<!DOCTYPE html><html  data-capo=""><head><script type="importmap">
  {
    "imports": {
      "@patternfly/elements/": "/scripts/v1/@patternfly/elements/",
      "@patternfly/pfe-clipboard/": "/scripts/v1/@patternfly/pfe-clipboard/",
      "@rhds/elements/": "/scripts/v1/@rhds/elements/elements/",
      "@cpelements/elements/": "/scripts/v1/@cpelements/elements/elements/"
    },
    "scopes": {
      "/": {
        "@floating-ui/core": "/scripts/v1/@floating-ui/core/dist/floating-ui.core.mjs",
        "@floating-ui/dom": "/scripts/v1/@floating-ui/dom/dist/floating-ui.dom.mjs",
        "@floating-ui/utils": "/scripts/v1/@floating-ui/utils/dist/floating-ui.utils.mjs",
        "@floating-ui/utils/dom": "/scripts/v1/@floating-ui/utils/dom/dist/floating-ui.utils.dom.mjs",
        "@lit/reactive-element": "/scripts/v1/@lit/reactive-element/reactive-element.js",
        "@lit/reactive-element/decorators/": "/scripts/v1/@lit/reactive-element/decorators/",
        "@patternfly/pfe-core": "/scripts/v1/@patternfly/pfe-core/core.js",
        "@patternfly/pfe-core/": "/scripts/v1/@patternfly/pfe-core/",
        "@rhds/tokens/media.js": "/scripts/v1/@rhds/tokens/js/media.js",
        "lit": "/scripts/v1/lit/index.js",
        "lit-element/lit-element.js": "/scripts/v1/lit-element/lit-element.js",
        "lit-html": "/scripts/v1/lit-html/lit-html.js",
        "lit-html/": "/scripts/v1/lit-html/",
        "lit/": "/scripts/v1/lit/",
        "tslib": "/scripts/v1/tslib/tslib.es6.mjs",
        "@cpelements/rh-table/dist/rh-table.js": "/scripts/v1/@cpelements/rh-table/dist/rh-table.js"
      }
    }
  }
</script><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Configuring GFS2 file systems | Red Hat Product Documentation</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<script type="text/javascript" id="trustarc" src="//static.redhat.com/libs/redhat/marketing/latest/trustarc/trustarc.js"></script>
<script src="//www.redhat.com/dtm.js"></script>
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Red+Hat+Display:wght@400;500;700&family=Red+Hat+Text:wght@400;500;700&display=swap">
<link rel="stylesheet" href="/styles/rh-table--lightdom.min.css">
<style>.section .titlepage{gap:.75rem}.section .titlepage,div.edit{align-items:center;display:flex}div.edit{font-size:.9rem;margin-bottom:8px}div.edit>a{align-items:center;display:flex}.edit pf-icon{margin-right:4px}</style>
<style>#error[data-v-df31ff14]{align-items:center;display:flex;flex-direction:column;justify-content:center;min-height:80vh}h1[data-v-df31ff14]{font-size:calc(var(--rh-font-size-body-text-md, 1rem)*4);font-weight:700;margin-bottom:0}h1[data-v-df31ff14],h1 span[data-v-df31ff14]{line-height:var(--rh-line-height-heading,1.3)}h1 span[data-v-df31ff14]{color:var(--rh-color-text-brand-on-light,#e00);display:block;text-transform:uppercase}h1 span[data-v-df31ff14],p[data-v-df31ff14]{font-size:var(--rh-font-size-body-text-lg,1.125rem)}aside[data-v-df31ff14]{align-items:center;background:var(--rh-color-surface-lightest,#fff);border:var(--rh-border-width-sm,1px) solid var(--rh-color-border-subtle-on-light,#c7c7c7);border-radius:var(--rh-border-radius-default,3px);border-top:calc(var(--rh-border-width-md, 2px)*2) solid var(--rh-color-text-brand-on-light,#e00);box-shadow:var(--rh-box-shadow-sm,0 2px 4px 0 hsla(0,0%,8%,.2));display:flex;flex-direction:column;justify-content:space-between;margin-top:var(--rh-space-2xl,32px);padding:var(--rh-space-xl,24px)}aside[data-v-df31ff14],aside>div[data-v-df31ff14]{width:100%}.text-container[data-v-df31ff14]{margin:auto;max-width:442px;text-align:center}.desktop[data-v-df31ff14]{display:none}.sr-only[data-v-df31ff14]{height:1px;overflow:hidden;padding:0;position:absolute;width:1px;clip:rect(0,0,0,0);border:0;clip-path:inset(50%);white-space:nowrap}form[data-v-df31ff14]{display:flex}button[data-v-df31ff14],input[data-v-df31ff14]{border:1px solid var(--rh-color-black-500,#8a8d90);box-sizing:border-box;height:40px}input[data-v-df31ff14]{border-right:none;flex:1;font-family:var(--rh-font-family-heading,RedHatDisplay,"Red Hat Display","Noto Sans Arabic","Noto Sans Hebrew","Noto Sans JP","Noto Sans KR","Noto Sans Malayalam","Noto Sans SC","Noto Sans TC","Noto Sans Thai",Helvetica,Arial,sans-serif);font-size:var(--rh-font-size-body-text-md,1rem);padding-left:var(--rh-space-md,8px);width:100%}button[data-v-df31ff14]{-webkit-appearance:none;-moz-appearance:none;appearance:none;background:transparent;border-left:none;display:flex;width:var(--rh-size-icon-04,40px)}button[data-v-df31ff14]:before{background:var(--rh-context-light-color-text-link,#06c);content:"";cursor:pointer;display:block;height:28px;margin:auto;width:28px}.search-icon[data-v-df31ff14]{margin:auto}ul[data-v-df31ff14]{max-width:275px;padding:0}ul li[data-v-df31ff14]{display:inline-block;list-style:none;margin-right:var(--rh-space-xl,24px);padding:var(--rh-space-xs,4px) 0}ul li a[data-v-df31ff14]{color:var(--rh-context-light-color-text-link,#06c);text-decoration:none}@media (min-width:992px){aside[data-v-df31ff14]{flex-direction:row}.mobile[data-v-df31ff14]{display:none}.desktop[data-v-df31ff14]{display:block}input[data-v-df31ff14]{width:auto}}</style>
<style>@keyframes fade-in{0%{opacity:0;visibility:hidden}1%{visibility:visible}to{opacity:1;visibility:visible}}@media (min-height:48em){.rhdocs{--rh-table--maxHeight:calc(100vh - 12.5rem)}}*,.rhdocs *,.rhdocs :after,.rhdocs :before,:after,:before{box-sizing:border-box}.rhdocs img,.rhdocs object,.rhdocs svg,img,object,svg{display:inline-block;max-width:100%;vertical-align:middle}.rhdocs hr{border:0;border-top:.0625rem solid #d2d2d2;clear:both;margin:1rem 0}.rhdocs a{color:#06c;text-decoration:underline}.rhdocs a:focus,.rhdocs a:hover{color:#036}.rhdocs a.anchor-heading{color:#151515;cursor:pointer;text-decoration:none;word-break:break-all}.rhdocs p{margin:1.49963rem 0}.rhdocs li>p{margin:0}.rhdocs h1,.rhdocs h2,.rhdocs h3,.rhdocs h4,.rhdocs h5,.rhdocs h6{font-family:RedHatDisplay,Red Hat Display,Helvetica Neue,Arial,sans-serif;font-weight:500;margin:0 0 .625rem}.rhdocs h1{font-size:2.25rem;margin:2rem 0}.rhdocs h2{font-size:1.625rem;margin:2rem 0}.rhdocs h3{font-size:1.5rem;font-weight:400}.rhdocs h4,.rhdocs h5{font-size:1.25rem}.rhdocs h5{font-weight:400}.rhdocs h6{font-size:1.125rem;font-weight:500;line-height:1.6667}.rhdocs ol,.rhdocs ul{margin:1rem 0;padding:0 0 0 1.5rem}.rhdocs ol ::marker,.rhdocs ul ::marker{font:inherit}.rhdocs li{margin:0 0 .5em;padding:0}.rhdocs li>p{margin:.5rem 0}.rhdocs li>ol,.rhdocs li>ul{margin:0}.rhdocs dl dd{margin:.5rem 0 .5rem 1rem}.rhdocs dl dd>p{margin:.5rem 0}.rhdocs .informaltable,.rhdocs .table-contents,.rhdocs .table-wrapper{max-height:var(--rh-table--maxHeight);overflow:auto}.rhdocs table{border:0;font-size:1rem;line-height:1.6667;table-layout:fixed}.rhdocs table caption{color:#585858;margin-bottom:.5rem;margin-top:.5rem;text-align:left}.rhdocs table td,.rhdocs table th{border:0;border-bottom:.0625rem solid #d2d2d2;border-bottom:.0625rem solid var(--pfe-table--Border,#d2d2d2);padding:.5em 1rem}.rhdocs table td.halign-left,.rhdocs table th.halign-left{text-align:left}.rhdocs table td.halign-center,.rhdocs table th.halign-center,table td.halign-center,table th.halign-center{text-align:center}.rhdocs table td.halign-right,.rhdocs table th.halign-right{text-align:right}.rhdocs table td.valign-top,.rhdocs table th.valign-top{vertical-align:top}.rhdocs table td.valign-middle,.rhdocs table th.valign-middle{vertical-align:middle}.rhdocs table td.valign-bottom,.rhdocs table th.valign-bottom{vertical-align:bottom}.rhdocs table thead td,.rhdocs table thead th{background:#f5f5f5;font-weight:600}.rhdocs rh-table table,.rhdocs rh-table.rh-table--expanded-vertically{max-height:-moz-max-content;max-height:max-content}.rhdocs pre.nowrap{overflow:auto;overflow-wrap:normal;white-space:pre;word-break:normal}.rhdocs .codeblock__wrapper pre{background:transparent}.rh-table--full-screen code,.rhdocs .content--md code,.rhdocs .content--sm code,.rhdocs .rh-table--full-screen code{overflow-wrap:normal;word-break:normal}.rhdocs[class] pre code,[class] pre code{background:inherit;color:inherit;font-family:inherit;font-size:inherit;font-weight:inherit;line-height:inherit;padding:0}.rhdocs .keycap,.rhdocs kbd{background-color:#eee;background-image:linear-gradient(180deg,#ddd,#eee,#fff);border-radius:.1875rem;box-shadow:0 -.0625rem 0 0 #fff,0 .0625rem 0 .1875rem #aaa;font-family:RedHatMono,Red Hat Mono,Consolas,monospace;font-size:90%;font-weight:400;margin:0 .25rem;padding:.125rem .375rem}.keycap strong,.rhdocs .keycap strong{font-weight:inherit}.rhdocs kbd.keyseq,kbd.keyseq{background:transparent;border:0;box-shadow:none;padding:0}.rhdocs kbd.keyseq kbd,kbd.keyseq kbd{display:inline-block;margin:0 .375rem}.rhdocs kbd.keyseq kbd:first-child,kbd.keyseq kbd:first-child{margin-left:0}.rhdocs b.button{font-size:90%;font-weight:700;padding:.1875rem}.rhdocs b.button:before{content:"["}.rhdocs b.button:after{content:"]"}html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}.rhdocs audio,.rhdocs canvas,.rhdocs progress,.rhdocs video{display:inline-block;vertical-align:baseline}.rhdocs audio:not([controls]){display:none;height:0}[hidden],template{display:none}.rhdocs a{background:transparent}.rhdocs a:active,.rhdocs a:hover{outline:0}.rhdocs a.anchor-heading:hover:before{color:#151515;content:"#";margin-left:-1.6rem;position:absolute}.rhdocs a.anchor-heading:focus-visible{color:#151515}@media screen and (max-width:990px){.rhdocs a.anchor-heading:hover:before{font-size:16px;margin-left:-1rem;padding-top:8px}.rhdocs h1 a.anchor-heading:hover:before{padding-top:12px}.rhdocs h4 a.anchor-heading:hover:before,.rhdocs h5 a.anchor-heading:hover:before{padding-top:4px}.rhdocs h6 a.anchor-heading:hover:before{padding-top:2px}}.rhdocs abbr[title]{border-bottom:.0625rem dotted}.rhdocs dfn{font-style:italic}.rhdocs h1{font-size:2em;margin:.67em 0}.rhdocs mark{background:#ff0;color:#000}.rhdocs small{font-size:80%}.rhdocs sub,.rhdocs sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}.rhdocs sup{top:-.5em}.rhdocs sub{bottom:-.25em}.rhdocs img{border:0}.rhdocs svg:not(:root){overflow:hidden}.rhdocs figure{margin:1em 2.5rem}.rhdocs hr{box-sizing:content-box;height:0}.rhdocs code,.rhdocs kbd,.rhdocs pre,.rhdocs samp{font-family:monospace,monospace;font-size:1em}.rhdocs button,.rhdocs optgroup,.rhdocs select,.rhdocs textarea,.rhdocsinput{color:inherit;font:inherit;margin:0}.rhdocs button.copy-link-btn{background:none;border:2px solid #fff;font:1px Red Hat Text}.rhdocs button.copy-link-btn:hover{border-bottom:2px solid #06c}.rhdocs button.copy-link-btn .link-icon{padding-bottom:4px}.rhdocs button{overflow:visible}.rhdocs button,.rhdocs select{text-transform:none}.rhdocs button,.rhdocs html input[type=button],.rhdocs input[type=reset],.rhdocs input[type=submit]{-moz-appearance:button;appearance:button;-webkit-appearance:button;cursor:pointer}.rhdocs button[disabled],.rhdocs html input[disabled]{cursor:default}.rhdocs button::-moz-focus-inner,.rhdocs input::-moz-focus-inner{border:0;padding:0}.rhdocs input{line-height:normal}.rhdocs input[type=checkbox],.rhdocs input[type=radio]{box-sizing:border-box;padding:0}.rhdocs input[type=number]::-webkit-inner-spin-button,.rhdocs input[type=number]::-webkit-outer-spin-button{height:auto}.rhdocs input[type=search]{-moz-appearance:textfield;appearance:textfield;-webkit-appearance:textfield;box-sizing:content-box}.rhdocs input[type=search]::-webkit-search-cancel-button,.rhdocs input[type=search]::-webkit-search-decoration{-webkit-appearance:none}.rhdocs fieldset{border:.0625rem solid silver;margin:0 .125rem;padding:.35em .625em .75em}.rhdocs legend{border:0;padding:0}.rhdocs textarea{overflow:auto}.rhdocs optgroup{font-weight:700}.rhdocs table{border-collapse:collapse;border-spacing:0}.rhdocs td,.rhdocs th{padding:0}.rhdocs ._additional-resources[class][class][id]:last-child{margin-top:-2rem}.rhdocs ._additional-resources[class][class]:only-child{grid-column:1/-1}._additional-resources[class][class] .additional-resources__heading,._additional-resources[class][class] .heading,._additional-resources[class][class] h1,._additional-resources[class][class] h2,._additional-resources[class][class] h3,._additional-resources[class][class] h4,._additional-resources[class][class] h5,._additional-resources[class][class] h6,._additional-resources[class][class] p.title{display:block;font-family:RedHatDisplay,Red Hat Display,Helvetica Neue,Arial,sans-serif;font-size:1.125rem;font-weight:700;line-height:1.5rem;margin:0 0 .5rem;padding:0;text-transform:uppercase}._additional-resources[class][class] ul{border:0;list-style:none;margin:0;padding:0;position:relative}.related-topic-content__wrapper ._additional-resources[class][class] ul{display:block}._additional-resources[class][class] ul:after{background-color:#fff;bottom:0;content:"";display:block;height:.125rem;position:absolute;width:100%}._additional-resources[class][class] li{border-bottom:.0625rem solid #d2d2d2;box-sizing:content-box;margin:0;padding:1rem 1.5rem 1rem 0;-moz-column-break-inside:avoid;break-inside:avoid}._additional-resources[class][class] li:only-child{grid-column:1/-1}._additional-resources[class][class] li:last-child{border:0}@media (min-width:1100px){._additional-resources[class][class] li:last-child{border-bottom:.0625rem solid #d2d2d2}}._additional-resources[class][class] li p:only-child{margin:0;padding:0}._additional-resources[class][class] li a{text-decoration:none}._additional-resources[class][class] li a:focus,._additional-resources[class][class] li a:hover{text-decoration:underline}.rhdocs table .admonitionblock>div:nth-child(2),.rhdocs table .caution>div:nth-child(2),.rhdocs table .important>div:nth-child(2),.rhdocs table .note>div:nth-child(2),.rhdocs table .tip>div:nth-child(2),.rhdocs table .warning>div:nth-child(2){margin:.5rem 0}.rhdocs table .admonitionblock>div:nth-child(2)>:first-child,.rhdocs table .caution>div:nth-child(2)>:first-child,.rhdocs table .important>div:nth-child(2)>:first-child,.rhdocs table .note>div:nth-child(2)>:first-child,.rhdocs table .tip>div:nth-child(2)>:first-child,.rhdocs table .warning>div:nth-child(2)>:first-child{margin-top:0}.rhdocs table .admonitionblock>div:nth-child(2)>:last-child,.rhdocs table .caution>div:nth-child(2)>:last-child,.rhdocs table .important>div:nth-child(2)>:last-child,.rhdocs table .note>div:nth-child(2)>:last-child,.rhdocs table .tip>div:nth-child(2)>:last-child,.rhdocs table .warning>div:nth-child(2)>:last-child{margin-bottom:0}.rhdocs .codeblock__wrapper+.codeblock__wrapper,.rhdocs pre+pre,.rhdocs pre[class]+pre[class]{margin-top:2rem}.rhdocs .codeblock__wrapper{background:#f8f8f8;overflow:visible;position:relative;transform:translate(0);z-index:0}.codeblock__wrapper:before{background-repeat:no-repeat;background-size:6.25rem 100%;bottom:var(--scrollbar__height,1px);content:"";display:block;height:7.125rem;max-height:100%;max-height:calc(100% - var(--scrollbar__height, 2px));position:absolute;right:var(--scrollbar__width,6px);top:.0625rem;width:4.0625rem;z-index:1}.rhdocs .codeblock__inner-wrapper,.rhdocs pre{max-height:calc(100vh - 6.25rem)}@media (min-height:48em){.rhdocs .codeblock__inner-wrapper,.rhdocs pre{max-height:calc(100vh - 12.5rem)}}.rhdocs .codeblock__inner-wrapper{display:grid;grid-template-columns:1fr 4.375rem}.rhdocs .codeblock__wrapper--expanded .codeblock__inner-wrapper{max-height:-moz-max-content;max-height:max-content}.codeblock__copy span{display:block;height:0;position:absolute;visibility:hidden;width:0}.codeblock__copy:focus{outline:.0625rem dashed currentcolor}.codeblock__copy svg#icon--copy{height:1rem;width:1rem}.codeblock__expand{-webkit-appearance:none;-moz-appearance:none;appearance:none;background:#f0efef;border:0;cursor:pointer;height:1.75rem;left:calc(100% - 2.75rem - var(--scrollbar__width, 0px));position:absolute;text-indent:-9999em;top:3.25rem;width:1.75rem;z-index:2}.codeblock__expand:before{background:#6a6e73;content:"";height:100%;left:0;-webkit-mask-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 320 512'%3E%3C!--! Font Awesome Pro 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2022 Fonticons, Inc.--%3E%3Cpath d='M182.6 9.4c-12.5-12.5-32.8-12.5-45.3 0l-96 96c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l41.4-41.4v293.4l-41.4-41.3c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l96 96c12.5 12.5 32.8 12.5 45.3 0l96-96c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192 402.7V109.3l41.4 41.4c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3l-96-96z'/%3E%3C/svg%3E");mask-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 320 512'%3E%3C!--! Font Awesome Pro 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2022 Fonticons, Inc.--%3E%3Cpath d='M182.6 9.4c-12.5-12.5-32.8-12.5-45.3 0l-96 96c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l41.4-41.4v293.4l-41.4-41.3c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l96 96c12.5 12.5 32.8 12.5 45.3 0l96-96c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192 402.7V109.3l41.4 41.4c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3l-96-96z'/%3E%3C/svg%3E");-webkit-mask-position:center center;mask-position:center center;-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:auto 1rem;mask-size:auto 1rem;position:absolute;top:0;width:100%}.codeblock__wrapper--expanded .codeblock__expand{background:#2b9af3}.codeblock__wrapper--expanded .codeblock__expand:before{background:#fff}.codeblock__expand:focus:before,.codeblock__expand:hover:before{background:#06c}.codeblock__wrapper--expanded .codeblock__expand:focus:before,.codeblock__wrapper--expanded .codeblock__expand:hover:before{background:#fff}.codeblock__expand:focus{outline:.0625rem dashed currentcolor}.rhdocs .calloutlist>ol,.rhdocs .colist>ol{counter-reset:colist;list-style:none;margin:1rem 0 2rem;padding:0}.rhdocs .calloutlist>ol>li,.rhdocs .colist>ol>li{counter-increment:colist;font-size:1rem;margin:.5rem 0;padding-left:1.75rem;position:relative}.rhdocs .calloutlist>ol>li .colist-num,.rhdocs .colist>ol>li .colist-num{display:none}.calloutlist>ol>li:before,.colist>ol>li:before{content:counter(colist);left:0;position:absolute;top:.1875rem}.calloutlist dt{clear:left;float:left;margin:0;padding:0 .5rem 0 0}.included-in-guides[class],.included-in-guides[class][id]:last-child{background:#fff;border:.0625rem solid #d2d2d2;border-radius:.1875rem;margin:2em 0 4em;padding:2rem 2rem 1rem}.included-in-guides[class][id]:last-child{margin-top:-2rem}.included-in-guides[class]:only-child{grid-column:1/-1}.included-in-guides[class] .additional-resources__heading,.included-in-guides[class] .heading,.included-in-guides[class] h1,.included-in-guides[class] h2,.included-in-guides[class] h3,.included-in-guides[class] h4,.included-in-guides[class] h5,.included-in-guides[class] h6,.included-in-guides[class] p.title{display:block;font-family:RedHatDisplay,Red Hat Display,Helvetica Neue,Arial,sans-serif;font-size:1.125rem;font-weight:700;line-height:1.5rem;margin:0 0 .5rem;padding:0;text-transform:uppercase}.included-in-guides[class] ul{border:0;list-style:none;margin:0;padding:0;position:relative}.related-topic-content__wrapper .included-in-guides[class] ul{display:block}.included-in-guides[class] ul:after{background-color:#fff;bottom:0;content:"";display:block;height:.125rem;position:absolute;width:100%}.included-in-guides[class] li{border-bottom:.0625rem solid #d2d2d2;box-sizing:content-box;margin:0;padding:1rem 1.5rem 1rem 0;-moz-column-break-inside:avoid;break-inside:avoid}.included-in-guides[class] li:only-child{grid-column:1/-1}.included-in-guides[class] li:last-child{border:0}@media (min-width:1100px){.included-in-guides[class] li:last-child{border-bottom:.0625rem solid #d2d2d2}}.included-in-guides[class] li p:only-child{margin:0;padding:0}.included-in-guides[class] li a{text-decoration:none}.included-in-guides[class] li a:focus,.included-in-guides[class] li a:hover{text-decoration:underline}.menuseq{display:inline-flex;overflow:hidden;text-indent:-9999em}.menuseq .menu,.menuseq .menuitem,.menuseq .submenu{display:block;position:relative;text-indent:0}.menuseq .menu+.menu:before,.menuseq .menu+.menuitem:before,.menuseq .menu+.submenu:before,.menuseq .menuitem+.menu:before,.menuseq .menuitem+.menuitem:before,.menuseq .menuitem+.submenu:before,.menuseq .submenu+.menu:before,.menuseq .submenu+.menuitem:before,.menuseq .submenu+.submenu:before{content:">";display:inline-block;font-weight:700;padding:0 .25em}.related-topic-content__wrapper{margin:2em 0}.related-topic-content__wrapper--for-guide{margin-bottom:-2.5rem;padding-bottom:.0625rem;position:relative;z-index:1}.related-topic-content__wrapper--for-guide:before{background:#f0f0f0;content:"";display:block;height:100%;left:-3rem;position:absolute;right:-4.5rem;top:0;width:auto;z-index:-1}@media (min-width:1100px){.related-topic-content__wrapper--for-guide:before{left:-2.5rem;right:-3.625rem}}.related-topic-content__wrapper--for-guide summary{padding:1em 2em 1em 2.1875rem}@media (min-width:950px){.related-topic-content__inner-wrapper{display:grid;gap:2em;grid-template-columns:repeat(2,minmax(0,1fr))}}.local-render .rhdocs-content{margin:0 auto}.rhdocs cp-documentation{display:block;padding-bottom:2.5rem}.rhdocs cp-documentation.PFElement,.rhdocs cp-documentation[pfelement]{padding:0}rh-table{display:block}::-webkit-scrollbar,:host .rhdocs ::-webkit-scrollbar{height:.625rem;width:.625rem}::-webkit-scrollbar,::-webkit-scrollbar-track,:host .rhdocs ::-webkit-scrollbar,:host .rhdocs ::-webkit-scrollbar-track{background-color:#d6d6d6}::-webkit-scrollbar-thumb,:host .rhdocs ::-webkit-scrollbar-thumb{background-color:#8e8e8e}*,:host .rhdocs *{scrollbar-color:#8e8e8e #d6d6d6}.rhdocs p:empty,p:empty{display:none}.rhdocs[class] h1 code,.rhdocs[class] h2 code,.rhdocs[class] h3 code,.rhdocs[class] h4 code,.rhdocs[class] h5 code,.rhdocs[class] h6 code,[class] h1 code,[class] h2 code,[class] h3 code,[class] h4 code,[class] h5 code,[class] h6 code{background:transparent;border:0;color:inherit;font:inherit;margin:0;padding:0}.pane-page-title h1,.rhdocs__header__primary-wrapper h1{font-family:RedHatDisplay,Red Hat Display,Helvetica Neue,Arial,sans-serif;font-size:2.25rem;line-height:1.333}.rhdocs details[class]{list-style:none;margin:1rem 0 3rem;padding:0}.rhdocs-toc[class]{background:#f2f2f2;margin:1rem 0 2rem;padding:1rem}.rhdocs-toc[class]>:last-child{margin-bottom:0}.rhdocs-toc[class] .rhdocs-toctitle{font-size:1.25rem;font-weight:400;line-height:1.6667;margin-top:0;text-transform:none}.rhdocs-toc[class] li{margin-bottom:.25em;padding-left:.5em}.preamble{margin:0 0 2rem}.sect1{margin:2rem 0 1rem}:host .sect1,cp-documentation .sect1{margin:0 0 2rem;padding:.0625rem 0 0}:host(.cp-documentation--has-external-header) .sect1:first-child>h2:first-child,:host(.cp-documentation--has-external-header) .sect1:first-child>h3:first-child{margin-top:0}.listingblock,.literalblock{margin:1rem 0}.quoteblock,.verseblock{border-left:.25rem solid #d2d2d2;margin:1rem 0;padding:1rem 1rem 1rem 2rem}.quoteblock.pullleft,.verseblock.pullleft{float:left;margin-right:3rem;width:25rem}@media (min-width:768px){.quoteblock.pullleft,.verseblock.pullleft{margin-left:-1rem}}.quoteblock.pullright,.verseblock.pullright{float:right;margin-left:3rem;width:25rem}@media (min-width:768){.quoteblock.pullright,.verseblock.pullright{margin-right:-2rem}}@media (min-width:1100px){.quoteblock.pullright,.verseblock.pullright{margin-right:-10rem}}.quoteblock>:first-child,.verseblock>:first-child{margin-top:0}.quoteblock .content,.verseblock .content{font-family:RedHatText,Red Hat Text,Helvetica Neue,Arial,sans-serif;font-size:1.25rem;line-height:1.6667}.quoteblock .attribution,.verseblock .attribution{font-size:.875rem;font-style:italic;font-weight:600;line-height:1.6667;text-transform:uppercase}.quoteblock .attribution .citetitle,.verseblock .attribution .citetitle{color:#585858}.quoteblock .attribution cite,.verseblock .attribution cite{font-size:1em}.quoteblock blockquote{font-style:italic;margin:0;padding:0}.quoteblock blockquote .content>:first-child{margin-top:0}.quoteblock blockquote .content>:first-child:before{color:#e00;content:"â€œ";display:block;float:left;font-size:2.75rem;font-style:normal;line-height:1.125em;margin-right:.5rem}.quoteblock blockquote .content>:first-child .content>:first-child:before{content:none}.imageblock{margin:1rem 0}.imageblock.pullleft{float:left;margin-right:3rem;width:25rem}@media (min-width:768px){.imageblock.pullleft{margin-left:-1rem}}.imageblock.pullright{float:right;margin-left:3rem;width:25rem}@media (min-width:768){.imageblock.pullright{margin-right:-2rem}}@media (min-width:1100px){.imageblock.pullright{margin-right:-10rem}}.imageblock.interrupter{margin:2rem 0}@media (min-width:768px){.imageblock.interrupter{margin-left:-1rem;margin-right:-2rem}.imageblock.interrupter .caption{margin-left:1rem;margin-right:2rem}}@media (min-width:1100px){.imageblock.interrupter{margin-right:-10rem}.imageblock.interrupter .caption{margin-right:10rem}}.imageblock.interrupter img{max-width:100%}.imageblock .caption{color:#585858;display:block;font-size:.875rem;line-height:1.6667;margin:.5rem 0 0}.rhdocs-footnotes{border-top:.0625rem solid #d2d2d2;margin:3rem 0 1rem;padding:1rem 0 0}.rhdocs-footnotes>ol{margin:0;padding:0 0 0 1.5rem}@supports (counter-reset:footnotenum){.rhdocs-footnotes>ol{counter-reset:footnotenum;list-style:none;padding:0}.rhdocs-footnotes>ol>li{counter-increment:footnotenum}.rhdocs-footnotes>ol>li:before{color:#585858;content:"[" counter(footnotenum) "]";display:inline-block;margin-right:.25rem}}.rhdocs-footer{background:#ededed;color:#151515;font-size:.875rem;line-height:1.6667;margin:3rem 0 0;padding:1rem}.center{margin-left:auto;margin-right:auto}.stretch{width:100%}.visually-hidden{overflow:hidden;position:absolute;clip:rect(0,0,0,0);border:0;height:.0625rem;margin:-.0625rem;padding:0;width:.0625rem}.rh-docs-legal-notice{margin-top:4em}pre,pre[class]{margin:0;padding:1.25em 1em;position:relative}code[class*=language-],pre[class*=language-]{color:#151515;-moz-tab-size:4;-o-tab-size:4;tab-size:4}code.language-none,code.language-text,code.language-txt,pre.language-none,pre.language-text,pre.language-txt{color:#151515}code[class*=language-] ::-moz-selection,code[class*=language-]::-moz-selection,pre[class*=language-] ::-moz-selection,pre[class*=language-]::-moz-selection{background:#cceae7;color:#263238}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{background:#cceae7;color:#263238}:not(pre)>code[class*=language-]{border-radius:.2em;padding:.1em;white-space:normal}.token.atrule{color:#40199a}.token.attr-name{color:#06c}.token.attr-value,.token.attribute{color:#b300b3}.token.boolean{color:#40199a}.token.builtin,.token.cdata,.token.char,.token.class,.token.class-name{color:#06c}.token.comment{color:#6a6e73}.token.constant{color:#40199a}.token.deleted{color:#c9190b}.token.doctype{color:#6a6e73}.token.entity{color:#c9190b}.token.function{color:#40199a}.token.hexcode{color:#b300b3}.token.id,.token.important{color:#40199a;font-weight:700}.token.inserted{color:#06c}.token.keyword{color:#40199a}.token.number{color:#b300b3}.token.operator{color:#06c}.token.prolog{color:#6a6e73}.token.property{color:#06c}.token.pseudo-class,.token.pseudo-element{color:#b300b3}.token.punctuation,.token.regex{color:#06c}.token.selector{color:#c9190b}.token.string{color:#b300b3}.token.symbol{color:#40199a}.token.unit{color:#b300b3}.token.url,.token.variable{color:#c9190b}.rhdocs.local-render{margin:0 auto;max-width:45.8125rem;padding:0 1.5rem}@media print{.field code,.field pre,code[class*=language-],pre,pre[class*=language-]{white-space:pre-wrap!important;word-wrap:break-word!important;overflow-wrap:break-word!important;word-break:break-word!important}}.book-nav__list[class]{display:flex;justify-content:space-between;line-height:var(--jupiter__lineHeight--xs,1.3333);list-style:none;margin:5rem 0 0;padding:0}@media (min-width:1200px){.book-nav__list[class]{display:grid;gap:2rem;grid-template-columns:repeat(2,minmax(0,1fr))}}.book-nav__item a{display:inline-block;font-size:.875rem;font-weight:500;padding-left:1.25rem;position:relative;text-transform:uppercase}.book-nav__item a:before{background:url(/sites/dxp-docs/penumbra-dist/jupiter/images/arrow-down-solid.svg) no-repeat;background-size:contain;content:"";display:block;height:.875rem;left:0;position:absolute;top:.125rem;transform:rotate(90deg);width:.875rem}.book>.titlepage:not(:last-child),.rhdocs .chapter,section[id]{padding-bottom:3.75rem}.book>.titlepage .chapter:last-child,.book>.titlepage section[id]:last-child,.chapter .chapter:last-child,.chapter section[id]:last-child,section[id] .chapter:last-child,section[id] section[id]:last-child{margin-bottom:-3.75rem}.rhdocs .codeblock__wrapper+section[id],pre+section[id]{padding-top:3.75rem}.rhdocs .cta-link{font-size:inherit}.rhdocs a{word-wrap:break-word;overflow-wrap:break-word}.rhdocs .caution,.rhdocs .important,.rhdocs .note,.rhdocs .tip,.rhdocs .warning{padding:.8888888889em;position:relative}.rhdocs .QSIPopOver{bottom:18.75rem!important;top:auto!important}.rhdocs .alert{position:relative}.rhdocs button.dismiss-button{background:none;border:0;cursor:pointer;height:2.5rem;margin-top:-1.25rem;padding:0;position:absolute;right:.3125rem;text-align:center;top:50%;width:2.5rem;z-index:50}.rhdocs button.dismiss-button:after{content:"\f109";display:inline-block;filter:alpha(opacity=30);font-family:rh-web-iconfont;font-size:1.3125rem;font-style:normal;font-variant:normal;font-weight:400;line-height:1;line-height:2.5rem;opacity:.3;text-decoration:inherit;text-rendering:optimizeLegibility;text-transform:none!important;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;font-smoothing:antialiased}.rhdocs .book>.titlepage,.rhdocs .chapter,.rhdocs section[id]{padding-bottom:var(--rh-space-4xl,64px)}.rhdocs .alert{border:0;border-radius:0}.rhdocs .alert>h2:first-child,.rhdocs .alert>h3:first-child,.rhdocs .alert>h4:first-child,.rhdocs .alert>h5:first-child,.rhdocs .alert>h6:first-child,.rhdocs .alert>p:first-child{margin-top:0!important}.rhdocs .alert>p:last-child{margin-bottom:0!important}.rhdocs .alert-w-icon[class]{padding-left:2.8125rem}.rhdocs .alert-w-icon .alert-icon{float:left;font-size:1.125rem;margin-left:-1.875rem;margin-right:.625rem}.rhdocs .alert-w-icon .alert-icon[class*=" rh-icon-"],.rhdocs .alert-w-icon .alert-icon[class^=rh-icon-]{font-size:2.25rem;line-height:1em;margin-left:-2.5rem;margin-top:-.375rem}.rhdocs .alert-w-icon .alert-icon[class*=" icon-innov-prev"],.rhdocs .alert-w-icon .alert-icon[class^=icon-innov-prev]{font-size:1.3125rem;margin-top:.25rem}.rhdocs .alert-w-icon.alert-plain{background:none;color:#151515;padding-left:5rem}.rhdocs .alert-w-icon.alert-plain .alert-icon{font-size:3rem;margin-left:-4.375rem;margin-right:0}.rhdocs .alert-w-icon.alert-plain.alert-success .alert-icon{color:#3f9c35}.rhdocs .alert-w-icon.alert-plain.alert-info .alert-icon{color:#0088ce}.rhdocs .alert-w-icon.alert-plain.alert-warning .alert-icon{color:#f0ab00}.rhdocs .alert-w-icon.alert-plain.alert-danger .alert-icon{color:#e00}#target_banner .copy-url{float:right;margin-top:0}#target_banner .dropdown-menu{font-size:inherit}.titlepage .svg-img[data*="title_logo.svg"]{margin:1.5rem 0;width:15rem}.para{margin:1.49963rem 0}.para[class]{margin-bottom:1.49963rem}dd{margin-bottom:2.5rem}.rhdocs .card-light,.rhdocs .card-light-gray,.rhdocs .card-light-grey{background:#f0f0f0;border:.0625rem solid #f0f0f0;color:#151515}.rhdocs .card-light-gray.push-bottom:first-child,.rhdocs .card-light-grey.push-bottom:first-child,.rhdocs .card-light.push-bottom:first-child{margin-bottom:3.125rem!important}.rhdocs .card-light a.card-link,.rhdocs .card-light h1,.rhdocs .card-light h2,.rhdocs .card-light h3,.rhdocs .card-light h4,.rhdocs .card-light h5,.rhdocs .card-light h6,.rhdocs .card-light-gray a.card-link,.rhdocs .card-light-gray h1,.rhdocs .card-light-gray h2,.rhdocs .card-light-gray h3,.rhdocs .card-light-gray h4,.rhdocs .card-light-gray h5,.rhdocs .card-light-gray h6,.rhdocs .card-light-grey a.card-link,.rhdocs .card-light-grey h1,.rhdocs .card-light-grey h2,.rhdocs .card-light-grey h3,.rhdocs .card-light-grey h4,.rhdocs .card-light-grey h5,.rhdocs .card-light-grey h6{color:#151515}.rhdocs .card-light-gray.card-active:after,.rhdocs .card-light-grey.card-active:after,.rhdocs .card-light.card-active:after{border-top-color:#f0f0f0}.rhdocs .card-md,.rhdocs .card-narrow{display:block;padding:1.1875rem;white-space:normal;word-wrap:break-word}.rhdocs .card .card-heading.card-heading-sm,.rhdocs .card-sm .card .card-heading{font-size:1.0625em;font-weight:500;line-height:1.5}.rhdocs .card .card-heading.card-heading-flush{margin-bottom:.25rem}.rhdocs .card .card-heading.card-heading-red{color:#d10000}.rhdocs .card>p{margin-top:0}.rhdocs .card>p:last-child{margin-bottom:0}.rhdocs .new-experience{background-color:#e7f1fa;border:.0625rem solid #bee1f4;font-size:1rem;margin:1.5rem;padding:1.5rem;position:relative;z-index:1}@media (min-width:48rem){.new-experience{display:flex}.new-experience--contained{left:50%;position:relative;transform:translateX(-50%);width:calc(100vw - 2.5rem)}}.new-experience__primary-content{flex-grow:1}@media (min-width:48rem){.new-experience__primary-content{margin-right:1.25rem}}.new-experience__title{font-size:inherit;font-weight:inherit;line-height:1.6;margin:0;padding:0}.new-experience__title+a,.new-experience__title+pfe-cta{display:inline-block;margin-top:1.5em}.new-experience__secondary-content{min-width:12.5rem}@media (min-width:48rem){.new-experience__secondary-content{text-align:right}}.example{border-left:.3125rem solid #ccc;margin-bottom:2rem;padding:1rem 0 1rem 1rem}dl.calloutlist[class]{display:grid;gap:1.25em .75em;grid-template-columns:min-content 1fr}dl.calloutlist[class] dt{float:none;margin:0;padding:0}dl.calloutlist[class] dd{margin:0;padding:0}dl.calloutlist[class] dd>:first-child{margin-top:0}dl.calloutlist[class] dd>:last-child{margin-bottom:0}.toast{background-color:#000;background-color:rgba(0,0,0,.9);bottom:.9375rem;box-shadow:0 .125rem .3125rem 0 rgba(0,0,0,.26);color:#fff;left:.9375rem;max-width:32.8125rem;min-width:6.25rem;padding:.9375rem;position:fixed;right:.9375rem;transform:translate3d(0,150%,0);transition:transform .2s cubic-bezier(.465,.183,.153,.946);will-change:transform;z-index:999}.toast.show{transform:translateZ(0)}.toast a{color:#fff;text-decoration:underline}.toast a:focus,.toast a:hover{color:#2b9af3}.toast a.btn{text-decoration:none}.toast .btn.btn-link{color:#fff}.toast .close{color:#fff;opacity:.3;text-decoration:none}.toast .close:focus,.toast .close:hover{color:#fff;opacity:.5}.no-csstransforms3d.csstransitions .toast{transition:all .2s cubic-bezier(.465,.183,.153,.946)}.no-csstransforms3d .toast{opacity:0;visibility:hidden}.no-csstransforms3d .toast.show{opacity:1;visibility:visible}.annotator-outer[class][class]{display:none;flex-direction:column;flex-grow:1;height:auto;margin:0;position:static;width:auto}@media (min-width:1400px){.annotator-outer[class][class]{display:flex}}.annotator-frame[class] *{height:auto}@media (min-width:1400px){.annotator-frame .h-sidebar-iframe[class]{position:static;width:calc(100% + 1.5rem)}}.annotator-toolbar[class][class]{position:static;width:auto}.annotator-toolbar>ul,.annotator-toolbar>ul>li{display:block;height:auto;list-style:none;margin:0;padding:0;width:auto}.annotator-toolbar>ul>li{display:flex;justify-content:flex-end}.annotator-frame[class] .annotator-frame-button--sidebar_toggle,.annotator-outer .annotator-frame-button[class][class],.app-content-wrapper *{font-family:RedHatText,Red Hat Text,Helvetica Neue,Arial,sans-serif!important}.annotator-outer .annotator-frame-button[class][class]{font-size:.9375rem;font-weight:500;height:auto;line-height:1.333;margin-right:1.875rem;padding:.75em 1em;position:static}@media (min-width:1400px){.annotator-outer .annotator-frame-button[class][class]{margin-right:0}}.annotator-outer iframe{flex-grow:1;margin-bottom:1.25rem}@media (min-width:1400px){.annotator-outer iframe{min-height:37.5rem}}.producttitle{color:#000;font-size:1.25rem;text-transform:uppercase}.producttitle .productnumber{color:var(--jupiter__palette__red--50,#e00)}.cp-modal-open,.zoom-open{overflow:hidden}.cp-modal,.cp-video-modal,.zoom-modal{bottom:0;display:none;filter:alpha(opacity=0);left:0;opacity:0;outline:0;overflow:hidden;position:fixed;right:0;top:0;transition:all .2s cubic-bezier(.465,.183,.153,.946);z-index:1040;z-index:1050;-webkit-overflow-scrolling:touch}.rhdocs .in.cp-modal,.rhdocs .in.cp-video-modal,.rhdocs .in.zoom-modal{display:block;filter:alpha(opacity=100);opacity:1;overflow-x:hidden;overflow-y:auto}.rhdocs .cp-modal .close,.rhdocs .cp-video-modal .close,.rhdocs .zoom-modal .close{background-color:#fff;border-radius:50%;color:#1a1a1a;font-size:1.75rem;height:28px;height:1.75rem;line-height:1.75rem;margin-bottom:.375rem;margin-top:0;opacity:.9;position:absolute;right:-.5rem;text-shadow:none;top:0;width:28px;width:1.75rem}.cp-modal .close:after,.cp-video-modal .close:after,.zoom-modal .close:after{line-height:1.75rem}.cp-modal-wrap,.zoom-wrap{margin:.625rem;padding-top:.5rem;position:relative}@media (min-width:48rem){.rhdocs .cp-modal-wrap,.rhdocs .zoom-wrap{margin:2.8125rem auto;width:38.4375rem}}@media (min-width:62rem){.rhdocs .cp-modal-wrap,.rhdocs .zoom-wrap{width:49.8958rem}}@media (min-width:75rem){.rhdocs .cp-modal-wrap,.rhdocs .zoom-wrap{width:60.3125rem}}.rhdocs .cp-modal-body :last-child{margin-bottom:0}.rhdocs .cp-modal-backdrop,.rhdocs .zoom-backdrop{background-color:#000;bottom:0;display:none;filter:alpha(opacity=0);left:0;opacity:0;position:fixed;right:0;top:0;transition:opacity .2s cubic-bezier(.465,.183,.153,.946);z-index:1040}.rhdocs .in.cp-modal-backdrop,.rhdocs .in.zoom-backdrop{display:block;filter:alpha(opacity=80);opacity:.8}.rhdocs .cp-modal-body{background:#fff;padding:1.875rem}.rhdocs .cp-modal[data-cp-modal-video=true] .cp-modal-body,.rhdocs .cp-video-modal .cp-modal-body{padding:0}.rhdocs [data-action=zoom]{position:relative}.rhdocs [data-action=zoom]:after{background:rgba(0,0,0,.4);bottom:0;color:#fff;display:inline-block;font-family:rh-web-iconfont;font-style:normal;font-variant:normal;font-weight:400;line-height:1;padding:.375rem;position:absolute;right:0;text-decoration:inherit;text-decoration:none!important;text-rendering:optimizeLegibility;text-transform:none!important;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;font-smoothing:antialiased}.rhdocs [data-action=zoom]:focus:after,.rhdocs [data-action=zoom]:hover:after{background:rgba(0,0,0,.9)}.rhdocs .zoom-wrap .zoom-larger{text-align:center}.rhdocs .zoom-wrap .zoom-larger a{color:#fff}.rhdocs .zoom-wrap .zoom-larger a:focus,.rhdocs .zoom-wrap .zoom-larger a:hover{color:#fff;text-decoration:underline}.rhdocs .zoom-wrap .zoom-larger a:after{content:"â¿»";display:inline-block;margin-left:.25rem}.rhdocs .zoom-body{background:#fff;border-radius:.5rem;margin:0 0 1rem;padding:1rem;text-align:center}.rhdocs .zoom-body .video-wrapper{height:0;overflow:hidden;padding-bottom:56.25%;position:relative}.rhdocs .zoom-body .video-wrapper[data-aspect-ratio="4:3"]{padding-bottom:75%}.rhdocs .zoom-body iframe{height:100%;left:0;position:absolute;top:0;width:100%}.rhdocs .para>.title[class],.rhdocs p.title[class]{font-size:1rem;font-style:normal;font-weight:700;line-height:1.6667;margin:1.25rem 0 0;text-transform:none}.rhdocs .para>.title[class]+.content>:first-child,.rhdocs .para>.title[class]+p,.rhdocs p.title[class]+.content>:first-child,.rhdocs p.title[class]+p{margin-top:0}.rhdocs [class] pre .caution,.rhdocs [class] pre .important,.rhdocs [class] pre .note,.rhdocs [class] pre .tip,.rhdocs [class] pre .warning{background:transparent;border:0;color:inherit;font:inherit;margin:0;padding:0}.rhdocs [class] pre .caution:after,.rhdocs [class] pre .important:after,.rhdocs [class] pre .note:after,.rhdocs [class] pre .tip:after,.rhdocs [class] pre .warning:after{content:none}.rhdocs [class] code.email{background-color:transparent;font:inherit;padding:0}.rhdocs [class] .author{margin-bottom:1.5rem}.rhdocs [class] .author .author{margin-bottom:0}.rhdocs table{margin:2rem 0}.rhdocs [class] table{width:auto}.rhdocs table .table-contents table{max-width:100%;overflow:auto}.rhdocs rh-table table{margin:0;max-width:9999em;overflow:visible}.rhdocs td,.rhdocs th{border-left:0;padding:.5em 1rem;transition:background .25s ease-out}.rhdocs td.content--md[class][class],.rhdocs th.content--md[class][class]{min-width:13em}.rhdocs td.content--lg[class][class],.rhdocs th.content--lg[class][class]{min-width:20em}.rhdocs thead th{padding-top:1.5em}.rhdocs caption{color:currentColor;color:var(--pfe-table__caption--Color,currentColor);font-weight:700;margin-bottom:.5rem;margin-top:.5rem;text-align:center}.rhdocs .revhistory table td,.rhdocs .revhistory table th{border-color:transparent}.rhdocs .revhistory table td{padding:.625rem .875rem}.rhdocs .revhistory table.simplelist{margin:0}@media print{#masthead{display:none!important}}.rh-table--is-full-screen #to-top{display:none}.rhdocs{--rh-table--maxHeight:calc(100vh - 6.25rem);color:#151515;font-family:var(--rh-font-family-body-text,RedHatText,"Red Hat Text","Noto Sans Arabic","Noto Sans Hebrew","Noto Sans JP","Noto Sans KR","Noto Sans Malayalam","Noto Sans SC","Noto Sans TC","Noto Sans Thai",Helvetica,Arial,sans-serif);font-size:var(--rh-body-copy-lage,1.125rem);line-height:1.6667;-moz-tab-size:4;-o-tab-size:4;tab-size:4}.rhdocs rh-codeblock::slotted(#content){border-radius:.25rem;padding:var (--rh-space-lg,16px)}.rhdocs rh-codeblock .screen{display:grid;grid-template-columns:1fr 4.375rem}.rhdocs rh-codeblock[class][class][class][class][class]{max-width:99999em}.rhdocs .codeblock__copy span{display:block;height:0;position:absolute;visibility:hidden;width:0}.rhdocs .codeblock__copy:focus{outline:.0625rem dashed currentcolor}.rhdocs .codeblock__copy svg#icon--copy{height:1rem;width:1rem}.rhdocs pre{border:0;max-height:-moz-max-content;max-height:max-content}.rhdocs pre,pre[class]{margin:0;padding:1.25em 1em;position:relative}.rhdocs rh-code-block>div.codeblock__inner-wrapper>pre,.rhdocs rh-code-block>div.codeblock__inner-wrapper>pre[class]{margin:0;padding:0;position:relative}.rhdocs code[class*=language-],pre[class*=language-]{color:#151515;-moz-tab-size:4;-o-tab-size:4;tab-size:4}.rhdocs code.literal{background:#eee;border-radius:.25rem;color:#000;font-size:.875rem;line-height:1.6667;overflow-wrap:break-word;padding:.125em .5em;word-break:break-word}.rhdocs code.literal,.rhdocs kbd,.rhdocs span.keycap{font-family:RedHatMono,Red Hat Mono,Consolas,monospace}.rhdocs kbd,.rhdocs span.keycap{background-color:#eee;background-image:linear-gradient(180deg,#ddd,#eee,#fff);border-radius:.1875rem;box-shadow:0 -.0625rem 0 0 #fff,0 .0625rem 0 .1875rem #aaa;font-size:90%;font-weight:400;margin:0 .25rem;padding:.125rem .375rem}.rhdocs ol,.rhdocs ul{margin:1rem 0;padding:0 0 0 1.5rem}.rhdocs ._additional-resources[class][class],.rhdocs ._additional-resources[class][class][id]:last-child{background:#fff;border:.0625rem solid #d2d2d2;border-radius:.1875rem;margin:2em 0 4em;padding:2rem 2rem 1rem}.rhdocs ._additional-resources[class][class] ul{border:0;list-style:none;margin:0;padding:0;position:relative}.rhdocs ._additional-resources[class][class] li{border-bottom:.0625rem solid #d2d2d2;box-sizing:content-box;margin:0;padding:1rem 1.5rem 1rem 0;-moz-column-break-inside:avoid;break-inside:avoid}.rhdocs ._additional-resources[class][class] li:last-child{border:0}.rhdocs section.section#additional_resource .additional-resources__heading,.rhdocs section.section#additional_resource .heading,.rhdocs section.section#additional_resource h1,.rhdocs section.section#additional_resource h2,.rhdocs section.section#additional_resource h3,.rhdocs section.section#additional_resource h4,.rhdocs section.section#additional_resource h5,.rhdocs section.section#additional_resource h6,.rhdocs section.section#additional_resource p.title{display:block;font-family:RedHatDisplay,Red Hat Display,Helvetica Neue,Arial,sans-serif;font-size:1.125rem;font-weight:700;line-height:1.5rem;margin:0 0 .5rem;padding:0;text-transform:uppercase}.rhdocs section.section:first-of-type{margin-top:var(--rh-space-4xl,64px)}.rhdocs section.section p{margin-bottom:var(--rh-space-lg,16px);margin-top:0;word-wrap:break-word}.rhdocs .section.section h1,.rhdocs .section.section h2,.rhdocs .section.section h3,.rhdocs .section.section h4,.rhdocs .section.section h5,.rhdocs .section.section h6,.rhdocs h1,.rhdocs h2,.rhdocs h3,.rhdocs h4,.rhdocs h5,.rhdocs h6{font-family:RedHatDisplay,Red Hat Display,Helvetica Neue,Arial,sans-serif;font-weight:400;line-height:1.3333}.rhdocs h1:first-of-type,.rhdocs h2:first-of-type,.rhdocs h3:first-of-type,.rhdocs h4:first-of-type,.rhdocs h5:first-of-type,.rhdocs h6:first-of-type{margin-top:0}.rhdocs h1,.rhdocs h2,.rhdocs h3,.rhdocs h4,.rhdocs h5,.rhdocs h6{font-family:RedHatDisplay,Red Hat Display,Helvetica,Arial,sans-serif;font-weight:400;line-height:1.3333}.rhdocs h2,.rhdocs section.section h2{font-size:var(--rh-font-size-heading-md,1.75rem)}.rhdocs h3,.rhdocs section.section h3{font-size:1.5rem;font-weight:400}.rhdocs dl dt{font-weight:600;margin:.5rem 0}.rhdocs dl{display:block;margin-block-end:1em;margin-block-start:1em;margin-inline-end:0;margin-inline-start:0}.rhdocs .para{margin:1.49963rem 0}.rhdocs dl.calloutlist[class] dt{float:none;margin:0;padding:0}.rhdocs dl.calloutlist[class] dd>:last-child{margin-bottom:0}.rhdocs dl.calloutlist[class]{display:grid;gap:1.25em .75em;grid-template-columns:fit-content(40%) 1fr}.rhdocs .calloutlist dt{clear:left;display:flex;flex-wrap:wrap;float:left;margin:0;padding:0 .5rem 0 0}.rhdocs .calloutlist dt a:not(:first-child){padding-left:4px}.rhdocs dl.calloutlist[class] dd{margin:0;padding:0}.rhdocs .callout,.rhdocs .colist>ol>li:before,.rhdocs .conum{background:#06c;border-radius:50%;color:#fff;display:inline-block;font-family:RedHatText,Red Hat Text,Helvetica Neue,Arial,sans-serif;font-size:.75rem;font-style:normal;font-weight:600;height:1.25rem;line-height:1.35rem;padding:0;position:relative;text-align:center;top:-.125em;vertical-align:middle;width:1.25rem}.rhdocs img,.rhdocs object,.rhdocs svg{display:inline-block;max-width:100%;vertical-align:middle}.rhdocs .titlepage .svg-img[data*="title_logo.svg"]{margin:1.5rem 0;width:15rem}.rhdocs[class] .author{margin-bottom:1.5rem}.rhdocs[class] .author .author{margin-bottom:0}.rhdocs .para>.title[class],p.title[class]{font-size:1rem;font-style:normal;font-weight:700;line-height:1.6667;margin:1.25rem 0 0}.rhdocs .example{border-left:.3125rem solid #ccc;margin-bottom:2rem;padding:1rem 0 1rem 1rem}.rhdocs code{background:#eee;color:#000;font-family:RedHatMono,Red Hat Mono,Consolas,monospace;font-size:.875rem;line-height:1.6667;overflow-wrap:break-word;padding:.125em .5em;word-break:break-word}.rhdocs .para[class]{margin-bottom:1.49963rem}.rhdocs[class] code.email{background-color:transparent;font:inherit;padding:0}rh-alert.admonition #description,rh-alert.admonition p{font-size:var(--rh-font-size-body-text-md,1rem)}rh-alert{width:-moz-fit-content;width:fit-content}.rhdocs .producttitle{color:#000;font-size:1.25rem;text-transform:uppercase}.rhdocs dl{margin:1rem 0}.rhdocs dl dt{font-weight:600;margin:.5rem 0}.rhdocs ol ol{list-style:lower-roman}.rhdocs .codeblock--processed pf-clipboard-copy::part(input),.rhdocs .codeblock--processed pf-clipboard-copy::part(span){display:none}.token.tag{color:#c9190b}.calloutlist div.para{margin:0}rh-alert.admonition{margin-bottom:var(--rh-space-lg,1rem)}.guibutton,.guimenu,.guimenuitem{font-weight:700}.guibutton{font-size:90%;padding:.1875rem}.guibutton:before{content:"["}.guibutton:after{content:"]"}.docs-content-container,.rhdocs{--rh-table--maxHeight:calc(100vh - 6.25rem);color:#151515;font-family:RedHatText,Red Hat Text,Helvetica Neue,Arial,sans-serif;font-size:1.125rem;line-height:1.6667;-moz-tab-size:4;-o-tab-size:4;tab-size:4}pre[hidden]{display:none}.codeblock[class][class][class][class][class]{max-width:99999em}.codeblock__wrapper{background:var(--rh-color-surface-lighter,#f2f2f2);margin:1rem 0;overflow:visible;position:relative;transform:translate(0);z-index:0}.codeblock__inner-wrapper:after{content:"";display:block;min-height:.625rem;width:4.375rem}.codeblock__copy{--pfe-clipboard--icon--Color--hover:#06c;-webkit-appearance:none;-moz-appearance:none;appearance:none;background:#f0efef;height:1.75rem;left:calc(100% - 2.75rem - var(--scrollbar__width, 0px));padding:.3125rem .375rem;position:absolute;top:1rem;width:1.75rem;z-index:2}.codeblock__inner-wrapper pre{border:0;max-height:-moz-max-content;max-height:max-content}.pfe-clipboard:not([copied]) .pfe-clipboard__text--success,:host(:not([copied])) .pfe-clipboard__text--success{display:none!important}.codeblock[class]{margin:0;overflow:visible;padding-right:0}pre{display:block;font-size:.8125rem;line-height:1.42857;margin:0 0 .625rem;word-break:break-all;word-wrap:break-word;background-color:var(--rh-color-surface-lighter,#f2f2f2);border:.0625rem solid #ccc;border-radius:.25rem;color:#333}.docs-content-container pre,.rhdocs pre{background:var(--rh-color-surface-lighter,#f2f2f2);color:#151515;font-family:RedHatMono,Red Hat Mono,Consolas,monospace;font-size:.875rem;line-height:1.6667;overflow-wrap:normal;white-space:pre;word-break:normal}.rhdocs pre[class]{line-height:1.6667;overflow-x:auto}rh-codeblock pre[class][class]{overflow-x:auto}.pfe-clipboard__text--success{background-color:#ddd;border:1px solid #000;border-radius:2px}*,:after,:before{box-sizing:border-box}:root{--rh-space-xs:4px;--rh-space-sm:6px;--rh-space-md:8px;--rh-space-lg:16px;--rh-space-xl:24px;--rh-space-2xl:32px;--rh-space-3xl:48px;--rh-space-4xl:64px;--rh-space-5xl:80px;--rh-space-6xl:96px;--rh-space-7xl:128px;--rh-font-size-body-text-xs:.75rem;--rh-font-size-body-text-sm:.875rem;--rh-font-size-body-text-md:1rem;--rh-font-size-body-text-lg:1.125rem;--rh-font-size-body-text-xl:1.25rem;--rh-font-size-body-text-2xl:1.5rem;--rh-font-size-heading-xs:1.25rem;--rh-font-size-heading-sm:1.5rem;--rh-font-size-heading-md:1.75rem;--rh-font-size-heading-lg:2.25rem;--rh-font-size-heading-xl:2.5rem;--rh-font-size-heading-2xl:3rem;--pfe-navigation--logo--maxWidth:200px;--pfe-navigation__logo--height:40px;--pfe-navigation--fade-transition-delay:500ms;--pfe-navigation__nav-bar--highlight-color:var(--rh-color-brand-red-on-dark,#e00);--pf-global--icon--FontSize--sm:.75rem}body,html{font-family:Red Hat Text,sans-serif;font-size:var(--rh-font-size-body-text-md,1rem);line-height:var(--rh-line-height-body-text,1.5);margin:0}h1,h2,h3,h4,h5,h6{font-family:Red Hat Display,sans-serif;font-weight:400;line-height:var(--rh-line-height-heading,1.3)}h1{font-size:var(--rh-font-size-heading-2xl,3rem);line-height:62px}h2{font-size:var(--rh-font-size-heading-xl,2.5rem);line-height:48px}h3{font-size:var(--rh-font-size-heading-lg,2.25rem)}h4{font-size:var(--rh-font-size-heading-md,2.25rem)}h5{font-size:var(--rh-font-size-heading-sm,2.25rem)}h6{font-size:var(--rh-font-size-heading-xs,2.25rem)}main{line-height:30px}section{padding-bottom:3rem;padding-top:3rem}img{height:auto;max-width:100%}a{color:var(--rh-color-interactive-blue-darker,#06c);text-decoration:none}a:hover{color:var(--rh-color-interactive-blue-darkest,#004080)}rh-alert.html-container a{text-decoration:underline}.container{padding-left:12px;padding-right:12px}.container,.container-fluid{margin-left:auto;margin-right:auto;width:100%}.container-fluid{padding:12px}@media (min-width:576px){.container{max-width:540px}}@media (min-width:768px){.container{max-width:720px}}@media (min-width:992px){.container{max-width:960px}}@media (min-width:1200px){.container{min-width:1140px}}@media (min-width:1400px){.container{min-width:1320px}}.grid{display:grid;gap:var(--rh-space-xl,24px)}.grid-center{margin:auto}.grid.grid-col-2{grid-template-columns:repeat(2,1fr)}.grid.grid-col-3{grid-template-columns:repeat(3,1fr)}.grid.grid-col-4{grid-template-columns:repeat(4,1fr)}.grid.grid-col-5{grid-template-columns:repeat(5,1fr)}.grid.grid-col-6{grid-template-columns:repeat(6,1fr)}.grid.grid-col-7{grid-template-columns:repeat(7,1fr)}.grid.grid-col-8{grid-template-columns:repeat(8,1fr)}.grid.grid-col-9{grid-template-columns:repeat(9,1fr)}.grid.grid-col-10{grid-template-columns:repeat(10,1fr)}.grid.grid-col-11{grid-template-columns:repeat(11,1fr)}.grid.grid-col-12{grid-template-columns:repeat(12,1fr)}@media (min-width:768px){.grid.grid-col-md-2{grid-template-columns:repeat(2,1fr)}.grid.grid-col-md-3{grid-template-columns:repeat(3,1fr)}.grid.grid-col-md-4{grid-template-columns:repeat(4,1fr)}.grid.grid-col-md-5{grid-template-columns:repeat(5,1fr)}.grid.grid-col-md-6{grid-template-columns:repeat(6,1fr)}.grid.grid-col-md-7{grid-template-columns:repeat(7,1fr)}.grid.grid-col-md-8{grid-template-columns:repeat(8,1fr)}.grid.grid-col-md-9{grid-template-columns:repeat(9,1fr)}.grid.grid-col-md-10{grid-template-columns:repeat(10,1fr)}.grid.grid-col-md-11{grid-template-columns:repeat(11,1fr)}.grid.grid-col-md-12{grid-template-columns:repeat(12,1fr)}}@media (min-width:992px){.grid.grid-col-lg-2{grid-template-columns:repeat(2,1fr)}.grid.grid-col-lg-3{grid-template-columns:repeat(3,1fr)}.grid.grid-col-lg-4{grid-template-columns:repeat(4,1fr)}.grid.grid-col-lg-5{grid-template-columns:repeat(5,1fr)}.grid.grid-col-lg-6{grid-template-columns:repeat(6,1fr)}.grid.grid-col-lg-7{grid-template-columns:repeat(7,1fr)}.grid.grid-col-lg-8{grid-template-columns:repeat(8,1fr)}.grid.grid-col-lg-9{grid-template-columns:repeat(9,1fr)}.grid.grid-col-lg-10{grid-template-columns:repeat(10,1fr)}.grid.grid-col-lg-11{grid-template-columns:repeat(11,1fr)}.grid.grid-col-lg-12{grid-template-columns:repeat(12,1fr)}}.span-1{grid-column:span 1}.span-2{grid-column:span 2}.span-3{grid-column:span 3}.span-4{grid-column:span 4}.span-5{grid-column:span 5}.span-6{grid-column:span 6}.span-7{grid-column:span 7}.span-8{grid-column:span 8}.span-9{grid-column:span 9}.span-10{grid-column:span 10}.span-11{grid-column:span 11}.span-12{grid-column:span 12}@media (min-width:399px){.span-xs-1{grid-column:span 1}.span-xs-2{grid-column:span 2}.span-xs-3{grid-column:span 3}.span-xs-4{grid-column:span 4}.span-xs-5{grid-column:span 5}.span-xs-6{grid-column:span 6}.span-xs-7{grid-column:span 7}.span-xs-8{grid-column:span 8}.span-xs-9{grid-column:span 9}.span-xs-10{grid-column:span 10}.span-xs-11{grid-column:span 11}.span-xs-12{grid-column:span 12}}@media (min-width:768px){.span-md-1{grid-column:span 1}.span-md-2{grid-column:span 2}.span-md-3{grid-column:span 3}.span-md-4{grid-column:span 4}.span-md-5{grid-column:span 5}.span-md-6{grid-column:span 6}.span-md-7{grid-column:span 7}.span-md-8{grid-column:span 8}.span-md-9{grid-column:span 9}.span-md-10{grid-column:span 10}.span-md-11{grid-column:span 11}.span-md-12{grid-column:span 12}}@media (min-width:992px){.span-lg-1{grid-column:span 1}.span-lg-2{grid-column:span 2}.span-lg-3{grid-column:span 3}.span-lg-4{grid-column:span 4}.span-lg-5{grid-column:span 5}.span-lg-6{grid-column:span 6}.span-lg-7{grid-column:span 7}.span-lg-8{grid-column:span 8}.span-lg-9{grid-column:span 9}.span-lg-10{grid-column:span 10}.span-lg-11{grid-column:span 11}.span-lg-12{grid-column:span 12}}@media (min-width:1025px){.span-xl-1{grid-column:span 1}.span-xl-2{grid-column:span 2}.span-xl-3{grid-column:span 3}.span-xl-4{grid-column:span 4}.span-xl-5{grid-column:span 5}.span-xl-6{grid-column:span 6}.span-xl-7{grid-column:span 7}.span-xl-8{grid-column:span 8}.span-xl-9{grid-column:span 9}.span-xl-10{grid-column:span 10}.span-xl-11{grid-column:span 11}.span-xl-12{grid-column:span 12}}@media (min-width:1200px){.span-2xl-1{grid-column:span 1}.span-2xl-2{grid-column:span 2}.span-2xl-3{grid-column:span 3}.span-2xl-4{grid-column:span 4}.span-2xl-5{grid-column:span 5}.span-2xl-6{grid-column:span 6}.span-2xl-7{grid-column:span 7}.span-2xl-8{grid-column:span 8}.span-2xl-9{grid-column:span 9}.span-2xl-10{grid-column:span 10}.span-2xl-11{grid-column:span 11}.span-2xl-12{grid-column:span 12}}.flex{display:flex;flex-direction:column;gap:var(--rh-space-lg,16px)}.flex-row{flex-direction:row}.flex-column{flex-direction:column}@media (min-width:768px){.flex-md-row{flex-direction:row}.flex-md-column{flex-direction:column}}.typography-h1{font-size:var(--rh-font-size-heading-2xl,3rem)}.typography-h2{font-size:var(--rh-font-size-heading-xl,2.5rem)}.typography-h3{font-size:var(--rh-font-size-heading-lg,2.25rem)}.typography-h4{font-size:var(--rh-font-size-heading-md,1.75rem)}.typography-h5{font-size:var(--rh-font-size-heading-sm,1.5rem)}.typography-h6{font-size:var(--rh-font-size-heading-xs,1.25rem)}.content section{padding:0}.content h1,.content h2,.content h3,.content h4,.content h5,.content h6{margin:var(--rh-space-lg,16px) 0}.sr-only{height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px;clip:rect(0,0,0,0);border:0}.list-unstyled{list-style:none;padding-left:0}.tooltip-content{align-items:center;display:flex;font-family:Red Hat Text;justify-content:center;text-transform:none}.tooltip-content .check-icon{margin-left:var(--rh-space-md,8px)}.doc-image-link{display:inline-block;text-decoration:none}.modal-img{display:block;width:100%}.modal-helper-text{margin-top:.5rem;text-align:center}.modal-helper-text a{color:#000;cursor:pointer}.modal-helper-text a:hover{text-decoration:underline}.modal-helper-text a:after{content:"⿻";margin-left:.25rem}pf-modal.pf-img-modal{--pf-c-modal-box--MaxHeight:90vh;overflow-y:scroll}pf-modal.pf-img-modal::part(close-button){background-color:#fff;border-radius:50%;color:#000;margin-right:-2rem;margin-top:-2rem}pf-modal.pf-img-modal::part(close-button):hover{opacity:.7}h2.truste-title{line-height:normal;margin-top:0}rh-alert p[slot=header]{color:#002952}@media (width < 992px){html:has(nav.mobile-nav .mobile-nav-wrapper){scroll-behavior:smooth;scroll-padding-top:4rem}html:has(nav.mobile-nav .mobile-jump-links){scroll-padding-top:7rem}html:has(nav.mobile-nav.hide-mobile-nav){scroll-padding-top:2rem}}.highlight{background:#fff4cc;color:#000}</style>
<style>rh-alert[data-v-84359384]{width:100%}</style>
<style>.search-btn[data-v-edc0d12c]{align-items:center;background-color:var(--rh-color-canvas-black,#151515);border:3px solid var(--rh-color-canvas-black,#151515);cursor:pointer;display:flex;flex-direction:column;height:100%;justify-content:center;outline:none;padding:14px var(--rh-space-md,8px)}.search-btn[data-v-edc0d12c]:focus{border-top:3px solid var(--rh-color-accent-brand-on-light,#e00);outline:2px dotted var(--rh-color-white,#fff)}.search-btn .search-icon[data-v-edc0d12c]{height:26px;padding:2px 0 var(--rh-space-xs,4px);width:20px}.search-btn .search-icon[data-v-edc0d12c],.search-icon-helper-text[data-v-edc0d12c]{color:var(--rh-color-white,#fff)}.search-mobile[data-v-edc0d12c]{margin-bottom:var(--rh-space-2xl,32px)}.search-mobile form[data-v-edc0d12c]{display:flex;gap:var(--rh-space-md,8px);margin:auto}.search-box[data-v-edc0d12c]{width:35rem}nav[data-v-edc0d12c]{background-color:#151515;justify-content:space-between;width:100%}a[data-v-edc0d12c],a[data-v-edc0d12c]:visited{color:#fff;display:inline-block;font-size:var(--rh-font-size-body-text-md,1rem)}.skip-link[class][class][data-v-edc0d12c]{font-size:var(--pf-global--FontSize--sm,.875rem);line-height:18px}.skip-link[class][class][data-v-edc0d12c]:focus{border-radius:.21429em;height:auto;left:50%;padding:.42857em .57143em;position:fixed;top:8px;transform:translateX(-50%);width:auto;z-index:99999;clip:auto;background:#fff;background:var(--pfe-navigation__skip-link--BackgroundColor,var(--pfe-theme--color--surface--lightest,#fff));color:#06c;color:var(--pfe-navigation__skip-link--Color,var(--pfe-theme--color--link,#06c));text-decoration:none}.visually-hidden[data-v-edc0d12c]{border:1px solid #06c;height:1px;overflow:hidden;padding:0;position:absolute;width:1px;clip:rect(0,0,0,0);white-space:nowrap}h3[data-v-edc0d12c]{color:#464646;font-family:var(--rh-font-family-heading,"Red Hat Display",Helvetica,Arial,sans-serif);font-size:var(--rh-font-size-body-text-lg,1.125rem)}.language-picker[data-v-edc0d12c]{align-items:center;background-color:#fff;display:flex;flex-direction:column;padding:var(--rh-space-xl,24px);width:100%}.language-picker h3[data-v-edc0d12c]{margin:0;padding:0 1rem 1rem}.language-picker ul[data-v-edc0d12c]{margin:0;padding:0}.language-picker a[data-v-edc0d12c]{color:#06c}.language-picker li[data-v-edc0d12c]{list-style:none}.language-dropdown[data-v-edc0d12c]{background:#fff;box-shadow:0 3px 6px rgba(0,0,0,.098);display:block!important;position:absolute;right:0;width:100%;z-index:104}.pfe-navigation.pfe-navigation--processed>[slot=secondary-links][data-v-edc0d12c]{height:auto;overflow:visible;visibility:visible;width:auto}.upper-navigation[data-v-edc0d12c]{padding:0 var(--rh-space-2xl,32px)}.upper-nav-container[data-v-edc0d12c]{border-bottom:1px solid #404040;margin:0}.upper-nav-hidden[data-v-edc0d12c]:not(:focus):not(:active){clip:rect(0 0 0 0);clip-path:inset(50%);height:1px;overflow:hidden;position:absolute;white-space:nowrap;width:1px}.upper-nav-menu[data-v-edc0d12c]{align-items:center;display:flex;justify-content:flex-end;line-height:1.444;list-style:none;margin-bottom:0;margin-top:0;padding-left:0}.upper-nav-menu[data-v-edc0d12c],.upper-nav-menu>li[data-v-edc0d12c]{position:relative}.upper-nav-menu>li:not(:first-child)>a[data-v-edc0d12c]:before,.upper-nav-menu>li:not(:first-child)>button[data-v-edc0d12c]:before{background-color:#404040;content:"";height:40%;left:0;position:absolute;top:30%;width:1px}li[data-v-edc0d12c]{display:list-item;margin:0;padding:0;text-align:-webkit-match-parent}.upper-nav-menu button.upper-nav-links[data-v-edc0d12c]{border:0;border-top:3px solid transparent;cursor:pointer;line-height:1.444}.upper-nav-menu button.upper-nav-links[aria-expanded=true][data-v-edc0d12c]{outline-color:#151515}.upper-nav-menu button.upper-nav-links[aria-expanded=true] .upper-nav-arrow[data-v-edc0d12c]{filter:invert(0) sepia(2%) saturate(21%) hue-rotate(257deg) brightness(108%) contrast(100%);transform:rotate(270deg)}.upper-nav-menu button.upper-nav-links[aria-expanded=true][data-v-edc0d12c]:before{display:none}.upper-nav-menu .upper-nav-links[data-v-edc0d12c]{background-color:var(--pfe-navigation--BackgroundColor,var(--pfe-theme--color--surface--darkest,#151515));border-top:3px solid transparent;color:#fff;display:block;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:var(--rh-font-size-body-text-sm,.875rem);outline:none;padding:12px 12px 14px;text-decoration:none}.upper-nav-menu .upper-nav-links[data-v-edc0d12c]:hover{border-top-color:#b8bbbe}.upper-nav-menu .upper-nav-links[data-v-edc0d12c]:focus-within{outline:1px dashed var(--rh-color-white,#fff);outline-offset:-1px}.upper-nav-menu .upper-nav-links[data-v-edc0d12c]:focus-within:before{display:none}.upper-nav-dropdown-container[data-v-edc0d12c]{background:#fff;box-shadow:0 3px 6px rgba(0,0,0,.098);display:none;padding:5px 30px 24px;position:absolute;right:0;top:100%;width:500px;z-index:105}.upper-nav-dropdown-container>ul[data-v-edc0d12c]{-moz-column-count:2;column-count:2;list-style-type:none;padding:0;width:auto}.upper-nav-dropdown-container>ul li[data-v-edc0d12c]{color:#151515;font-family:var(--rh-font-family-heading,"Red Hat Display",Helvetica,Arial,sans-serif);font-size:var(--rh-font-size-body-text-sm,.875rem);list-style-type:none;margin-bottom:0}.upper-nav-dropdown-container>ul li span[data-v-edc0d12c]{font-weight:var(--rh-font-weight-body-text-medium,500)}.upper-nav-dropdown-container>ul ul[data-v-edc0d12c]{padding-left:0;padding-top:9px}.upper-nav-dropdown-container>ul>li[data-v-edc0d12c]{padding-top:19px;-moz-column-break-inside:avoid;break-inside:avoid}.upper-nav-dropdown-container>ul>li>ul>li[data-v-edc0d12c]{line-height:1.45;padding:4px 0}.upper-nav-menu .upper-nav-arrow[data-v-edc0d12c]{display:inline-block;filter:invert(100%) sepia(8%) saturate(7%) hue-rotate(1turn) brightness(100%) contrast(93%);height:18px;margin-left:5px;transform:rotate(90deg);vertical-align:middle;width:8px}#pfe-navigation__secondary-links .show[data-v-edc0d12c],.upper-navigation .show[data-v-edc0d12c]{display:block}.upper-nav-menu .upper-nav-links[aria-expanded=true][data-v-edc0d12c]:active,.upper-nav-menu .upper-nav-links[aria-expanded=true][data-v-edc0d12c]:focus,.upper-nav-menu .upper-nav-links[aria-expanded=true][data-v-edc0d12c]:hover{background-color:#fff;color:#151515}.upper-nav-menu .upper-nav-links[aria-expanded=true][data-v-edc0d12c]{background-color:#fff;border-top-color:#b8bbbe;color:#000;position:relative;z-index:1}.upper-nav-dropdown-container>ul a[data-v-edc0d12c]{color:var(--rh-color-accent-base-on-light,#06c);font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:14px;text-decoration:none}.pfe-navigation__search[data-v-edc0d12c]{background-color:var(--rh-color-white,#fff)}.pfe-navigation__search form[data-v-edc0d12c]{display:flex;gap:var(--rh-space-md,8px);margin:auto;max-width:992px}pfe-navigation [slot=secondary-links] .buttons[data-v-edc0d12c]{display:flex;flex-wrap:wrap;gap:var(--rh-space-md,8px);margin-top:4px}pfe-navigation [slot=secondary-links] .buttons a[data-v-edc0d12c]{border:1px solid #d2d2d2;border-radius:3px;color:#06c;cursor:pointer;flex-basis:calc(50% - 5px);font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-weight:var(--rh-font-weight-code-regular,400);padding:1em;text-align:center;text-decoration:none}pfe-navigation [slot=secondary-links] .mobile-lang-select[data-v-edc0d12c]{border:1px solid #d2d2d2;border-bottom-color:#3c3f42;cursor:pointer;display:flex;margin:3rem 0;position:relative}pfe-navigation [slot=secondary-links] .mobile-lang-select label[data-v-edc0d12c]{bottom:100%;color:#000;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:14px;font-weight:500;margin-bottom:5px;position:absolute}pfe-navigation [slot=secondary-links] .mobile-lang-select select[data-v-edc0d12c]{-webkit-appearance:none;-moz-appearance:none;appearance:none;background-color:#fff;border-style:none;color:#000;flex-basis:100%;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:16px;line-height:24px;padding:6px 24px 6px 8px}select[data-v-edc0d12c]{background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' width='10' height='6' fill='none' viewBox='0 0 10 6'%3E%3Cpath fill='%23151515' d='M.678 0h8.644c.596 0 .895.797.497 1.195l-4.372 4.58c-.298.3-.695.3-.993 0L.18 1.196C-.216.797.081 0 .678 0'/%3E%3C/svg%3E");background-position:98% 50%;background-repeat:no-repeat}#inputLabel[data-v-edc0d12c]{align-items:center;display:flex;position:relative}#inputLabel form[data-v-edc0d12c]{width:100%}.input-box[data-v-edc0d12c]{font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:var(--rh-font-size-body-text-md,1rem);height:36px;padding:0 8px;width:100%}.input-box[data-v-edc0d12c]::-moz-placeholder{color:#6a6e73;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:var(--rh-font-size-body-text-md,1rem);font-weight:var(--rh-font-weight-code-regular,400);line-height:24px}.input-box[data-v-edc0d12c]::placeholder{color:#6a6e73;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:var(--rh-font-size-body-text-md,1rem);font-weight:var(--rh-font-weight-code-regular,400);line-height:24px}@media(max-width:960px){.search-box[data-v-edc0d12c]{width:28rem}}@media (max-width:768px){.right-navigation[data-v-edc0d12c],.upper-navigation[data-v-edc0d12c]{display:none}}@media (min-width:767px){.pfe-navigation__search form[data-v-edc0d12c]{padding:var(--rh-space-2xl,32px) 0}}</style>
<style>.element-invisible,.sr-only,.visually-hidden{height:1px;overflow:hidden;padding:0;position:absolute;width:1px;clip:rect(0,0,0,0);border:0;white-space:nowrap}@keyframes reveal-nav{0%{max-height:72px;max-height:var(--pfe-navigation__nav-bar--Height,72px);opacity:0;visibility:hidden}99%{max-height:72px;max-height:var(--pfe-navigation__nav-bar--Height,72px)}to{max-height:9999em;opacity:1;visibility:visible}}@keyframes reveal-nav-parts{0%{max-height:72px;max-height:var(--pfe-navigation__nav-bar--Height,72px);opacity:0;visibility:hidden}1%{visibility:visible}99%{max-height:72px;max-height:var(--pfe-navigation__nav-bar--Height,72px)}to{max-height:9999em;opacity:1;visibility:visible}}@media print{.pfe-navigation__menu,pfe-navigation [slot]{display:none!important}}pfe-navigation{--pfe-broadcasted--text:var(--pfe-theme--color--text,#151515);--pfe-broadcasted--text--muted:var(--pfe-theme--color--text--muted,#6a6e73);--pfe-broadcasted--link:var(--pfe-theme--color--link,#06c);--pfe-broadcasted--link--hover:var(--pfe-theme--color--link--hover,#004080);--pfe-broadcasted--link--focus:var(--pfe-theme--color--link--focus,#004080);--pfe-broadcasted--link--visited:var(--pfe-theme--color--link--visited,#6753ac);--pfe-broadcasted--link-decoration:var(--pfe-theme--link-decoration,none);--pfe-broadcasted--link-decoration--hover:var(--pfe-theme--link-decoration--hover,underline);--pfe-broadcasted--link-decoration--focus:var(--pfe-theme--link-decoration--focus,underline);--pfe-broadcasted--link-decoration--visited:var(--pfe-theme--link-decoration--visited,none)}@supports (display:grid){pfe-navigation{animation:reveal-nav .1618s 4s 1 forwards;max-height:72px;max-height:var(--pfe-navigation__nav-bar--Height,72px)}pfe-navigation>*{animation:reveal-nav-parts .1618s 4s 1 forwards;opacity:0;transition:opacity .1618s ease-in-out;transition:opacity var(--pfe-reveal-duration,.1618s) ease-in-out;visibility:hidden}}pfe-navigation.pfe-navigation--processed,pfe-navigation.pfe-navigation--processed>*{animation:none;opacity:1;visibility:visible}pfe-navigation pfe-primary-detail{display:none}pfe-navigation[pfelement]{display:block}pfe-navigation-dropdown{color:#151515;color:var(--pfe-navigation__dropdown--Color,#151515)}#pfe-navigation[breakpoint=desktop] .hidden-at-desktop[class][class][class],#pfe-navigation[breakpoint=mobile] .hidden-at-mobile[class][class][class],#pfe-navigation[breakpoint=tablet] .hidden-at-tablet[class][class][class],pfe-navigation[breakpoint=desktop] .hidden-at-desktop[class][class][class],pfe-navigation[breakpoint=mobile] .hidden-at-mobile[class][class][class],pfe-navigation[breakpoint=tablet] .hidden-at-tablet[class][class][class]{display:none}#pfe-navigation,#pfe-navigation *,pfe-navigation,pfe-navigation *{box-sizing:border-box}#pfe-navigation [pfelement] .pfe-navigation__log-in-link,pfe-navigation [pfelement] .pfe-navigation__log-in-link{display:none}#pfe-navigation,pfe-navigation{align-items:stretch;background:#151515;background:var(--pfe-navigation__nav-bar--Background,#151515);color:#fff;color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));display:flex;font-family:Red Hat Text,RedHatText,Arial,Helvetica,sans-serif;font-family:var(--pfe-navigation--FontFamily,Red Hat Text,RedHatText,Arial,Helvetica,sans-serif);font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);height:72px;height:var(--pfe-navigation__nav-bar--Height,72px);height:auto;line-height:1.5;margin:0;max-width:9999em;min-height:72px;min-height:var(--pfe-navigation__nav-bar--Height,72px);padding:0 16px;position:relative;z-index:95;z-index:var(--pfe-navigation--ZIndex,var(--pfe-theme--zindex--navigation,95))}@media (min-width:768px){#pfe-navigation,pfe-navigation{flex-wrap:wrap;margin:0;max-width:9999em;padding:0 16px}}@media (min-width:1200px){#pfe-navigation,pfe-navigation{margin:0 auto;padding:0 32px}}#pfe-navigation .pfe-navigation__dropdown,#pfe-navigation pfe-navigation-dropdown,pfe-navigation .pfe-navigation__dropdown,pfe-navigation pfe-navigation-dropdown{display:none}#pfe-navigation>[slot=account],#pfe-navigation>[slot=search],#pfe-navigation>[slot=secondary-links],pfe-navigation>[slot=account],pfe-navigation>[slot=search],pfe-navigation>[slot=secondary-links]{height:0;overflow:hidden;visibility:hidden;width:0}@media (min-width:768px){#pfe-navigation nav.pfe-navigation,pfe-navigation nav.pfe-navigation{align-items:stretch;display:flex;flex-wrap:wrap}}@media (min-width:992px){#pfe-navigation nav.pfe-navigation,pfe-navigation nav.pfe-navigation{flex-wrap:nowrap}}#pfe-navigation .pfe-navigation__logo-wrapper,pfe-navigation .pfe-navigation__logo-wrapper{align-items:center;display:flex;justify-content:flex-start;margin:0;min-width:150px;padding:10px 16px 10px 0}@media (min-width:768px){.pfe-navigation--no-main-menu #pfe-navigation .pfe-navigation__logo-wrapper,.pfe-navigation--no-main-menu pfe-navigation .pfe-navigation__logo-wrapper{margin-right:auto}}.pfe-navigation--collapse-secondary-links .pfe-navigation--no-main-menu #pfe-navigation .pfe-navigation__logo-wrapper,.pfe-navigation--collapse-secondary-links .pfe-navigation--no-main-menu pfe-navigation .pfe-navigation__logo-wrapper{margin-right:0}#pfe-navigation .pfe-navigation__logo-link,pfe-navigation .pfe-navigation__logo-link{border-radius:3px;display:block;margin-left:-8px;outline:0;padding:6px 8px;position:relative}#pfe-navigation .pfe-navigation__logo-link:focus,pfe-navigation .pfe-navigation__logo-link:focus{outline:0}#pfe-navigation .pfe-navigation__logo-link:focus:after,pfe-navigation .pfe-navigation__logo-link:focus:after{border:1px dashed #fff;bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation .pfe-navigation__logo-image,pfe-navigation .pfe-navigation__logo-image{display:block;height:auto;width:100%}@media (min-width:576px){#pfe-navigation .pfe-navigation__logo-image,pfe-navigation .pfe-navigation__logo-image{height:40px;height:var(--pfe-navigation__logo--height,40px);width:auto}}@media print{#pfe-navigation .pfe-navigation__logo-image,pfe-navigation .pfe-navigation__logo-image{display:none}}#pfe-navigation .pfe-navigation__logo-image:only-child,pfe-navigation .pfe-navigation__logo-image:only-child{display:block}@media (min-width:576px){#pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--small,pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--small{height:32px;height:var(--pfe-navigation__logo--height,32px)}}@media print{#pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--screen,pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--screen{display:none!important}}@media screen{#pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--print,pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--print{display:none!important}}#pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--screen.pfe-navigation__logo-image--print,pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--screen.pfe-navigation__logo-image--print{display:inline-block!important}#pfe-navigation .pfe-navigation__fallback-links a,#pfe-navigation .pfe-navigation__log-in-link,#pfe-navigation .pfe-navigation__menu-link,#pfe-navigation .pfe-navigation__secondary-link,pfe-navigation .pfe-navigation__fallback-links a,pfe-navigation .pfe-navigation__log-in-link,pfe-navigation .pfe-navigation__menu-link,pfe-navigation .pfe-navigation__secondary-link{--pfe-icon--color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));align-items:center;-webkit-appearance:none;-moz-appearance:none;appearance:none;background:0 0;border:0;color:#06c;color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));color:#fff;color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));cursor:pointer;display:flex;font-family:inherit;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);justify-content:flex-start;justify-content:center;margin:0;outline:0;padding:8px 24px;position:relative;text-align:center;text-decoration:none;white-space:nowrap;width:100%}@media print{#pfe-navigation .pfe-navigation__fallback-links a,#pfe-navigation .pfe-navigation__log-in-link,#pfe-navigation .pfe-navigation__menu-link,#pfe-navigation .pfe-navigation__secondary-link,pfe-navigation .pfe-navigation__fallback-links a,pfe-navigation .pfe-navigation__log-in-link,pfe-navigation .pfe-navigation__menu-link,pfe-navigation .pfe-navigation__secondary-link{display:none!important}}@media (min-width:768px){#pfe-navigation .pfe-navigation__fallback-links a,#pfe-navigation .pfe-navigation__log-in-link,#pfe-navigation .pfe-navigation__menu-link,#pfe-navigation .pfe-navigation__secondary-link,pfe-navigation .pfe-navigation__fallback-links a,pfe-navigation .pfe-navigation__log-in-link,pfe-navigation .pfe-navigation__menu-link,pfe-navigation .pfe-navigation__secondary-link{--pfe-icon--color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));color:#fff;color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));display:flex;flex-direction:column;font-size:12px;font-size:var(--pfe-navigation--FontSize--xs,12px);height:72px;height:var(--pfe-navigation__nav-bar--Height,72px);justify-content:flex-end;padding:14px 8px;width:auto}@supports (display:grid){#pfe-navigation .pfe-navigation__fallback-links a,#pfe-navigation .pfe-navigation__log-in-link,#pfe-navigation .pfe-navigation__menu-link,#pfe-navigation .pfe-navigation__secondary-link,pfe-navigation .pfe-navigation__fallback-links a,pfe-navigation .pfe-navigation__log-in-link,pfe-navigation .pfe-navigation__menu-link,pfe-navigation .pfe-navigation__secondary-link{align-items:center;display:grid;grid-template-rows:26px 18px;justify-items:center}}#pfe-navigation .pfe-navigation__fallback-links a[class]:focus,#pfe-navigation .pfe-navigation__fallback-links a[class]:hover,#pfe-navigation .pfe-navigation__log-in-link[class]:focus,#pfe-navigation .pfe-navigation__log-in-link[class]:hover,#pfe-navigation .pfe-navigation__menu-link[class]:focus,#pfe-navigation .pfe-navigation__menu-link[class]:hover,#pfe-navigation .pfe-navigation__secondary-link[class]:focus,#pfe-navigation .pfe-navigation__secondary-link[class]:hover,pfe-navigation .pfe-navigation__fallback-links a[class]:focus,pfe-navigation .pfe-navigation__fallback-links a[class]:hover,pfe-navigation .pfe-navigation__log-in-link[class]:focus,pfe-navigation .pfe-navigation__log-in-link[class]:hover,pfe-navigation .pfe-navigation__menu-link[class]:focus,pfe-navigation .pfe-navigation__menu-link[class]:hover,pfe-navigation .pfe-navigation__secondary-link[class]:focus,pfe-navigation .pfe-navigation__secondary-link[class]:hover{box-shadow:inset 0 4px 0 0 #06c;box-shadow:inset 0 4px 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}}#pfe-navigation .pfe-navigation__fallback-links a:focus,#pfe-navigation .pfe-navigation__fallback-links a:hover,#pfe-navigation .pfe-navigation__log-in-link:focus,#pfe-navigation .pfe-navigation__log-in-link:hover,#pfe-navigation .pfe-navigation__menu-link:focus,#pfe-navigation .pfe-navigation__menu-link:hover,#pfe-navigation .pfe-navigation__secondary-link:focus,#pfe-navigation .pfe-navigation__secondary-link:hover,pfe-navigation .pfe-navigation__fallback-links a:focus,pfe-navigation .pfe-navigation__fallback-links a:hover,pfe-navigation .pfe-navigation__log-in-link:focus,pfe-navigation .pfe-navigation__log-in-link:hover,pfe-navigation .pfe-navigation__menu-link:focus,pfe-navigation .pfe-navigation__menu-link:hover,pfe-navigation .pfe-navigation__secondary-link:focus,pfe-navigation .pfe-navigation__secondary-link:hover{box-shadow:inset 4px 0 0 0 #06c;box-shadow:inset 4px 0 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}@media (min-width:768px){#pfe-navigation .pfe-navigation__fallback-links a:focus,#pfe-navigation .pfe-navigation__fallback-links a:hover,#pfe-navigation .pfe-navigation__log-in-link:focus,#pfe-navigation .pfe-navigation__log-in-link:hover,#pfe-navigation .pfe-navigation__menu-link:focus,#pfe-navigation .pfe-navigation__menu-link:hover,#pfe-navigation .pfe-navigation__secondary-link:focus,#pfe-navigation .pfe-navigation__secondary-link:hover,pfe-navigation .pfe-navigation__fallback-links a:focus,pfe-navigation .pfe-navigation__fallback-links a:hover,pfe-navigation .pfe-navigation__log-in-link:focus,pfe-navigation .pfe-navigation__log-in-link:hover,pfe-navigation .pfe-navigation__menu-link:focus,pfe-navigation .pfe-navigation__menu-link:hover,pfe-navigation .pfe-navigation__secondary-link:focus,pfe-navigation .pfe-navigation__secondary-link:hover{box-shadow:inset 0 4px 0 0 #06c;box-shadow:inset 0 4px 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}}.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__fallback-links a:focus,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__fallback-links a:hover,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__log-in-link:focus,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__log-in-link:hover,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__menu-link:focus,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__menu-link:hover,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__secondary-link:focus,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__secondary-link:hover,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__fallback-links a:focus,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__fallback-links a:hover,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__log-in-link:focus,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__log-in-link:hover,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__menu-link:focus,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__menu-link:hover,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__secondary-link:focus,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__secondary-link:hover{box-shadow:inset 4px 0 0 0 #06c;box-shadow:inset 4px 0 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}#pfe-navigation .pfe-navigation__fallback-links a:focus,#pfe-navigation .pfe-navigation__log-in-link:focus,#pfe-navigation .pfe-navigation__menu-link:focus,#pfe-navigation .pfe-navigation__secondary-link:focus,pfe-navigation .pfe-navigation__fallback-links a:focus,pfe-navigation .pfe-navigation__log-in-link:focus,pfe-navigation .pfe-navigation__menu-link:focus,pfe-navigation .pfe-navigation__secondary-link:focus{outline:0}#pfe-navigation .pfe-navigation__fallback-links a:focus:after,#pfe-navigation .pfe-navigation__log-in-link:focus:after,#pfe-navigation .pfe-navigation__menu-link:focus:after,#pfe-navigation .pfe-navigation__secondary-link:focus:after,pfe-navigation .pfe-navigation__fallback-links a:focus:after,pfe-navigation .pfe-navigation__log-in-link:focus:after,pfe-navigation .pfe-navigation__menu-link:focus:after,pfe-navigation .pfe-navigation__secondary-link:focus:after{border:1px dashed;bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation .pfe-navigation__fallback-links a pfe-icon,#pfe-navigation .pfe-navigation__log-in-link pfe-icon,#pfe-navigation .pfe-navigation__menu-link pfe-icon,#pfe-navigation .pfe-navigation__secondary-link pfe-icon,pfe-navigation .pfe-navigation__fallback-links a pfe-icon,pfe-navigation .pfe-navigation__log-in-link pfe-icon,pfe-navigation .pfe-navigation__menu-link pfe-icon,pfe-navigation .pfe-navigation__secondary-link pfe-icon{pointer-events:none}#pfe-navigation .pfe-navigation__fallback-links a .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__fallback-links a>pfe-icon,#pfe-navigation .pfe-navigation__log-in-link .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__log-in-link>pfe-icon,#pfe-navigation .pfe-navigation__menu-link .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__menu-link>pfe-icon,#pfe-navigation .pfe-navigation__secondary-link .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__secondary-link>pfe-icon,pfe-navigation .pfe-navigation__fallback-links a .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__fallback-links a>pfe-icon,pfe-navigation .pfe-navigation__log-in-link .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__log-in-link>pfe-icon,pfe-navigation .pfe-navigation__menu-link .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__menu-link>pfe-icon,pfe-navigation .pfe-navigation__secondary-link .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__secondary-link>pfe-icon{--pfe-icon--size:18px;padding-right:5px}@media (min-width:768px){#pfe-navigation .pfe-navigation__fallback-links a .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__fallback-links a>pfe-icon,#pfe-navigation .pfe-navigation__log-in-link .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__log-in-link>pfe-icon,#pfe-navigation .pfe-navigation__menu-link .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__menu-link>pfe-icon,#pfe-navigation .pfe-navigation__secondary-link .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__secondary-link>pfe-icon,pfe-navigation .pfe-navigation__fallback-links a .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__fallback-links a>pfe-icon,pfe-navigation .pfe-navigation__log-in-link .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__log-in-link>pfe-icon,pfe-navigation .pfe-navigation__menu-link .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__menu-link>pfe-icon,pfe-navigation .pfe-navigation__secondary-link .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__secondary-link>pfe-icon{padding-right:0;padding:2px 0 4px}}.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__fallback-links a .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__fallback-links a>pfe-icon,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__log-in-link .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__log-in-link>pfe-icon,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__menu-link .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__menu-link>pfe-icon,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__secondary-link .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__secondary-link>pfe-icon,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__fallback-links a .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__fallback-links a>pfe-icon,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__log-in-link .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__log-in-link>pfe-icon,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__menu-link .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__menu-link>pfe-icon,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__secondary-link .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__secondary-link>pfe-icon{padding:0 16px 0 0}#pfe-navigation .pfe-navigation__fallback-links a pfe-icon,#pfe-navigation .pfe-navigation__log-in-link pfe-icon,#pfe-navigation .pfe-navigation__menu-link pfe-icon,#pfe-navigation .pfe-navigation__secondary-link pfe-icon,pfe-navigation .pfe-navigation__fallback-links a pfe-icon,pfe-navigation .pfe-navigation__log-in-link pfe-icon,pfe-navigation .pfe-navigation__menu-link pfe-icon,pfe-navigation .pfe-navigation__secondary-link pfe-icon{display:block;height:18px}#pfe-navigation .pfe-navigation__fallback-links a[class],#pfe-navigation .pfe-navigation__fallback-links a[href],#pfe-navigation .pfe-navigation__log-in-link[class],#pfe-navigation .pfe-navigation__log-in-link[href],#pfe-navigation .pfe-navigation__menu-link[class],#pfe-navigation .pfe-navigation__menu-link[href],#pfe-navigation .pfe-navigation__secondary-link[class],#pfe-navigation .pfe-navigation__secondary-link[href],pfe-navigation .pfe-navigation__fallback-links a[class],pfe-navigation .pfe-navigation__fallback-links a[href],pfe-navigation .pfe-navigation__log-in-link[class],pfe-navigation .pfe-navigation__log-in-link[href],pfe-navigation .pfe-navigation__menu-link[class],pfe-navigation .pfe-navigation__menu-link[href],pfe-navigation .pfe-navigation__secondary-link[class],pfe-navigation .pfe-navigation__secondary-link[href]{align-items:center;justify-content:center}#pfe-navigation .pfe-navigation__account-toggle,#pfe-navigation [slot=account]>a[href],pfe-navigation .pfe-navigation__account-toggle,pfe-navigation [slot=account]>a[href]{--pfe-icon--color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));align-items:center;-webkit-appearance:none;-moz-appearance:none;appearance:none;background:0 0;border:0;color:#06c;color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));cursor:pointer;font-family:inherit;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);justify-content:flex-start;margin:0;outline:0;position:relative;text-align:center;text-decoration:none;white-space:nowrap;width:100%;--pfe-icon--color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));color:#fff;color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));display:flex;flex-direction:column;font-size:12px;font-size:var(--pfe-navigation--FontSize--xs,12px);height:72px;height:var(--pfe-navigation__nav-bar--Height,72px);justify-content:flex-end;padding:14px 8px;width:auto}@media print{#pfe-navigation .pfe-navigation__account-toggle,#pfe-navigation [slot=account]>a[href],pfe-navigation .pfe-navigation__account-toggle,pfe-navigation [slot=account]>a[href]{display:none!important}}#pfe-navigation .pfe-navigation__account-toggle:focus,#pfe-navigation .pfe-navigation__account-toggle:hover,#pfe-navigation [slot=account]>a[href]:focus,#pfe-navigation [slot=account]>a[href]:hover,pfe-navigation .pfe-navigation__account-toggle:focus,pfe-navigation .pfe-navigation__account-toggle:hover,pfe-navigation [slot=account]>a[href]:focus,pfe-navigation [slot=account]>a[href]:hover{box-shadow:inset 4px 0 0 0 #06c;box-shadow:inset 4px 0 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}#pfe-navigation .pfe-navigation__account-toggle:focus,#pfe-navigation [slot=account]>a[href]:focus,pfe-navigation .pfe-navigation__account-toggle:focus,pfe-navigation [slot=account]>a[href]:focus{outline:0}#pfe-navigation .pfe-navigation__account-toggle:focus:after,#pfe-navigation [slot=account]>a[href]:focus:after,pfe-navigation .pfe-navigation__account-toggle:focus:after,pfe-navigation [slot=account]>a[href]:focus:after{border:1px dashed;bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation .pfe-navigation__account-toggle pfe-icon,#pfe-navigation [slot=account]>a[href] pfe-icon,pfe-navigation .pfe-navigation__account-toggle pfe-icon,pfe-navigation [slot=account]>a[href] pfe-icon{pointer-events:none}@supports (display:grid){#pfe-navigation .pfe-navigation__account-toggle,#pfe-navigation [slot=account]>a[href],pfe-navigation .pfe-navigation__account-toggle,pfe-navigation [slot=account]>a[href]{align-items:center;display:grid;grid-template-rows:26px 18px;justify-items:center}}#pfe-navigation .pfe-navigation__account-toggle[class]:focus,#pfe-navigation .pfe-navigation__account-toggle[class]:hover,#pfe-navigation [slot=account]>a[href][class]:focus,#pfe-navigation [slot=account]>a[href][class]:hover,pfe-navigation .pfe-navigation__account-toggle[class]:focus,pfe-navigation .pfe-navigation__account-toggle[class]:hover,pfe-navigation [slot=account]>a[href][class]:focus,pfe-navigation [slot=account]>a[href][class]:hover{box-shadow:inset 0 4px 0 0 #06c;box-shadow:inset 0 4px 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}@media print{#pfe-navigation .pfe-navigation__account-toggle,#pfe-navigation [slot=account]>a[href],pfe-navigation .pfe-navigation__account-toggle,pfe-navigation [slot=account]>a[href]{display:none}}#pfe-navigation .pfe-navigation__account-toggle pfe-icon,#pfe-navigation [slot=account]>a[href] pfe-icon,pfe-navigation .pfe-navigation__account-toggle pfe-icon,pfe-navigation [slot=account]>a[href] pfe-icon{--pfe-icon--size:18px;padding:2px 0 4px}@media (min-width:768px){#pfe-navigation .pfe-navigation__account-toggle pfe-icon,#pfe-navigation [slot=account]>a[href] pfe-icon,pfe-navigation .pfe-navigation__account-toggle pfe-icon,pfe-navigation [slot=account]>a[href] pfe-icon{padding-right:0}}#pfe-navigation .pfe-navigation__account-toggle:focus,#pfe-navigation .pfe-navigation__account-toggle:hover,#pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true],#pfe-navigation [slot=account]>a[href][href]:focus,#pfe-navigation [slot=account]>a[href][href]:hover,pfe-navigation .pfe-navigation__account-toggle:focus,pfe-navigation .pfe-navigation__account-toggle:hover,pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true],pfe-navigation [slot=account]>a[href][href]:focus,pfe-navigation [slot=account]>a[href][href]:hover{box-shadow:inset 0 4px 0 0 #06c;box-shadow:inset 0 4px 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}#pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true],pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true]{--pfe-icon--color:var(--pfe-navigation__nav-bar--Color--active,var(--pfe-theme--color--text,#151515));background:#fff;background:var(--pfe-navigation__nav-bar--toggle--BackgroundColor--active,var(--pfe-theme--color--surface--lightest,#fff));color:#151515;color:var(--pfe-navigation__nav-bar--Color--active,var(--pfe-theme--color--text,#151515))}#pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true]:focus,pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true]:focus{outline:0}#pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true]:focus:after,pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true]:focus:after{border:1px dashed #151515;border:1px dashed var(--pfe-navigation__nav-bar--Color--active,var(--pfe-theme--color--text,#151515));bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation .pfe-navigation__fallback-links,#pfe-navigation .pfe-navigation__menu,pfe-navigation .pfe-navigation__fallback-links,pfe-navigation .pfe-navigation__menu{font-size:inherit;list-style:none;margin:0;padding:0}@media (min-width:768px){#pfe-navigation .pfe-navigation__fallback-links,#pfe-navigation .pfe-navigation__menu,pfe-navigation .pfe-navigation__fallback-links,pfe-navigation .pfe-navigation__menu{align-items:stretch;display:flex}}#pfe-navigation .pfe-navigation__fallback-links li,#pfe-navigation .pfe-navigation__menu li,pfe-navigation .pfe-navigation__fallback-links li,pfe-navigation .pfe-navigation__menu li{font-size:inherit;margin:0;padding:0}#pfe-navigation .pfe-navigation__fallback-links li:before,#pfe-navigation .pfe-navigation__menu li:before,pfe-navigation .pfe-navigation__fallback-links li:before,pfe-navigation .pfe-navigation__menu li:before{content:none}#pfe-navigation .pfe-navigation__fallback-links,pfe-navigation .pfe-navigation__fallback-links{margin-left:auto}#pfe-navigation .pfe-navigation__menu-link,pfe-navigation .pfe-navigation__menu-link{display:flex;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);white-space:nowrap}#pfe-navigation.pfe-navigation--processed,pfe-navigation.pfe-navigation--processed{display:block;padding:0}#pfe-navigation.pfe-navigation--processed:before,pfe-navigation.pfe-navigation--processed:before{content:none}#pfe-navigation.pfe-navigation--processed>[slot=account],#pfe-navigation.pfe-navigation--processed>[slot=search],#pfe-navigation.pfe-navigation--processed>[slot=secondary-links],pfe-navigation.pfe-navigation--processed>[slot=account],pfe-navigation.pfe-navigation--processed>[slot=search],pfe-navigation.pfe-navigation--processed>[slot=secondary-links]{height:auto;overflow:visible;visibility:visible;width:auto}#pfe-navigation.pfe-navigation--processed pfe-navigation-dropdown,pfe-navigation.pfe-navigation--processed pfe-navigation-dropdown{display:block}#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown,#pfe-navigation.pfe-navigation--processed pfe-navigation-dropdown,#pfe-navigation.pfe-navigation--processed>[slot],pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown,pfe-navigation.pfe-navigation--processed pfe-navigation-dropdown,pfe-navigation.pfe-navigation--processed>[slot]{animation:none;opacity:1}#pfe-navigation.pfe-navigation--processed [slot=secondary-links],pfe-navigation.pfe-navigation--processed [slot=secondary-links]{display:block;height:auto;list-style:none;margin:0 0 8px;padding:0;width:auto}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links],pfe-navigation.pfe-navigation--processed [slot=secondary-links]{margin:0}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links],.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]{margin:0 0 8px}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button{--pfe-icon--color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));align-items:center;-webkit-appearance:none;-moz-appearance:none;appearance:none;background:0 0;border:0;color:#06c;color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));cursor:pointer;display:flex;font-family:inherit;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);justify-content:flex-start;margin:0;outline:0;padding:8px 24px;position:relative;text-align:center;text-decoration:none;white-space:nowrap;width:100%}@media print{#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button{display:none!important}}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button{--pfe-icon--color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));color:#fff;color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));display:flex;flex-direction:column;font-size:12px;font-size:var(--pfe-navigation--FontSize--xs,12px);height:72px;height:var(--pfe-navigation__nav-bar--Height,72px);justify-content:flex-end;padding:14px 8px;width:auto}@supports (display:grid){#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button{align-items:center;display:grid;grid-template-rows:26px 18px;justify-items:center}}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[class]:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[class]:hover,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[class]:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[class]:hover,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[class]:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[class]:hover,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[class]:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[class]:hover{box-shadow:inset 0 4px 0 0 #06c;box-shadow:inset 0 4px 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:hover,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:hover,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:hover,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:hover{box-shadow:inset 4px 0 0 0 #06c;box-shadow:inset 4px 0 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:hover,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:hover,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:hover,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:hover{box-shadow:inset 0 4px 0 0 #06c;box-shadow:inset 0 4px 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:hover,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:hover,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:hover,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:hover{box-shadow:inset 4px 0 0 0 #06c;box-shadow:inset 4px 0 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus{outline:0}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus:after,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus:after,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus:after,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus:after{border:1px dashed;bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a pfe-icon,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button pfe-icon{pointer-events:none}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a .secondary-link__icon-wrapper,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a>pfe-icon,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button .secondary-link__icon-wrapper,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button>pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a .secondary-link__icon-wrapper,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a>pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button .secondary-link__icon-wrapper,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button>pfe-icon{--pfe-icon--size:18px;padding-right:5px}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a .secondary-link__icon-wrapper,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a>pfe-icon,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button .secondary-link__icon-wrapper,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button>pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a .secondary-link__icon-wrapper,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a>pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button .secondary-link__icon-wrapper,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button>pfe-icon{padding-right:0;padding:2px 0 4px}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a>pfe-icon,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button>pfe-icon,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a>pfe-icon,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button>pfe-icon{padding:0 16px 0 0}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a pfe-icon,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button pfe-icon{display:block;height:18px}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus{outline:0}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus:after,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus:after,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus:after,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus:after{border:1px dashed #fff;border:1px dashed var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus{box-shadow:none}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[aria-expanded=true],#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[aria-expanded=true],pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[aria-expanded=true],pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[aria-expanded=true]{--pfe-icon--color:var(--pfe-navigation__nav-bar--Color--active,var(--pfe-theme--color--text,#151515));background:#fff;background:var(--pfe-navigation__nav-bar--toggle--BackgroundColor--active,var(--pfe-theme--color--surface--lightest,#fff));color:#151515;color:var(--pfe-navigation__nav-bar--Color--active,var(--pfe-theme--color--text,#151515))}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[aria-expanded=true],.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[aria-expanded=true],.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[aria-expanded=true],.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[aria-expanded=true]{background:0 0;box-shadow:none}#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown__wrapper--single-column,pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown__wrapper--single-column{position:relative}#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown__wrapper,pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown__wrapper{display:block}#pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class],pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class]{height:0;transition:height .25s ease-in-out;transition:var(--pfe-navigation--accordion-transition,height .25s ease-in-out)}@media (prefers-reduced-motion){#pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class],pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class]{transition:none}}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class],pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class]{position:absolute;right:0;top:72px;top:var(--pfe-navigation__nav-bar--Height,72px)}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class],.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class]{position:static}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class][aria-hidden=false],pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class][aria-hidden=false]{height:auto}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class][aria-hidden=false],.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class][aria-hidden=false]{height:0}#pfe-navigation.pfe-navigation--processed[breakpoint=mobile] [slot=secondary-links][mobile-slider] .pfe-navigation__dropdown-wrapper,pfe-navigation.pfe-navigation--processed[breakpoint=mobile] [slot=secondary-links][mobile-slider] .pfe-navigation__dropdown-wrapper{left:100vw;left:calc(100vw - 32px);left:calc(100vw - var(--pfe-navigation__mobile-dropdown--PaddingHorizontal,32px));position:absolute;top:0;width:100vw}#pfe-navigation.pfe-navigation--processed[breakpoint=mobile] [slot=secondary-links][mobile-slider] .pfe-navigation__dropdown-wrapper[aria-hidden=false],pfe-navigation.pfe-navigation--processed[breakpoint=mobile] [slot=secondary-links][mobile-slider] .pfe-navigation__dropdown-wrapper[aria-hidden=false]{height:100vh;height:calc(100vh - 72px);height:calc(100vh - var(--pfe-navigation__nav-bar--Height,72px));overflow-y:scroll}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles,#pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown-wrapper,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles,pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown-wrapper{background:#fff;background:var(--pfe-navigation__dropdown--Background,var(--pfe-theme--color--surface--lightest,#fff));padding:0 24px;padding:0 var(--pfe-navigation__dropdown--full-width--spacing--mobile,24px)}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles,#pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown-wrapper,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles,pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown-wrapper{padding:0 64px24px;padding:0 var(--pfe-navigation__dropdown--full-width--spacing--desktop,64px) var(--pfe-navigation__dropdown--full-width--spacing--mobile,24px)}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown-wrapper{padding:0 24px;padding:0 var(--pfe-navigation__dropdown--full-width--spacing--mobile,24px)}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a{border:1px solid transparent;color:#06c;color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));display:inline-block}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a:hover{color:#036;color:var(--pfe-navigation__dropdown--link--Color--hover,#036);text-decoration:underline}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a:focus{border:1px dashed;outline:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level],#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level],#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level],pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level],pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6{margin:32px 0 .75em;margin:var(--pfe-navigation--gutter,32px) 0 .75em;padding:0;-moz-column-break-inside:avoid;break-inside:avoid;color:#464646;color:var(--pfe-navigation__dropdown--headings--Color,#464646);font-family:Red Hat Display,RedHatDisplay,Arial,Helvetica,sans-serif;font-family:var(--pfe-navigation--FontFamilyHeadline,Red Hat Display,RedHatDisplay,Arial,Helvetica,sans-serif);font-size:1.125rem;font-size:var(--pf-global--FontSize--lg,1.125rem);font-weight:500}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level]:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level]:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level]:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level]:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6:first-child{margin-top:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a{border:1px solid transparent;color:#464646;color:var(--pfe-navigation__dropdown--headings--Color,#464646);text-decoration:underline}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a:hover{color:#036;color:var(--pfe-navigation__dropdown--link--Color--hover,#036);text-decoration:none}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a:focus{border:1px dashed;outline:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container li,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles li,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container li,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles li{margin:0 0 16px;-moz-column-break-inside:avoid;break-inside:avoid}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-card,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-card,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-card,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-card,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta{-moz-column-break-inside:avoid;break-inside:avoid}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[pfe-priority=primary],#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[priority=primary],#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[pfe-priority=primary],#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[priority=primary],pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[pfe-priority=primary],pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[priority=primary],pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[pfe-priority=primary],pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[priority=primary]{--pfe-cta--BackgroundColor:var(--pfe-navigation__dropdown--pfe-cta--BackgroundColor,#e00);--pfe-cta--BackgroundColor--hover:var(--pfe-navigation__dropdown--pfe-cta--hover--BackgroundColor,#c00);--pfe-theme--ui--border-width:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[pfe-priority=primary]:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[pfe-priority=primary]:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[priority=primary]:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[priority=primary]:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[pfe-priority=primary]:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[pfe-priority=primary]:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[priority=primary]:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[priority=primary]:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[pfe-priority=primary]:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[pfe-priority=primary]:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[priority=primary]:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[priority=primary]:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[pfe-priority=primary]:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[pfe-priority=primary]:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[priority=primary]:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[priority=primary]:hover{--pfe-cta--BackgroundColor:var(--pfe-navigation__dropdown--pfe-cta--hover--BackgroundColor,#c00)}pfe-card #pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta,pfe-card #pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta,pfe-card pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta,pfe-card pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta{margin-top:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container li,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container ul,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles li,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles ul,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container li,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container ul,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles li,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles ul{list-style:none;margin:0;padding:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles,pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles{color:#151515;color:var(--pfe-navigation__dropdown--Color,#151515);-moz-column-count:auto;column-count:auto;display:block;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);gap:0;margin-left:auto;margin-right:auto;max-width:1136px;max-width:var(--pfe-navigation--content-max-width,1136px);padding-bottom:12px;padding-top:12px;width:calc(100% + 32px)}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles,pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles{-moz-column-count:3;column-count:3;display:block;gap:32px;gap:var(--pfe-navigation--gutter,32px);padding-bottom:12px;padding-top:12px}}@media (min-width:1200px){#pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles,pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles{-moz-column-count:auto;column-count:auto;display:flex;flex-wrap:wrap;padding-bottom:32px;padding-top:32px}@supports (display:grid){#pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles,pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles{display:grid;gap:32px;gap:var(--pfe-navigation--gutter,32px);grid-auto-flow:row;grid-template-columns:repeat(4,minmax(0,1fr))}}}.pfe-navigation--collapse-main-menu #pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,.pfe-navigation--collapse-main-menu #pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles,.pfe-navigation--collapse-main-menu pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,.pfe-navigation--collapse-main-menu pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles{-moz-column-count:3;column-count:3;display:block;gap:32px;gap:var(--pfe-navigation--gutter,32px);padding-bottom:12px;padding-top:12px}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles{-moz-column-count:auto;column-count:auto;display:block;gap:0;margin-left:-16px;margin-right:-16px;max-width:1136px;max-width:var(--pfe-navigation--content-max-width,1136px);padding-bottom:12px;padding-top:12px;width:calc(100% + 32px)}.pfe-navigation__menu-item--open #pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,.pfe-navigation__menu-item--open #pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles,.pfe-navigation__menu-item--open pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,.pfe-navigation__menu-item--open pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles{transition-delay:0s;visibility:visible}#pfe-navigation.pfe-navigation--processed .pfe-navigation-grid>*,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles>*,pfe-navigation.pfe-navigation--processed .pfe-navigation-grid>*,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles>*{margin:0 0 18px;-moz-column-break-inside:avoid;break-inside:avoid}@media (min-width:1200px){#pfe-navigation.pfe-navigation--processed .pfe-navigation-grid>*,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles>*,pfe-navigation.pfe-navigation--processed .pfe-navigation-grid>*,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles>*{margin:0}}.pfe-navigation--collapse-main-menu #pfe-navigation.pfe-navigation--processed .pfe-navigation-grid>*,.pfe-navigation--collapse-main-menu #pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles>*,.pfe-navigation--collapse-main-menu pfe-navigation.pfe-navigation--processed .pfe-navigation-grid>*,.pfe-navigation--collapse-main-menu pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles>*{margin:0 0 18px}#pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,pfe-navigation.pfe-navigation--processed .pfe-navigation-grid{max-width:100%}#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--1-x,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--1-x{display:block}#pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown,pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown{background:#fff}#pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher,pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher{margin-left:auto;margin-right:auto;max-width:1136px;max-width:var(--pfe-navigation--content-max-width,1136px);padding:12px 24px;padding:12px var(--pfe-navigation__dropdown--full-width--spacing--mobile,24px)}@media (min-width:1200px){#pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher,pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher{padding:32px 64px;padding:32px var(--pfe-navigation__dropdown--full-width--spacing--desktop,64px)}}.pfe-navigation--collapse-main-menu #pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher,.pfe-navigation--collapse-main-menu pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher{padding:12px 24px;padding:12px var(--pfe-navigation__dropdown--full-width--spacing--mobile,24px)}#pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher .container,pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher .container{margin:0;padding:0;width:auto}#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--invisible[class],pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--invisible[class]{padding:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--invisible pfe-navigation-dropdown,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--invisible pfe-navigation-dropdown{visibility:hidden}#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class],pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class]{padding:0}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class],pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class]{box-shadow:0 1px 2px rgba(0,0,0,.12);box-shadow:var(--pfe-navigation__dropdown--BoxShadow,0 1px 2px rgba(0,0,0,.12));max-width:100%;min-width:13em;padding:0 32px;position:absolute;top:100%}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class],.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class]{box-shadow:none;max-width:100%;position:static}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class],pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class]{right:0}}#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class],pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class]{width:100%}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class],pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class]{left:0;position:absolute;right:0}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class],.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class]{position:static}#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class] .pfe-navigation__dropdown,pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class] .pfe-navigation__dropdown{width:100%}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles{padding-left:16px;padding-right:16px}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles form,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles form{align-items:center;display:flex}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button,#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input{padding:10px;transition:box-shadow .2s}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input{border:1px solid #f0f0f0;border-bottom-color:#8b8e91;color:#717579;flex-basis:0%;flex-grow:1;flex-shrink:1;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);margin-right:8px}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input::-moz-placeholder,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input::-moz-placeholder{color:#717579}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input::placeholder,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input::placeholder{color:#717579}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button{background-color:#e00;border:1px solid #e00;border-radius:2px;color:#fff;flex-basis:auto;flex-grow:0;flex-shrink:1;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem)}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:hover{outline:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:focus:after,#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:hover:after,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:focus:after,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:hover:after{border:1px dashed #000;bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:hover{outline:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:focus:after,#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:hover:after,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:focus:after,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:hover:after{border:1px dashed #fff;bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation .pfe-navigation__site-switcher__back-wrapper,pfe-navigation .pfe-navigation__site-switcher__back-wrapper{border-bottom:1px solid #d2d2d2;border-bottom:var(--pfe-navigation__dropdown--separator--Border,1px solid var(--pfe-theme--color--ui--border--lighter,#d2d2d2));display:block}@media (min-width:768px){#pfe-navigation .pfe-navigation__site-switcher__back-wrapper,pfe-navigation .pfe-navigation__site-switcher__back-wrapper{display:none}}.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__site-switcher__back-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__site-switcher__back-wrapper{display:block}#pfe-navigation .pfe-navigation__site-switcher__back-button,pfe-navigation .pfe-navigation__site-switcher__back-button{background-color:transparent;border:1px solid transparent;color:#06c;color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));cursor:pointer;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);padding:21px 21px 21px 45px;position:relative;text-align:left;width:100%}#pfe-navigation .pfe-navigation__site-switcher__back-button:before,pfe-navigation .pfe-navigation__site-switcher__back-button:before{border:2px solid #06c;border:2px solid var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));border-right:0;border-top:0;content:"";display:block;height:8px;left:35px;position:absolute;right:auto;top:27px;transform:rotate(45deg);transform-origin:left top;width:8px}#pfe-navigation .pfe-navigation__site-switcher__back-button:focus,#pfe-navigation .pfe-navigation__site-switcher__back-button:hover,pfe-navigation .pfe-navigation__site-switcher__back-button:focus,pfe-navigation .pfe-navigation__site-switcher__back-button:hover{border:1px dashed #151515;border-top:1px dashed #151515;border:1px dashed var(--pfe-navigation__dropdown--Color,#151515);color:#036;color:var(--pfe-navigation__dropdown--link--Color--hover,#036);outline:0}#pfe-navigation.pfe-navigation--processed site-switcher,pfe-navigation.pfe-navigation--processed site-switcher{-moz-columns:auto;columns:auto;display:block}#pfe-navigation.pfe-navigation--stuck,pfe-navigation.pfe-navigation--stuck{left:0;position:fixed;top:0;width:100%;z-index:95;z-index:var(--pfe-theme--zindex--navigation,95)}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__outer-menu-wrapper__inner,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__outer-menu-wrapper__inner{opacity:1!important}#pfe-navigation.pfe-navigation--in-crusty-browser pfe-navigation-account,#pfe-navigation.pfe-navigation--in-crusty-browser rh-account-dropdown,pfe-navigation.pfe-navigation--in-crusty-browser pfe-navigation-account,pfe-navigation.pfe-navigation--in-crusty-browser rh-account-dropdown{display:none!important}#pfe-navigation.pfe-navigation--in-crusty-browser[open-toggle=pfe-navigation__account-toggle] pfe-navigation-account,#pfe-navigation.pfe-navigation--in-crusty-browser[open-toggle=pfe-navigation__account-toggle] rh-account-dropdown,pfe-navigation.pfe-navigation--in-crusty-browser[open-toggle=pfe-navigation__account-toggle] pfe-navigation-account,pfe-navigation.pfe-navigation--in-crusty-browser[open-toggle=pfe-navigation__account-toggle] rh-account-dropdown{display:block!important}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-item,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-item{display:block}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-link[aria-expanded=true],pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-link[aria-expanded=true]{--pfe-icon--color:var(--pfe-navigation__nav-bar--Color--active,var(--pfe-theme--color--text,#151515));background:#fff;background:var(--pfe-navigation__nav-bar--toggle--BackgroundColor--active,var(--pfe-theme--color--surface--lightest,#fff));color:#151515;color:var(--pfe-navigation__nav-bar--Color--active,var(--pfe-theme--color--text,#151515))}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-link[aria-expanded=true]:focus,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-link[aria-expanded=true]:focus{outline:0}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-link[aria-expanded=true]:focus:after,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-link[aria-expanded=true]:focus:after{border:1px dashed;bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown{display:flex;flex-wrap:wrap}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown>.style-scope,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown>.style-scope{flex-basis:25%}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown .pfe-navigation__footer.style-scope,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown .pfe-navigation__footer.style-scope{flex-basis:100%}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown .pfe-navigation__footer.style-scope>.style-scope,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown .pfe-navigation__footer.style-scope>.style-scope{margin-right:16px}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column,#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column ul,#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column>.style-scope,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column ul,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column>.style-scope{display:flex;flex-direction:column;flex-wrap:nowrap}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column>.style-scope,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column>.style-scope{flex-basis:auto}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle,#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link,#pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link,pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a{color:#fff!important;justify-content:center!important}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle[aria-expanded=true],#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link[aria-expanded=true],#pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a[aria-expanded=true],pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle[aria-expanded=true],pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link[aria-expanded=true],pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a[aria-expanded=true]{color:#151515!important}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__account-wrapper--logged-in .pfe-navigation__log-in-link,#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle .secondary-link__icon-wrapper,#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle pfe-icon,#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link .secondary-link__icon-wrapper,#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link pfe-icon,#pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a .secondary-link__icon-wrapper,#pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a pfe-icon,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__account-wrapper--logged-in .pfe-navigation__log-in-link,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle .secondary-link__icon-wrapper,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle pfe-icon,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link .secondary-link__icon-wrapper,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link pfe-icon,pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a .secondary-link__icon-wrapper,pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a pfe-icon{display:none!important}[id=pfe-navigation__account-dropdown][class][class]{display:block;height:auto;left:0;padding:0;position:absolute;top:72px;top:var(--pfe-navigation__nav-bar--Height,72px);width:100%}[id=pfe-navigation__account-dropdown].pfe-navigation__dropdown-wrapper--invisible[class]{display:none}.pfe-navigation__dropdown-wrapper{overflow:hidden}@media (min-width:768px){.pfe-navigation__custom-dropdown--single-column{min-width:25em}}.pfe-navigation--collapse-secondary-links .pfe-navigation__custom-dropdown--single-column{min-width:0}.secondary-link__icon-wrapper{align-items:center;display:flex;justify-content:center}.secondary-link__alert-count{background:#06c;background:var(--pfe-navigation__nav-bar--alert-color,var(--pfe-theme--color--link,#06c));border-radius:20px;color:#fff;color:var(--pfe-navigation__nav-bar--Color--on-highlight,var(--pfe-theme--color--text--on-saturated,#fff));display:block;font-size:12px;font-size:var(--pfe-navigation--FontSize--xs,12px);line-height:20px;margin:0 4px 0 2px;min-width:23px;overflow:hidden;padding:0 8px}.secondary-link__alert-count:empty{display:none}#pfe-navigation__1x-skip-links{left:0;position:absolute;top:0}#pfe-navigation__1x-skip-links,#pfe-navigation__1x-skip-links li{height:0;list-style:none;margin:0;padding:0;width:0}.skip-link[class][class]{font-size:.875rem;font-size:var(--pf-global--FontSize--sm,.875rem);line-height:18px}.skip-link[class][class]:focus{border-radius:.21429em;height:auto;left:50%;padding:.42857em .57143em;position:fixed;top:8px;transform:translateX(-50%);width:auto;z-index:99999;clip:auto;background:#fff;background:var(--pfe-navigation__skip-link--BackgroundColor,var(--pfe-theme--color--surface--lightest,#fff));color:#06c;color:var(--pfe-navigation__skip-link--Color,var(--pfe-theme--color--link,#06c));text-decoration:none}pfe-navigation pfe-navigation-account[slot=account]{background:#fff;background:var(--pfe-navigation__dropdown--Background,var(--pfe-theme--color--surface--lightest,#fff));width:100%}</style>
<style>:host([size=sm]) #container[data-v-8589d091]{--_size:var(--pf-global--icon--FontSize--sm,12px)}.content-wrapper[data-v-8589d091]{height:auto;margin:0 auto;min-height:46vh}.content[data-v-8589d091]{max-width:1000px}#left-content[data-v-8589d091]{max-width:330px;z-index:1}.line-below-chp[data-v-8589d091]{margin:var(--rh-space-xl,24px) 0 var(--rh-space-3xl,48px)}.toc-container[data-v-8589d091]{border-right:1px solid var(--rh-color-gray-30,#c7c7c7);min-height:100vh;position:sticky;top:0;transition:transform .3s ease-in-out}nav#toc[data-v-8589d091]{height:auto;overflow-y:auto;padding-bottom:var(--rh-space-2xl,32px)}.max-height-85[data-v-8589d091]{max-height:85vh}.max-height-75[data-v-8589d091]{max-height:75vh}.toc-filter[data-v-8589d091]{background-color:#fff;padding:var(--rh-space-lg,16px) var(--rh-space-2xl,32px);position:sticky;top:-1px;width:100%;z-index:1}#text[data-v-8589d091],.toc-filter[data-v-8589d091]{align-items:center;display:flex}#text[data-v-8589d091]{flex:1;flex-direction:row}#search-icon[data-v-8589d091]{color:#151515;left:2.5rem;position:absolute;top:55%;transform:translateY(-50%)}pf-icon[data-v-8589d091]{--pf-icon--size:16px}#text:focus-within #icon[data-v-8589d091],#text:hover #icon[data-v-8589d091]{color:#151515}#text[data-v-8589d091]:after,#text[data-v-8589d091]:before{content:"";inset:0;pointer-events:none;position:absolute}#text-input[data-v-8589d091]:focus,#text-input:focus+#utilities[data-v-8589d091]{border-bottom:2px solid #06c;outline:none}#text-input[data-v-8589d091]{background-color:transparent;border:1px solid #f0f0f0;border-bottom-color:#8a8d90;color:#151515;font-family:inherit;font-size:100%;grid-area:text-input;line-height:1.5;overflow:hidden;padding:.375rem .25rem .375rem 2rem;position:relative;text-overflow:ellipsis;white-space:nowrap;width:100%}#utilities[data-v-8589d091]{align-items:center;border:1px solid #f0f0f0;border-bottom:1px solid #8a8d90;border-left:0;display:flex}#utilities rh-badge[data-v-8589d091]{border-radius:80px;font-weight:var(--rh-font-weight-heading-medium,500);--_background-color:#e0e0e0;margin-right:8px}#clear-button[data-v-8589d091]{--pf-c-button--PaddingTop:0.625rem;--pf-c-button--PaddingRight:.25rem;--pf-c-button--PaddingBottom:0.625rem;--pf-c-button--PaddingLeft:.25rem;margin-right:8px}#text-input.no-right-border[data-v-8589d091]{border-right:0}.btn-container[data-v-8589d091]{bottom:0;display:flex;justify-content:flex-end;padding:1rem;pointer-events:none;position:fixed;right:0;z-index:2}.top-scroll-btn[data-v-8589d091]{--pf-c-button--BorderRadius:64px;pointer-events:all}.focusable[data-v-8589d091]:focus-visible{border:2px solid var(--rh-color-interactive-blue,#06c)}.mobile-nav-wrapper[data-v-8589d091]{align-items:center;border-bottom:1px solid #c7c7c7;display:flex;height:auto;justify-content:space-between;padding:var(--rh-space-sm,.5rem)}.mobile-nav[data-v-8589d091]{align-items:center;background-color:var(--rh-color-bg-page,#fff);min-height:51px;position:sticky;top:0;z-index:5}.active-mobile-menu[data-v-8589d091]{color:#151515;padding-left:.5rem}.hidden[data-v-8589d091]{display:none}.mobile-nav-btn[data-v-8589d091]{background-color:transparent;border:none;font-family:inherit;font-size:.875rem;font-weight:500;margin:0;min-height:40px;min-width:40px}.border-right[data-v-8589d091]{border-right:1px solid #c7c7c7}.toc-focus-container[data-v-8589d091]{position:sticky;top:0;z-index:2}.toc-focus-btn[data-v-8589d091]{align-items:center;background-color:var(--rh-color-white,#fff);border:1px solid var(--rh-color-blue-50,#06c);border-radius:50%;cursor:pointer;display:flex;height:40px;justify-content:center;position:absolute;right:-20px;top:15px;width:40px}.toc-focus-btn[data-v-8589d091]:focus-visible,.toc-focus-btn[data-v-8589d091]:hover{background-color:var(--rh-color-blue-10,#e0f0ff);box-shadow:var(--rh-box-shadow-sm,0 2px 4px 0 hsla(0,0%,8%,.2))}.toc-focus-btn[data-v-8589d091]:focus-visible{border:2px solid var(--rh-color-blue-50,#06c)}.toc-focus-btn-icon[data-v-8589d091]{color:var(--rh-color-blue-50,#06c)}.toc-wrapper[data-v-8589d091]{padding:0}.product-container[data-v-8589d091]{border-bottom:1px solid var(--rh-color-gray-30,#c7c7c7);padding:var(--rh-space-xl,24px) var(--rh-space-2xl,32px)}.product-container h1.product-title[data-v-8589d091]{font-size:var(--rh-font-size-code-xl,1.25rem);line-height:30px;margin-bottom:0;margin-top:0}.product-container .product-version[data-v-8589d091]{align-items:center;display:flex;flex-wrap:wrap;margin-top:var(--rh-space-lg,16px)}.product-version .version-label[data-v-8589d091]{color:var(--rh-color-canvas-black,#151515);font-family:Red Hat Text;font-size:var(--rh-font-size-body-text-sm,.875rem);font-weight:500}.product-version .version-select-dropdown[data-v-8589d091]{margin:var(--rh-space-md,8px) var(--rh-space-sm,6px);max-width:9.75rem;min-height:2rem;min-width:3rem;overflow:hidden;width:-moz-min-content;width:min-content;word-wrap:nowrap;-webkit-appearance:none;-moz-appearance:none;background:var(--rh-color-white,#fff);background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' width='10' height='6' fill='none' viewBox='0 0 10 6'%3E%3Cpath fill='%23151515' d='M.678 0h8.644c.596 0 .895.797.497 1.195l-4.372 4.58c-.298.3-.695.3-.993 0L.18 1.196C-.216.797.081 0 .678 0'/%3E%3C/svg%3E");background-position-x:85%;background-position-y:50%;background-repeat:no-repeat;border:1px solid var(--rh-color-gray-30,#c7c7c7);border-bottom:0;box-shadow:0 -1px 0 0 var(--rh-color-gray-60,#4d4d4d) inset;cursor:pointer;font-size:var(--rh-font-size-body-text-md,1rem);padding:var(--rh-space-md,8px);padding-right:24px;text-overflow:ellipsis}pf-popover[data-v-8589d091]{margin-top:var(--rh-space-md,8px);--pf-c-popover__arrow--BackgroundColor:var(--rh-color-canvas-black,#151515);--pf-c-popover__content--BackgroundColor:var(--rh-color-canvas-black,#151515);--pf-c-popover--BoxShadow:0px 4px 8px 0px #15151540;--pf-c-popover__title-text--Color:var(--rh-color-white,#fff);--pf-c-popover--MaxWidth:300px;--pf-c-popover--MinWidth:300px;--pf-c-popover--c-button--Top:20px;--pf-c-popover--c-button--Right:4px;--pf-c-button--m-plain--hover--Color:var(--rh-color-white,#fff)}pf-popover[data-v-8589d091]::part(content){padding:var(--rh-space-2xl,32px)}pf-popover[data-v-8589d091]::part(body){margin-top:var(--rh-space-lg,16px)}pf-popover[data-v-8589d091]::part(close-button){--pf-c-button--m-plain--focus--Color:var(--rh-color-gray-30,#c7c7c7);--pf-c-button--m-plain--Color:var(--rh-color-gray-30,#c7c7c7)}.popover-header-text[data-v-8589d091]{color:var(--rh-color-white,#fff);font-size:var(--rh-font-size-code-md,1rem);margin:0;max-width:80%;padding:0}.popover-body-link[data-v-8589d091]{color:var(--rh-color-blue-30,#92c5f9);text-decoration:none}.popover-trigger-btn[data-v-8589d091]{align-items:center;background:none;border:none;cursor:pointer;display:flex;justify-content:center}#first-button[data-v-8589d091]{width:80%}#second-button[data-v-8589d091]{text-align:right;width:20%}#toc-btn[data-v-8589d091]{text-align:left;width:100%}.toc-error[data-v-8589d091]{margin:0;max-width:100%}#layout label[data-v-8589d091]{font-weight:500}.page-layout-options[data-v-8589d091]{background-color:#fff;display:flex;flex-direction:column;padding-bottom:var(--rh-space-lg,16px)}.sticky-top[data-v-8589d091]{position:sticky;top:0}summary[data-v-8589d091]{cursor:pointer;list-style:none;position:relative}summary[data-v-8589d091]::-webkit-details-marker{display:none}details#jump-links-details .jump-links-heading[data-v-8589d091]{background-color:var(--rh-color-white,#fff);display:block;margin-top:var(--rh-space-lg,16px);padding:var(--rh-space-lg,16px) 0 0 var(--rh-space-xl,24px);position:sticky;top:0}details#jump-links-details .jump-links-heading[data-v-8589d091]:before{border-right:3px solid #151515;border-top:3px solid #151515;color:#151515;content:"";display:flex;height:9px;left:2px;position:absolute;top:26px;transform:rotate(-135deg);width:9px}details#jump-links-details[open] .jump-links-heading[data-v-8589d091]:before{transform:rotate(135deg)}.table-of-contents>ol[data-v-8589d091]{list-style:none;margin:0;padding:0}nav.table-of-contents[data-v-8589d091]{z-index:1}nav.table-of-contents ol[data-v-8589d091]{list-style:none;margin:0;padding:0}#mobile-browse-docs[data-v-8589d091]{font-weight:var(--rh-font-weight-body-text-medium,500);padding-left:var(--rh-space-2xl,32px)}.docs-content-container[data-v-8589d091]{font-size:var(--rh-font-size-body-text-lg,1.125rem);font-weight:var(--rh-font-weight-body-text-regular,400);line-height:1.6667;padding-left:6rem;padding-right:6rem;padding-top:var(--rh-space-3xl,4rem)}.chapter-title[data-v-8589d091]{font-family:Red Hat Display}h1.chapter-title[data-v-8589d091]{font-size:var(--rh-fontsize-heading-xl,2.25rem);line-height:46.8px;margin:0;padding:0}.chapter .section h4[data-v-8589d091]{font-size:24px;font-weight:400}.banner-wrapper[data-v-8589d091]{padding:3rem 6rem 0}.page-format-dropdown[data-v-8589d091]{-webkit-appearance:none;-moz-appearance:none;appearance:none;background:#fff;background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAABtSURBVHgBhc4xCoAwDAXQxE6li0K76w0cPZqjHsEb6AlcHb2BR9ADdG7GmIKTWv0QSOAFPjrndgAo4TtHxszTD4JoVAhh1VoXiNgkUO+971Q8iGgxxlSy1jc3CGof39baWTrzNSOkkksEbG/oBGEJIn6gD3jAAAAAAElFTkSuQmCC");background-position:8.5rem;background-repeat:no-repeat;background-size:auto;border:1px solid #c7c7c7;box-shadow:inset 0 -1px 0 0 #4d4d4d;font-family:Red Hat Text;font-size:var(--rh-font-size-body-text-md,1rem);margin-top:var(--rh-space-xs,4px);max-width:168px;min-height:36px;min-width:168px;overflow:hidden;padding:0 var(--rh-space-xl,24px) 0 var(--rh-space-md,8px);text-overflow:ellipsis;white-space:nowrap}.content-format-selectors[data-v-8589d091]{color:#151515;font-family:Red Hat Text;font-size:var(--rh-font-size-body-text-sm,.875rem);font-weight:var(--rh-font-weight-body-text-medium,500);margin-right:var(--rh-space-2xl,32px);max-width:250px;min-width:250px;padding-top:var(--rh-space-2xl,32px)}.chapter .section .simpara[data-v-8589d091],.chapter .section p[data-v-8589d091]{font-family:Red Hat Text;font-size:var(--rh-font-size-body-text-lg,1.125rem);font-weight:var(--rh-font-weight-body-text-regular,400);line-height:30px}#toggle-focus-mode[data-v-8589d091]{padding-right:var(--rh-space-lg,1rem)}@keyframes slideaway-left-8589d091{0%{display:block}to{opacity:0;transform:translateX(-40px)}}@keyframes slideaway-right-8589d091{0%{display:block}to{opacity:0;transform:translateX(40px)}}@keyframes enter-left-8589d091{0%{display:none;transform:translateX(-40px)}to{opacity:1}}@keyframes enter-right-8589d091{0%{display:none;transform:translateX(40px)}to{opacity:1}}.hide-left[data-v-8589d091]{animation:slideaway-left-8589d091 .2s;display:none}.enter-left[data-v-8589d091]{animation:enter-left-8589d091 .3s;border-right:none;display:block}.enter-right[data-v-8589d091]{animation:enter-right-8589d091 .3s;display:block}.hide-right[data-v-8589d091]{animation:slideaway-right-8589d091 .2s;display:none}.toc-container.enter-toc-container-left[data-v-8589d091]{transform:translateX(-85%)}.alert-section[data-v-8589d091]{padding-bottom:3rem}rh-alert[data-v-8589d091]{width:auto}@media (min-width:992px){#mobile-nav[data-v-8589d091],#toc-list-mobile[data-v-8589d091]{display:none}}@media (min-width:992px) and (max-width:1400px){.docs-ocp-content-container[data-v-8589d091]{padding:var(--rh-space-4xl,64px) var(--rh-space-lg,16px) 0}}@media (width < 992px){#breadcrumbs[data-v-8589d091],.content-format-selectors[data-v-8589d091],.toc-container[data-v-8589d091]{display:none}#mobile-nav-content-wrapper[data-v-8589d091]{border-bottom:1px solid #c7c7c7;border-top:1px solid #c7c7c7}#toc-wrapper-mobile[data-v-8589d091]{padding:var(--rh-space-md,1.5rem)}.product-container[data-v-8589d091]{padding:var(--rh-space-2xl,32px) var(--rh-space-lg,16px)}.product-container.shrink-product-padding[data-v-8589d091]{padding:var(--rh-space-lg,16px)}.product-version .version-select-dropdown[data-v-8589d091]{margin:0 var(--rh-space-lg,16px)}#page-content-options-mobile[data-v-8589d091]{padding:var(--rh-space-lg,2rem)}label[for=page-format][data-v-8589d091],label[for=toggle-focus-mode][data-v-8589d091]{display:block}.page-format-dropdown[data-v-8589d091]{background-position:97%;max-width:100%}nav#mobile-toc-menu[data-v-8589d091]{max-height:50vh;overflow-y:scroll}.mobile-jump-links #first-button[data-v-8589d091]{width:100%}#jump-links-btn[data-v-8589d091]{text-align:left;width:100%}#mobile-jump-links-content-wrapper[data-v-8589d091]{border-bottom:1px solid #c7c7c7;border-top:1px solid #c7c7c7;padding:0 var(--rh-space-lg,16px) var(--rh-space-2xl,32px)}.table-of-contents #browse-docs[data-v-8589d091]{margin-top:1rem;padding-top:var(--rh-space-md,1.5rem)}.mobile-nav[data-v-8589d091]{display:block}.hide-mobile-nav[data-v-8589d091],.mobile-nav[data-v-8589d091]{transition:transform .3s ease-in-out}.hide-mobile-nav[data-v-8589d091]{transform:translateY(-100%)}.docs-content-container[data-v-8589d091]{padding-left:1.25rem;padding-right:1.25rem;padding-top:var(--rh-space-xl,24px)}.banner-wrapper[data-v-8589d091]{padding:0 1rem}.toc-filter-mobile[data-v-8589d091]{align-items:center;display:flex;padding:var(--rh-space-lg,16px);width:100%}#text-input[data-v-8589d091]{padding-left:.5rem}.toc-filter[data-v-8589d091]{display:none}}@media (width <=576px){.content-format-selectors[data-v-8589d091]{display:none}}.informaltable[data-v-8589d091],.rhdocs .informaltable[data-v-8589d091],.rhdocs .table-contents[data-v-8589d091],.rhdocs .table-wrapper[data-v-8589d091],.table-contents[data-v-8589d091],.table-wrapper[data-v-8589d091]{max-height:var(--rh-table--maxHeight);overflow:auto}rh-table[data-v-8589d091]{display:block;margin:2rem 0;max-width:100%}.pvof-doc__wrapper[data-v-8589d091],.rhdocs[data-v-8589d091]{--rh-table--maxHeight:calc(100vh - 12.5rem)}</style>
<style>:is(rh-footer-block) a[data-v-97dd2752]{text-decoration:underline}</style>
<style>:is(rh-footer,:is(rh-footer-universal,rh-global-footer)) a{color:var(--rh-color-link-inline-on-dark,var(--rh-color-interactive-blue-lighter,#92c5f9));text-decoration:none}:is(rh-footer,:is(rh-footer-universal,rh-global-footer)) a:hover{color:var(--rh-color-link-inline-hover-on-dark,var(--rh-color-interactive-blue-lightest,#b9dafc));text-decoration:underline}:is(rh-footer,:is(rh-footer-universal,rh-global-footer)) a:is(:focus,:focus-within){color:var(--rh-color-link-inline-focus-on-dark,var(--rh-color-interactive-blue-lightest,#b9dafc));text-decoration:underline}:is(rh-footer,:is(rh-footer-universal,rh-global-footer)) a:visited{color:var(--rh-color-link-inline-visited-on-dark,var(--rh-color-interactive-blue-lightest,#b9dafc));text-decoration:none}:is(rh-footer,:is(rh-footer-universal,rh-global-footer)) a[slot^=logo]{display:block}:is(rh-footer) a[slot^=logo]>img{display:block;height:100%;height:var(--rh-size-icon-04,40px);width:auto}:is(rh-footer,rh-footer-universal,rh-global-footer) :is(h1,h2,h3,h4,h5,h6){font-family:var(--rh-font-family-heading,RedHatDisplay,"Red Hat Display","Noto Sans Arabic","Noto Sans Hebrew","Noto Sans JP","Noto Sans KR","Noto Sans Malayalam","Noto Sans SC","Noto Sans TC","Noto Sans Thai",Helvetica,Arial,sans-serif);line-height:var(--rh-line-height-heading,1.3)}rh-footer [slot=links]:is(h1,h2,h3,h4,h5):nth-of-type(n+5){--_link-header-margin:calc(var(--rh-space-2xl, 32px) - var(--rh-space-lg, 16px))}rh-footer [slot^=links] a{gap:var(--rh-footer-links-gap,var(--rh-space-md,8px))}:is(rh-footer,:is(rh-footer-universal,rh-global-footer)) [slot^=links] li{display:contents;margin:0;padding:0}:is(rh-footer,:is(rh-footer-universal,rh-global-footer)) [slot^=links] a{color:var(--rh-color-text-primary-on-dark,#fff)!important;display:block;font-size:var(--rh-footer-link-font-size,var(--rh-font-size-body-text-sm,.875rem));width:-moz-fit-content;width:fit-content}:is(rh-footer-universal,rh-global-footer) [slot^=links] a{font-size:inherit}:is(rh-footer,rh-footer-universal,rh-global-footer){--rh-footer-section-side-gap:var(--rh-space-lg,16px)}@media screen and (min-width:768px){:is(rh-footer,rh-footer-universal,rh-global-footer){--rh-footer-section-side-gap:var(--rh-space-2xl,32px)}}@media screen and (min-width:1440px){:is(rh-footer,rh-footer-universal,rh-global-footer){--rh-footer-section-side-gap:var(--rh-space-4xl,64px)}}rh-footer:not(:defined){background-color:var(--rh-color-surface-darker,#1f1f1f);display:grid;grid-template-areas:"footer" "global";grid-template-rows:1fr auto;min-height:var(--rh-footer-nojs-min-height,750px);width:100%}:is(rh-footer-universal,rh-global-footer):not(:defined):before{grid-area:global}rh-footer:not(:defined)>[slot=logo]{padding:var(--rh-space-2xl,32px) var(--_section-side-gap)}:is(rh-footer-universal,rh-global-footer):not(:defined)>*,rh-footer:not(:defined)>:not([slot=logo],:is(rh-footer-universal,rh-global-footer)){border:0;clip:rect(1px,1px,1px,1px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}:is(rh-footer-universal,rh-global-footer):not(:defined){background-color:var(--rh-color-surface-darkest,#151515);display:block;min-height:176px;width:100%}rh-footer-universal rh-footer-copyright{grid-column:-1/1}</style>
<style>.status-legal .status-page-widget[data-v-5f538988]{display:block;margin:.5rem 0;width:11.3125rem;width:-moz-max-content;width:max-content}.status-page-widget[data-v-5f538988]{align-items:center;display:flex;flex-direction:row}.status-legal .status-page-widget .status-description[data-v-5f538988]{color:#ccc;font-weight:600;letter-spacing:.0125rem;line-height:1.5;margin-right:.5rem}.status-good[data-v-5f538988]{background-color:#3e8536}.status-critical[data-v-5f538988]{background-color:#a30100}.status-partial[data-v-5f538988]{background-color:#f5c12d}.status-maintentance[data-v-5f538988]{background-color:#316dc1}.status-minor[data-v-5f538988]{background-color:#b85c00}.status-description[data-v-5f538988]{color:#ccc;font-weight:500;letter-spacing:.2px;line-height:1.5}.current-status-indicator[data-v-5f538988]{border-radius:6px;display:inline-block;height:12px;margin:0 0 0 5px;width:12px}.current-status-indicator.small[data-v-5f538988]{border-radius:4px;display:inline-block;height:8px;margin:0 0 0 5px;width:8px}</style>
<style>.breadcrumbs[data-v-798f280c]{align-items:center;background-color:#f6f6f6;display:flex;gap:var(--rh-space-xl,24px);padding:var(--rh-space-lg,16px) var(--rh-space-2xl,32px)}nav[data-v-798f280c]{flex:1}ol[data-v-798f280c]{background-color:#f6f6f6;font-size:.875rem;list-style:none;margin:0;padding:0}li[data-v-798f280c]{color:#151515;display:inline}a[data-v-798f280c]:after{border-bottom-color:transparent;border-left-color:transparent;box-shadow:inset .25rem .25rem 0 .0625rem #8a8d8d;content:"";display:inline-block;height:1.07143em;margin:0 .5em;position:relative;right:0;top:.75em;transform:translateY(-.5em) rotate(135deg) scale(.5);width:1.07143em}</style>
<style>ol[data-v-fa0dae77]{margin:0;padding:0}li[data-v-fa0dae77],ol[data-v-fa0dae77],ul[data-v-fa0dae77]{list-style:none;margin:0}.chapter-title[data-v-fa0dae77]{font-size:1em}.sub-chapter-title[data-v-fa0dae77],.sub-chapter-title a[data-v-fa0dae77]{font-size:.875rem}#toc .link[data-v-fa0dae77],#toc-mobile .link[data-v-fa0dae77],.heading[data-v-fa0dae77],.sub-nav .link[data-v-fa0dae77],.sub-nav .link .link[data-v-fa0dae77]{display:block;padding:var(--rh-space-md,8px) var(--rh-space-2xl,32px);padding-right:2.5em;text-decoration:none;transition:background-color .25s}.heading[data-v-fa0dae77]:hover,.link[data-v-fa0dae77]:hover,.sub-nav .link[data-v-fa0dae77]:hover{background:var(--rh-color-surface-lighter,#f2f2f2);box-shadow:inset 3px 0 0 0 #d2d2d2;color:#151515}details[open]:first-of-type>.heading[data-v-fa0dae77]:after{transform:rotate(135deg)}.item[data-v-fa0dae77]{line-height:22px}#toc .link[data-v-fa0dae77],#toc-mobile .link[data-v-fa0dae77]{color:var(--rh-color-text-primary-on-light,#151515)}.sub-nav[data-v-fa0dae77],.toc-wrapper[data-v-fa0dae77]{list-style:none;margin:0}.toc-wrapper[data-v-fa0dae77]{min-width:100%;padding:0}.sub-nav[data-v-fa0dae77]{font-size:1em;line-height:24px;padding-left:1rem;padding-left:16px}.sub-nav .link[data-v-fa0dae77]:hover{color:#151515}.active[data-v-fa0dae77]{background:var(--rh-color-surface-lighter,#f2f2f2);box-shadow:inset 3px 0 0 0 var(--rh-color-icon-primary-on-light,#e00)}.chapter-landing-page[data-v-fa0dae77]{font-weight:500}summary[data-v-fa0dae77]{cursor:pointer;list-style:none;position:relative}summary[data-v-fa0dae77]::-webkit-details-marker{display:none}@keyframes slideDown-fa0dae77{0%{height:0;opacity:0}to{height:var(--details-height-open,"100%");opacity:1}}html[data-v-fa0dae77]{--details-transition-time:400ms}details[data-v-fa0dae77]{max-height:var(--details-height-closed,auto);transition:all ease-out var(--details-transition-time,0)}details[open][data-v-fa0dae77]{max-height:var(--details-height-open,auto)}details .heading.sub-chapter-title[data-v-fa0dae77]:after,details .heading[data-v-fa0dae77]:after{border-right:3px solid #151515;border-top:3px solid #151515;color:#151515;content:"";display:flex;float:right;height:9px;margin-left:16px;position:absolute;right:var(--rh-space-xl,24px);top:14px;transform:rotate(45deg);width:9px}details .heading.sub-chapter-title[data-v-fa0dae77]:after{height:8px;width:8px}</style>
<style>.item[data-v-b883c74f]{line-height:22px}#toc .link[data-v-b883c74f],#toc-mobile .link[data-v-b883c74f]{color:var(--rh-color-text-primary-on-light,#151515)}#toc .link[data-v-b883c74f],#toc-mobile .link[data-v-b883c74f],.heading[data-v-b883c74f],.sub-nav .link[data-v-b883c74f],.sub-nav .link .link[data-v-b883c74f]{display:block;padding:var(--rh-space-md,8px) var(--rh-space-2xl,32px);padding-right:2.5em;text-decoration:none;transition:background-color .25s}.heading[data-v-b883c74f]:hover,.link[data-v-b883c74f]:hover,.sub-nav .link[data-v-b883c74f]:hover{background:var(--rh-color-surface-lighter,#f2f2f2);box-shadow:inset 3px 0 0 0 #d2d2d2;color:#151515}ol[data-v-b883c74f]{margin:0;padding:0}li[data-v-b883c74f],ol[data-v-b883c74f],ul[data-v-b883c74f]{list-style:none;margin:0}.chapter-title[data-v-b883c74f]{font-size:1em}.sub-nav[data-v-b883c74f]{font-size:1em;line-height:24px;list-style:none;margin:0;padding-left:1rem;padding-left:16px}.sub-nav .link[data-v-b883c74f]:hover{color:#151515}.active[data-v-b883c74f]{background:var(--rh-color-surface-lighter,#f2f2f2);box-shadow:inset 3px 0 0 0 var(--rh-color-icon-primary-on-light,#e00)}.sub-chapter-title[data-v-b883c74f],.sub-chapter-title a[data-v-b883c74f]{font-size:.875rem}summary[data-v-b883c74f]{cursor:pointer;list-style:none;position:relative}summary[data-v-b883c74f]::-webkit-details-marker{display:none}html[data-v-b883c74f]{--details-transition-time:400ms}.chapter-landing-page[data-v-b883c74f]{font-weight:500}details[open]:first-of-type>.heading[data-v-b883c74f]:after{transform:rotate(135deg)}details[data-v-b883c74f]{max-height:var(--details-height-closed,auto);transition:all ease-out var(--details-transition-time,0)}details[open][data-v-b883c74f]{max-height:var(--details-height-open,auto)}details .heading.sub-chapter-title[data-v-b883c74f]:after,details .heading[data-v-b883c74f]:after{border-right:3px solid #151515;border-top:3px solid #151515;color:#151515;content:"";display:flex;float:right;height:9px;margin-left:16px;position:absolute;right:var(--rh-space-xl,24px);top:14px;transform:rotate(45deg);width:9px}details .heading.sub-chapter-title[data-v-b883c74f]:after{height:8px;width:8px}</style>
<style>.html-container[data-v-9c2a9ddb]{padding:2rem 1.5rem 0}rh-alert[data-v-9c2a9ddb]{color:#151515}@media (max-width:772px){.html-container[data-v-9c2a9ddb]{padding:3rem 1rem 0}}</style>
<style>.search-container[data-v-69710f44]{height:100%}.form-box[data-v-69710f44],.search-container[data-v-69710f44]{justify-content:center}.form-box[data-v-69710f44],.search-box[data-v-69710f44],.search-container[data-v-69710f44]{align-items:center;display:flex;width:100%}.search-box[data-v-69710f44]{justify-content:space-between;position:relative}ul[data-v-69710f44]{list-style-type:none}#search-list[data-v-69710f44]{background:#fff;border:1px solid #f0f0f0;box-shadow:0 4px 4px 0 #00000040;color:#151515;display:block;left:0;margin:0;padding:0;position:absolute;top:37px;width:100%;z-index:200}#search-list li[data-v-69710f44]{align-items:center;display:flex;justify-content:space-between;padding:.75rem}#search-list .active[data-v-69710f44],#search-list li[data-v-69710f44]:hover{background:#f0f0f0}.group-title[data-v-69710f44]{border-top:1px solid #4d4d4d;color:#4d4d4d;cursor:default;font-size:12px;font-weight:400;pointer-events:none}.group-title[data-v-69710f44],.group-title[data-v-69710f44]:hover{background:#fff}.search-item-text-elipsis[data-v-69710f44]{display:inline-block;max-width:48%;overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.search-item-text[data-v-69710f44]{padding-left:1.75rem}.search-item-chip[data-v-69710f44]{float:right}.search-product-version[data-v-69710f44]{background:#fff;border:1px solid #f0f0f0;border-radius:3px;font-size:1rem;font-weight:400;line-height:24px}.search-icon-form[data-v-69710f44]{color:var(--rh-color-gray-50,#707070);left:12px;position:absolute}.input-search-box[data-v-69710f44]{-webkit-appearance:none;-moz-appearance:none;appearance:none;border:0;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:var(--rh-font-size-body-text-md,1rem);height:36px;padding:0 40px;width:100%}.input-clear-btn[data-v-69710f44]{align-items:center;background-color:transparent;border:none;display:flex;height:36px;justify-content:center;margin-right:-30px;outline:none;transform:translateX(-30px)}.input-clear-btn[data-v-69710f44]:focus{border:1px solid var(--rh-color-accent-base-on-light,#06c)}.input-clear-btn:focus .input-clear-icon[data-v-69710f44]{color:var(--rh-color-canvas-black,#151515)}.input-clear-icon[data-v-69710f44]{color:#6b6e72;cursor:pointer}.input-clear-icon[data-v-69710f44]:hover{color:var(--rh-color-canvas-black,#151515)}.form-submit-btn[data-v-69710f44]::part(button){align-items:center;background-color:var(--rh-color-gray-20,#e0e0e0);border-radius:0;display:flex;height:36px;justify-content:center;--_default-border-color:var(--rh-color-gray-20,#e0e0e0)}.input-close-btn[data-v-69710f44]{background:none;border:none;cursor:pointer;margin:0 var(--rh-space-lg,16px)}.input-close-icon[data-v-69710f44]{color:var(--rh-color-white,#fff)}@media (max-width:992px){.form-box[data-v-69710f44]{gap:var(--rh-space-md,8px);margin:auto;width:100%}.input-search-box[data-v-69710f44]{border:1px solid var(--rh-color-gray-30,#c7c7c7)}.input-search-box[data-v-69710f44]::-moz-placeholder{color:#6a6e73;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:var(--rh-font-size-body-text-md,1rem);font-weight:var(--rh-font-weight-code-regular,400);line-height:24px}.input-search-box[data-v-69710f44]::placeholder{color:#6a6e73;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:var(--rh-font-size-body-text-md,1rem);font-weight:var(--rh-font-weight-code-regular,400);line-height:24px}.search-item-text-elipsis[data-v-69710f44]{max-width:60%}}@media (max-width:767px){.input-close-btn[data-v-69710f44]{display:none}.search-container[data-v-69710f44]{border:1px solid #f0f0f0}}</style>
<link rel="stylesheet" href="/_nuxt/entry.DNAluCmw.css" integrity="sha384-FnrZajt9k3u4tri3ClqikI96k+jP7kyrvgKKgdzBr5Y8CQEi3gusKE0+eMSLvGxm">
<link rel="stylesheet" href="/_nuxt/SearchAutocomplete.DkrJaF8R.css" integrity="sha384-Zw8gf6w7SrpWDmknvrab5QanIN+DSJRS/bPB9/lgDkhR7JhDmG/VF++MTMKU8mKB">
<link rel="stylesheet" href="/_nuxt/Breadcrumbs.BLkLxUMB.css" integrity="sha384-f6iEfCywVoZB0/hIKTRaxywtS21KzhBx2JOI/uVjjU9aqFcP7X9cqLH29w2HQvLe">
<link rel="stylesheet" href="/_nuxt/Alert.fTkXFs3h.css" integrity="sha384-yFMpT5E64LAQi3qJ10AJJ1oOH3DrI68OvLSjSivjNCQXZ25pmFxlPavPQ0TEyEpq">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/C3PvZR0C.js" integrity="sha384-H9Can2ny34kmIBk2SNFJRaBH6FotFjI6LVF8tQ4HqJnqY0a1tGDF/4RHxK2LFs7c">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/Ds9UYxCD.js" integrity="sha384-PFyBx6Pvk48yXM+prD4CDErW/3HFtPEqORVyAch5qDvJSyubA93+vR4jsWGRQKZs">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BUXK7nH-.js" integrity="sha384-Kqf3r1QrD7+NOn9EuK/ba9sTBZjn7vLAjnCoRtq7IKz+QV0JUUBFf1Snf1yVkkgr">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BxBriHwM.js" integrity="sha384-bejD6BN5N1iVRo5Ymn+LdVN0jvrnJTzgqgFD9xQ5+Yi46U6ff+s5W2AISzRWvJaM">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DB5Lt3x5.js" integrity="sha384-Ae/gV/4Dt4jnJApC0CWbckKJ7omq+W9Hb4jjdMyhxGJpwk6FlO7pGYMD3bPO/h1y">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/D8BFaW0S.js" integrity="sha384-/m09v/yYk14L9Hra6/9jYtMTn2IdkKQvzpFPRQ5P/baRKwdZQN98873hqvwuYXFS">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/_58Q44fW.js" integrity="sha384-EdSd8jpnetO/BWDAc8C4SpQExZ7pYjMygNPvbLmiA1TaYsNUdVOqIIyOIzaErEeU">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DOCUiUcq.js" integrity="sha384-9H868K5q7MLzC8TSRXT3pVqDzsOVxZLYvUQ8cLGpHPZCFEyEJjnwSHE+/7FWrJPV">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CiTVMRA7.js" integrity="sha384-xBIJ2xO0t62/oREKqSxPtW1tRxKY+5YYXbGcJ0QR9IRM28JhL+53NOiAvVFn3rmF">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/CR3F0y4K.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/Bn-QuJwp.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/T_zvNmZe.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/Ciddaed-.js">
<link rel="prefetch" as="image" type="image/svg+xml" href="/_nuxt/Footer_Cloud.DpSdW8MR.svg">
<script type="module">
      import "@rhds/elements/rh-cta/rh-cta.js";
      </script>
<script type="module">
      import '@rhds/elements/rh-alert/rh-alert.js';
      </script>
<script type="module">
        import "/scripts/v1/@cpelements/pfe-navigation/dist/pfe-navigation.min.js";
        import "/scripts/v1/@rhds/elements/elements/rh-button/rh-button.js";
      </script>
<script type="module">import "@rhds/elements/rh-footer/rh-footer.js"</script>
<link rel="canonical" href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/configuring_gfs2_file_systems/index">
<script type="module">
      import "@rhds/elements/rh-alert/rh-alert.js";
      import "@rhds/elements/rh-code-block/rh-code-block.js";
      import '@rhds/elements/rh-cta/rh-cta.js';
      import '@patternfly/elements/pf-switch/pf-switch.js';
      import '@cpelements/rh-table/dist/rh-table.js';
      import '@patternfly/pfe-clipboard/dist/pfe-clipboard.min.js';
      import '@patternfly/elements/pf-button/pf-button.js';
      import '@patternfly/elements/pf-modal/pf-modal.js';
      import '@patternfly/elements/pf-icon/pf-icon.js';
      import '@patternfly/elements/pf-popover/pf-popover.js';
      import '@patternfly/elements/pf-tooltip/pf-tooltip.js';
      import '@rhds/elements/rh-badge/rh-badge.js';
      </script>
<meta name="description" content="Configuring GFS2 file systems | Red Hat Documentation">
<meta name="app-version" content="v0.0.1">
<script type="module">
        import "/scripts/v1/@rhds/elements/elements/rh-button/rh-button.js";
      </script>
<script type="module">
      import '@rhds/elements/rh-alert/rh-alert.js';
      </script>
<script type="module">
        import "/scripts/v1/@rhds/elements/elements/rh-button/rh-button.js";
        import "@patternfly/elements/pf-badge/pf-badge.js";
      </script>
<script type="module" src="/_nuxt/C3PvZR0C.js" crossorigin integrity="sha384-H9Can2ny34kmIBk2SNFJRaBH6FotFjI6LVF8tQ4HqJnqY0a1tGDF/4RHxK2LFs7c"></script></head><body><div id="__nuxt"><!--[--><!--[--><!----><header data-v-edc0d12c><a href="#pfe-navigation" id="global-skip-to-nav" class="skip-link visually-hidden" data-v-edc0d12c>Skip to navigation</a><a href="#main-content" class="skip-link visually-hidden" data-v-edc0d12c>Skip to content</a><nav id="upper-navigation" class="upper-navigation" aria-labelledby="upper-navigation-label" data-analytics-region="upper-navigation" data-v-edc0d12c><p id="upper-navigation-label" class="upper-nav-hidden" data-v-edc0d12c>Featured links</p><div class="upper-nav-container" data-v-edc0d12c><ul class="upper-nav-menu" data-v-edc0d12c><li data-v-edc0d12c><a href="https://access.redhat.com/" class="upper-nav-links" data-analytics-text="Support" data-analytics-category="Featured Links" data-v-edc0d12c>Support</a></li><li data-v-edc0d12c><a href="https://console.redhat.com/" class="upper-nav-links" data-analytics-text="Console" data-analytics-category="Featured Links" data-v-edc0d12c>Console</a></li><li data-v-edc0d12c><a href="https://developers.redhat.com/" class="upper-nav-links" data-analytics-text="Developers" data-analytics-category="Featured Links" data-v-edc0d12c>Developers</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/en/products/trials" class="upper-nav-links" data-analytics-text="Start a trial" data-analytics-category="Featured Links" data-v-edc0d12c>Start a trial</a></li><li data-v-edc0d12c><button id="all-red-hat" class="upper-nav-links" data-analytics-text="All Red Hat" data-analytics-category="Featured Links" aria-expanded="false" data-analytics-linktype="tab" data-v-edc0d12c>All Red Hat<svg class="upper-nav-arrow" xmlns="http://www.w3.org/2000/svg" width="1024" height="1024" viewBox="0 0 1024 1024" aria-hidden="true" data-v-edc0d12c=""><path d="M810.642 511.557c0 8.905-3.447 16.776-10.284 23.613L322.31 1013.216c-6.835 6.837-14.706 10.284-23.61 10.284s-16.776-3.447-23.613-10.284l-51.303-51.303c-6.837-6.837-10.284-14.707-10.284-23.612s3.447-16.775 10.284-23.61L626.972 511.5 223.784 108.31c-6.837-6.835-10.284-14.706-10.284-23.61s3.447-16.776 10.284-23.613l51.303-51.303C281.924 2.947 289.794-.5 298.7-.5s16.775 3.447 23.61 10.284L800.36 487.83c6.837 6.837 10.284 14.708 10.284 23.613v.114" data-v-edc0d12c=""/></svg></button><div class="upper-nav-dropdown-container" data-v-edc0d12c><ul data-v-edc0d12c><li data-v-edc0d12c><span data-v-edc0d12c>For customers</span><ul data-v-edc0d12c><li data-v-edc0d12c><a href="https://access.redhat.com/support" data-pzn-audience="customers" data-analytics-category="All Red Hat|For customers" data-analytics-text="Customer support" data-v-edc0d12c>Customer support</a></li><li data-v-edc0d12c><a href="/products" data-pzn-audience="customers" data-analytics-category="All Red Hat|For customers" data-analytics-text="Documentation" data-v-edc0d12c>Documentation</a></li><li data-v-edc0d12c><a href="https://access.redhat.com/support/cases" data-pzn-audience="customers" data-analytics-category="All Red Hat|For customers" data-analytics-text="Support cases" data-v-edc0d12c>Support Cases</a></li><li data-v-edc0d12c><a href="https://access.redhat.com/management" data-pzn-audience="customers" data-analytics-category="All Red Hat|For customers" data-analytics-text="Subscription management" data-v-edc0d12c>Subscription management</a></li><li data-v-edc0d12c><a href="https://catalog.redhat.com/" data-analytics-category="All Red Hat|For customers" data-analytics-text="Red Hat Ecosystem Catalog" data-v-edc0d12c>Red Hat Ecosystem Catalog</a></li><li data-v-edc0d12c><a href="https://catalog.redhat.com/partners" data-analytics-category="All Red Hat|For customers" data-analytics-text="Find a partner" data-v-edc0d12c>Find a partner</a></li></ul></li><li data-v-edc0d12c><span data-v-edc0d12c>For partners</span><ul data-v-edc0d12c><li data-v-edc0d12c><a href="https://connect.redhat.com/login" data-pzn-audience="partners" data-analytics-category="All Red Hat|For partners" data-analytics-text="Partner login" data-v-edc0d12c>Partner login</a></li><li data-v-edc0d12c><a href="https://connect.redhat.com/en/support" data-pzn-audience="partners" data-analytics-category="All Red Hat|For partners" data-analytics-text="Partner support" data-v-edc0d12c>Partner support</a></li><li data-v-edc0d12c><a href="https://connect.redhat.com/" data-pzn-audience="partners" data-analytics-category="All Red Hat|For partners" data-analytics-text="Become a partner " data-v-edc0d12c>Become a partner</a></li></ul></li><li data-v-edc0d12c><span data-v-edc0d12c>Try, buy, &amp; sell</span><ul data-v-edc0d12c><li data-v-edc0d12c><a href="https://marketplace.redhat.com/en-us" data-analytics-category="All Red Hat|Try, buy, &amp; sell" data-analytics-text="Red Hat Marketplace" data-v-edc0d12c>Red Hat Marketplace</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/en/store" data-analytics-category="All Red Hat|Try, buy, &amp; sell" data-analytics-text="Red Hat Store" data-v-edc0d12c>Red Hat Store</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/en/contact" data-analytics-category="All Red Hat|Try, buy, &amp; sell" data-analytics-text="Contact sales" data-v-edc0d12c>Contact Sales</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/en/products/trials" data-analytics-category="All Red Hat|Try, buy, &amp; sell" data-analytics-text="Start a trial" data-v-edc0d12c>Start a trial</a></li></ul></li><li data-v-edc0d12c><span data-v-edc0d12c>Learning resources</span><ul data-v-edc0d12c><li data-v-edc0d12c><a href="https://www.redhat.com/en/services/training-and-certification" data-analytics-category="All Red Hat|Learning resources" data-analytics-text="Training and certification " data-v-edc0d12c>Training and certification</a></li><li data-v-edc0d12c><a href="https://developers.redhat.com/" data-pzn-audience="developers|community" data-analytics-category="All Red Hat|Learning resources" data-analytics-text="For developers" data-v-edc0d12c>For developers</a></li><li data-v-edc0d12c><a href="https://cloud.redhat.com/learn" data-analytics-category="All Red Hat|Learning resources" data-analytics-text="Hybrid cloud learning hub" data-v-edc0d12c>Hybrid cloud learning hub</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/en/interactive-labs" data-analytics-category="All Red Hat|Learning resources" data-analytics-text="Interactive labs" data-v-edc0d12c>Interactive labs</a></li><li data-v-edc0d12c><a href="https://learn.redhat.com/" data-analytics-category="All Red Hat|Learning resources" data-analytics-text="Learning community" data-v-edc0d12c>Learning community</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/en/tv" data-analytics-category="All Red Hat|Learning resources" data-analytics-text="Red Hat TV" data-v-edc0d12c>Red Hat TV</a></li></ul></li><li data-v-edc0d12c><span data-v-edc0d12c>Open source communities</span><ul data-v-edc0d12c><li data-v-edc0d12c><a href="https://www.ansible.com/community" data-analytics-category="All Red Hat|Open source communities" data-analytics-text="Ansible" data-v-edc0d12c>Ansible</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/sysadmin/" id="community" data-analytics-category="All Red Hat|Open source communities" data-analytics-text="For system administrators" data-v-edc0d12c>For system administrators</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/architect/" data-pzn-audience="community" data-analytics-category="All Red Hat|Open source communities" data-analytics-text="For architects" data-v-edc0d12c>For architects</a></li></ul></li></ul></div></li></ul></div></nav><pfe-navigation full-width id="pfe-navigation" pf-sticky="true" lang="en" data-v-edc0d12c><nav class="pfe-navigation" aria-label="Main Navigation" data-v-edc0d12c><div class="pfe-navigation__logo-wrapper" id="pfe-navigation__logo-wrapper" data-v-edc0d12c><a href="/en" class="pfe-navigation__logo-link" data-v-edc0d12c><img class="pfe-navigation__logo-image pfe-navigation__logo-image--screen pfe-navigation__logo-image--small" src="/Logo-Red_Hat-Documentation-A-Reverse-RGB.svg" width="240" height="40" alt="Red Hat Documentation" data-v-edc0d12c></a></div></nav><span data-v-edc0d12c></span><div slot="secondary-links" data-v-edc0d12c><div class="hidden-at-desktop hidden-at-tablet search-mobile" data-v-edc0d12c><div id="search-form" class="search-container" opensearchbox="true" data-v-edc0d12c data-v-69710f44><form role="search" class="form-box" autocomplete="off" data-v-69710f44><div class="search-box" data-v-69710f44><pf-icon icon="search" size="md" class="search-icon-form" data-v-69710f44></pf-icon><input type="text" id="input-search" class="input-search-box" placeholder="Search documentation" value aria-autocomplete="list" data-v-69710f44><!----><!----><!----><rh-button disabled variant="tertiary" class="form-submit-btn" data-v-69710f44><img src="data:image/svg+xml,%3csvg%20width=&#39;14&#39;%20height=&#39;14&#39;%20viewBox=&#39;0%200%2014%2014&#39;%20fill=&#39;none&#39;%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%3e%3cpath%20d=&#39;M7%200L5.6%201.4L10.3%206H0V8H10.3L5.6%2012.6L7%2014L14%207L7%200Z&#39;%20fill=&#39;%23707070&#39;/%3e%3c/svg%3e" alt="Submit button" data-v-69710f44></rh-button></div></form><!----></div></div><div class="hidden-at-desktop hidden-at-tablet buttons" data-v-edc0d12c><a href="https://access.redhat.com/" data-analytics-category="More Red Hat" data-analytics-text="Support" data-v-edc0d12c>Support</a><a href="https://console.redhat.com/" data-analytics-category="More Red Hat" data-analytics-text="Console" data-v-edc0d12c>Console</a><a href="https://developers.redhat.com/" data-analytics-category="More Red Hat" data-analytics-text="Developers" data-v-edc0d12c>Developers</a><a href="https://www.redhat.com/en/products/trials" data-analytics-category="More Red Hat" data-analytics-text="Start a trial" data-v-edc0d12c>Start a trial</a><a href="https://www.redhat.com/en/contact" data-analytics-category="More Red Hat" data-analytics-text="Contact" data-v-edc0d12c>Contact</a></div><div class="hidden-at-desktop hidden-at-tablet mobile-lang-select" data-v-edc0d12c><label for="lang_selection" data-v-edc0d12c>Select your language</label><select id="lang_selection" data-v-edc0d12c><!--[--><option value="en" xml:lang="en" hreflang="en" data-v-edc0d12c>English</option><option value="fr" xml:lang="fr" hreflang="fr" data-v-edc0d12c>Français</option><option value="ko" xml:lang="ko" hreflang="ko" data-v-edc0d12c>한국어</option><option value="ja" xml:lang="ja" hreflang="ja" data-v-edc0d12c>日本語</option><option value="zh-cn" xml:lang="zh-cn" hreflang="zh-cn" data-v-edc0d12c>中文 (中国)</option><option value="de" xml:lang="de" hreflang="de" data-v-edc0d12c>Deutsch</option><option value="it" xml:lang="it" hreflang="it" data-v-edc0d12c>Italiano</option><option value="pt-br" xml:lang="pt-br" hreflang="pt-br" data-v-edc0d12c>Português</option><option value="es" xml:lang="es" hreflang="es" data-v-edc0d12c>Español</option><!--]--></select></div></div></pfe-navigation></header><main id="main-content"><!--[--><!--[--><!--[--><!----><!----><!--]--><div class="breadcrumbs" id="breadcrumbs" data-v-8589d091 data-v-798f280c><nav aria-label="Breadcrumb" class="breadcrumb" data-v-798f280c><ol data-v-798f280c><li data-v-798f280c><a href="/" data-v-798f280c>Home</a></li><li data-v-798f280c><a href="/en/products" data-v-798f280c>Products</a></li><!--[--><li data-v-798f280c><a href="/en/documentation/red_hat_enterprise_linux/" data-v-798f280c>Red Hat Enterprise Linux</a></li><li data-v-798f280c><a href="/en/documentation/red_hat_enterprise_linux/9/" data-v-798f280c>9</a></li><li data-v-798f280c><!--[-->Configuring GFS2 file systems<!--]--></li><!--]--></ol></nav><span data-v-798f280c></span></div><!----><nav id="mobile-nav" class="mobile-nav" aria-label="mobile menu" data-v-8589d091><div class="mobile-nav-wrapper" data-v-8589d091><div id="first-button" data-v-8589d091><button id="toc-btn" aria-expanded="false" aria-controls="mobile-nav-content-wrapper" class="mobile-nav-btn" data-v-8589d091><span class="sr-only" data-v-8589d091>Open </span>Table of contents</button></div><div id="second-button" data-v-8589d091><button id="settings-btn" aria-expanded="false" aria-controls="mobile-nav-content-wrapper" class="mobile-nav-btn" data-v-8589d091><pf-icon icon="cog" size="md" data-v-8589d091></pf-icon><span class="sr-only" data-v-8589d091>Open page settings</span></button></div></div><div id="mobile-nav-content-wrapper" class="hidden" role="navigation" tabindex="0" data-v-8589d091><div id="toc-mobile" class="hidden" aria-labelledby="toc-btn" data-v-8589d091><div class="shrink-product-padding product-container" id="product-container-mobile" data-v-8589d091><h1 class="product-title" data-v-8589d091>Red Hat Enterprise Linux</h1><!----></div><div class="toc-filter-mobile" data-v-8589d091><span id="text" part="text" data-v-8589d091><input id="text-input" aria-label="Search input" part="text-input" placeholder="Filter table of contents" type="text" class value data-v-8589d091><!----></span></div><div id="toc-wrapper-mobile" class="span-xs-12 span-sm-4 span-md-3" lang="en" data-v-8589d091><nav id="mobile-toc-menu" class="table-of-contents" aria-label="table of content - mobile" data-v-8589d091><!----></nav></div><!----></div><div id="page-content-options-mobile" class="hidden" aria-labelledby="settings-btn" data-v-8589d091><div class="page-layout-options" data-v-8589d091><label for="page-format-mobile" data-v-8589d091>Format</label><select id="page-format-mobile" class="page-format-dropdown" data-v-8589d091><option class="page-type" value="html" data-v-8589d091>Multi-page</option><option selected class="page-type" value="html-single" data-v-8589d091>Single-page</option><option class="page-type" value="pdf" data-v-8589d091>View full doc as PDF</option></select></div></div></div><!----><!----></nav><div class="grid grid-col-12 content-wrapper" data-v-8589d091><aside id="left-content" class="span-xs-12 span-sm-4 span-md-3" lang="en-us" aria-label="left navigation" xml:lang="en-us" data-v-8589d091><div class="toc-container" id="toc-container" visible="true" data-v-8589d091><div class="toc-focus-container" id="toc-focus-container" data-v-8589d091><button class="toc-focus-btn" aria-label="toggle left menu" aria-controls="toc-container" aria-expanded="true" data-v-8589d091><pf-icon size="md" icon="angle-left" class="toc-focus-btn-icon" data-v-8589d091></pf-icon></button></div><div class="product-container" id="product-container-desktop" data-v-8589d091><h1 class="product-title" data-v-8589d091>Red Hat Enterprise Linux</h1><!----></div><div class="toc-filter" id="toc-filter" data-v-8589d091><span id="text" part="text" data-v-8589d091><span id="search-icon" part="search-icon" data-v-8589d091><pf-icon icon="filter" size="md" data-v-8589d091></pf-icon></span><input id="text-input" aria-label="Search input" part="text-input" placeholder="Filter table of contents" type="text" class value data-v-8589d091><!----></span></div><div id="toc-wrapper" class="toc-wrapper" data-v-8589d091><nav id="toc" class="max-height-85 table-of-contents" aria-label="Table of contents" data-v-8589d091><ol id="toc-list" data-v-8589d091><!--[--><li class="item chapter" data-v-fa0dae77><a class="link active" href id="chapter-index" data-v-fa0dae77>Configuring GFS2 file systems</a></li><li class="item chapter" data-v-fa0dae77><a class="link" href="#proc_providing-feedback-on-red-hat-documentation_configuring-gfs2-file-systems" id="chapter-landing--configuring_gfs2_file_systems-proc_providing-feedback-on-red-hat-documentation_configuring-gfs2-file-systems" data-v-fa0dae77>Providing feedback on Red Hat documentation</a></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_planning-gfs2-deployment-configuring-gfs2-file-systems" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_planning-gfs2-deployment-configuring-gfs2-file-systems--summary" data-v-fa0dae77>1. Planning a GFS2 file system deployment</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_planning-gfs2-deployment-configuring-gfs2-file-systems" id="chapter-landing--configuring_gfs2_file_systems-assembly_planning-gfs2-deployment-configuring-gfs2-file-systems" data-v-fa0dae77>Planning a GFS2 file system deployment</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_gfs2-filesystem-format-planning-gfs2-deployment" id="sub-link-to-configuring_gfs2_file_systems-con_gfs2-filesystem-format-planning-gfs2-deployment" data-v-b883c74f>1.1. GFS2 file system format version 1802</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_basic-gfs2-parameters-planning-gfs2-deployment" id="sub-link-to-configuring_gfs2_file_systems-con_basic-gfs2-parameters-planning-gfs2-deployment" data-v-b883c74f>1.2. Key GFS2 parameters to determine</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--con_gfs2-support-limits-planning-gfs2-deployment" data-v-b883c74f><summary class="heading sub-chapter-title" id="con_gfs2-support-limits-planning-gfs2-deployment--summary" data-v-b883c74f>1.3. GFS2 support considerations</summary><ol id="sub-nav--con_gfs2-support-limits-planning-gfs2-deployment" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#con_gfs2-support-limits-planning-gfs2-deployment" id="chapter-landing--configuring_gfs2_file_systems-con_gfs2-support-limits-planning-gfs2-deployment" data-v-b883c74f>GFS2 support considerations</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#maximum_file_system_and_cluster_size" id="sub-link-to-configuring_gfs2_file_systems-maximum_file_system_and_cluster_size" data-v-b883c74f>1.3.1. Maximum file system and cluster size</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#minimum_cluster_size" id="sub-link-to-configuring_gfs2_file_systems-minimum_cluster_size" data-v-b883c74f>1.3.2. Minimum cluster size</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#shared_storage_considerations" id="sub-link-to-configuring_gfs2_file_systems-shared_storage_considerations" data-v-b883c74f>1.3.3. Shared storage considerations</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_gfs2-formattiing-considerations-planning-gfs2-deployment" id="sub-link-to-configuring_gfs2_file_systems-con_gfs2-formattiing-considerations-planning-gfs2-deployment" data-v-b883c74f>1.4. GFS2 formatting considerations</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_gfs2-cluster-considerations-planning-gfs2-deployment" id="sub-link-to-configuring_gfs2_file_systems-con_gfs2-cluster-considerations-planning-gfs2-deployment" data-v-b883c74f>1.5. Considerations for GFS2 in a cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_basic-gfs2-hardware-considerations-planning-gfs2-deployment" id="sub-link-to-configuring_gfs2_file_systems-con_basic-gfs2-hardware-considerations-planning-gfs2-deployment" data-v-b883c74f>1.6. Hardware considerations</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_gfs2-usage-considerations-configuring-gfs2-file-systems" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_gfs2-usage-considerations-configuring-gfs2-file-systems--summary" data-v-fa0dae77>2. Recommendations for GFS2 usage</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_gfs2-usage-considerations-configuring-gfs2-file-systems" id="chapter-landing--configuring_gfs2_file_systems-assembly_gfs2-usage-considerations-configuring-gfs2-file-systems" data-v-fa0dae77>Recommendations for GFS2 usage</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-atime-gfs2-usage-considerations" id="sub-link-to-configuring_gfs2_file_systems-proc_configuring-atime-gfs2-usage-considerations" data-v-b883c74f>2.1. Configuring atime updates</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_vfs-tuning-options-gfs2-usage-considerations" id="sub-link-to-configuring_gfs2_file_systems-con_vfs-tuning-options-gfs2-usage-considerations" data-v-b883c74f>2.2. VFS tuning options: research and experiment</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_selinux-on-gfs2-gfs2-usage-considerations" id="sub-link-to-configuring_gfs2_file_systems-con_selinux-on-gfs2-gfs2-usage-considerations" data-v-b883c74f>2.3. SELinux on GFS2</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_nfs-over-gfs2-gfs2-usage-considerations" id="sub-link-to-configuring_gfs2_file_systems-con_nfs-over-gfs2-gfs2-usage-considerations" data-v-b883c74f>2.4. Setting up NFS over GFS2</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_samba-over-gfs2-gfs2-usage-considerations" id="sub-link-to-configuring_gfs2_file_systems-con_samba-over-gfs2-gfs2-usage-considerations" data-v-b883c74f>2.5. Samba (SMB or Windows) file serving over GFS2</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_vms-for-gfs2-gfs2-usage-considerations" id="sub-link-to-configuring_gfs2_file_systems-con_vms-for-gfs2-gfs2-usage-considerations" data-v-b883c74f>2.6. Configuring virtual machines for GFS2</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--con_gfs2-block-allocation-issues-gfs2-usage-considerations" data-v-b883c74f><summary class="heading sub-chapter-title" id="con_gfs2-block-allocation-issues-gfs2-usage-considerations--summary" data-v-b883c74f>2.7. Block allocation</summary><ol id="sub-nav--con_gfs2-block-allocation-issues-gfs2-usage-considerations" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#con_gfs2-block-allocation-issues-gfs2-usage-considerations" id="chapter-landing--configuring_gfs2_file_systems-con_gfs2-block-allocation-issues-gfs2-usage-considerations" data-v-b883c74f>Block allocation</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#leave_free_space_in_the_file_system" id="sub-link-to-configuring_gfs2_file_systems-leave_free_space_in_the_file_system" data-v-b883c74f>2.7.1. Leave free space in the file system</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#have_each_node_allocate_its_own_files_if_possible" id="sub-link-to-configuring_gfs2_file_systems-have_each_node_allocate_its_own_files_if_possible" data-v-b883c74f>2.7.2. Have each node allocate its own files, if possible</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#preallocate_if_possible" id="sub-link-to-configuring_gfs2_file_systems-preallocate_if_possible" data-v-b883c74f>2.7.3. Preallocate, if possible</a></li><!--]--><!--]--></ol></details></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_creating-mounting-gfs2-configuring-gfs2-file-systems" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_creating-mounting-gfs2-configuring-gfs2-file-systems--summary" data-v-fa0dae77>3. Administering GFS2 file systems</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_creating-mounting-gfs2-configuring-gfs2-file-systems" id="chapter-landing--configuring_gfs2_file_systems-assembly_creating-mounting-gfs2-configuring-gfs2-file-systems" data-v-fa0dae77>Administering GFS2 file systems</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_creating-gfs2-creating-mounting-gfs2" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_creating-gfs2-creating-mounting-gfs2--summary" data-v-b883c74f>3.1. GFS2 file system creation</summary><ol id="sub-nav--proc_creating-gfs2-creating-mounting-gfs2" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_creating-gfs2-creating-mounting-gfs2" id="chapter-landing--configuring_gfs2_file_systems-proc_creating-gfs2-creating-mounting-gfs2" data-v-b883c74f>GFS2 file system creation</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#the_gfs2_mkfs_command" id="sub-link-to-configuring_gfs2_file_systems-the_gfs2_mkfs_command" data-v-b883c74f>3.1.1. The GFS2 mkfs command</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#creating_a_gfs2_file_system" id="sub-link-to-configuring_gfs2_file_systems-creating_a_gfs2_file_system" data-v-b883c74f>3.1.2. Creating a GFS2 file system</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_mounting-gfs2-filesystem_creating-mounting-gfs2" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_mounting-gfs2-filesystem_creating-mounting-gfs2--summary" data-v-b883c74f>3.2. Mounting a GFS2 file system</summary><ol id="sub-nav--proc_mounting-gfs2-filesystem_creating-mounting-gfs2" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_mounting-gfs2-filesystem_creating-mounting-gfs2" id="chapter-landing--configuring_gfs2_file_systems-proc_mounting-gfs2-filesystem_creating-mounting-gfs2" data-v-b883c74f>Mounting a GFS2 file system</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#mounting_a_gfs2_file_system_with_no_options_specified" id="sub-link-to-configuring_gfs2_file_systems-mounting_a_gfs2_file_system_with_no_options_specified" data-v-b883c74f>3.2.1. Mounting a GFS2 file system with no options specified</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#mounting_a_gfs2_file_system_that_specifies_mount_options" id="sub-link-to-configuring_gfs2_file_systems-mounting_a_gfs2_file_system_that_specifies_mount_options" data-v-b883c74f>3.2.2. Mounting a GFS2 file system that specifies mount options</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#unmounting_a_gfs2_file_system" id="sub-link-to-configuring_gfs2_file_systems-unmounting_a_gfs2_file_system" data-v-b883c74f>3.2.3. Unmounting a GFS2 file system</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_backing-up-a-gfs2-filesystem-creating-mounting-gfs2" id="sub-link-to-configuring_gfs2_file_systems-proc_backing-up-a-gfs2-filesystem-creating-mounting-gfs2" data-v-b883c74f>3.3. Backing up a GFS2 file system</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_suspending-activity-on-a-gfs2-filesystem-creating-mounting-gfs2" id="sub-link-to-configuring_gfs2_file_systems-proc_suspending-activity-on-a-gfs2-filesystem-creating-mounting-gfs2" data-v-b883c74f>3.4. Suspending activity on a GFS2 file system</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_growing-gfs2-filesystem-creating-mounting-gfs2" id="sub-link-to-configuring_gfs2_file_systems-proc_growing-gfs2-filesystem-creating-mounting-gfs2" data-v-b883c74f>3.5. Growing a GFS2 file system</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_adding-gfs2-journal-creating-mounting-gfs2" id="sub-link-to-configuring_gfs2_file_systems-proc_adding-gfs2-journal-creating-mounting-gfs2" data-v-b883c74f>3.6. Adding journals to a GFS2 file system</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_gfs2-disk-quota-administration-configuring-gfs2-file-systems" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_gfs2-disk-quota-administration-configuring-gfs2-file-systems--summary" data-v-fa0dae77>4. GFS2 quota management</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_gfs2-disk-quota-administration-configuring-gfs2-file-systems" id="chapter-landing--configuring_gfs2_file_systems-assembly_gfs2-disk-quota-administration-configuring-gfs2-file-systems" data-v-fa0dae77>GFS2 quota management</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_configuring-gfs2-disk-quotas-gfs2-disk-quota-administration" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_configuring-gfs2-disk-quotas-gfs2-disk-quota-administration--summary" data-v-b883c74f>4.1. Configuring GFS2 disk quotas</summary><ol id="sub-nav--proc_configuring-gfs2-disk-quotas-gfs2-disk-quota-administration" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_configuring-gfs2-disk-quotas-gfs2-disk-quota-administration" id="chapter-landing--configuring_gfs2_file_systems-proc_configuring-gfs2-disk-quotas-gfs2-disk-quota-administration" data-v-b883c74f>Configuring GFS2 disk quotas</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#setting_up_quotas_in_enforcement_or_accounting_mode" id="sub-link-to-configuring_gfs2_file_systems-setting_up_quotas_in_enforcement_or_accounting_mode" data-v-b883c74f>4.1.1. Setting up quotas in enforcement or accounting mode</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#creating_the_quota_database_files" id="sub-link-to-configuring_gfs2_file_systems-creating_the_quota_database_files" data-v-b883c74f>4.1.2. Creating the quota database files</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#assigning_quotas_per_user" id="sub-link-to-configuring_gfs2_file_systems-assigning_quotas_per_user" data-v-b883c74f>4.1.3. Assigning quotas per user</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#assigning_quotas_per_group" id="sub-link-to-configuring_gfs2_file_systems-assigning_quotas_per_group" data-v-b883c74f>4.1.4. Assigning quotas per group</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_managing-gfs2-disk-quotas-gfs2-disk-quota-administration" id="sub-link-to-configuring_gfs2_file_systems-proc_managing-gfs2-disk-quotas-gfs2-disk-quota-administration" data-v-b883c74f>4.2. Managing GFS2 disk Quotas</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_keeping-gfs2-quotas-accurate-with-quotacheck-gfs2-disk-quota-administration" id="sub-link-to-configuring_gfs2_file_systems-proc_keeping-gfs2-quotas-accurate-with-quotacheck-gfs2-disk-quota-administration" data-v-b883c74f>4.3. Keeping GFS2 disk quotas accurate with the quotacheck command</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_synchronizing-gfs2-quotas-gfs2-disk-quota-administration" id="sub-link-to-configuring_gfs2_file_systems-proc_synchronizing-gfs2-quotas-gfs2-disk-quota-administration" data-v-b883c74f>4.4. Synchronizing quotas with the quotasync Command</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_gfs2-filesystem-repair-configuring-gfs2-file-systems" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_gfs2-filesystem-repair-configuring-gfs2-file-systems--summary" data-v-fa0dae77>5. GFS2 file system repair</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_gfs2-filesystem-repair-configuring-gfs2-file-systems" id="chapter-landing--configuring_gfs2_file_systems-assembly_gfs2-filesystem-repair-configuring-gfs2-file-systems" data-v-fa0dae77>GFS2 file system repair</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_determining-needed-memory-for-fsckgfs2-gfs2-filesystem-repair" id="sub-link-to-configuring_gfs2_file_systems-proc_determining-needed-memory-for-fsckgfs2-gfs2-filesystem-repair" data-v-b883c74f>5.1. Determining required memory for running fsck.gfs2</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_repairing-a-gfs2-filesystem-gfs2-filesystem-repair" id="sub-link-to-configuring_gfs2_file_systems-proc_repairing-a-gfs2-filesystem-gfs2-filesystem-repair" data-v-b883c74f>5.2. Repairing a gfs2 filesystem</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_gfs2-performance-configuring-gfs2-file-systems" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_gfs2-performance-configuring-gfs2-file-systems--summary" data-v-fa0dae77>6. Improving GFS2 performance</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_gfs2-performance-configuring-gfs2-file-systems" id="chapter-landing--configuring_gfs2_file_systems-assembly_gfs2-performance-configuring-gfs2-file-systems" data-v-fa0dae77>Improving GFS2 performance</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_gfs2-defragment-gfs2-performance" id="sub-link-to-configuring_gfs2_file_systems-proc_gfs2-defragment-gfs2-performance" data-v-b883c74f>6.1. GFS2 file system defragmentation</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_gfs2-node-locking-gfs2-performance" id="sub-link-to-configuring_gfs2_file_systems-con_gfs2-node-locking-gfs2-performance" data-v-b883c74f>6.2. GFS2 node locking</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_posix-locking-issues-gfs2-performance" id="sub-link-to-configuring_gfs2_file_systems-con_posix-locking-issues-gfs2-performance" data-v-b883c74f>6.3. Issues with Posix locking</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_troubleshooting-gfs2-performance-gfs2-performance" id="sub-link-to-configuring_gfs2_file_systems-con_troubleshooting-gfs2-performance-gfs2-performance" data-v-b883c74f>6.4. Performance tuning with GFS2</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_gfs2-lockdump-gfs2-performance" id="sub-link-to-configuring_gfs2_file_systems-con_gfs2-lockdump-gfs2-performance" data-v-b883c74f>6.5. Troubleshooting GFS2 performance with the GFS2 lock dump</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#enabling-data-journaling-gfs2-performance" id="sub-link-to-configuring_gfs2_file_systems-enabling-data-journaling-gfs2-performance" data-v-b883c74f>6.6. Enabling data journaling</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_troubleshooting-gfs2-configuring-gfs2-file-systems" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_troubleshooting-gfs2-configuring-gfs2-file-systems--summary" data-v-fa0dae77>7. Diagnosing and correcting problems with GFS2 file systems</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_troubleshooting-gfs2-configuring-gfs2-file-systems" id="chapter-landing--configuring_gfs2_file_systems-assembly_troubleshooting-gfs2-configuring-gfs2-file-systems" data-v-fa0dae77>Diagnosing and correcting problems with GFS2 file systems</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_gfs2-filesystem-unavailable-troubleshooting-gfs2" id="sub-link-to-configuring_gfs2_file_systems-ref_gfs2-filesystem-unavailable-troubleshooting-gfs2" data-v-b883c74f>7.1. GFS2 file system unavailable to a node (the GFS2 withdraw function)</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_gfs2-filesystem-hangs-one-node-troubleshooting-gfs2" id="sub-link-to-configuring_gfs2_file_systems-ref_gfs2-filesystem-hangs-one-node-troubleshooting-gfs2" data-v-b883c74f>7.2. GFS2 file system hangs and requires reboot of one node</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_gfs2-filesystem-hangs-all-nodes-troubleshooting-gfs2" id="sub-link-to-configuring_gfs2_file_systems-ref_gfs2-filesystem-hangs-all-nodes-troubleshooting-gfs2" data-v-b883c74f>7.3. GFS2 file system hangs and requires reboot of all nodes</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_gfs2-nomount-new-cluster-node-troubleshooting-gfs2" id="sub-link-to-configuring_gfs2_file_systems-ref_gfs2-nomount-new-cluster-node-troubleshooting-gfs2" data-v-b883c74f>7.4. GFS2 file system does not mount on newly added cluster node</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_gfs2-used-space-empty-filesystem-troubleshooting-gfs2" id="sub-link-to-configuring_gfs2_file_systems-ref_gfs2-used-space-empty-filesystem-troubleshooting-gfs2" data-v-b883c74f>7.5. Space indicated as used in empty file system</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_gathering-gfs2-data-troubleshooting-gfs2" id="sub-link-to-configuring_gfs2_file_systems-proc_gathering-gfs2-data-troubleshooting-gfs2" data-v-b883c74f>7.6. Gathering GFS2 data for troubleshooting</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-gfs2-in-a-cluster-configuring-gfs2-file-systems" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-gfs2-in-a-cluster-configuring-gfs2-file-systems--summary" data-v-fa0dae77>8. GFS2 file systems in a cluster</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-gfs2-in-a-cluster-configuring-gfs2-file-systems" id="chapter-landing--configuring_gfs2_file_systems-assembly_configuring-gfs2-in-a-cluster-configuring-gfs2-file-systems" data-v-fa0dae77>GFS2 file systems in a cluster</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster" id="sub-link-to-configuring_gfs2_file_systems-proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster" data-v-b883c74f>8.1. Configuring a GFS2 file system in a cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster--summary" data-v-b883c74f>8.2. Configuring an encrypted GFS2 file system in a cluster</summary><ol id="sub-nav--proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster" id="chapter-landing--configuring_gfs2_file_systems-proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster" data-v-b883c74f>Configuring an encrypted GFS2 file system in a cluster</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#configure_a_shared_logical_volume_in_a_pacemaker_cluster" id="sub-link-to-configuring_gfs2_file_systems-configure_a_shared_logical_volume_in_a_pacemaker_cluster" data-v-b883c74f>8.2.1. Configure a shared logical volume in a Pacemaker cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#encrypt_the_logical_volume_and_create_a_crypt_resource" id="sub-link-to-configuring_gfs2_file_systems-encrypt_the_logical_volume_and_create_a_crypt_resource" data-v-b883c74f>8.2.2. Encrypt the logical volume and create a crypt resource</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#format_the_encrypted_logical_volume_with_a_gfs2_file_system_and_create_a_file_system_resource_for_the_cluster" id="sub-link-to-configuring_gfs2_file_systems-format_the_encrypted_logical_volume_with_a_gfs2_file_system_and_create_a_file_system_resource_for_the_cluster" data-v-b883c74f>8.2.3. Format the encrypted logical volume with a GFS2 file system and create a file system resource for the cluster</a></li><!--]--><!--]--></ol></details></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--con_gfs2-tracepoints-configuring-gfs2-file-systems" data-v-fa0dae77><summary class="heading chapter-title" id="con_gfs2-tracepoints-configuring-gfs2-file-systems--summary" data-v-fa0dae77>9. GFS2 tracepoints and the glock debugfs interface</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#con_gfs2-tracepoints-configuring-gfs2-file-systems" id="chapter-landing--configuring_gfs2_file_systems-con_gfs2-tracepoints-configuring-gfs2-file-systems" data-v-fa0dae77>GFS2 tracepoints and the glock debugfs interface</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#gfs2_tracepoint_types" id="sub-link-to-configuring_gfs2_file_systems-gfs2_tracepoint_types" data-v-b883c74f>9.1. GFS2 tracepoint types</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ap-tracepoints-gfs2" id="sub-link-to-configuring_gfs2_file_systems-ap-tracepoints-gfs2" data-v-b883c74f>9.2. Tracepoints</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ap-glocks-gfs2" id="sub-link-to-configuring_gfs2_file_systems-ap-glocks-gfs2" data-v-b883c74f>9.3. Glocks</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ap-glock-debugfs-gfs2" id="sub-link-to-configuring_gfs2_file_systems-ap-glock-debugfs-gfs2" data-v-b883c74f>9.4. The glock debugfs interface</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ap-glock-holders-gfs2" id="sub-link-to-configuring_gfs2_file_systems-ap-glock-holders-gfs2" data-v-b883c74f>9.5. Glock holders</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ap-glock-tracepoints-gfs2" id="sub-link-to-configuring_gfs2_file_systems-ap-glock-tracepoints-gfs2" data-v-b883c74f>9.6. Glock tracepoints</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ap-bmap-tracepoints-gfs2" id="sub-link-to-configuring_gfs2_file_systems-ap-bmap-tracepoints-gfs2" data-v-b883c74f>9.7. Bmap tracepoints</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ap-log-gracepoints-gfs2" id="sub-link-to-configuring_gfs2_file_systems-ap-log-gracepoints-gfs2" data-v-b883c74f>9.8. Log tracepoints</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ap-glockstats-gfs2" id="sub-link-to-configuring_gfs2_file_systems-ap-glockstats-gfs2" data-v-b883c74f>9.9. Glock statistics</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ap-references-gfs2" id="sub-link-to-configuring_gfs2_file_systems-ap-references-gfs2" data-v-b883c74f>9.10. References</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_analyzing-gfs2-with-pcp-configuring-gfs2-file-systems" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_analyzing-gfs2-with-pcp-configuring-gfs2-file-systems--summary" data-v-fa0dae77>10. Monitoring and analyzing GFS2 file systems using Performance Co-Pilot (PCP)</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_analyzing-gfs2-with-pcp-configuring-gfs2-file-systems" id="chapter-landing--configuring_gfs2_file_systems-assembly_analyzing-gfs2-with-pcp-configuring-gfs2-file-systems" data-v-fa0dae77>Monitoring and analyzing GFS2 file systems using Performance Co-Pilot (PCP)</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_installing-gfs2-pdma-analyzing-gfs2-with-pcp" id="sub-link-to-configuring_gfs2_file_systems-proc_installing-gfs2-pdma-analyzing-gfs2-with-pcp" data-v-b883c74f>10.1. Installing the GFS2 PMDA</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_examining-number-of-glocks-analyzing-gfs2-with-pcp" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_examining-number-of-glocks-analyzing-gfs2-with-pcp--summary" data-v-b883c74f>10.2. Displaying information about the available performance metrics with the pminfo tool</summary><ol id="sub-nav--proc_examining-number-of-glocks-analyzing-gfs2-with-pcp" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_examining-number-of-glocks-analyzing-gfs2-with-pcp" id="chapter-landing--configuring_gfs2_file_systems-proc_examining-number-of-glocks-analyzing-gfs2-with-pcp" data-v-b883c74f>Displaying information about the available performance metrics with the pminfo tool</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#examining_the_number_of_glock_structures_that_currently_exist_per_file_system" id="sub-link-to-configuring_gfs2_file_systems-examining_the_number_of_glock_structures_that_currently_exist_per_file_system" data-v-b883c74f>10.2.1. Examining the number of glock structures that currently exist per file system</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#examining_the_number_of_glock_structures_that_exist_per_file_system_by_type" id="sub-link-to-configuring_gfs2_file_systems-examining_the_number_of_glock_structures_that_exist_per_file_system_by_type" data-v-b883c74f>10.2.2. Examining the number of glock structures that exist per file system by type</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#checking_the_number_of_glock_structures_that_are_in_a_wait_state" id="sub-link-to-configuring_gfs2_file_systems-checking_the_number_of_glock_structures_that_are_in_a_wait_state" data-v-b883c74f>10.2.3. Checking the number of glock structures that are in a wait state</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#checking_file_system_operation_latency_using_the_kernel_tracepoint_based_metrics" id="sub-link-to-configuring_gfs2_file_systems-checking_file_system_operation_latency_using_the_kernel_tracepoint_based_metrics" data-v-b883c74f>10.2.4. Checking file system operation latency using the kernel tracepoint based metrics</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_available-gfs2-PCP-metrics-analyzing-gfs2-with-pcp" id="sub-link-to-configuring_gfs2_file_systems-ref_available-gfs2-PCP-metrics-analyzing-gfs2-with-pcp" data-v-b883c74f>10.3. Complete listing of available metrics for GFS2 in PCP</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_installing-minimal-PCP-setup-analyzing-gfs2-with-pcp" id="sub-link-to-configuring_gfs2_file_systems-proc_installing-minimal-PCP-setup-analyzing-gfs2-with-pcp" data-v-b883c74f>10.4. Performing minimal PCP setup to gather file system data</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#additional_resources" id="sub-link-to-configuring_gfs2_file_systems-additional_resources" data-v-b883c74f>10.5. Additional resources</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><a class="link" href="#idm139688929828816" id="chapter-landing--configuring_gfs2_file_systems-idm139688929828816" data-v-fa0dae77>Legal Notice</a></li><!--]--></ol></nav><!----></div></div></aside><article class="content span-xs-12 span-sm-6 span-md-12 span-lg-7" aria-live="polite" data-v-8589d091><!----><div lang="en-us" xml:lang="en-us" class="docs-content-container" data-v-8589d091><!----><!----><h1 data-id="content_chapter" class="chapter-title" data-v-8589d091>Configuring GFS2 file systems</h1><hr class="line-below-chp" data-v-8589d091><section class="rhdocs" data-v-8589d091><body><div xml:lang="en-US" class="book" id="idm139688935047696"><div class="titlepage"><div><div class="producttitle"><span class="productname">Red Hat Enterprise Linux</span> <span class="productnumber">9</span></div><div><h3 class="subtitle">Planning, administering, troubleshooting, and configuring GFS2 file systems in a high availability cluster</h3></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat</span> <span class="orgdiv">Customer Content Services</span></div></div><div><a href="#idm139688929828816">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				The Red Hat Enterprise Linux (RHEL) Resilient Storage Add-On provides the Red Hat Global File System 2 (GFS2), a cluster file system that manages coherency between multiple nodes sharing a common block device. This title provides information about planning a GFS2 file system deployment as well as procedures for configuring, troubleshooting, and tuning GFS2 file systems.
			</div></div></div></div><hr></div><section class="preface" id="proc_providing-feedback-on-red-hat-documentation_configuring-gfs2-file-systems"><div class="titlepage"><div><div><h2 class="title">Providing feedback on Red Hat documentation</h2></div></div></div><p class="_abstract _abstract">
			We appreciate your feedback on our documentation. Let us know how we can improve it.
		</p><div class="orderedlist"><p class="title"><strong>Submitting feedback through Jira (account required)</strong></p><ol class="orderedlist" type="1"><li class="listitem">
					Log in to the <a class="link" href="https://issues.redhat.com/projects/RHELDOCS/issues">Jira</a> website.
				</li><li class="listitem">
					Click <span class="strong strong"><strong>Create</strong></span> in the top navigation bar
				</li><li class="listitem">
					Enter a descriptive title in the <span class="strong strong"><strong>Summary</strong></span> field.
				</li><li class="listitem">
					Enter your suggestion for improvement in the <span class="strong strong"><strong>Description</strong></span> field. Include links to the relevant parts of the documentation.
				</li><li class="listitem">
					Click <span class="strong strong"><strong>Create</strong></span> at the bottom of the dialogue.
				</li></ol></div></section><section class="chapter" id="assembly_planning-gfs2-deployment-configuring-gfs2-file-systems"><div class="titlepage"><div><div><h2 class="title">Chapter 1. Planning a GFS2 file system deployment</h2></div></div></div><p class="_abstract _abstract">
			The Red Hat Global File System 2 (GFS2) file system is a 64-bit symmetric cluster file system which provides a shared name space and manages coherency between multiple nodes sharing a common block device. A GFS2 file system is intended to provide a feature set which is as close as possible to a local file system, while at the same time enforcing full cluster coherency between nodes. To achieve this, the nodes employ a cluster-wide locking scheme for file system resources. This locking scheme uses communication protocols such as TCP/IP to exchange locking information.
		</p><p>
			In a few cases, the Linux file system API does not allow the clustered nature of GFS2 to be totally transparent; for example, programs using POSIX locks in GFS2 should avoid using the <code class="literal">GETLK</code> function since, in a clustered environment, the process ID may be for a different node in the cluster. In most cases however, the functionality of a GFS2 file system is identical to that of a local file system.
		</p><p>
			The Red Hat Enterprise Linux (RHEL) Resilient Storage Add-On provides GFS2, and it depends on the RHEL High Availability Add-On to provide the cluster management required by GFS2.
		</p><p>
			The <code class="literal">gfs2.ko</code> kernel module implements the GFS2 file system and is loaded on GFS2 cluster nodes.
		</p><p>
			To get the best performance from GFS2, it is important to take into account the performance considerations which stem from the underlying design. Just like a local file system, GFS2 relies on the page cache in order to improve performance by local caching of frequently used data. In order to maintain coherency across the nodes in the cluster, cache control is provided by the <span class="emphasis"><em>glock</em></span> state machine.
		</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
				Make sure that your deployment of the Red Hat High Availability Add-On meets your needs and can be supported. Consult with an authorized Red Hat representative to verify your configuration prior to deployment.
			</p></div></rh-alert><section class="section" id="con_gfs2-filesystem-format-planning-gfs2-deployment"><div class="titlepage"><div><div><h3 class="title">1.1. GFS2 file system format version 1802</h3></div></div></div><p class="_abstract _abstract">
				As of Red Hat Enterprise Linux 9, GFS2 file systems are created with format version 1802.
			</p><p>
				Format version 1802 enables the following features:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Extended attributes in the <code class="literal">trusted</code> namespace ("trusted.* xattrs") are recognized by <code class="literal">gfs2</code> and <code class="literal">gfs2-utils</code>.
					</li><li class="listitem">
						The <code class="literal">rgrplvb</code> option is active by default. This allows <code class="literal">gfs2</code> to attach updated resource group data to DLM lock requests, so the node acquiring the lock does not need to update the resource group information from disk. This improves performance in some cases.
					</li></ul></div><p>
				Filesystems created with the new format version will not be able to be mounted under earlier RHEL versions and older versions of the <code class="literal">fsck.gfs2</code> utility will not be able to check them.
			</p><p>
				Users can create a file system with the older format version by running the <code class="literal">mkfs.gfs2</code> command with the option <code class="literal">-o format=1801</code>.
			</p><p>
				Users can upgrade the format version of an older file system running <code class="literal">tunegfs2 -r 1802 <span class="emphasis"><em>device</em></span></code> on an unmounted file system. Downgrading the format version is not supported.
			</p></section><section class="section" id="con_basic-gfs2-parameters-planning-gfs2-deployment"><div class="titlepage"><div><div><h3 class="title">1.2. Key GFS2 parameters to determine</h3></div></div></div><p class="_abstract _abstract">
				There are a number of key GFS2 parameters you should plan for before you install and configure a GFS2 file system.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">GFS2 nodes</span></dt><dd>
							Determine which nodes in the cluster will mount the GFS2 file systems.
						</dd><dt><span class="term">Number of file systems</span></dt><dd>
							Determine how many GFS2 file systems to create initially. More file systems can be added later.
						</dd><dt><span class="term">File system name</span></dt><dd>
							Each GFS2 file system should have a unique name. This name is usually the same as the LVM logical volume name and is used as the DLM lock table name when a GFS2 file system is mounted. For example, this guide uses file system names <code class="literal">mydata1</code> and <code class="literal">mydata2</code> in some example procedures.
						</dd><dt><span class="term">Journals</span></dt><dd>
							Determine the number of journals for your GFS2 file systems. GFS2 requires one journal for each node in the cluster that needs to mount the file system. For example, if you have a 16-node cluster but need to mount only the file system from two nodes, you need only two journals. GFS2 allows you to add journals dynamically at a later point with the <code class="literal">gfs2_jadd</code> utility as additional servers mount a file system.
						</dd><dt><span class="term">Storage devices and partitions</span></dt><dd>
							Determine the storage devices and partitions to be used for creating logical volumes (using <code class="literal">lvmlockd</code>) in the file systems.
						</dd><dt><span class="term">Time protocol</span></dt><dd><p class="simpara">
							Make sure that the clocks on the GFS2 nodes are synchronized. It is recommended that you use the Precision Time Protocol (PTP) or, if necessary for your configuration, the Network Time Protocol (NTP) software provided with your Red Hat Enterprise Linux distribution.
						</p><p class="simpara">
							The system clocks in GFS2 nodes must be within a few minutes of each other to prevent unnecessary inode time stamp updating. Unnecessary inode time stamp updating severely impacts cluster performance.
						</p></dd></dl></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					You may see performance problems with GFS2 when many create and delete operations are issued from more than one node in the same directory at the same time. If this causes performance problems in your system, you should localize file creation and deletions by a node to directories specific to that node as much as possible.
				</p></div></rh-alert></section><section class="section" id="con_gfs2-support-limits-planning-gfs2-deployment"><div class="titlepage"><div><div><h3 class="title">1.3. GFS2 support considerations</h3></div></div></div><p>
				To be eligible for support from Red Hat for a cluster running a GFS2 file system, you must take into account the support policies for GFS2 file systems.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					For full information about Red Hat’s support policies, requirements, and limitations for RHEL High Availability clusters, see <a class="link" href="https://access.redhat.com/articles/2912891/">Support Policies for RHEL High Availability Clusters</a>.
				</p></div></rh-alert><section class="section" id="maximum_file_system_and_cluster_size"><div class="titlepage"><div><div><h4 class="title">1.3.1. Maximum file system and cluster size</h4></div></div></div><p>
					The following table summarizes the current maximum file system size and number of nodes that GFS2 supports.
				</p><rh-table id="tb-table-gfs2-max"><table class="lt-4-cols lt-7-rows"><caption>Table 1.1. GFS2 Support Limits</caption><colgroup><col style="width: 50%; " class="col_1"><!--Empty--><col style="width: 50%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm139688932456128" scope="col">Parameter</th><th align="left" valign="top" id="idm139688929152064" scope="col">Maximum</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139688932456128"> <p>
									Number of nodes
								</p>
								 </td><td align="left" valign="top" headers="idm139688929152064"> <p>
									16 (x86, Power8 on PowerVM)
								</p>
								 <p>
									4 (s390x under z/VM)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688932456128"> <p>
									File system size
								</p>
								 </td><td align="left" valign="top" headers="idm139688929152064"> <p>
									100TB on all supported architectures
								</p>
								 </td></tr></tbody></table></rh-table><p>
					GFS2 is based on a 64-bit architecture, which can theoretically accommodate an 8 EB file system. If your system requires larger GFS2 file systems than are currently supported, contact your Red Hat service representative.
				</p><p>
					When determining the size of your file system, you should consider your recovery needs. Running the <code class="literal command">fsck.gfs2</code> command on a very large file system can take a long time and consume a large amount of memory. Additionally, in the event of a disk or disk subsystem failure, recovery time is limited by the speed of your backup media. For information about the amount of memory the <code class="literal command">fsck.gfs2</code> command requires, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_gfs2-filesystem-repair-configuring-gfs2-file-systems#proc_determining-needed-memory-for-fsckgfs2-gfs2-filesystem-repair">Determining required memory for running fsck.gfs2</a>.
				</p></section><section class="section" id="minimum_cluster_size"><div class="titlepage"><div><div><h4 class="title">1.3.2. Minimum cluster size</h4></div></div></div><p>
					Although a GFS2 file system can be implemented in a standalone system or as part of a cluster configuration, Red Hat does not support the use of GFS2 as a single-node file system, with the following exceptions:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Red Hat supports single-node GFS2 file systems for mounting snapshots of cluster file systems as might be needed, for example, for backup purposes.
						</li><li class="listitem"><p class="simpara">
							A single-node cluster mounting GFS2 file systems (which uses DLM) is supported for the purposes of a secondary-site Disaster Recovery (DR) node. This exception is for DR purposes only and not for transferring the main cluster workload to the secondary site.
						</p><p class="simpara">
							For example, copying off the data from the filesystem mounted on the secondary site while the primary site is offline is supported. However, migrating a workload from the primary site directly to a single-node cluster secondary site is unsupported. If the full work load needs to be migrated to the single-node secondary site then the secondary site must be the same size as the primary site.
						</p><p class="simpara">
							Red Hat recommends that when you mount a GFS2 file system in a single-node cluster you specify the <code class="literal">errors=panic</code> mount option so that the single-node cluster will panic when a GFS2 withdraw occurs since the single-node cluster will not be able to fence itself when encountering file system errors.
						</p></li></ul></div><p>
					Red Hat supports a number of high-performance single-node file systems that are optimized for single node and thus have generally lower overhead than a cluster file system. Red Hat recommends using these file systems in preference to GFS2 in cases where only a single node needs to mount the file system. For information about the file systems that Red Hat Enterprise Linux 9 supports, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_file_systems/index">Managing file systems</a>.
				</p></section><section class="section" id="shared_storage_considerations"><div class="titlepage"><div><div><h4 class="title">1.3.3. Shared storage considerations</h4></div></div></div><p>
					While a GFS2 file system may be used outside of LVM, Red Hat supports only GFS2 file systems that are created on a shared LVM logical volume.
				</p><p>
					When you configure a GFS2 file system as a cluster file system, you must ensure that all nodes in the cluster have access to the shared storage. Asymmetric cluster configurations in which some nodes have access to the shared storage and others do not are not supported. This does not require that all nodes actually mount the GFS2 file system itself.
				</p></section></section><section class="section" id="con_gfs2-formattiing-considerations-planning-gfs2-deployment"><div class="titlepage"><div><div><h3 class="title">1.4. GFS2 formatting considerations</h3></div></div></div><p class="_abstract _abstract">
				To format your GFS2 file system to optimize performance, you should take these recommendations into account.
			</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
					Make sure that your deployment of the Red Hat High Availability Add-On meets your needs and can be supported. Consult with an authorized Red Hat representative to verify your configuration prior to deployment.
				</p></div></rh-alert><h5 id="file_system_size_smaller_is_better">File System Size: Smaller Is Better</h5><p>
				GFS2 is based on a 64-bit architecture, which can theoretically accommodate an 8 EB file system. However, the current supported maximum size of a GFS2 file system for 64-bit hardware is 100TB.
			</p><p>
				Note that even though GFS2 large file systems are possible, that does not mean they are recommended. The rule of thumb with GFS2 is that smaller is better: it is better to have 10 1TB file systems than one 10TB file system.
			</p><p>
				There are several reasons why you should keep your GFS2 file systems small:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Less time is required to back up each file system.
					</li><li class="listitem">
						Less time is required if you need to check the file system with the <code class="literal command">fsck.gfs2</code> command.
					</li><li class="listitem">
						Less memory is required if you need to check the file system with the <code class="literal command">fsck.gfs2</code> command.
					</li></ul></div><p>
				In addition, fewer resource groups to maintain mean better performance.
			</p><p>
				Of course, if you make your GFS2 file system too small, you might run out of space, and that has its own consequences. You should consider your own use cases before deciding on a size.
			</p><h5 id="block_size_default_4k_blocks_are_preferred">Block Size: Default (4K) Blocks Are Preferred</h5><p>
				The <code class="literal command">mkfs.gfs2</code> command attempts to estimate an optimal block size based on device topology. In general, 4K blocks are the preferred block size because 4K is the default page size (memory) for Red Hat Enterprise Linux. Unlike some other file systems, GFS2 does most of its operations using 4K kernel buffers. If your block size is 4K, the kernel has to do less work to manipulate the buffers.
			</p><p>
				It is recommended that you use the default block size, which should yield the highest performance. You may need to use a different block size only if you require efficient storage of many very small files.
			</p><h5 id="journal_size_default_128mb_is_usually_optimal">Journal Size: Default (128MB) Is Usually Optimal</h5><p>
				When you run the <code class="literal command">mkfs.gfs2</code> command to create a GFS2 file system, you may specify the size of the journals. If you do not specify a size, it will default to 128MB, which should be optimal for most applications.
			</p><p>
				Some system administrators might think that 128MB is excessive and be tempted to reduce the size of the journal to the minimum of 8MB or a more conservative 32MB. While that might work, it can severely impact performance. Like many journaling file systems, every time GFS2 writes metadata, the metadata is committed to the journal before it is put into place. This ensures that if the system crashes or loses power, you will recover all of the metadata when the journal is automatically replayed at mount time. However, it does not take much file system activity to fill an 8MB journal, and when the journal is full, performance slows because GFS2 has to wait for writes to the storage.
			</p><p>
				It is generally recommended to use the default journal size of 128MB. If your file system is very small (for example, 5GB), having a 128MB journal might be impractical. If you have a larger file system and can afford the space, using 256MB journals might improve performance.
			</p><h5 id="size_and_number_of_resource_groups">Size and Number of Resource Groups</h5><p>
				When a GFS2 file system is created with the <code class="literal command">mkfs.gfs2</code> command, it divides the storage into uniform slices known as resource groups. It attempts to estimate an optimal resource group size (ranging from 32MB to 2GB). You can override the default with the <code class="literal command">-r</code> option of the <code class="literal command">mkfs.gfs2</code> command.
			</p><p>
				Your optimal resource group size depends on how you will use the file system. Consider how full it will be and whether or not it will be severely fragmented.
			</p><p>
				You should experiment with different resource group sizes to see which results in optimal performance. It is a best practice to experiment with a test cluster before deploying GFS2 into full production.
			</p><p>
				If your file system has too many resource groups, each of which is too small, block allocations can waste too much time searching tens of thousands of resource groups for a free block. The more full your file system, the more resource groups that will be searched, and every one of them requires a cluster-wide lock. This leads to slow performance.
			</p><p>
				If, however, your file system has too few resource groups, each of which is too big, block allocations might contend more often for the same resource group lock, which also impacts performance. For example, if you have a 10GB file system that is carved up into five resource groups of 2GB, the nodes in your cluster will fight over those five resource groups more often than if the same file system were carved into 320 resource groups of 32MB. The problem is exacerbated if your file system is nearly full because every block allocation might have to look through several resource groups before it finds one with a free block. GFS2 tries to mitigate this problem in two ways:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						First, when a resource group is completely full, it remembers that and tries to avoid checking it for future allocations until a block is freed from it. If you never delete files, contention will be less severe. However, if your application is constantly deleting blocks and allocating new blocks on a file system that is mostly full, contention will be very high and this will severely impact performance.
					</li><li class="listitem">
						Second, when new blocks are added to an existing file (for example, by appending) GFS2 will attempt to group the new blocks together in the same resource group as the file. This is done to increase performance: on a spinning disk, seek operations take less time when they are physically close together.
					</li></ul></div><p>
				The worst case scenario is when there is a central directory in which all the nodes create files because all of the nodes will constantly fight to lock the same resource group.
			</p></section><section class="section" id="con_gfs2-cluster-considerations-planning-gfs2-deployment"><div class="titlepage"><div><div><h3 class="title">1.5. Considerations for GFS2 in a cluster</h3></div></div></div><p class="_abstract _abstract">
				When determining the number of nodes that your system will contain, note that there is a trade-off between high availability and performance. With a larger number of nodes, it becomes increasingly difficult to make workloads scale. For that reason, Red Hat does not support using GFS2 for cluster file system deployments greater than 16 nodes.
			</p><p>
				Deploying a cluster file system is not a "drop in" replacement for a single node deployment. Red Hat recommends that you allow a period of around 8-12 weeks of testing on new installations in order to test the system and ensure that it is working at the required performance level. During this period, any performance or functional issues can be worked out and any queries should be directed to the Red Hat support team.
			</p><p>
				Red Hat recommends that customers considering deploying clusters have their configurations reviewed by Red Hat support before deployment to avoid any possible support issues later on.
			</p></section><section class="section" id="con_basic-gfs2-hardware-considerations-planning-gfs2-deployment"><div class="titlepage"><div><div><h3 class="title">1.6. Hardware considerations</h3></div></div></div><p class="_abstract _abstract">
				Take the following hardware considerations into account when deploying a GFS2 file system.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Use higher quality storage options
					</p><p class="simpara">
						GFS2 can operate on cheaper shared storage options, such as iSCSI or Fibre Channel over Ethernet (FCoE), but you will get better performance if you buy higher quality storage with larger caching capacity. Red Hat performs most quality, sanity, and performance tests on SAN storage with Fibre Channel interconnect. As a general rule, it is always better to deploy something that has been tested first.
					</p></li><li class="listitem"><p class="simpara">
						Test network equipment before deploying
					</p><p class="simpara">
						Higher quality, faster network equipment makes cluster communications and GFS2 run faster with better reliability. However, you do not have to purchase the most expensive hardware. Some of the most expensive network switches have problems passing multicast packets, which are used for passing <code class="literal">fcntl</code> locks (flocks), whereas cheaper commodity network switches are sometimes faster and more reliable. Red Hat recommends trying equipment before deploying it into full production.
					</p></li></ul></div></section></section><section class="chapter" id="assembly_gfs2-usage-considerations-configuring-gfs2-file-systems"><div class="titlepage"><div><div><h2 class="title">Chapter 2. Recommendations for GFS2 usage</h2></div></div></div><p class="_abstract _abstract">
			When deploying a GFS2 file system, there are a variety of general recommendations you should take into account.
		</p><section class="section" id="proc_configuring-atime-gfs2-usage-considerations"><div class="titlepage"><div><div><h3 class="title">2.1. Configuring <code class="literal">atime</code> updates</h3></div></div></div><p class="_abstract _abstract">
				Each file inode and directory inode has three time stamps associated with it:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal command">ctime</code> — The last time the inode status was changed
					</li><li class="listitem">
						<code class="literal command">mtime</code> — The last time the file (or directory) data was modified
					</li><li class="listitem">
						<code class="literal command">atime</code> — The last time the file (or directory) data was accessed
					</li></ul></div><p>
				If <code class="literal command">atime</code> updates are enabled as they are by default on GFS2 and other Linux file systems, then every time a file is read its inode needs to be updated.
			</p><p>
				Because few applications use the information provided by <code class="literal command">atime</code>, those updates can require a significant amount of unnecessary write traffic and file locking traffic. That traffic can degrade performance; therefore, it may be preferable to turn off or reduce the frequency of <code class="literal command">atime</code> updates.
			</p><p>
				The following methods of reducing the effects of <code class="literal command">atime</code> updating are available:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Mount with <code class="literal">relatime</code> (relative atime), which updates the <code class="literal command">atime</code> if the previous <code class="literal command">atime</code> update is older than the <code class="literal command">mtime</code> or <code class="literal command">ctime</code> update. This is the default mount option for GFS2 file systems.
					</li><li class="listitem">
						Mount with <code class="literal command">noatime</code> or <code class="literal command">nodiratime</code>. Mounting with <code class="literal command">noatime</code> disables <code class="literal command">atime</code> updates for both files and directories on that file system, while mounting with <code class="literal command">nodiratime</code> disables <code class="literal command">atime</code> updates only for directories on that file system, It is generally recommended that you mount GFS2 file systems with the <code class="literal">noatime</code> or <code class="literal">nodiratime</code> mount option whenever possible, with the preference for <code class="literal">noatime</code> where the application allows for this. For more information about the effect of these arguments on GFS2 file system performance, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_gfs2-performance-configuring-gfs2-file-systems#con_gfs2-node-locking-gfs2-performance">GFS2 Node Locking</a>.
					</li></ul></div><p>
				Use the following command to mount a GFS2 file system with the <code class="literal option">noatime</code> Linux mount option.
			</p><pre class="literallayout">mount <span class="emphasis"><em>BlockDevice</em></span> <span class="emphasis"><em>MountPoint</em></span> -o noatime</pre><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">BlockDevice</code></span></dt><dd>
							Specifies the block device where the GFS2 file system resides.
						</dd><dt><span class="term"><code class="literal">MountPoint</code></span></dt><dd>
							Specifies the directory where the GFS2 file system should be mounted.
						</dd></dl></div><p>
				In this example, the GFS2 file system resides on <code class="literal">/dev/vg01/lvol0</code> and is mounted on directory <code class="literal">/mygfs2</code> with <code class="literal command">atime</code> updates turned off.
			</p><pre class="literallayout"># <span class="strong strong"><strong>mount /dev/vg01/lvol0 /mygfs2 -o noatime</strong></span></pre></section><section class="section" id="con_vfs-tuning-options-gfs2-usage-considerations"><div class="titlepage"><div><div><h3 class="title">2.2. VFS tuning options: research and experiment</h3></div></div></div><p class="_abstract _abstract">
				Like all Linux file systems, GFS2 sits on top of a layer called the virtual file system (VFS). The VFS provides good defaults for the cache settings for most workloads and should not need changing in most cases. If, however, you have a workload that is not running efficiently (for example, cache is too large or too small) then you may be able to improve the performance by using the <code class="literal command">sysctl</code>(8) command to adjust the values of the <code class="literal">sysctl</code> files in the <code class="literal">/proc/sys/vm</code> directory. Documentation for these files can be found in the kernel source tree <code class="literal">Documentation/sysctl/vm.txt</code>.
			</p><p>
				For example, the values for <code class="literal">dirty_background_ratio</code> and <code class="literal">vfs_cache_pressure</code> may be adjusted depending on your situation. To fetch the current values, use the following commands:
			</p><pre class="literallayout"># <span class="strong strong"><strong>sysctl -n vm.dirty_background_ratio</strong></span>
# <span class="strong strong"><strong>sysctl -n vm.vfs_cache_pressure</strong></span></pre><p>
				The following commands adjust the values:
			</p><pre class="literallayout"># <span class="strong strong"><strong>sysctl -w vm.dirty_background_ratio=20</strong></span>
# <span class="strong strong"><strong>sysctl -w vm.vfs_cache_pressure=500</strong></span></pre><p>
				You can permanently change the values of these parameters by editing the <code class="literal">/etc/sysctl.conf</code> file.
			</p><p>
				To find the optimal values for your use cases, research the various VFS options and experiment on a test cluster before deploying into full production.
			</p></section><section class="section" id="con_selinux-on-gfs2-gfs2-usage-considerations"><div class="titlepage"><div><div><h3 class="title">2.3. SELinux on GFS2</h3></div></div></div><p class="_abstract _abstract">
				Use of Security Enhanced Linux (SELinux) with GFS2 incurs a small performance penalty. To avoid this overhead, you may choose not to use SELinux with GFS2 even on a system with SELinux in enforcing mode. When mounting a GFS2 file system, you can ensure that SELinux will not attempt to read the <code class="literal">seclabel</code> element on each file system object by using one of the <code class="literal">context</code> options as described on the <code class="literal">mount</code>(8) man page; SELinux will assume that all content in the file system is labeled with the <code class="literal">seclabel</code> element provided in the <code class="literal">context</code> mount options. This will also speed up processing as it avoids another disk read of the extended attribute block that could contain <code class="literal">seclabel</code> elements.
			</p><p>
				For example, on a system with SELinux in enforcing mode, you can use the following <code class="literal command">mount</code> command to mount the GFS2 file system if the file system is going to contain Apache content. This label will apply to the entire file system; it remains in memory and is not written to disk.
			</p><pre class="literallayout"># <span class="strong strong"><strong>mount -t gfs2 -o context=system_u:object_r:httpd_sys_content_t:s0 /dev/mapper/xyz/mnt/gfs2</strong></span></pre><p>
				If you are not sure whether the file system will contain Apache content, you can use the labels <code class="literal">public_content_rw_t</code> or <code class="literal">public_content_t</code>, or you could define a new label altogether and define a policy around it.
			</p><p>
				Note that in a Pacemaker cluster you should always use Pacemaker to manage a GFS2 file system. You can specify the mount options when you create a GFS2 file system resource.
			</p></section><section class="section" id="con_nfs-over-gfs2-gfs2-usage-considerations"><div class="titlepage"><div><div><h3 class="title">2.4. Setting up NFS over GFS2</h3></div></div></div><p class="_abstract _abstract">
				Due to the added complexity of the GFS2 locking subsystem and its clustered nature, setting up NFS over GFS2 requires taking many precautions.
			</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
					If the GFS2 file system is NFS exported, then you must mount the file system with the <code class="literal">localflocks</code> option. Because utilizing the <code class="literal">localflocks</code> option prevents you from safely accessing the GFS2 filesystem from multiple locations, and it is not viable to export GFS2 from multiple nodes simultaneously, it is a support requirement that the GFS2 file system be mounted on only one node at a time when using this configuration. The intended effect of this is to force POSIX locks from each server to be local: non-clustered, independent of each other. This is because a number of problems exist if GFS2 attempts to implement POSIX locks from NFS across the nodes of a cluster. For applications running on NFS clients, localized POSIX locks means that two clients can hold the same lock concurrently if the two clients are mounting from different servers, which could cause data corruption. If all clients mount NFS from one server, then the problem of separate servers granting the same locks independently goes away. If you are not sure whether to mount your file system with the <code class="literal">localflocks</code> option, you should not use the option. Contact Red Hat support immediately to discuss the appropriate configuration to avoid data loss. Exporting GFS2 via NFS, while technically supported in some circumstances, is not recommended.
				</p><p>
					For all other (non-NFS) GFS2 applications, do not mount your file system using <code class="literal">localflocks</code>, so that GFS2 will manage the POSIX locks and flocks between all the nodes in the cluster (on a cluster-wide basis). If you specify <code class="literal">localflocks</code> and do not use NFS, the other nodes in the cluster will not have knowledge of each other’s POSIX locks and flocks, thus making them unsafe in a clustered environment
				</p></div></rh-alert><p>
				In addition to the locking considerations, you should take the following into account when configuring an NFS service over a GFS2 file system.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Red Hat supports only Red Hat High Availability Add-On configurations using NFSv3 with locking in an active/passive configuration with the following characteristics. This configuration provides High Availability (HA) for the file system and reduces system downtime since a failed node does not result in the requirement to execute the <code class="literal command">fsck</code> command when failing the NFS server from one node to another.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								The back-end file system is a GFS2 file system running on a 2 to 16 node cluster.
							</li><li class="listitem">
								An NFSv3 server is defined as a service exporting the entire GFS2 file system from a single cluster node at a time.
							</li><li class="listitem">
								The NFS server can fail over from one cluster node to another (active/passive configuration).
							</li><li class="listitem">
								No access to the GFS2 file system is allowed <span class="emphasis"><em>except</em></span> through the NFS server. This includes both local GFS2 file system access as well as access through Samba or Clustered Samba. Accessing the file system locally via the cluster node from which it is mounted may result in data corruption.
							</li><li class="listitem">
								There is no NFS quota support on the system.
							</li></ul></div></li><li class="listitem">
						The <code class="literal">fsid=</code> NFS option is mandatory for NFS exports of GFS2.
					</li><li class="listitem">
						If problems arise with your cluster (for example, the cluster becomes inquorate and fencing is not successful), the clustered logical volumes and the GFS2 file system will be frozen and no access is possible until the cluster is quorate. You should consider this possibility when determining whether a simple failover solution such as the one defined in this procedure is the most appropriate for your system.
					</li></ul></div></section><section class="section" id="con_samba-over-gfs2-gfs2-usage-considerations"><div class="titlepage"><div><div><h3 class="title">2.5. Samba (SMB or Windows) file serving over GFS2</h3></div></div></div><p class="_abstract _abstract">
				You can use Samba (SMB or Windows) file serving from a GFS2 file system with CTDB, which allows active/active configurations.
			</p><p>
				Simultaneous access to the data in the Samba share from outside of Samba is not supported. There is currently no support for GFS2 cluster leases, which slows Samba file serving. For further information about support policies for Samba, see <a class="link" href="https://access.redhat.com/articles/3278591">Support Policies for RHEL Resilient Storage - ctdb General Policies</a> and <a class="link" href="https://access.redhat.com/articles/3252211">Support Policies for RHEL Resilient Storage - Exporting gfs2 contents via other protocols</a>.
			</p></section><section class="section" id="con_vms-for-gfs2-gfs2-usage-considerations"><div class="titlepage"><div><div><h3 class="title">2.6. Configuring virtual machines for GFS2</h3></div></div></div><p class="_abstract _abstract">
				When using a GFS2 file system with a virtual machine, it is important that your VM storage settings on each node be configured properly in order to force the cache off. For example, including these settings for <code class="literal">cache</code> and <code class="literal">io</code> in the <code class="literal">libvirt</code> domain should allow GFS2 to behave as expected.
			</p><pre class="literallayout">&lt;driver name='qemu' type='raw' cache='none' io='native'/&gt;</pre><p>
				Alternately, you can configure the <code class="literal">shareable</code> attribute within the device element. This indicates that the device is expected to be shared between domains (as long as hypervisor and OS support this). If <code class="literal">shareable</code> is used, <code class="literal">cache='no'</code> should be used for that device.
			</p></section><section class="section" id="con_gfs2-block-allocation-issues-gfs2-usage-considerations"><div class="titlepage"><div><div><h3 class="title">2.7. Block allocation</h3></div></div></div><p class="_abstract _abstract">
				Even though applications that only write data typically do not care how or where a block is allocated, some knowledge of how block allocation works can help you optimize performance.
			</p><section class="section" id="leave_free_space_in_the_file_system"><div class="titlepage"><div><div><h4 class="title">2.7.1. Leave free space in the file system</h4></div></div></div><p>
					When a GFS2 file system is nearly full, the block allocator starts to have a difficult time finding space for new blocks to be allocated. As a result, blocks given out by the allocator tend to be squeezed into the end of a resource group or in tiny slices where file fragmentation is much more likely. This file fragmentation can cause performance problems. In addition, when a GFS2 file system is nearly full, the GFS2 block allocator spends more time searching through multiple resource groups, and that adds lock contention that would not necessarily be there on a file system that has ample free space. This also can cause performance problems.
				</p><p>
					For these reasons, it is recommended that you not run a file system that is more than 85 percent full, although this figure may vary depending on workload.
				</p></section><section class="section" id="have_each_node_allocate_its_own_files_if_possible"><div class="titlepage"><div><div><h4 class="title">2.7.2. Have each node allocate its own files, if possible</h4></div></div></div><p>
					When developing applications for use with GFS2 file systems, it is recommended that you have each node allocate it own files, if possible. Due to the way the distributed lock manager (DLM) works, there will be more lock contention if all files are allocated by one node and other nodes need to add blocks to those files.
				</p><p>
					The term "lock master" has been used historically to denote a node which is currently the coordinator of lock requests, which originate locally or from a remote node in the cluster. This term for the lock request coordinator is slightly misleading because it is really a resource (in DLM terminology) in relation to which lock requests are either queued, granted or declined. In the sense in which the term is used in the DLM, it should be taken to refer to "first among equals", since the DLM is a peer-to-peer system.
				</p><p>
					In the Linux kernel DLM implementation, the node on which the lock is first used becomes the coordinator of lock requests, and after that point it does not change. This is an implementation detail of the Linux kernel DLM and not a property of DLMs in general. It is possible that a future update may allow the coordination of lock requests for a particular lock to move between nodes.
				</p><p>
					The location where lock requests are coordinated is transparent to the initiator of the lock request, except by the effect on the latency of the request. One consequence of the current implementation is that if there is an imbalance of the initial workload (for example, one node scans through the whole filesystem before others perform any I/O commands) this can result in higher lock latencies for other nodes in the cluster compared with the node that performed the initial scan of the filesystem.
				</p><p>
					As in many file systems, the GFS2 allocator tries to keep blocks in the same file close to one another to reduce the movement of disk heads and boost performance. A node that allocates blocks to a file will likely need to use and lock the same resource groups for the new blocks (unless all the blocks in that resource group are in use). The file system will run faster if the lock request coordinator for the resource group containing the file allocates its data blocks (it is faster to have the node that first opened the file do all the writing of new blocks).
				</p></section><section class="section" id="preallocate_if_possible"><div class="titlepage"><div><div><h4 class="title">2.7.3. Preallocate, if possible</h4></div></div></div><p>
					If files are preallocated, block allocations can be avoided altogether and the file system can run more efficiently. GFS2 includes the <code class="literal command">fallocate(1)</code> system call, which you can use to preallocate blocks of data.
				</p></section></section></section><section class="chapter" id="assembly_creating-mounting-gfs2-configuring-gfs2-file-systems"><div class="titlepage"><div><div><h2 class="title">Chapter 3. Administering GFS2 file systems</h2></div></div></div><p class="_abstract _abstract">
			There are a variety of commands and options that you use to create, mount, grow, and manage GFS2 file systems.
		</p><section class="section" id="proc_creating-gfs2-creating-mounting-gfs2"><div class="titlepage"><div><div><h3 class="title">3.1. GFS2 file system creation</h3></div></div></div><p class="_abstract _abstract">
				You create a GFS2 file system with the <code class="literal command">mkfs.gfs2</code> command. A file system is created on an activated LVM volume.
			</p><section class="section" id="the_gfs2_mkfs_command"><div class="titlepage"><div><div><h4 class="title">3.1.1. The GFS2 mkfs command</h4></div></div></div><p>
					The following information is required to run the <code class="literal command">mkfs.gfs2</code> command to create a clustered GFS2 file system:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Lock protocol/module name, which is <code class="literal">lock_dlm</code> for a cluster
						</li><li class="listitem">
							Cluster name
						</li><li class="listitem">
							Number of journals (one journal required for each node that may be mounting the file system)
						</li></ul></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						Once you have created a GFS2 file system with the <code class="literal command">mkfs.gfs2</code> command, you cannot decrease the size of the file system. You can, however, increase the size of an existing file system with the <code class="literal command">gfs2_grow</code> command.
					</p></div></rh-alert><p>
					The format for creating a clustered GFS2 file system is as follows. Note that Red Hat does not support the use of GFS2 as a single-node file system.
				</p><pre class="literallayout">mkfs.gfs2 -p lock_dlm -t <span class="emphasis"><em>ClusterName:FSName</em></span> -j <span class="emphasis"><em>NumberJournals</em></span> <span class="emphasis"><em>BlockDevice</em></span></pre><p>
					If you prefer, you can create a GFS2 file system by using the <code class="literal command">mkfs</code> command with the <code class="literal">-t</code> parameter specifying a file system of type <code class="literal">gfs2</code>, followed by the GFS2 file system options.
				</p><pre class="literallayout">mkfs -t gfs2 -p lock_dlm -t <span class="emphasis"><em>ClusterName:FSName</em></span> -j <span class="emphasis"><em>NumberJournals</em></span> <span class="emphasis"><em>BlockDevice</em></span></pre><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
						Improperly specifying the <span class="emphasis"><em>ClusterName:FSName</em></span> parameter may cause file system or lock space corruption.
					</p></div></rh-alert><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">ClusterName</code></span></dt><dd>
								The name of the cluster for which the GFS2 file system is being created.
							</dd><dt><span class="term"><code class="literal">FSName</code></span></dt><dd>
								The file system name, which can be 1 to 16 characters long. The name must be unique for all <code class="literal">lock_dlm</code> file systems over the cluster.
							</dd><dt><span class="term"><code class="literal">NumberJournals</code></span></dt><dd>
								Specifies the number of journals to be created by the <code class="literal command">mkfs.gfs2</code> command. One journal is required for each node that mounts the file system. For GFS2 file systems, more journals can be added later without growing the file system.
							</dd><dt><span class="term"><code class="literal">BlockDevice</code></span></dt><dd>
								Specifies a logical or other block device
							</dd></dl></div><p>
					The following table describes the <code class="literal command">mkfs.gfs2</code> command options (flags and parameters).
				</p><rh-table id="tb-table-gfs2-mkfs"><table class="lt-4-cols lt-7-rows"><caption>Table 3.1. Command Options: mkfs.gfs2</caption><colgroup><col style="width: 33%; " class="col_1"><!--Empty--><col style="width: 33%; " class="col_2"><!--Empty--><col style="width: 33%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm139688937186224" scope="col">Flag</th><th align="left" valign="top" id="idm139688937185136" scope="col">Parameter</th><th align="left" valign="top" id="idm139688930182128" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139688937186224"> <p>
									<code class="literal option">-c</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688937185136"> <p>
									<code class="literal">Megabytes</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688930182128"> <p>
									Sets the initial size of each journal’s quota change file to <code class="literal">Megabytes</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688937186224"> <p>
									<code class="literal option">-D</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688937185136"> </td><td align="left" valign="top" headers="idm139688930182128"> <p>
									Enables debugging output.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688937186224"> <p>
									<code class="literal option">-h</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688937185136"> </td><td align="left" valign="top" headers="idm139688930182128"> <p>
									Help. Displays available options.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688937186224"> <p>
									<code class="literal option">-J</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688937185136"> <p>
									<code class="literal">Megabytes</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688930182128"> <p>
									Specifies the size of the journal in megabytes. Default journal size is 128 megabytes. The minimum size is 8 megabytes. Larger journals improve performance, although they use more memory than smaller journals.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688937186224"> <p>
									<code class="literal option">-j</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688937185136"> <p>
									<code class="literal">Number</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688930182128"> <p>
									Specifies the number of journals to be created by the <code class="literal command">mkfs.gfs2</code> command. One journal is required for each node that mounts the file system. If this option is not specified, one journal will be created. For GFS2 file systems, you can add additional journals at a later time without growing the file system.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688937186224"> <p>
									<code class="literal option">-O</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688937185136"> </td><td align="left" valign="top" headers="idm139688930182128"> <p>
									Prevents the <code class="literal command">mkfs.gfs2</code> command from asking for confirmation before writing the file system.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688937186224"> <p>
									<code class="literal option">-p</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688937185136"> <p>
									<code class="literal">LockProtoName</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688930182128"> <p>
									* Specifies the name of the locking protocol to use. Recognized locking protocols include:
								</p>
								 <p>
									* <code class="literal">lock_dlm</code> — The standard locking module, required for a clustered file system.
								</p>
								 <p>
									* <code class="literal">lock_nolock</code> — Used when GFS2 is acting as a local file system (one node only). Red Hat does not support the use of GFS2 as a single-node file system in a production environment. <code class="literal">lock_nolock</code> should be used only for the purposes of backup or for a secondary-site Disaster Recovery node, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/configuring_gfs2_file_systems/index#minimum_cluster_size">Minimum cluster size</a>. When using <code class="literal">lock_nolock</code>, you must ensure that the GFS2 file system is being used by only one system at a time.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688937186224"> <p>
									<code class="literal option">-q</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688937185136"> </td><td align="left" valign="top" headers="idm139688930182128"> <p>
									Quiet. Do not display anything.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688937186224"> <p>
									<code class="literal option">-r</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688937185136"> <p>
									<code class="literal">Megabytes</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688930182128"> <p>
									Specifies the size of the resource groups in megabytes. The minimum resource group size is 32 megabytes. The maximum resource group size is 2048 megabytes. A large resource group size may increase performance on very large file systems. If this is not specified, <code class="literal command">mkfs.gfs2</code> chooses the resource group size based on the size of the file system: average size file systems will have 256 megabyte resource groups, and bigger file systems will have bigger resource groups for better performance.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688937186224"> <p>
									<code class="literal option">-t</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688937185136"> <p>
									<code class="literal">LockTableName</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688930182128"> <p>
									* A unique identifier that specifies the lock table field when you use the <code class="literal">lock_dlm</code> protocol; the <code class="literal">lock_nolock</code> protocol does not use this parameter.
								</p>
								 <p>
									* This parameter has two parts separated by a colon (no spaces) as follows: <code class="literal">ClusterName:FSName</code>.
								</p>
								 <p>
									* <code class="literal">ClusterName</code> is the name of the cluster for which the GFS2 file system is being created; only members of this cluster are permitted to use this file system.
								</p>
								 <p>
									* <code class="literal">FSName</code>, the file system name, can be 1 to 16 characters in length, and the name must be unique among all file systems in the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688937186224"> <p>
									<code class="literal option">-V</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688937185136"> </td><td align="left" valign="top" headers="idm139688930182128"> <p>
									Displays command version information.
								</p>
								 </td></tr></tbody></table></rh-table></section><section class="section" id="creating_a_gfs2_file_system"><div class="titlepage"><div><div><h4 class="title">3.1.2. Creating a GFS2 file system</h4></div></div></div><p>
					The following example creates two GFS2 file systems. For both of these file systems, lock_dlm` is the locking protocol that the file system uses, since this is a clustered file system. Both file systems can be used in the cluster named <code class="literal">alpha</code>.
				</p><p>
					For the first file system, file system name is <code class="literal">mydata1</code>. it contains eight journals and is created on <code class="literal">/dev/vg01/lvol0</code>. For the second file system, the file system name is <code class="literal">mydata2</code>. It contains eight journals and is created on <code class="literal">/dev/vg01/lvol1</code>.
				</p><pre class="literallayout"># <span class="strong strong"><strong>mkfs.gfs2 -p lock_dlm -t alpha:mydata1 -j 8 /dev/vg01/lvol0</strong></span>
# <span class="strong strong"><strong>mkfs.gfs2 -p lock_dlm -t alpha:mydata2 -j 8 /dev/vg01/lvol1</strong></span></pre></section></section><section class="section" id="proc_mounting-gfs2-filesystem_creating-mounting-gfs2"><div class="titlepage"><div><div><h3 class="title">3.2. Mounting a GFS2 file system</h3></div></div></div><p class="_abstract _abstract">
				Before you can mount a GFS2 file system, the file system must exist, the volume where the file system exists must be activated, and the supporting clustering and locking systems must be started. After those requirements have been met, you can mount the GFS2 file system as you would any Linux file system.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					You should always use Pacemaker to manage the GFS2 file system in a production environment rather than manually mounting the file system with a <code class="literal command">mount</code> command, as this may cause issues at system shutdown.
				</p></div></rh-alert><p>
				To manipulate file ACLs, you must mount the file system with the <code class="literal command">-o acl</code> mount option. If a file system is mounted without the <code class="literal command">-o acl</code> mount option, users are allowed to view ACLs (with <code class="literal command">getfacl</code>), but are not allowed to set them (with <code class="literal command">setfacl</code>).
			</p><section class="section" id="mounting_a_gfs2_file_system_with_no_options_specified"><div class="titlepage"><div><div><h4 class="title">3.2.1. Mounting a GFS2 file system with no options specified</h4></div></div></div><p>
					In this example, the GFS2 file system on <code class="literal">/dev/vg01/lvol0</code> is mounted on the <code class="literal">/mygfs2</code> directory.
				</p><pre class="literallayout"># <span class="strong strong"><strong>mount /dev/vg01/lvol0 /mygfs2</strong></span></pre></section><section class="section" id="mounting_a_gfs2_file_system_that_specifies_mount_options"><div class="titlepage"><div><div><h4 class="title">3.2.2. Mounting a GFS2 file system that specifies mount options</h4></div></div></div><p>
					The following is the format for the command to mount a GFS2 file system that specifies mount options.
				</p><pre class="literallayout">mount <span class="emphasis"><em>BlockDevice</em></span> <span class="emphasis"><em>MountPoint</em></span> -o <span class="emphasis"><em>option</em></span></pre><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">BlockDevice</code></span></dt><dd>
								Specifies the block device where the GFS2 file system resides.
							</dd><dt><span class="term"><code class="literal">MountPoint</code></span></dt><dd>
								Specifies the directory where the GFS2 file system should be mounted.
							</dd></dl></div><p>
					The <code class="literal option">-o option</code> argument consists of GFS2-specific options or acceptable standard Linux <code class="literal command">mount -o</code> options, or a combination of both. Multiple <code class="literal">option</code> parameters are separated by a comma and no spaces.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						The <code class="literal command">mount</code> command is a Linux system command. In addition to using these GFS2-specific options, you can use other, standard, <code class="literal command">mount</code> command options (for example, <code class="literal option">-r</code>). For information about other Linux <code class="literal command">mount</code> command options, see the Linux <code class="literal command">mount</code> man page.
					</p></div></rh-alert><p>
					The following table describes the available GFS2-specific <code class="literal option">-o option</code> values that can be passed to GFS2 at mount time.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						This table includes descriptions of options that are used with local file systems only. Note, however, that Red Hat does not support the use of GFS2 as a single-node file system. Red Hat will continue to support single-node GFS2 file systems for mounting snapshots of cluster file systems (for example, for backup purposes).
					</p></div></rh-alert><rh-table id="tb-table-gfs2-mount"><table class="lt-4-cols lt-7-rows"><caption>Table 3.2. GFS2-Specific Mount Options</caption><colgroup><col style="width: 50%; " class="col_1"><!--Empty--><col style="width: 50%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm139688951536048" scope="col">Option</th><th align="left" valign="top" id="idm139688951534960" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139688951536048"> <p>
									<code class="literal">acl</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688951534960"> <p>
									Allows manipulating file ACLs. If a file system is mounted without the <code class="literal command">acl</code> mount option, users are allowed to view ACLs (with <code class="literal command">getfacl</code>), but are not allowed to set them (with <code class="literal command">setfacl</code>).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688951536048"> <p>
									<code class="literal">data=[ordered|writeback]</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688951534960"> <p>
									When <code class="literal">data=ordered</code> is set, the user data modified by a transaction is flushed to the disk before the transaction is committed to disk. This should prevent the user from seeing uninitialized blocks in a file after a crash. When <code class="literal">data=writeback</code> mode is set, the user data is written to the disk at any time after it is dirtied; this does not provide the same consistency guarantee as <code class="literal">ordered</code> mode, but it should be slightly faster for some workloads. The default value is <code class="literal">ordered</code> mode.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688951536048"> <p>
									<code class="literal">ignore_local_fs</code>
								</p>
								 <p>
									<code class="literal">Caution:</code> This option should not be used when GFS2 file systems are shared.
								</p>
								 </td><td align="left" valign="top" headers="idm139688951534960"> <p>
									Forces GFS2 to treat the file system as a multi-host file system. By default, using <code class="literal">lock_nolock</code> automatically turns on the <code class="literal">localflocks</code> flag.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688951536048"> <p>
									<code class="literal">localflocks</code>
								</p>
								 <p>
									<code class="literal">Caution:</code> This option should not be used when GFS2 file systems are shared.
								</p>
								 </td><td align="left" valign="top" headers="idm139688951534960"> <p>
									Tells GFS2 to let the VFS (virtual file system) layer do all flock and fcntl. The <code class="literal">localflocks</code> flag is automatically turned on by <code class="literal">lock_nolock</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688951536048"> <p>
									<code class="literal">lockproto=</code><code class="literal">LockModuleName</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688951534960"> <p>
									Allows the user to specify which locking protocol to use with the file system. If <code class="literal">LockModuleName</code> is not specified, the locking protocol name is read from the file system superblock.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688951536048"> <p>
									<code class="literal">locktable=</code><code class="literal">LockTableName</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688951534960"> <p>
									Allows the user to specify which locking table to use with the file system.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688951536048"> <p>
									<code class="literal">quota=[off/account/on]</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688951534960"> <p>
									Turns quotas on or off for a file system. Setting the quotas to be in the <code class="literal">account</code> state causes the per UID/GID usage statistics to be correctly maintained by the file system; limit and warn values are ignored. The default value is <code class="literal">off</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688951536048"> <p>
									<code class="literal command">errors=panic|withdraw</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688951534960"> <p>
									When <code class="literal">errors=panic</code> is specified, file system errors will cause a kernel panic. When <code class="literal">errors=withdraw</code> is specified, which is the default behavior, file system errors will cause the system to withdraw from the file system and make it inaccessible until the next reboot; in some cases the system may remain running.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688951536048"> <p>
									<code class="literal">discard/nodiscard</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688951534960"> <p>
									Causes GFS2 to generate "discard" I/O requests for blocks that have been freed. These can be used by suitable hardware to implement thin provisioning and similar schemes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688951536048"> <p>
									<code class="literal">barrier/nobarrier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688951534960"> <p>
									Causes GFS2 to send I/O barriers when flushing the journal. The default value is <code class="literal">on</code>. This option is automatically turned <code class="literal">off</code> if the underlying device does not support I/O barriers. Use of I/O barriers with GFS2 is highly recommended at all times unless the block device is designed so that it cannot lose its write cache content (for example, if it is on a UPS or it does not have a write cache).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688951536048"> <p>
									<code class="literal">quota_quantum=<span class="emphasis"><em>secs</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688951534960"> <p>
									Sets the number of seconds for which a change in the quota information may sit on one node before being written to the quota file. This is the preferred way to set this parameter. The value is an integer number of seconds greater than zero. The default is 60 seconds. Shorter settings result in faster updates of the lazy quota information and less likelihood of someone exceeding their quota. Longer settings make file system operations involving quotas faster and more efficient.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688951536048"> <p>
									<code class="literal">statfs_quantum=<span class="emphasis"><em>secs</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688951534960"> <p>
									Setting <code class="literal">statfs_quantum</code> to 0 is the preferred way to set the slow version of <code class="literal command">statfs</code>. The default value is 30 secs which sets the maximum time period before <code class="literal">statfs</code> changes will be synced to the master <code class="literal">statfs</code> file. This can be adjusted to allow for faster, less accurate <code class="literal">statfs</code> values or slower more accurate values. When this option is set to 0, <code class="literal">statfs</code> will always report the true values.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139688951536048"> <p>
									<code class="literal">statfs_percent=<span class="emphasis"><em>value</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm139688951534960"> <p>
									Provides a bound on the maximum percentage change in the <code class="literal">statfs</code> information about a local basis before it is synced back to the master <code class="literal">statfs</code> file, even if the time period has not expired. If the setting of <code class="literal">statfs_quantum</code> is 0, then this setting is ignored.
								</p>
								 </td></tr></tbody></table></rh-table></section><section class="section" id="unmounting_a_gfs2_file_system"><div class="titlepage"><div><div><h4 class="title">3.2.3. Unmounting a GFS2 file system</h4></div></div></div><p>
					GFS2 file systems that have been mounted manually rather than automatically through Pacemaker will not be known to the system when file systems are unmounted at system shutdown. As a result, the GFS2 resource agent will not unmount the GFS2 file system. After the GFS2 resource agent is shut down, the standard shutdown process kills off all remaining user processes, including the cluster infrastructure, and tries to unmount the file system. This unmount will fail without the cluster infrastructure and the system will hang.
				</p><p>
					To prevent the system from hanging when the GFS2 file systems are unmounted, you should do one of the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Always use Pacemaker to manage the GFS2 file system.
						</li><li class="listitem">
							If a GFS2 file system has been mounted manually with the <code class="literal command">mount</code> command, be sure to unmount the file system manually with the <code class="literal command">umount</code> command before rebooting or shutting down the system.
						</li></ul></div><p>
					If your file system hangs while it is being unmounted during system shutdown under these circumstances, perform a hardware reboot. It is unlikely that any data will be lost since the file system is synced earlier in the shutdown process.
				</p><p>
					The GFS2 file system can be unmounted the same way as any Linux file system, by using the <code class="literal command">umount</code> command.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						The <code class="literal command">umount</code> command is a Linux system command. Information about this command can be found in the Linux <code class="literal command">umount</code> command man pages.
					</p></div></rh-alert><p>
					Usage
				</p><pre class="literallayout">umount <span class="emphasis"><em>MountPoint</em></span></pre><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">MountPoint</code></span></dt><dd>
								Specifies the directory where the GFS2 file system is currently mounted.
							</dd></dl></div></section></section><section class="section" id="proc_backing-up-a-gfs2-filesystem-creating-mounting-gfs2"><div class="titlepage"><div><div><h3 class="title">3.3. Backing up a GFS2 file system</h3></div></div></div><p class="_abstract _abstract">
				It is important to make regular backups of your GFS2 file system in case of emergency, regardless of the size of your file system. Many system administrators feel safe because they are protected by RAID, multipath, mirroring, snapshots, and other forms of redundancy, but there is no such thing as safe enough.
			</p><p>
				It can be a problem to create a backup since the process of backing up a node or set of nodes usually involves reading the entire file system in sequence. If this is done from a single node, that node will retain all the information in cache until other nodes in the cluster start requesting locks. Running this type of backup program while the cluster is in operation will negatively impact performance.
			</p><p>
				Dropping the caches once the backup is complete reduces the time required by other nodes to regain ownership of their cluster locks and caches. This is still not ideal, however, because the other nodes will have stopped caching the data that they were caching before the backup process began. You can drop caches using the following command after the backup is complete:
			</p><pre class="literallayout">echo -n 3 &gt; /proc/sys/vm/drop_caches</pre><p>
				It is faster if each node in the cluster backs up its own files so that the task is split between the nodes. You might be able to accomplish this with a script that uses the <code class="literal command">rsync</code> command on node-specific directories.
			</p><p>
				Red Hat recommends making a GFS2 backup by creating a hardware snapshot on the SAN, presenting the snapshot to another system, and backing it up there. The backup system should mount the snapshot with <code class="literal command">-o lockproto=lock_nolock</code> since it will not be in a cluster. Note, however, that Red Hat does not support the use of GFS2 as a single-node file system in a production environment. This option should be used only for the purposes of backup or for a secondary-site Disaster Recovery node, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/configuring_gfs2_file_systems/index#minimum_cluster_size">Minimum cluster size</a>. When using this option, you must ensure that the GFS2 file system is being used by only one system at a time.
			</p></section><section class="section" id="proc_suspending-activity-on-a-gfs2-filesystem-creating-mounting-gfs2"><div class="titlepage"><div><div><h3 class="title">3.4. Suspending activity on a GFS2 file system</h3></div></div></div><p class="_abstract _abstract">
				You can suspend write activity to a file system by using the <code class="literal command">dmsetup suspend</code> command. Suspending write activity allows hardware-based device snapshots to be used to capture the file system in a consistent state. The <code class="literal command">dmsetup resume</code> command ends the suspension.
			</p><p>
				The format for the command to suspend activity on a GFS2 file system is as follows.
			</p><pre class="literallayout">dmsetup suspend <span class="emphasis"><em>MountPoint</em></span></pre><p>
				This example suspends writes to file system <code class="literal">/mygfs2</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>dmsetup suspend /mygfs2</strong></span></pre><p>
				The format for the command to end suspension of activity on a GFS2 file system is as follows.
			</p><pre class="literallayout">dmsetup resume <span class="emphasis"><em>MountPoint</em></span></pre><p>
				This example ends suspension of writes to file system <code class="literal">/mygfs2</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>dmsetup resume /mygfs2</strong></span></pre></section><section class="section" id="proc_growing-gfs2-filesystem-creating-mounting-gfs2"><div class="titlepage"><div><div><h3 class="title">3.5. Growing a GFS2 file system</h3></div></div></div><p class="_abstract _abstract">
				The <code class="literal command">gfs2_grow</code> command is used to expand a GFS2 file system after the device where the file system resides has been expanded. Running the <code class="literal command">gfs2_grow</code> command on an existing GFS2 file system fills all spare space between the current end of the file system and the end of the device with a newly initialized GFS2 file system extension. All nodes in the cluster can then use the extra storage space that has been added.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					You cannot decrease the size of a GFS2 file system.
				</p></div></rh-alert><p>
				The <code class="literal command">gfs2_grow</code> command must be run on a mounted file system. The following procedure increases the size of the GFS2 file system in a cluster that is mounted on the logical volume <code class="literal">shared_vg/shared_lv1</code> with a mount point of <code class="literal">/mnt/gfs2</code>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Perform a backup of the data on the file system.
					</li><li class="listitem"><p class="simpara">
						If you do not know the logical volume that is used by the file system to be expanded, you can determine this by running the <code class="literal command">df <span class="emphasis"><em>mountpoint</em></span></code> command. This will display the device name in the following format:
					</p><p class="simpara">
						<code class="literal">/dev/mapper/<span class="emphasis"><em>vg</em></span>-<span class="emphasis"><em>lv</em></span></code>
					</p><p class="simpara">
						For example, the device name <code class="literal">/dev/mapper/shared_vg-shared_lv1</code> indicates that the logical volume is <code class="literal">shared_vg/shared_lv1</code>.
					</p></li><li class="listitem"><p class="simpara">
						On one node of the cluster, expand the underlying cluster volume with the <code class="literal command">lvextend</code> command.
					</p><pre class="literallayout"># <span class="strong strong"><strong>lvextend -L+1G shared_vg/shared_lv1</strong></span>
Size of logical volume shared_vg/shared_lv1 changed from 5.00 GiB (1280 extents) to 6.00 GiB (1536 extents).
WARNING: extending LV with a shared lock, other hosts may require LV refresh.
Logical volume shared_vg/shared_lv1 successfully resized.</pre></li><li class="listitem"><p class="simpara">
						One one node of the cluster, increase the size of the GFS2 file system. Do not extend the file system if the logical volume was not refreshed on all of the nodes, otherwise the file system data may become unavailable throughout the cluster.
					</p><pre class="literallayout"># <span class="strong strong"><strong>gfs2_grow /mnt/gfs2</strong></span>
FS: Mount point:             /mnt/gfs2
FS: Device:                  /dev/mapper/shared_vg-shared_lv1
FS: Size:                    1310719 (0x13ffff)
DEV: Length:                 1572864 (0x180000)
The file system will grow by 1024MB.
gfs2_grow complete.</pre></li><li class="listitem"><p class="simpara">
						Run the <code class="literal command">df</code> command on all nodes to check that the new space is now available in the file system. Note that it may take up to 30 seconds for the <code class="literal command">df</code> command on all nodes to show the same file system size
					</p><pre class="literallayout"># <span class="strong strong"><strong>df -h /mnt/gfs2</strong></span>]
Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/shared_vg-shared_lv1  6.0G  4.5G  1.6G  75% /mnt/gfs2</pre></li></ol></div></section><section class="section" id="proc_adding-gfs2-journal-creating-mounting-gfs2"><div class="titlepage"><div><div><h3 class="title">3.6. Adding journals to a GFS2 file system</h3></div></div></div><p class="_abstract _abstract">
				GFS2 requires one journal for each node in a cluster that needs to mount the file system. If you add additional nodes to the cluster, you can add journals to a GFS2 file system with the <code class="literal command">gfs2_jadd</code> command. You can add journals to a GFS2 file system dynamically at any point without expanding the underlying logical volume. The <code class="literal command">gfs2_jadd</code> command must be run on a mounted file system, but it needs to be run on only one node in the cluster. All the other nodes sense that the expansion has occurred.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					If a GFS2 file system is full, the <code class="literal command">gfs2_jadd</code> command will fail, even if the logical volume containing the file system has been extended and is larger than the file system. This is because in a GFS2 file system, journals are plain files rather than embedded metadata, so simply extending the underlying logical volume will not provide space for the journals.
				</p></div></rh-alert><p>
				Before adding journals to a GFS2 file system, you can find out how many journals the GFS2 file system currently contains with the <code class="literal command">gfs2_edit -p jindex</code> command, as in the following example:
			</p><pre class="literallayout"># <span class="strong strong"><strong>gfs2_edit -p jindex /dev/sasdrives/scratch|grep journal</strong></span>
   3/3 [fc7745eb] 4/25 (0x4/0x19): File    journal0
   4/4 [8b70757d] 5/32859 (0x5/0x805b): File    journal1
   5/5 [127924c7] 6/65701 (0x6/0x100a5): File    journal2</pre><p>
				The format for the basic command to add journals to a GFS2 file system is as follows.
			</p><pre class="literallayout">gfs2_jadd -j <span class="emphasis"><em>Number</em></span> <span class="emphasis"><em>MountPoint</em></span></pre><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">Number</code></span></dt><dd>
							Specifies the number of new journals to be added.
						</dd><dt><span class="term"><code class="literal">MountPoint</code></span></dt><dd>
							Specifies the directory where the GFS2 file system is mounted.
						</dd></dl></div><p>
				In this example, one journal is added to the file system on the <code class="literal">/mygfs2</code> directory.
			</p><pre class="literallayout"># <span class="strong strong"><strong>gfs2_jadd -j 1 /mygfs2</strong></span></pre></section></section><section class="chapter" id="assembly_gfs2-disk-quota-administration-configuring-gfs2-file-systems"><div class="titlepage"><div><div><h2 class="title">Chapter 4. GFS2 quota management</h2></div></div></div><p class="_abstract _abstract">
			File system quotas are used to limit the amount of file system space a user or group can use. A user or group does not have a quota limit until one is set. When a GFS2 file system is mounted with the <code class="literal">quota=on</code> or <code class="literal">quota=account</code> option, GFS2 keeps track of the space used by each user and group even when there are no limits in place. GFS2 updates quota information in a transactional way so system crashes do not require quota usages to be reconstructed. To prevent a performance slowdown, a GFS2 node synchronizes updates to the quota file only periodically. The fuzzy quota accounting can allow users or groups to slightly exceed the set limit. To minimize this, GFS2 dynamically reduces the synchronization period as a hard quota limit is approached.
		</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
				GFS2 supports the standard Linux quota facilities. In order to use this you will need to install the <span class="strong strong"><strong><span class="application application">quota</span></strong></span> RPM. This is the preferred way to administer quotas on GFS2 and should be used for all new deployments of GFS2 using quotas.
			</p></div></rh-alert><p>
			For more information about disk quotas, see the <code class="literal command">man</code> pages of the following commands:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<code class="literal command">quotacheck</code>
				</li><li class="listitem">
					<code class="literal command">edquota</code>
				</li><li class="listitem">
					<code class="literal command">repquota</code>
				</li><li class="listitem">
					<code class="literal command">quota</code>
				</li></ul></div><section class="section" id="proc_configuring-gfs2-disk-quotas-gfs2-disk-quota-administration"><div class="titlepage"><div><div><h3 class="title">4.1. Configuring GFS2 disk quotas</h3></div></div></div><p class="_abstract _abstract">
				To implement disk quotas for GFS2 file systems, there are three steps to perform.
			</p><p>
				The steps to perform to implement disk quotas are as follows:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Set up quotas in enforcement or accounting mode.
					</li><li class="listitem">
						Initialize the quota database file with current block usage information.
					</li><li class="listitem">
						Assign quota policies. (In accounting mode, these policies are not enforced.)
					</li></ol></div><p>
				Each of these steps is discussed in detail in the following sections.
			</p><section class="section" id="setting_up_quotas_in_enforcement_or_accounting_mode"><div class="titlepage"><div><div><h4 class="title">4.1.1. Setting up quotas in enforcement or accounting mode</h4></div></div></div><p>
					In GFS2 file systems, quotas are disabled by default. To enable quotas for a file system, mount the file system with the <code class="literal">quota=on</code> option specified.
				</p><p>
					To mount a file system with quotas enabled, specify <code class="literal">quota=on</code> for the <code class="literal">options</code> argument when creating the GFS2 file system resource in a cluster. For example, the following command specifies that the GFS2 <code class="literal">Filesystem</code> resource being created will be mounted with quotas enabled.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create gfs2mount Filesystem options="quota=on" device=BLOCKDEVICE directory=MOUNTPOINT fstype=gfs2 clone</strong></span></pre><p>
					It is possible to keep track of disk usage and maintain quota accounting for every user and group without enforcing the limit and warn values. To do this, mount the file system with the <code class="literal">quota=account</code> option specified.
				</p><p>
					To mount a file system with quotas disabled, specify <code class="literal">quota=off</code> for the <code class="literal">options</code> argument when creating the GFS2 file system resource in a cluster.
				</p></section><section class="section" id="creating_the_quota_database_files"><div class="titlepage"><div><div><h4 class="title">4.1.2. Creating the quota database files</h4></div></div></div><p>
					After each quota-enabled file system is mounted, the system is capable of working with disk quotas. However, the file system itself is not yet ready to support quotas. The next step is to run the <code class="literal command">quotacheck</code> command.
				</p><p>
					The <code class="literal command">quotacheck</code> command examines quota-enabled file systems and builds a table of the current disk usage per file system. The table is then used to update the operating system’s copy of disk usage. In addition, the file system’s disk quota files are updated.
				</p><p>
					To create the quota files on the file system, use the <code class="literal option">-u</code> and the <code class="literal option">-g</code> options of the <code class="literal command">quotacheck</code> command; both of these options must be specified for user and group quotas to be initialized. For example, if quotas are enabled for the <code class="literal">/home</code> file system, create the files in the <code class="literal">/home</code> directory:
				</p><pre class="literallayout"># <span class="strong strong"><strong>quotacheck -ug /home</strong></span></pre></section><section class="section" id="assigning_quotas_per_user"><div class="titlepage"><div><div><h4 class="title">4.1.3. Assigning quotas per user</h4></div></div></div><p>
					The last step is assigning the disk quotas with the <code class="literal command">edquota</code> command. Note that if you have mounted your file system in accounting mode (with the <code class="literal">quota=account</code> option specified), the quotas are not enforced.
				</p><p>
					To configure the quota for a user, as root in a shell prompt, execute the command:
				</p><pre class="literallayout"># <span class="strong strong"><strong>edquota username</strong></span></pre><p>
					Perform this step for each user who needs a quota. For example, if a quota is enabled for the <code class="literal">/home</code> partition (<code class="literal">/dev/VolGroup00/LogVol02</code> in the example below) and the command <code class="literal command">edquota testuser</code> is executed, the following is shown in the editor configured as the default for the system:
				</p><pre class="literallayout">Disk quotas for user testuser (uid 501):
Filesystem                blocks     soft     hard    inodes   soft   hard
/dev/VolGroup00/LogVol02  440436        0        0</pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						The text editor defined by the <code class="literal">EDITOR</code> environment variable is used by <code class="literal command">edquota</code>. To change the editor, set the <code class="literal">EDITOR</code> environment variable in your <code class="literal">~/.bash_profile</code> file to the full path of the editor of your choice.
					</p></div></rh-alert><p>
					The first column is the name of the file system that has a quota enabled for it. The second column shows how many blocks the user is currently using. The next two columns are used to set soft and hard block limits for the user on the file system.
				</p><p>
					The soft block limit defines the maximum amount of disk space that can be used.
				</p><p>
					The hard block limit is the absolute maximum amount of disk space that a user or group can use. Once this limit is reached, no further disk space can be used.
				</p><p>
					The GFS2 file system does not maintain quotas for inodes, so these columns do not apply to GFS2 file systems and will be blank.
				</p><p>
					If any of the values are set to 0, that limit is not set. In the text editor, change the limits. For example:
				</p><pre class="literallayout">Disk quotas for user testuser (uid 501):
Filesystem                blocks     soft     hard    inodes   soft   hard
/dev/VolGroup00/LogVol02  440436   500000   550000</pre><p>
					To verify that the quota for the user has been set, use the following command:
				</p><pre class="literallayout"># <span class="strong strong"><strong>quota testuser</strong></span></pre><p>
					You can also set quotas from the command line with the <code class="literal">setquota</code> command. For information about the <code class="literal">setquota</code> command, see the <code class="literal">setquota</code>(8) man page.
				</p></section><section class="section" id="assigning_quotas_per_group"><div class="titlepage"><div><div><h4 class="title">4.1.4. Assigning quotas per group</h4></div></div></div><p>
					Quotas can also be assigned on a per-group basis. Note that if you have mounted your file system in accounting mode (with the <code class="literal">account=on</code> option specified), the quotas are not enforced.
				</p><p>
					To set a group quota for the <code class="literal">devel</code> group (the group must exist prior to setting the group quota), use the following command:
				</p><pre class="literallayout"># <span class="strong strong"><strong>edquota -g devel</strong></span></pre><p>
					This command displays the existing quota for the group in the text editor:
				</p><pre class="literallayout">Disk quotas for group devel (gid 505):
Filesystem                blocks    soft     hard    inodes   soft   hard
/dev/VolGroup00/LogVol02  440400       0        0</pre><p>
					The GFS2 file system does not maintain quotas for inodes, so these columns do not apply to GFS2 file systems and will be blank. Modify the limits, then save the file.
				</p><p>
					To verify that the group quota has been set, use the following command:
				</p><pre class="literallayout">$ <span class="strong strong"><strong>quota -g devel</strong></span></pre></section></section><section class="section" id="proc_managing-gfs2-disk-quotas-gfs2-disk-quota-administration"><div class="titlepage"><div><div><h3 class="title">4.2. Managing GFS2 disk Quotas</h3></div></div></div><p class="_abstract _abstract">
				If quotas are implemented, they need some maintenance, mostly in the form of watching to see if the quotas are exceeded and making sure the quotas are accurate.
			</p><p>
				If users repeatedly exceed their quotas or consistently reach their soft limits, a system administrator has a few choices to make depending on what type of users they are and how much disk space impacts their work. The administrator can either help the user determine how to use less disk space or increase the user’s disk quota.
			</p><p>
				You can create a disk usage report by running the <code class="literal command">repquota</code> utility. For example, the command <code class="literal command">repquota /home</code> produces this output:
			</p><pre class="literallayout">*** Report for user quotas on device /dev/mapper/VolGroup00-LogVol02
Block grace time: 7days; Inode grace time: 7days
			Block limits			File limits
User		used	soft	hard	grace	used	soft	hard	grace
----------------------------------------------------------------------
root      --      36       0       0              4     0     0
kristin   --     540       0       0            125     0     0
testuser  --  440400  500000  550000          37418     0     0</pre><p>
				To view the disk usage report for all (option <code class="literal option">-a</code>) quota-enabled file systems, use the command:
			</p><pre class="literallayout"># <span class="strong strong"><strong>repquota -a</strong></span></pre><p>
				The <code class="literal">--</code> displayed after each user is a quick way to determine whether the block limits have been exceeded. If the block soft limit is exceeded, a <code class="literal">+</code> appears in place of the first <code class="literal">-</code> in the output. The second <code class="literal">-</code> indicates the inode limit, but GFS2 file systems do not support inode limits so that character will remain as <code class="literal">-</code>. GFS2 file systems do not support a grace period, so the <code class="literal">grace</code> column will remain blank.
			</p><p>
				Note that the <code class="literal command">repquota</code> command is not supported over NFS, irrespective of the underlying file system.
			</p></section><section class="section" id="proc_keeping-gfs2-quotas-accurate-with-quotacheck-gfs2-disk-quota-administration"><div class="titlepage"><div><div><h3 class="title">4.3. Keeping GFS2 disk quotas accurate with the quotacheck command</h3></div></div></div><p class="_abstract _abstract">
				If you enable quotas on your file system after a period of time when you have been running with quotas disabled, you should run the <code class="literal">quotacheck</code> command to create, check, and repair quota files. Additionally, you may want to run the <code class="literal">quotacheck</code> command if you think your quota files may not be accurate, as may occur when a file system is not unmounted cleanly after a system crash.
			</p><p>
				For more information about the <code class="literal">quotacheck</code> command, see the <code class="literal">quotacheck(8)</code> man page.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					Run <code class="literal command">quotacheck</code> when the file system is relatively idle on all nodes because disk activity may affect the computed quota values.
				</p></div></rh-alert></section><section class="section" id="proc_synchronizing-gfs2-quotas-gfs2-disk-quota-administration"><div class="titlepage"><div><div><h3 class="title">4.4. Synchronizing quotas with the quotasync Command</h3></div></div></div><p class="_abstract _abstract">
				GFS2 stores all quota information in its own internal file on disk. A GFS2 node does not update this quota file for every file system write; rather, by default it updates the quota file once every 60 seconds. This is necessary to avoid contention among nodes writing to the quota file, which would cause a slowdown in performance.
			</p><p>
				As a user or group approaches their quota limit, GFS2 dynamically reduces the time between its quota-file updates to prevent the limit from being exceeded. The normal time period between quota synchronizations is a tunable parameter, <code class="literal">quota_quantum</code>. You can change this from its default value of 60 seconds using the <code class="literal">quota_quantum=</code> mount option, as described in the "GFS2-Specific Mount Options" table in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_creating-mounting-gfs2-configuring-gfs2-file-systems#mounting_a_gfs2_file_system_that_specifies_mount_options">Mounting a GFS2 file system that specifies mount options</a>.
			</p><p>
				The <code class="literal">quota_quantum</code> parameter must be set on each node and each time the file system is mounted. Changes to the <code class="literal command">quota_quantum</code> parameter are not persistent across unmounts. You can update the <code class="literal">quota_quantum</code> value with the <code class="literal command">mount -o remount</code>.
			</p><p>
				You can use the <code class="literal command">quotasync</code> command to synchronize the quota information from a node to the on-disk quota file between the automatic updates performed by GFS2. Usage <span class="strong strong"><strong><span class="application application">Synchronizing Quota Information</span></strong></span>
			</p><pre class="literallayout">quotasync [-ug] -a|<span class="emphasis"><em>mountpoint</em></span>...</pre><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">u</code></span></dt><dd>
							Sync the user quota files.
						</dd><dt><span class="term"><code class="literal">g</code></span></dt><dd>
							Sync the group quota files
						</dd><dt><span class="term"><code class="literal">a</code></span></dt><dd>
							Sync all file systems that are currently quota-enabled and support sync. When -a is absent, a file system mountpoint should be specified.
						</dd><dt><span class="term"><code class="literal">mountpoint</code></span></dt><dd>
							Specifies the GFS2 file system to which the actions apply.
						</dd></dl></div><p>
				You can tune the time between synchronizations by specifying a <code class="literal">quota-quantum</code> mount option.
			</p><pre class="literallayout"># <span class="strong strong"><strong>mount -o quota_quantum=<span class="emphasis"><em>secs</em></span>,remount <span class="emphasis"><em>BlockDevice</em></span> <span class="emphasis"><em>MountPoint</em></span></strong></span></pre><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">MountPoint</code></span></dt><dd>
							Specifies the GFS2 file system to which the actions apply.
						</dd><dt><span class="term"><code class="literal">secs</code></span></dt><dd>
							Specifies the new time period between regular quota-file synchronizations by GFS2. Smaller values may increase contention and slow down performance.
						</dd></dl></div><p>
				The following example synchronizes all the cached dirty quotas from the node it is run on to the on-disk quota file for the file system <code class="literal">/mnt/mygfs2</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>quotasync -ug /mnt/mygfs2</strong></span></pre><p>
				This following example changes the default time period between regular quota-file updates to one hour (3600 seconds) for file system <code class="literal">/mnt/mygfs2</code> when remounting that file system on logical volume <code class="literal">/dev/volgroup/logical_volume</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>mount -o quota_quantum=3600,remount /dev/volgroup/logical_volume /mnt/mygfs2</strong></span></pre></section></section><section class="chapter" id="assembly_gfs2-filesystem-repair-configuring-gfs2-file-systems"><div class="titlepage"><div><div><h2 class="title">Chapter 5. GFS2 file system repair</h2></div></div></div><p class="_abstract _abstract">
			When nodes fail with the file system mounted, file system journaling allows fast recovery. However, if a storage device loses power or is physically disconnected, file system corruption may occur. (Journaling cannot be used to recover from storage subsystem failures.) When that type of corruption occurs, you can recover the GFS2 file system by using the <code class="literal command">fsck.gfs2</code> command.
		</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
				The <code class="literal command">fsck.gfs2</code> command must be run only on a file system that is unmounted from all nodes. When the file system is being managed as a Pacemaker cluster resource, you can disable the file system resource, which unmounts the file system. After running the <code class="literal command">fsck.gfs2</code> command, you enable the file system resource again. The <span class="emphasis"><em>timeout</em></span> value specified with the <code class="literal">--wait</code> option of the <code class="literal command">pcs resource disable</code> indicates a value in seconds.
			</p><pre class="literallayout">pcs resource disable --wait=<span class="emphasis"><em>timeoutvalue</em></span> <span class="emphasis"><em>resource_id</em></span>
[fsck.gfs2]
pcs resource enable <span class="emphasis"><em>resource_id</em></span></pre><p>
				Note that even if a file system is part of a resource group, as in an encrypted file system deployment, you need to disable only the file system resource in order to run the fsck command on the file system. You must not disable the entire resource group.
			</p></div></rh-alert><p>
			To ensure that <code class="literal command">fsck.gfs2</code> command does not run on a GFS2 file system at boot time, you can set the <code class="literal">run_fsck</code> parameter of the <code class="literal">options</code> argument when creating the GFS2 file system resource in a cluster. Specifying <code class="literal">"run_fsck=no"</code> will indicate that you should not run the <code class="literal command">fsck</code> command.
		</p><section class="section" id="proc_determining-needed-memory-for-fsckgfs2-gfs2-filesystem-repair"><div class="titlepage"><div><div><h3 class="title">5.1. Determining required memory for running fsck.gfs2</h3></div></div></div><p class="_abstract _abstract">
				Running the <code class="literal command">fsck.gfs2</code> command may require system memory above and beyond the memory used for the operating system and kernel. Larger file systems in particular may require additional memory to run this command.
			</p><p>
				The following table shows approximate values of memory that may be required to run <code class="literal">fsck.gfs2</code> file systems on GFS2 file systems that are 1TB, 10TB, and 100TB in size with a block size of 4K.
			</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--><col style="width: 67%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm139688927449616" scope="col">GFS2 file system size</th><th align="left" valign="top" id="idm139688927448528" scope="col">Approximate memory required to run <code class="literal">fsck.gfs2</code></th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139688927449616"> <p>
								1 TB
							</p>
							 </td><td align="left" valign="top" headers="idm139688927448528"> <p>
								0.16 GB
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927449616"> <p>
								10 TB
							</p>
							 </td><td align="left" valign="top" headers="idm139688927448528"> <p>
								1.6 GB
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927449616"> <p>
								100 TB
							</p>
							 </td><td align="left" valign="top" headers="idm139688927448528"> <p>
								16 GB
							</p>
							 </td></tr></tbody></table></rh-table><p>
				Note that a smaller block size for the file system would require a larger amount of memory. For example, GFS2 file systems with a block size of 1K would require four times the amount of memory indicated in this table.
			</p></section><section class="section" id="proc_repairing-a-gfs2-filesystem-gfs2-filesystem-repair"><div class="titlepage"><div><div><h3 class="title">5.2. Repairing a gfs2 filesystem</h3></div></div></div><p class="_abstract _abstract">
				The format of the <code class="literal">fsck.gfs2</code> command to repair a GFS2 filesystem is as follows:
			</p><pre class="literallayout">fsck.gfs2 -y <span class="emphasis"><em>BlockDevice</em></span></pre><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal option">-y</code></span></dt><dd>
							The <code class="literal option">-y</code> flag causes all questions to be answered with <code class="literal">yes</code>. With the <code class="literal option">-y</code> flag specified, the <code class="literal command">fsck.gfs2</code> command does not prompt you for an answer before making changes.
						</dd><dt><span class="term"><code class="literal">BlockDevice</code></span></dt><dd>
							Specifies the block device where the GFS2 file system resides.
						</dd></dl></div><p>
				In this example, the GFS2 file system residing on block device <code class="literal">/dev/testvg/testlv</code> is repaired. All queries to repair are automatically answered with <code class="literal">yes</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>fsck.gfs2 -y /dev/testvg/testlv</strong></span>
Initializing fsck
Validating Resource Group index.
Level 1 RG check.
(level 1 passed)
Clearing journals (this may take a while)...
Journals cleared.
Starting pass1
Pass1 complete
Starting pass1b
Pass1b complete
Starting pass1c
Pass1c complete
Starting pass2
Pass2 complete
Starting pass3
Pass3 complete
Starting pass4
Pass4 complete
Starting pass5
Pass5 complete
Writing changes to disk
fsck.gfs2 complete</pre></section></section><section class="chapter" id="assembly_gfs2-performance-configuring-gfs2-file-systems"><div class="titlepage"><div><div><h2 class="title">Chapter 6. Improving GFS2 performance</h2></div></div></div><p class="_abstract _abstract">
			There are many aspects of GFS2 configuration that you can analyze to improve file system performance.
		</p><p>
			For general recommendations for deploying and upgrading Red Hat Enterprise Linux clusters using the High Availability Add-On and Red Hat Global File System 2 (GFS2) see the article <a class="link" href="https://access.redhat.com/kb/docs/DOC-40821">Red Hat Enterprise Linux Cluster, High Availability, and GFS Deployment Best Practices</a> on the Red Hat Customer Portal.
		</p><section class="section" id="proc_gfs2-defragment-gfs2-performance"><div class="titlepage"><div><div><h3 class="title">6.1. GFS2 file system defragmentation</h3></div></div></div><p class="_abstract _abstract">
				While there is no defragmentation tool for GFS2 on Red Hat Enterprise Linux, you can defragment individual files by identifying them with the <code class="literal">filefrag</code> tool, copying them to temporary files, and renaming the temporary files to replace the originals.
			</p></section><section class="section" id="con_gfs2-node-locking-gfs2-performance"><div class="titlepage"><div><div><h3 class="title">6.2. GFS2 node locking</h3></div></div></div><p class="_abstract _abstract">
				In order to get the best performance from a GFS2 file system, it is important to understand some of the basic theory of its operation. A single node file system is implemented alongside a cache, the purpose of which is to eliminate latency of disk accesses when using frequently requested data. In Linux the page cache (and historically the buffer cache) provide this caching function.
			</p><p>
				With GFS2, each node has its own page cache which may contain some portion of the on-disk data. GFS2 uses a locking mechanism called <span class="emphasis"><em>glocks</em></span> (pronounced gee-locks) to maintain the integrity of the cache between nodes. The glock subsystem provides a cache management function which is implemented using the <span class="emphasis"><em>distributed lock manager</em></span> (DLM) as the underlying communication layer.
			</p><p>
				The glocks provide protection for the cache on a per-inode basis, so there is one lock per inode which is used for controlling the caching layer. If that glock is granted in shared mode (DLM lock mode: PR) then the data under that glock may be cached upon one or more nodes at the same time, so that all the nodes may have local access to the data.
			</p><p>
				If the glock is granted in exclusive mode (DLM lock mode: EX) then only a single node may cache the data under that glock. This mode is used by all operations which modify the data (such as the <code class="literal command">write</code> system call).
			</p><p>
				If another node requests a glock which cannot be granted immediately, then the DLM sends a message to the node or nodes which currently hold the glocks blocking the new request to ask them to drop their locks. Dropping glocks can be (by the standards of most file system operations) a long process. Dropping a shared glock requires only that the cache be invalidated, which is relatively quick and proportional to the amount of cached data.
			</p><p>
				Dropping an exclusive glock requires a log flush, and writing back any changed data to disk, followed by the invalidation as per the shared glock.
			</p><p>
				The difference between a single node file system and GFS2, then, is that a single node file system has a single cache and GFS2 has a separate cache on each node. In both cases, latency to access cached data is of a similar order of magnitude, but the latency to access uncached data is much greater in GFS2 if another node has previously cached that same data.
			</p><p>
				Operations such as <code class="literal">read</code> (buffered), <code class="literal">stat,</code> and <code class="literal">readdir</code> only require a shared glock. Operations such as <code class="literal">write</code> (buffered), <code class="literal">mkdir</code>, <code class="literal">rmdir</code>, and <code class="literal">unlink</code> require an exclusive glock. Direct I/O read/write operations require a deferred glock if no allocation is taking place, or an exclusive glock if the write requires an allocation (that is, extending the file, or hole filling).
			</p><p>
				There are two main performance considerations which follow from this. First, read-only operations parallelize extremely well across a cluster, since they can run independently on every node. Second, operations requiring an exclusive glock can reduce performance, if there are multiple nodes contending for access to the same inode(s). Consideration of the working set on each node is thus an important factor in GFS2 file system performance such as when, for example, you perform a file system backup, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_creating-mounting-gfs2-configuring-gfs2-file-systems#proc_backing-up-a-gfs2-filesystem-creating-mounting-gfs2">Backing up a GFS2 file system</a>.
			</p><p>
				A further consequence of this is that we recommend the use of the <code class="literal">noatime</code> or <code class="literal">nodiratime</code> mount option with GFS2 whenever possible, with the preference for <code class="literal">noatime</code> where the application allows for this. This prevents reads from requiring exclusive locks to update the <code class="literal">atime</code> timestamp.
			</p><p>
				For users who are concerned about the working set or caching efficiency, GFS2 provides tools that allow you to monitor the performance of a GFS2 file system: Performance Co-Pilot and GFS2 tracepoints.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					Due to the way in which GFS2’s caching is implemented the best performance is obtained when either of the following takes place:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							An inode is used in a read-only fashion across all nodes.
						</li><li class="listitem">
							An inode is written or modified from a single node only.
						</li></ul></div><p>
					Note that inserting and removing entries from a directory during file creation and deletion counts as writing to the directory inode.
				</p><p>
					It is possible to break this rule provided that it is broken relatively infrequently. Ignoring this rule too often will result in a severe performance penalty.
				</p><p>
					If you <code class="literal command">mmap</code>() a file on GFS2 with a read/write mapping, but only read from it, this only counts as a read.
				</p><p>
					If you do not set the <code class="literal">noatime</code> <code class="literal command">mount</code> parameter, then reads will also result in writes to update the file timestamps. We recommend that all GFS2 users should mount with <code class="literal">noatime</code> unless they have a specific requirement for <code class="literal">atime</code>.
				</p></div></rh-alert></section><section class="section" id="con_posix-locking-issues-gfs2-performance"><div class="titlepage"><div><div><h3 class="title">6.3. Issues with Posix locking</h3></div></div></div><p class="_abstract _abstract">
				When using Posix locking, you should take the following into account:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Use of Flocks will yield faster processing than use of Posix locks.
					</li><li class="listitem">
						Programs using Posix locks in GFS2 should avoid using the <code class="literal">GETLK</code> function since, in a clustered environment, the process ID may be for a different node in the cluster.
					</li></ul></div></section><section class="section" id="con_troubleshooting-gfs2-performance-gfs2-performance"><div class="titlepage"><div><div><h3 class="title">6.4. Performance tuning with GFS2</h3></div></div></div><p class="_abstract _abstract">
				It is usually possible to alter the way in which a troublesome application stores its data in order to gain a considerable performance advantage.
			</p><p>
				A typical example of a troublesome application is an email server. These are often laid out with a spool directory containing files for each user (<code class="literal">mbox</code>), or with a directory for each user containing a file for each message (<code class="literal">maildir</code>). When requests arrive over IMAP, the ideal arrangement is to give each user an affinity to a particular node. That way their requests to view and delete email messages will tend to be served from the cache on that one node. Obviously if that node fails, then the session can be restarted on a different node.
			</p><p>
				When mail arrives by means of SMTP, then again the individual nodes can be set up so as to pass a certain user’s mail to a particular node by default. If the default node is not up, then the message can be saved directly into the user’s mail spool by the receiving node. Again this design is intended to keep particular sets of files cached on just one node in the normal case, but to allow direct access in the case of node failure.
			</p><p>
				This setup allows the best use of GFS2’s page cache and also makes failures transparent to the application, whether <code class="literal">imap</code> or <code class="literal">smtp</code>.
			</p><p>
				Backup is often another tricky area. Again, if it is possible it is greatly preferable to back up the working set of each node directly from the node which is caching that particular set of inodes. If you have a backup script which runs at a regular point in time, and that seems to coincide with a spike in the response time of an application running on GFS2, then there is a good chance that the cluster may not be making the most efficient use of the page cache.
			</p><p>
				Obviously, if you are in the position of being able to stop the application in order to perform a backup, then this will not be a problem. On the other hand, if a backup is run from just one node, then after it has completed a large portion of the file system will be cached on that node, with a performance penalty for subsequent accesses from other nodes. This can be mitigated to a certain extent by dropping the VFS page cache on the backup node after the backup has completed with following command:
			</p><pre class="literallayout">echo -n 3 &gt;/proc/sys/vm/drop_caches</pre><p>
				However this is not as good a solution as taking care to ensure the working set on each node is either shared, mostly read-only across the cluster, or accessed largely from a single node.
			</p></section><section class="section" id="con_gfs2-lockdump-gfs2-performance"><div class="titlepage"><div><div><h3 class="title">6.5. Troubleshooting GFS2 performance with the GFS2 lock dump</h3></div></div></div><p class="_abstract _abstract">
				If your cluster performance is suffering because of inefficient use of GFS2 caching, you may see large and increasing I/O wait times. You can make use of GFS2’s lock dump information to determine the cause of the problem.
			</p><p>
				The GFS2 lock dump information can be gathered from the <code class="literal">debugfs</code> file which can be found at the following path name, assuming that <code class="literal">debugfs</code> is mounted on <code class="literal">/sys/kernel/debug/</code>:
			</p><pre class="literallayout">/sys/kernel/debug/gfs2/<span class="emphasis"><em>fsname</em></span>/glocks</pre><p>
				The content of the file is a series of lines. Each line starting with G: represents one glock, and the following lines, indented by a single space, represent an item of information relating to the glock immediately before them in the file.
			</p><p>
				The best way to use the <code class="literal">debugfs</code> file is to use the <code class="literal command">cat</code> command to take a copy of the complete content of the file (it might take a long time if you have a large amount of RAM and a lot of cached inodes) while the application is experiencing problems, and then looking through the resulting data at a later date.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					It can be useful to make two copies of the <code class="literal">debugfs</code> file, one a few seconds or even a minute or two after the other. By comparing the holder information in the two traces relating to the same glock number, you can tell whether the workload is making progress (it is just slow) or whether it has become stuck (which is always a bug and should be reported to Red Hat support immediately).
				</p></div></rh-alert><p>
				Lines in the <code class="literal">debugfs</code> file starting with H: (holders) represent lock requests either granted or waiting to be granted. The flags field on the holders line f: shows which: The 'W' flag refers to a waiting request, the 'H' flag refers to a granted request. The glocks which have large numbers of waiting requests are likely to be those which are experiencing particular contention.
			</p><p>
				The following tables show the meanings of the glock flags and glock holder flags.
			</p><rh-table id="tb-glock-flags"><table class="lt-4-cols lt-7-rows"><caption>Table 6.1. Glock flags</caption><colgroup><col style="width: 33%; " class="col_1"><!--Empty--><col style="width: 33%; " class="col_2"><!--Empty--><col style="width: 33%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm139688931730816" scope="col">Flag</th><th align="left" valign="top" id="idm139688931729728" scope="col">Name</th><th align="left" valign="top" id="idm139688931728640" scope="col">Meaning</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139688931730816"> <p>
								b
							</p>
							 </td><td align="left" valign="top" headers="idm139688931729728"> <p>
								Blocking
							</p>
							 </td><td align="left" valign="top" headers="idm139688931728640"> <p>
								Valid when the locked flag is set, and indicates that the operation that has been requested from the DLM may block. This flag is cleared for demotion operations and for "try" locks. The purpose of this flag is to allow gathering of stats of the DLM response time independent from the time taken by other nodes to demote locks.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931730816"> <p>
								d
							</p>
							 </td><td align="left" valign="top" headers="idm139688931729728"> <p>
								Pending demote
							</p>
							 </td><td align="left" valign="top" headers="idm139688931728640"> <p>
								A deferred (remote) demote request
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931730816"> <p>
								D
							</p>
							 </td><td align="left" valign="top" headers="idm139688931729728"> <p>
								Demote
							</p>
							 </td><td align="left" valign="top" headers="idm139688931728640"> <p>
								A demote request (local or remote)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931730816"> <p>
								f
							</p>
							 </td><td align="left" valign="top" headers="idm139688931729728"> <p>
								Log flush
							</p>
							 </td><td align="left" valign="top" headers="idm139688931728640"> <p>
								The log needs to be committed before releasing this glock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931730816"> <p>
								F
							</p>
							 </td><td align="left" valign="top" headers="idm139688931729728"> <p>
								Frozen
							</p>
							 </td><td align="left" valign="top" headers="idm139688931728640"> <p>
								Replies from remote nodes ignored - recovery is in progress. This flag is not related to file system freeze, which uses a different mechanism, but is used only in recovery.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931730816"> <p>
								i
							</p>
							 </td><td align="left" valign="top" headers="idm139688931729728"> <p>
								Invalidate in progress
							</p>
							 </td><td align="left" valign="top" headers="idm139688931728640"> <p>
								In the process of invalidating pages under this glock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931730816"> <p>
								I
							</p>
							 </td><td align="left" valign="top" headers="idm139688931729728"> <p>
								Initial
							</p>
							 </td><td align="left" valign="top" headers="idm139688931728640"> <p>
								Set when DLM lock is associated with this glock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931730816"> <p>
								l
							</p>
							 </td><td align="left" valign="top" headers="idm139688931729728"> <p>
								Locked
							</p>
							 </td><td align="left" valign="top" headers="idm139688931728640"> <p>
								The glock is in the process of changing state
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931730816"> <p>
								L
							</p>
							 </td><td align="left" valign="top" headers="idm139688931729728"> <p>
								LRU
							</p>
							 </td><td align="left" valign="top" headers="idm139688931728640"> <p>
								Set when the glock is on the LRU list
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931730816"> <p>
								o
							</p>
							 </td><td align="left" valign="top" headers="idm139688931729728"> <p>
								Object
							</p>
							 </td><td align="left" valign="top" headers="idm139688931728640"> <p>
								Set when the glock is associated with an object (that is, an inode for type 2 glocks, and a resource group for type 3 glocks)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931730816"> <p>
								p
							</p>
							 </td><td align="left" valign="top" headers="idm139688931729728"> <p>
								Demote in progress
							</p>
							 </td><td align="left" valign="top" headers="idm139688931728640"> <p>
								The glock is in the process of responding to a demote request
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931730816"> <p>
								q
							</p>
							 </td><td align="left" valign="top" headers="idm139688931729728"> <p>
								Queued
							</p>
							 </td><td align="left" valign="top" headers="idm139688931728640"> <p>
								Set when a holder is queued to a glock, and cleared when the glock is held, but there are no remaining holders. Used as part of the algorithm the calculates the minimum hold time for a glock.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931730816"> <p>
								r
							</p>
							 </td><td align="left" valign="top" headers="idm139688931729728"> <p>
								Reply pending
							</p>
							 </td><td align="left" valign="top" headers="idm139688931728640"> <p>
								Reply received from remote node is awaiting processing
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931730816"> <p>
								y
							</p>
							 </td><td align="left" valign="top" headers="idm139688931729728"> <p>
								Dirty
							</p>
							 </td><td align="left" valign="top" headers="idm139688931728640"> <p>
								Data needs flushing to disk before releasing this glock
							</p>
							 </td></tr></tbody></table></rh-table><rh-table id="tb-glock-holderflags"><table class="lt-4-cols lt-7-rows"><caption>Table 6.2. Glock holder flags</caption><colgroup><col style="width: 33%; " class="col_1"><!--Empty--><col style="width: 33%; " class="col_2"><!--Empty--><col style="width: 33%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm139688931006304" scope="col">Flag</th><th align="left" valign="top" id="idm139688931005216" scope="col">Name</th><th align="left" valign="top" id="idm139688931004128" scope="col">Meaning</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139688931006304"> <p>
								a
							</p>
							 </td><td align="left" valign="top" headers="idm139688931005216"> <p>
								Async
							</p>
							 </td><td align="left" valign="top" headers="idm139688931004128"> <p>
								Do not wait for glock result (will poll for result later)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931006304"> <p>
								A
							</p>
							 </td><td align="left" valign="top" headers="idm139688931005216"> <p>
								Any
							</p>
							 </td><td align="left" valign="top" headers="idm139688931004128"> <p>
								Any compatible lock mode is acceptable
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931006304"> <p>
								c
							</p>
							 </td><td align="left" valign="top" headers="idm139688931005216"> <p>
								No cache
							</p>
							 </td><td align="left" valign="top" headers="idm139688931004128"> <p>
								When unlocked, demote DLM lock immediately
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931006304"> <p>
								e
							</p>
							 </td><td align="left" valign="top" headers="idm139688931005216"> <p>
								No expire
							</p>
							 </td><td align="left" valign="top" headers="idm139688931004128"> <p>
								Ignore subsequent lock cancel requests
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931006304"> <p>
								E
							</p>
							 </td><td align="left" valign="top" headers="idm139688931005216"> <p>
								exact
							</p>
							 </td><td align="left" valign="top" headers="idm139688931004128"> <p>
								Must have exact lock mode
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931006304"> <p>
								F
							</p>
							 </td><td align="left" valign="top" headers="idm139688931005216"> <p>
								First
							</p>
							 </td><td align="left" valign="top" headers="idm139688931004128"> <p>
								Set when holder is the first to be granted for this lock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931006304"> <p>
								H
							</p>
							 </td><td align="left" valign="top" headers="idm139688931005216"> <p>
								Holder
							</p>
							 </td><td align="left" valign="top" headers="idm139688931004128"> <p>
								Indicates that requested lock is granted
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931006304"> <p>
								p
							</p>
							 </td><td align="left" valign="top" headers="idm139688931005216"> <p>
								Priority
							</p>
							 </td><td align="left" valign="top" headers="idm139688931004128"> <p>
								Enqueue holder at the head of the queue
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931006304"> <p>
								t
							</p>
							 </td><td align="left" valign="top" headers="idm139688931005216"> <p>
								Try
							</p>
							 </td><td align="left" valign="top" headers="idm139688931004128"> <p>
								A "try" lock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931006304"> <p>
								T
							</p>
							 </td><td align="left" valign="top" headers="idm139688931005216"> <p>
								Try 1CB
							</p>
							 </td><td align="left" valign="top" headers="idm139688931004128"> <p>
								A "try" lock that sends a callback
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688931006304"> <p>
								W
							</p>
							 </td><td align="left" valign="top" headers="idm139688931005216"> <p>
								Wait
							</p>
							 </td><td align="left" valign="top" headers="idm139688931004128"> <p>
								Set while waiting for request to complete
							</p>
							 </td></tr></tbody></table></rh-table><p>
				Having identified a glock which is causing a problem, the next step is to find out which inode it relates to. The glock number (n: on the G: line) indicates this. It is of the form <span class="emphasis"><em>type</em></span>/<span class="emphasis"><em>number</em></span> and if <span class="emphasis"><em>type</em></span> is 2, then the glock is an inode glock and the <span class="emphasis"><em>number</em></span> is an inode number. To track down the inode, you can then run <code class="literal command">find -inum <span class="emphasis"><em>number</em></span></code> where <span class="emphasis"><em>number</em></span> is the inode number converted from the hex format in the glocks file into decimal.
			</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
					If you run the <code class="literal command">find</code> command on a file system when it is experiencing lock contention, you are likely to make the problem worse. It is a good idea to stop the application before running the <code class="literal command">find</code> command when you are looking for contended inodes.
				</p></div></rh-alert><p>
				The following table shows the meanings of the different glock types.
			</p><rh-table id="tb-glock-types"><table class="lt-4-cols lt-7-rows"><caption>Table 6.3. Glock types</caption><colgroup><col style="width: 33%; " class="col_1"><!--Empty--><col style="width: 33%; " class="col_2"><!--Empty--><col style="width: 33%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm139688929069632" scope="col">Type number</th><th align="left" valign="top" id="idm139688929068544" scope="col">Lock type</th><th align="left" valign="top" id="idm139688929067456" scope="col">Use</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139688929069632"> <p>
								1
							</p>
							 </td><td align="left" valign="top" headers="idm139688929068544"> <p>
								Trans
							</p>
							 </td><td align="left" valign="top" headers="idm139688929067456"> <p>
								Transaction lock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688929069632"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm139688929068544"> <p>
								Inode
							</p>
							 </td><td align="left" valign="top" headers="idm139688929067456"> <p>
								Inode metadata and data
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688929069632"> <p>
								3
							</p>
							 </td><td align="left" valign="top" headers="idm139688929068544"> <p>
								Rgrp
							</p>
							 </td><td align="left" valign="top" headers="idm139688929067456"> <p>
								Resource group metadata
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688929069632"> <p>
								4
							</p>
							 </td><td align="left" valign="top" headers="idm139688929068544"> <p>
								Meta
							</p>
							 </td><td align="left" valign="top" headers="idm139688929067456"> <p>
								The superblock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688929069632"> <p>
								5
							</p>
							 </td><td align="left" valign="top" headers="idm139688929068544"> <p>
								Iopen
							</p>
							 </td><td align="left" valign="top" headers="idm139688929067456"> <p>
								Inode last closer detection
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688929069632"> <p>
								6
							</p>
							 </td><td align="left" valign="top" headers="idm139688929068544"> <p>
								Flock
							</p>
							 </td><td align="left" valign="top" headers="idm139688929067456"> <p>
								<code class="literal command">flock</code>(2) syscall
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688929069632"> <p>
								8
							</p>
							 </td><td align="left" valign="top" headers="idm139688929068544"> <p>
								Quota
							</p>
							 </td><td align="left" valign="top" headers="idm139688929067456"> <p>
								Quota operations
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688929069632"> <p>
								9
							</p>
							 </td><td align="left" valign="top" headers="idm139688929068544"> <p>
								Journal
							</p>
							 </td><td align="left" valign="top" headers="idm139688929067456"> <p>
								Journal mutex
							</p>
							 </td></tr></tbody></table></rh-table><p>
				If the glock that was identified was of a different type, then it is most likely to be of type 3: (resource group). If you see significant numbers of processes waiting for other types of glock under normal loads, report this to Red Hat support.
			</p><p>
				If you do see a number of waiting requests queued on a resource group lock there may be a number of reasons for this. One is that there are a large number of nodes compared to the number of resource groups in the file system. Another is that the file system may be very nearly full (requiring, on average, longer searches for free blocks). The situation in both cases can be improved by adding more storage and using the <code class="literal command">gfs2_grow</code> command to expand the file system.
			</p></section><section class="section" id="enabling-data-journaling-gfs2-performance"><div class="titlepage"><div><div><h3 class="title">6.6. Enabling data journaling</h3></div></div></div><p class="_abstract _abstract">
				Ordinarily, GFS2 writes only metadata to its journal. File contents are subsequently written to disk by the kernel’s periodic sync that flushes file system buffers. An <code class="literal command">fsync()</code> call on a file causes the file’s data to be written to disk immediately. The call returns when the disk reports that all data is safely written.
			</p><p>
				Data journaling can result in a reduced <code class="literal command">fsync()</code> time for very small files because the file data is written to the journal in addition to the metadata. This advantage rapidly reduces as the file size increases. Writing to medium and larger files will be much slower with data journaling turned on.
			</p><p>
				Applications that rely on <code class="literal command">fsync()</code> to sync file data may see improved performance by using data journaling. Data journaling can be enabled automatically for any GFS2 files created in a flagged directory (and all its subdirectories). Existing files with zero length can also have data journaling turned on or off.
			</p><p>
				Enabling data journaling on a directory sets the directory to "inherit jdata", which indicates that all files and directories subsequently created in that directory are journaled. You can enable and disable data journaling on a file with the <code class="literal command">chattr</code> command.
			</p><p>
				The following commands enable data journaling on the <code class="literal">/mnt/gfs2/gfs2_dir/newfile</code> file and then check whether the flag has been set properly.
			</p><pre class="literallayout"># <span class="strong strong"><strong>chattr +j /mnt/gfs2/gfs2_dir/newfile</strong></span>
# <span class="strong strong"><strong>lsattr /mnt/gfs2/gfs2_dir</strong></span>
---------j--- /mnt/gfs2/gfs2_dir/newfile</pre><p>
				The following commands disable data journaling on the <code class="literal">/mnt/gfs2/gfs2_dir/newfile</code> file and then check whether the flag has been set properly.
			</p><pre class="literallayout"># <span class="strong strong"><strong>chattr -j /mnt/gfs2/gfs2_dir/newfile</strong></span>
# <span class="strong strong"><strong>lsattr /mnt/gfs2/gfs2_dir</strong></span>
------------- /mnt/gfs2/gfs2_dir/newfile</pre><p>
				You can also use the <code class="literal command">chattr</code> command to set the <code class="literal">j</code> flag on a directory. When you set this flag for a directory, all files and directories subsequently created in that directory are journaled. The following set of commands sets the <code class="literal">j</code> flag on the <code class="literal">gfs2_dir</code> directory, then checks whether the flag has been set properly. After this, the commands create a new file called <code class="literal">newfile</code> in the <code class="literal">/mnt/gfs2/gfs2_dir</code> directory and then check whether the <code class="literal">j</code> flag has been set for the file. Since the <code class="literal">j</code> flag is set for the directory, then <code class="literal">newfile</code> should also have journaling enabled.
			</p><pre class="literallayout"># <span class="strong strong"><strong>chattr -j /mnt/gfs2/gfs2_dir</strong></span>
# <span class="strong strong"><strong>lsattr /mnt/gfs2</strong></span>
---------j--- /mnt/gfs2/gfs2_dir
# <span class="strong strong"><strong>touch /mnt/gfs2/gfs2_dir/newfile</strong></span>
# <span class="strong strong"><strong>lsattr /mnt/gfs2/gfs2_dir</strong></span>
---------j--- /mnt/gfs2/gfs2_dir/newfile</pre></section></section><section class="chapter" id="assembly_troubleshooting-gfs2-configuring-gfs2-file-systems"><div class="titlepage"><div><div><h2 class="title">Chapter 7. Diagnosing and correcting problems with GFS2 file systems</h2></div></div></div><p class="_abstract _abstract">
			The following procedures describe some common GFS2 issues and provide information on how to address them.
		</p><section class="section" id="ref_gfs2-filesystem-unavailable-troubleshooting-gfs2"><div class="titlepage"><div><div><h3 class="title">7.1. GFS2 file system unavailable to a node (the GFS2 withdraw function)</h3></div></div></div><p class="_abstract _abstract">
				The GFS2 <span class="emphasis"><em>withdraw</em></span> function is a data integrity feature of the GFS2 file system that prevents potential file system damage due to faulty hardware or kernel software. If the GFS2 kernel module detects an inconsistency while using a GFS2 file system on any given cluster node, it withdraws from the file system, leaving it unavailable to that node until it is unmounted and remounted (or the machine detecting the problem is rebooted). All other mounted GFS2 file systems remain fully functional on that node. (The GFS2 withdraw function is less severe than a kernel panic, which causes the node to be fenced.)
			</p><p>
				The main categories of inconsistency that can cause a GFS2 withdraw are as follows:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Inode consistency error
					</li><li class="listitem">
						Resource group consistency error
					</li><li class="listitem">
						Journal consistency error
					</li><li class="listitem">
						Magic number metadata consistency error
					</li><li class="listitem">
						Metadata type consistency error
					</li></ul></div><p>
				An example of an inconsistency that would cause a GFS2 withdraw is an incorrect block count for a file’s inode. When GFS2 deletes a file, it systematically removes all the data and metadata blocks referenced by that file. When done, it checks the inode’s block count. If the block count is not 1 (meaning all that is left is the disk inode itself), that indicates a file system inconsistency, since the inode’s block count did not match the actual blocks used for the file. In many cases, the problem may have been caused by faulty hardware (faulty memory, motherboard, HBA, disk drives, cables, and so forth). It may also have been caused by a kernel bug (another kernel module accidentally overwriting GFS2’s memory), or actual file system damage (caused by a GFS2 bug).
			</p><p>
				In most cases, the best way to recover from a withdrawn GFS2 file system is to reboot or fence the node. The withdrawn GFS2 file system will give you an opportunity to relocate services to another node in the cluster. After services are relocated you can reboot the node or force a fence with this command.
			</p><pre class="literallayout">pcs stonith fence <span class="emphasis"><em>node</em></span></pre><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
					Do not try to unmount and remount the file system manually with the <code class="literal command">umount</code> and <code class="literal command">mount</code> commands. You must use the <code class="literal command">pcs</code> command, otherwise Pacemaker will detect the file system service has disappeared and fence the node.
				</p></div></rh-alert><p>
				The consistency problem that caused the withdraw may make stopping the file system service impossible as it may cause the system to hang.
			</p><p>
				If the problem persists after a remount, you should stop the file system service to unmount the file system from all nodes in the cluster, then perform a file system check with the <code class="literal">fsck.gfs2</code> command before restarting the service with the following procedure.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Reboot the affected node.
					</li><li class="listitem"><p class="simpara">
						Disable the non-clone file system service in Pacemaker to unmount the file system from every node in the cluster.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource disable --wait=100 mydata_fs</strong></span></pre></li><li class="listitem"><p class="simpara">
						From one node of the cluster, run the <code class="literal command">fsck.gfs2</code> command on the file system device to check for and repair any file system damage.
					</p><pre class="literallayout"># <span class="strong strong"><strong>fsck.gfs2 -y /dev/vg_mydata/mydata &gt; /tmp/fsck.out</strong></span></pre></li><li class="listitem"><p class="simpara">
						Remount the GFS2 file system from all nodes by re-enabling the file system service:
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource enable --wait=100 mydata_fs</strong></span></pre></li></ol></div><p>
				You can override the GFS2 withdraw function by mounting the file system with the <code class="literal">-o errors=panic</code> option specified in the file system service.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource update mydata_fs “options=noatime,errors=panic”</strong></span></pre><p>
				When this option is specified, any errors that would normally cause the system to withdraw force a kernel panic instead. This stops the node’s communications, which causes the node to be fenced. This is especially useful for clusters that are left unattended for long periods of time without monitoring or intervention.
			</p><p>
				Internally, the GFS2 withdraw function works by disconnecting the locking protocol to ensure that all further file system operations result in I/O errors. As a result, when the withdraw occurs, it is normal to see a number of I/O errors from the device mapper device reported in the system logs.
			</p></section><section class="section" id="ref_gfs2-filesystem-hangs-one-node-troubleshooting-gfs2"><div class="titlepage"><div><div><h3 class="title">7.2. GFS2 file system hangs and requires reboot of one node</h3></div></div></div><p class="_abstract _abstract">
				If your GFS2 file system hangs and does not return commands run against it, but rebooting one specific node returns the system to normal, this may be indicative of a locking problem or bug. Should this occur, gather GFS2 data during one of these occurences and open a support ticket with Red Hat Support, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_troubleshooting-gfs2-configuring-gfs2-file-systems#proc_gathering-gfs2-data-troubleshooting-gfs2">Gathering GFS2 data for troubleshooting</a>.
			</p></section><section class="section" id="ref_gfs2-filesystem-hangs-all-nodes-troubleshooting-gfs2"><div class="titlepage"><div><div><h3 class="title">7.3. GFS2 file system hangs and requires reboot of all nodes</h3></div></div></div><p class="_abstract _abstract">
				If your GFS2 file system hangs and does not return commands run against it, requiring that you reboot all nodes in the cluster before using it, check for the following issues.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You may have had a failed fence. GFS2 file systems will freeze to ensure data integrity in the event of a failed fence. Check the messages logs to see if there are any failed fences at the time of the hang. Ensure that fencing is configured correctly.
					</li><li class="listitem"><p class="simpara">
						The GFS2 file system may have withdrawn. Check through the messages logs for the word <code class="literal">withdraw</code> and check for any messages and call traces from GFS2 indicating that the file system has been withdrawn. A withdraw is indicative of file system corruption, a storage failure, or a bug. At the earliest time when it is convenient to unmount the file system, you should perform the following procedure:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Reboot the node on which the withdraw occurred.
							</p><pre class="literallayout"># <span class="strong strong"><strong>/sbin/reboot</strong></span></pre></li><li class="listitem"><p class="simpara">
								Stop the file system resource to unmount the GFS2 file system on all nodes.
							</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource disable --wait=100 mydata_fs</strong></span></pre></li><li class="listitem"><p class="simpara">
								Capture the metadata with the <code class="literal command">gfs2_edit savemeta…​</code> command. You should ensure that there is sufficient space for the file, which in some cases may be large. In this example, the metadata is saved to a file in the <code class="literal">/root</code> directory.
							</p><pre class="literallayout"># <span class="strong strong"><strong>gfs2_edit savemeta /dev/vg_mydata/mydata /root/gfs2metadata.gz</strong></span></pre></li><li class="listitem"><p class="simpara">
								Update the <code class="literal">gfs2-utils</code> package.
							</p><pre class="literallayout"># <span class="strong strong"><strong>sudo dnf update gfs2-utils</strong></span></pre></li><li class="listitem"><p class="simpara">
								On one node, run the <code class="literal command">fsck.gfs2</code> command on the file system to ensure file system integrity and repair any damage.
							</p><pre class="literallayout"># <span class="strong strong"><strong>fsck.gfs2 -y /dev/vg_mydata/mydata &gt; /tmp/fsck.out</strong></span></pre></li><li class="listitem"><p class="simpara">
								After the <code class="literal command">fsck.gfs2</code> command has completed, re-enable the file system resource to return it to service:
							</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource enable --wait=100 mydata_fs</strong></span></pre></li><li class="listitem"><p class="simpara">
								Open a support ticket with Red Hat Support. Inform them you experienced a GFS2 withdraw and provide logs and the debugging information generated by the <code class="literal command">sosreports</code> and <code class="literal command">gfs2_edit savemeta</code> commands.
							</p><p class="simpara">
								In some instances of a GFS2 withdraw, commands can hang that are trying to access the file system or its block device. In these cases a hard reboot is required to reboot the cluster.
							</p><p class="simpara">
								For information about the GFS2 withdraw function, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_troubleshooting-gfs2-configuring-gfs2-file-systems#ref_gfs2-filesystem-unavailable-troubleshooting-gfs2">GFS2 filesystem unavailable to a node (the GFS2 withdraw function)</a>.
							</p></li></ol></div></li><li class="listitem">
						This error may be indicative of a locking problem or bug. Gather data during one of these occurrences and open a support ticket with Red Hat Support, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_troubleshooting-gfs2-configuring-gfs2-file-systems#proc_gathering-gfs2-data-troubleshooting-gfs2">Gathering GFS2 data for troubleshooting</a>.
					</li></ul></div></section><section class="section" id="ref_gfs2-nomount-new-cluster-node-troubleshooting-gfs2"><div class="titlepage"><div><div><h3 class="title">7.4. GFS2 file system does not mount on newly added cluster node</h3></div></div></div><p class="_abstract _abstract">
				If you add a new node to a cluster and find that you cannot mount your GFS2 file system on that node, you may have fewer journals on the GFS2 file system than nodes attempting to access the GFS2 file system. You must have one journal per GFS2 host you intend to mount the file system on (with the exception of GFS2 file systems mounted with the <code class="literal">spectator</code> mount option set, since these do not require a journal). You can add journals to a GFS2 file system with the <code class="literal command">gfs2_jadd</code> command, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_creating-mounting-gfs2-configuring-gfs2-file-systems#proc_adding-gfs2-journal-creating-mounting-gfs2">Adding journals to a GFS2 file system</a>.
			</p></section><section class="section" id="ref_gfs2-used-space-empty-filesystem-troubleshooting-gfs2"><div class="titlepage"><div><div><h3 class="title">7.5. Space indicated as used in empty file system</h3></div></div></div><p class="_abstract _abstract">
				If you have an empty GFS2 file system, the <code class="literal command">df</code> command will show that there is space being taken up. This is because GFS2 file system journals consume space (number of journals * journal size) on disk. f you created a GFS2 file system with a large number of journals or specified a large journal size then you will be see (number of journals * journal size) as already in use when you execute the <code class="literal command">df</code> command. Even if you did not specify a large number of journals or large journals, small GFS2 file systems (in the 1GB or less range) will show a large amount of space as being in use with the default GFS2 journal size.
			</p></section><section class="section" id="proc_gathering-gfs2-data-troubleshooting-gfs2"><div class="titlepage"><div><div><h3 class="title">7.6. Gathering GFS2 data for troubleshooting</h3></div></div></div><p class="_abstract _abstract">
				If your GFS2 file system hangs and does not return commands run against it and you find that you need to open a ticket with Red Hat Support, you should first gather the following data:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						The GFS2 lock dump for the file system on each node:
					</p><pre class="literallayout">cat /sys/kernel/debug/gfs2/<span class="emphasis"><em>fsname</em></span>/glocks &gt;glocks.<span class="emphasis"><em>fsname</em></span>.<span class="emphasis"><em>nodename</em></span></pre></li><li class="listitem"><p class="simpara">
						The DLM lock dump for the file system on each node: You can get this information with the <code class="literal command">dlm_tool</code>:
					</p><pre class="literallayout">dlm_tool lockdebug -sv <span class="emphasis"><em>lsname</em></span></pre><p class="simpara">
						In this command, <span class="emphasis"><em>lsname</em></span> is the lockspace name used by DLM for the file system in question. You can find this value in the output from the <code class="literal command">group_tool</code> command.
					</p></li><li class="listitem">
						The output from the <code class="literal command">sysrq -t</code> command.
					</li><li class="listitem">
						The contents of the <code class="literal">/var/log/messages</code> file.
					</li></ul></div><p>
				Once you have gathered that data, you can open a ticket with Red Hat Support and provide the data you have collected.
			</p></section></section><section class="chapter" id="assembly_configuring-gfs2-in-a-cluster-configuring-gfs2-file-systems"><div class="titlepage"><div><div><h2 class="title">Chapter 8. GFS2 file systems in a cluster</h2></div></div></div><p class="_abstract _abstract">
			Use the following administrative procedures to configure GFS2 file systems in a Red Hat high availability cluster.
		</p><section class="section" id="proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster"><div class="titlepage"><div><div><h3 class="title">8.1. Configuring a GFS2 file system in a cluster</h3></div></div></div><p class="_abstract _abstract">
				You can set up a Pacemaker cluster that includes GFS2 file systems with the following procedure. In this example, you create three GFS2 file systems on three logical volumes in a two-node cluster.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install and start the cluster software on both cluster nodes and create a basic two-node cluster.
					</li><li class="listitem">
						Configure fencing for the cluster.
					</li></ul></div><p>
				For information about creating a Pacemaker cluster and configuring fencing for the cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters">Creating a Red Hat High-Availability cluster with Pacemaker</a>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On both nodes in the cluster, enable the repository for Resilient Storage that corresponds to your system architecture. For example, to enable the Resilient Storage repository for an x86_64 system, you can enter the following <code class="literal">subscription-manager</code> command:
					</p><pre class="literallayout"># <span class="strong strong"><strong>subscription-manager repos --enable=rhel-9-for-x86_64-resilientstorage-rpms</strong></span></pre><p class="simpara">
						Note that the Resilient Storage repository is a superset of the High Availability repository. If you enable the Resilient Storage repository you do not also need to enable the High Availability repository.
					</p></li><li class="listitem"><p class="simpara">
						On both nodes of the cluster, install the <code class="literal">lvm2-lockd</code>, <code class="literal">gfs2-utils</code>, and <code class="literal">dlm</code> packages. To support these packages, you must be subscribed to the AppStream channel and the Resilient Storage channel.
					</p><pre class="literallayout"># <span class="strong strong"><strong>dnf install lvm2-lockd gfs2-utils dlm</strong></span></pre></li><li class="listitem"><p class="simpara">
						On both nodes of the cluster, set the <code class="literal">use_lvmlockd</code> configuration option in the <code class="literal">/etc/lvm/lvm.conf</code> file to <code class="literal">use_lvmlockd=1</code>.
					</p><pre class="literallayout">...
use_lvmlockd = 1
...</pre></li><li class="listitem"><p class="simpara">
						Set the global Pacemaker parameter <code class="literal">no-quorum-policy</code> to <code class="literal">freeze</code>.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							By default, the value of <code class="literal">no-quorum-policy</code> is set to <code class="literal">stop</code>, indicating that once quorum is lost, all the resources on the remaining partition will immediately be stopped. Typically this default is the safest and most optimal option, but unlike most resources, GFS2 requires quorum to function. When quorum is lost both the applications using the GFS2 mounts and the GFS2 mount itself cannot be correctly stopped. Any attempts to stop these resources without quorum will fail which will ultimately result in the entire cluster being fenced every time quorum is lost.
						</p><p>
							To address this situation, set <code class="literal">no-quorum-policy</code> to <code class="literal">freeze</code> when GFS2 is in use. This means that when quorum is lost, the remaining partition will do nothing until quorum is regained.
						</p></div></rh-alert><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs property set no-quorum-policy=freeze</strong></span></pre></li><li class="listitem"><p class="simpara">
						Set up a <code class="literal">dlm</code> resource. This is a required dependency for configuring a GFS2 file system in a cluster. This example creates the <code class="literal">dlm</code> resource as part of a resource group named <code class="literal">locking</code>.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence</strong></span></pre></li><li class="listitem"><p class="simpara">
						Clone the <code class="literal">locking</code> resource group so that the resource group can be active on both nodes of the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource clone locking interleave=true</strong></span></pre></li><li class="listitem"><p class="simpara">
						Set up an <code class="literal">lvmlockd</code> resource as part of the <code class="literal">locking</code> resource group.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence</strong></span></pre></li><li class="listitem"><p class="simpara">
						Check the status of the cluster to ensure that the <code class="literal">locking</code> resource group has started on both nodes of the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status --full</strong></span>
Cluster name: my_cluster
[...]

Online: [ z1.example.com (1) z2.example.com (2) ]

Full list of resources:

 smoke-apc      (stonith:fence_apc):    Started z1.example.com
 Clone Set: locking-clone [locking]
     Resource Group: locking:0
         dlm    (ocf::pacemaker:controld):      Started z1.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
     Resource Group: locking:1
         dlm    (ocf::pacemaker:controld):      Started z2.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
     Started: [ z1.example.com z2.example.com ]</pre></li><li class="listitem"><p class="simpara">
						On one node of the cluster, create two shared volume groups. One volume group will contain two GFS2 file systems, and the other volume group will contain one GFS2 file system.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							If your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order">Configuring startup order for resource dependencies not managed by Pacemaker</a>.
						</p></div></rh-alert><p class="simpara">
						The following command creates the shared volume group <code class="literal">shared_vg1</code> on <code class="literal">/dev/vdb</code>.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>vgcreate --shared shared_vg1 /dev/vdb</strong></span>
  Physical volume "/dev/vdb" successfully created.
  Volume group "shared_vg1" successfully created
  VG shared_vg1 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre><p class="simpara">
						The following command creates the shared volume group <code class="literal">shared_vg2</code> on <code class="literal">/dev/vdc</code>.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>vgcreate --shared shared_vg2 /dev/vdc</strong></span>
  Physical volume "/dev/vdc" successfully created.
  Volume group "shared_vg2" successfully created
  VG shared_vg2 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre></li><li class="listitem"><p class="simpara">
						On the second node in the cluster:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								If the use of a devices file is enabled with the <code class="literal">use_devicesfile = 1</code> parameter in the <code class="literal">lvm.conf</code> file, add the shared devices to the devices file This feature is enabled by default.
							</p><pre class="literallayout">[root@z2 ~]# <span class="strong strong"><strong>lvmdevices --adddev /dev/vdb</strong></span>
[root@z2 ~]# <span class="strong strong"><strong>lvmdevices --adddev /dev/vdc</strong></span></pre></li><li class="listitem"><p class="simpara">
								Start the lock manager for each of the shared volume groups.
							</p><pre class="literallayout">[root@z2 ~]# <span class="strong strong"><strong>vgchange --lockstart shared_vg1</strong></span>
  VG shared_vg1 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...
[root@z2 ~]# <span class="strong strong"><strong>vgchange --lockstart shared_vg2</strong></span>
  VG shared_vg2 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						On one node in the cluster, create the shared logical volumes and format the volumes with a GFS2 file system. One journal is required for each node that mounts the file system. Ensure that you create enough journals for each of the nodes in your cluster. The format of the lock table name is <span class="emphasis"><em>ClusterName:FSName</em></span> where <span class="emphasis"><em>ClusterName</em></span> is the name of the cluster for which the GFS2 file system is being created and <span class="emphasis"><em>FSName</em></span> is the file system name, which must be unique for all <code class="literal">lock_dlm</code> file systems over the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvcreate --activate sy -L5G -n shared_lv1 shared_vg1</strong></span>
  Logical volume "shared_lv1" created.
[root@z1 ~]# <span class="strong strong"><strong>lvcreate --activate sy -L5G -n shared_lv2 shared_vg1</strong></span>
  Logical volume "shared_lv2" created.
[root@z1 ~]# <span class="strong strong"><strong>lvcreate --activate sy -L5G -n shared_lv1 shared_vg2</strong></span>
  Logical volume "shared_lv1" created.

[root@z1 ~]# <span class="strong strong"><strong>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo1 /dev/shared_vg1/shared_lv1</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo2 /dev/shared_vg1/shared_lv2</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo3 /dev/shared_vg2/shared_lv1</strong></span></pre></li><li class="listitem"><p class="simpara">
						Create an <code class="literal">LVM-activate</code> resource for each logical volume to automatically activate that logical volume on all nodes.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create an <code class="literal">LVM-activate</code> resource named <code class="literal">sharedlv1</code> for the logical volume <code class="literal">shared_lv1</code> in volume group <code class="literal">shared_vg1</code>. This command also creates the resource group <code class="literal">shared_vg1</code> that includes the resource. In this example, the resource group has the same name as the shared volume group that includes the logical volume.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedlv1 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd</strong></span></pre></li><li class="listitem"><p class="simpara">
								Create an <code class="literal">LVM-activate</code> resource named <code class="literal">sharedlv2</code> for the logical volume <code class="literal">shared_lv2</code> in volume group <code class="literal">shared_vg1</code>. This resource will also be part of the resource group <code class="literal">shared_vg1</code>.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedlv2 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv2 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd</strong></span></pre></li><li class="listitem"><p class="simpara">
								Create an <code class="literal">LVM-activate</code> resource named <code class="literal">sharedlv3</code> for the logical volume <code class="literal">shared_lv1</code> in volume group <code class="literal">shared_vg2</code>. This command also creates the resource group <code class="literal">shared_vg2</code> that includes the resource.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedlv3 --group shared_vg2 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg2 activation_mode=shared vg_access_mode=lvmlockd</strong></span></pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Clone the two new resource groups.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource clone shared_vg1 interleave=true</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>pcs resource clone shared_vg2 interleave=true</strong></span></pre></li><li class="listitem"><p class="simpara">
						Configure ordering constraints to ensure that the <code class="literal">locking</code> resource group that includes the <code class="literal">dlm</code> and <code class="literal">lvmlockd</code> resources starts first.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs constraint order start locking-clone then shared_vg1-clone</strong></span>
Adding locking-clone shared_vg1-clone (kind: Mandatory) (Options: first-action=start then-action=start)
[root@z1 ~]# <span class="strong strong"><strong>pcs constraint order start locking-clone then shared_vg2-clone</strong></span>
Adding locking-clone shared_vg2-clone (kind: Mandatory) (Options: first-action=start then-action=start)</pre></li><li class="listitem"><p class="simpara">
						Configure colocation constraints to ensure that the <code class="literal">vg1</code> and <code class="literal">vg2</code> resource groups start on the same node as the <code class="literal">locking</code> resource group.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs constraint colocation add shared_vg1-clone with locking-clone</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>pcs constraint colocation add shared_vg2-clone with locking-clone</strong></span></pre></li><li class="listitem"><p class="simpara">
						On both nodes in the cluster, verify that the logical volumes are active. There may be a delay of a few seconds.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvs</strong></span>
  LV         VG          Attr       LSize
  shared_lv1 shared_vg1  -wi-a----- 5.00g
  shared_lv2 shared_vg1  -wi-a----- 5.00g
  shared_lv1 shared_vg2  -wi-a----- 5.00g

[root@z2 ~]# <span class="strong strong"><strong>lvs</strong></span>
  LV         VG          Attr       LSize
  shared_lv1 shared_vg1  -wi-a----- 5.00g
  shared_lv2 shared_vg1  -wi-a----- 5.00g
  shared_lv1 shared_vg2  -wi-a----- 5.00g</pre></li><li class="listitem"><p class="simpara">
						Create a file system resource to automatically mount each GFS2 file system on all nodes.
					</p><p class="simpara">
						You should not add the file system to the <code class="literal">/etc/fstab</code> file because it will be managed as a Pacemaker cluster resource. Mount options can be specified as part of the resource configuration with <code class="literal">options=<span class="emphasis"><em>options</em></span></code>. Run the <code class="literal command">pcs resource describe Filesystem</code> command to display the full configuration options.
					</p><p class="simpara">
						The following commands create the file system resources. These commands add each resource to the resource group that includes the logical volume resource for that file system.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/shared_vg1/shared_lv1" directory="/mnt/gfs1" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedfs2 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/shared_vg1/shared_lv2" directory="/mnt/gfs2" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedfs3 --group shared_vg2 ocf:heartbeat:Filesystem device="/dev/shared_vg2/shared_lv1" directory="/mnt/gfs3" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</strong></span></pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Verify that the GFS2 file systems are mounted on both nodes of the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>mount | grep gfs2</strong></span>
/dev/mapper/shared_vg1-shared_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg1-shared_lv2 on /mnt/gfs2 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg2-shared_lv1 on /mnt/gfs3 type gfs2 (rw,noatime,seclabel)

[root@z2 ~]# <span class="strong strong"><strong>mount | grep gfs2</strong></span>
/dev/mapper/shared_vg1-shared_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg1-shared_lv2 on /mnt/gfs2 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg2-shared_lv1 on /mnt/gfs3 type gfs2 (rw,noatime,seclabel)</pre></li><li class="listitem"><p class="simpara">
						Check the status of the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status --full</strong></span>
Cluster name: my_cluster
[...]

Full list of resources:

 smoke-apc      (stonith:fence_apc):    Started z1.example.com
 Clone Set: locking-clone [locking]
     Resource Group: locking:0
         dlm    (ocf::pacemaker:controld):      Started z2.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
     Resource Group: locking:1
         dlm    (ocf::pacemaker:controld):      Started z1.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
     Started: [ z1.example.com z2.example.com ]
 Clone Set: shared_vg1-clone [shared_vg1]
     Resource Group: shared_vg1:0
         sharedlv1      (ocf::heartbeat:LVM-activate):  Started z2.example.com
         sharedlv2      (ocf::heartbeat:LVM-activate):  Started z2.example.com
         sharedfs1      (ocf::heartbeat:Filesystem):    Started z2.example.com
         sharedfs2      (ocf::heartbeat:Filesystem):    Started z2.example.com
     Resource Group: shared_vg1:1
         sharedlv1      (ocf::heartbeat:LVM-activate):  Started z1.example.com
         sharedlv2      (ocf::heartbeat:LVM-activate):  Started z1.example.com
         sharedfs1      (ocf::heartbeat:Filesystem):    Started z1.example.com
         sharedfs2      (ocf::heartbeat:Filesystem):    Started z1.example.com
     Started: [ z1.example.com z2.example.com ]
 Clone Set: shared_vg2-clone [shared_vg2]
     Resource Group: shared_vg2:0
         sharedlv3      (ocf::heartbeat:LVM-activate):  Started z2.example.com
         sharedfs3      (ocf::heartbeat:Filesystem):    Started z2.example.com
     Resource Group: shared_vg2:1
         sharedlv3      (ocf::heartbeat:LVM-activate):  Started z1.example.com
         sharedfs3      (ocf::heartbeat:Filesystem):    Started z1.example.com
     Started: [ z1.example.com z2.example.com ]

...</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/index">Configuring GFS2 file systems</a>
					</li><li class="listitem">
						<a class="link" href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/deploying_rhel_9_on_microsoft_azure/index#configuring-rhel-high-availability-on-azure_cloud-content-azure">Configuring a Red Hat High Availability cluster on Microsoft Azure</a>
					</li><li class="listitem">
						<a class="link" href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/deploying_rhel_9_on_amazon_web_services/index#configuring-a-red-hat-high-availability-cluster-on-aws_deploying-a-virtual-machine-on-aws">Configuring a Red Hat High Availability cluster on AWS</a>
					</li><li class="listitem">
						<a class="link" href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/deploying_rhel_9_on_google_cloud_platform/index#configuring-rhel-ha-on-gcp_cloud-content-gcp">Configuring a Red Hat High Availability Cluster on Google Cloud Platform</a>
					</li></ul></div></section><section class="section" id="proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster"><div class="titlepage"><div><div><h3 class="title">8.2. Configuring an encrypted GFS2 file system in a cluster</h3></div></div></div><p class="_abstract _abstract">
				You can create a Pacemaker cluster that includes a LUKS encrypted GFS2 file system with the following procedure. In this example, you create one GFS2 file systems on a logical volume and encrypt the file system. Encrypted GFS2 file systems are supported using the <code class="literal">crypt</code> resource agent, which provides support for LUKS encryption.
			</p><p>
				There are three parts to this procedure:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Configuring a shared logical volume in a Pacemaker cluster
					</li><li class="listitem">
						Encrypting the logical volume and creating a <code class="literal">crypt</code> resource
					</li><li class="listitem">
						Formatting the encrypted logical volume with a GFS2 file system and creating a file system resource for the cluster
					</li></ul></div><section class="section" id="configure_a_shared_logical_volume_in_a_pacemaker_cluster"><div class="titlepage"><div><div><h4 class="title">8.2.1. Configure a shared logical volume in a Pacemaker cluster</h4></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install and start the cluster software on two cluster nodes and create a basic two-node cluster.
						</li><li class="listitem">
							Configure fencing for the cluster.
						</li></ul></div><p>
					For information about creating a Pacemaker cluster and configuring fencing for the cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters">Creating a Red Hat High-Availability cluster with Pacemaker</a>.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On both nodes in the cluster, enable the repository for Resilient Storage that corresponds to your system architecture. For example, to enable the Resilient Storage repository for an x86_64 system, you can enter the following <code class="literal">subscription-manager</code> command:
						</p><pre class="literallayout"># <span class="strong strong"><strong>subscription-manager repos --enable=rhel-9-for-x86_64-resilientstorage-rpms</strong></span></pre><p class="simpara">
							Note that the Resilient Storage repository is a superset of the High Availability repository. If you enable the Resilient Storage repository you do not also need to enable the High Availability repository.
						</p></li><li class="listitem"><p class="simpara">
							On both nodes of the cluster, install the <code class="literal">lvm2-lockd</code>, <code class="literal">gfs2-utils</code>, and <code class="literal">dlm</code> packages. To support these packages, you must be subscribed to the AppStream channel and the Resilient Storage channel.
						</p><pre class="literallayout"># <span class="strong strong"><strong>dnf install lvm2-lockd gfs2-utils dlm</strong></span></pre></li><li class="listitem"><p class="simpara">
							On both nodes of the cluster, set the <code class="literal">use_lvmlockd</code> configuration option in the <code class="literal">/etc/lvm/lvm.conf</code> file to <code class="literal">use_lvmlockd=1</code>.
						</p><pre class="literallayout">...
use_lvmlockd = 1
...</pre></li><li class="listitem"><p class="simpara">
							Set the global Pacemaker parameter <code class="literal">no-quorum-policy</code> to <code class="literal">freeze</code>.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								By default, the value of <code class="literal">no-quorum-policy</code> is set to <code class="literal">stop</code>, indicating that when quorum is lost, all the resources on the remaining partition will immediately be stopped. Typically this default is the safest and most optimal option, but unlike most resources, GFS2 requires quorum to function. When quorum is lost both the applications using the GFS2 mounts and the GFS2 mount itself cannot be correctly stopped. Any attempts to stop these resources without quorum will fail which will ultimately result in the entire cluster being fenced every time quorum is lost.
							</p><p>
								To address this situation, set <code class="literal">no-quorum-policy</code> to <code class="literal">freeze</code> when GFS2 is in use. This means that when quorum is lost, the remaining partition will do nothing until quorum is regained.
							</p></div></rh-alert><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs property set no-quorum-policy=freeze</strong></span></pre></li><li class="listitem"><p class="simpara">
							Set up a <code class="literal">dlm</code> resource. This is a required dependency for configuring a GFS2 file system in a cluster. This example creates the <code class="literal">dlm</code> resource as part of a resource group named <code class="literal">locking</code>.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence</strong></span></pre></li><li class="listitem"><p class="simpara">
							Clone the <code class="literal">locking</code> resource group so that the resource group can be active on both nodes of the cluster.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource clone locking interleave=true</strong></span></pre></li><li class="listitem"><p class="simpara">
							Set up an <code class="literal">lvmlockd</code> resource as part of the group <code class="literal">locking</code>.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence</strong></span></pre></li><li class="listitem"><p class="simpara">
							Check the status of the cluster to ensure that the <code class="literal">locking</code> resource group has started on both nodes of the cluster.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status --full</strong></span>
Cluster name: my_cluster
[...]

Online: [ z1.example.com (1) z2.example.com (2) ]

Full list of resources:

 smoke-apc      (stonith:fence_apc):    Started z1.example.com
 Clone Set: locking-clone [locking]
     Resource Group: locking:0
         dlm    (ocf::pacemaker:controld):      Started z1.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
     Resource Group: locking:1
         dlm    (ocf::pacemaker:controld):      Started z2.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
     Started: [ z1.example.com z2.example.com ]</pre></li><li class="listitem"><p class="simpara">
							On one node of the cluster, create a shared volume group.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								If your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order">Configuring startup order for resource dependencies not managed by Pacemaker</a>.
							</p></div></rh-alert><p class="simpara">
							The following command creates the shared volume group <code class="literal">shared_vg1</code> on <code class="literal">/dev/sda1</code>.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>vgcreate --shared shared_vg1 /dev/sda1</strong></span>
  Physical volume "/dev/sda1" successfully created.
  Volume group "shared_vg1" successfully created
  VG shared_vg1 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre></li><li class="listitem"><p class="simpara">
							On the second node in the cluster:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									If the use of a devices file is enabled with the <code class="literal">use_devicesfile = 1</code> parameter in the <code class="literal">lvm.conf</code> file, add the shared device to the devices file on the second node in the cluster. This feature is enabled by default.
								</p><pre class="literallayout">[root@z2 ~]# <span class="strong strong"><strong>lvmdevices --adddev /dev/sda1</strong></span></pre></li><li class="listitem"><p class="simpara">
									Start the lock manager for the shared volume group.
								</p><pre class="literallayout">[root@z2 ~]# <span class="strong strong"><strong>vgchange --lockstart shared_vg1</strong></span>
  VG shared_vg1 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							On one node in the cluster, create the shared logical volume.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvcreate --activate sy -L5G -n shared_lv1 shared_vg1</strong></span>
  Logical volume "shared_lv1" created.</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">LVM-activate</code> resource for the logical volume to automatically activate the logical volume on all nodes.
						</p><p class="simpara">
							The following command creates an <code class="literal">LVM-activate</code> resource named <code class="literal">sharedlv1</code> for the logical volume <code class="literal">shared_lv1</code> in volume group <code class="literal">shared_vg1</code>. This command also creates the resource group <code class="literal">shared_vg1</code> that includes the resource. In this example, the resource group has the same name as the shared volume group that includes the logical volume.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedlv1 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd</strong></span></pre></li><li class="listitem"><p class="simpara">
							Clone the new resource group.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource clone shared_vg1 interleave=true</strong></span></pre></li><li class="listitem"><p class="simpara">
							Configure an ordering constraints to ensure that the <code class="literal">locking</code> resource group that includes the <code class="literal">dlm</code> and <code class="literal">lvmlockd</code> resources starts first.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs constraint order start locking-clone then shared_vg1-clone</strong></span>
Adding locking-clone shared_vg1-clone (kind: Mandatory) (Options: first-action=start then-action=start)</pre></li><li class="listitem"><p class="simpara">
							Configure a colocation constraints to ensure that the <code class="literal">vg1</code> and <code class="literal">vg2</code> resource groups start on the same node as the <code class="literal">locking</code> resource group.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs constraint colocation add shared_vg1-clone with locking-clone</strong></span></pre></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						On both nodes in the cluster, verify that the logical volume is active. There may be a delay of a few seconds.
					</p></div><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvs</strong></span>
  LV         VG          Attr       LSize
  shared_lv1 shared_vg1  -wi-a----- 5.00g

[root@z2 ~]# <span class="strong strong"><strong>lvs</strong></span>
  LV         VG          Attr       LSize
  shared_lv1 shared_vg1  -wi-a----- 5.00g</pre></section><section class="section" id="encrypt_the_logical_volume_and_create_a_crypt_resource"><div class="titlepage"><div><div><h4 class="title">8.2.2. Encrypt the logical volume and create a crypt resource</h4></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have configured a shared logical volume in a Pacemaker cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On one node in the cluster, create a new file that will contain the crypt key and set the permissions on the file so that it is readable only by root.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>touch /etc/crypt_keyfile</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>chmod 600 /etc/crypt_keyfile</strong></span></pre></li><li class="listitem"><p class="simpara">
							Create the crypt key.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>dd if=/dev/urandom bs=4K count=1 of=/etc/crypt_keyfile</strong></span>
1+0 records in
1+0 records out
4096 bytes (4.1 kB, 4.0 KiB) copied, 0.000306202 s, 13.4 MB/s
[root@z1 ~]# <span class="strong strong"><strong>scp /etc/crypt_keyfile root@z2.example.com:/etc/</strong></span></pre></li><li class="listitem"><p class="simpara">
							Distribute the crypt keyfile to the other nodes in the cluster, using the <code class="literal">-p</code> parameter to preserve the permissions you set.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>scp -p /etc/crypt_keyfile root@z2.example.com:/etc/</strong></span></pre></li><li class="listitem"><p class="simpara">
							Create the encrypted device on the LVM volume where you will configure the encrypted GFS2 file system.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>cryptsetup luksFormat /dev/shared_vg1/shared_lv1 --type luks2 --key-file=/etc/crypt_keyfile</strong></span>
WARNING!
========
This will overwrite data on /dev/shared_vg1/shared_lv1 irrevocably.

Are you sure? (Type 'yes' in capital letters): YES</pre></li><li class="listitem"><p class="simpara">
							Create the crypt resource as part of the <code class="literal">shared_vg1</code> volume group.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create crypt --group shared_vg1 ocf:heartbeat:crypt crypt_dev="luks_lv1" crypt_type=luks2 key_file=/etc/crypt_keyfile encrypted_dev="/dev/shared_vg1/shared_lv1"</strong></span></pre></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						Ensure that the crypt resource has created the crypt device, which in this example is <code class="literal">/dev/mapper/luks_lv1</code>.
					</p></div><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>ls -l /dev/mapper/</strong></span>
...
lrwxrwxrwx 1 root root 7 Mar 4 09:52 luks_lv1 -&gt; ../dm-3
...</pre></section><section class="section" id="format_the_encrypted_logical_volume_with_a_gfs2_file_system_and_create_a_file_system_resource_for_the_cluster"><div class="titlepage"><div><div><h4 class="title">8.2.3. Format the encrypted logical volume with a GFS2 file system and create a file system resource for the cluster</h4></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have encrypted the logical volume and created a crypt resource.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On one node in the cluster, format the volume with a GFS2 file system. One journal is required for each node that mounts the file system. Ensure that you create enough journals for each of the nodes in your cluster. The format of the lock table name is <span class="emphasis"><em>ClusterName:FSName</em></span> where <span class="emphasis"><em>ClusterName</em></span> is the name of the cluster for which the GFS2 file system is being created and <span class="emphasis"><em>FSName</em></span> is the file system name, which must be unique for all <code class="literal">lock_dlm</code> file systems over the cluster.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>mkfs.gfs2 -j3 -p lock_dlm -t my_cluster:gfs2-demo1 /dev/mapper/luks_lv1</strong></span>
/dev/mapper/luks_lv1 is a symbolic link to /dev/dm-3
This will destroy any data on /dev/dm-3
Are you sure you want to proceed? [y/n] y
Discarding device contents (may take a while on large devices): Done
Adding journals: Done
Building resource groups: Done
Creating quota file: Done
Writing superblock and syncing: Done
Device:                    /dev/mapper/luks_lv1
Block size:                4096
Device size:               4.98 GB (1306624 blocks)
Filesystem size:           4.98 GB (1306622 blocks)
Journals:                  3
Journal size:              16MB
Resource groups:           23
Locking protocol:          "lock_dlm"
Lock table:                "my_cluster:gfs2-demo1"
UUID:                      de263f7b-0f12-4d02-bbb2-56642fade293</pre></li><li class="listitem"><p class="simpara">
							Create a file system resource to automatically mount the GFS2 file system on all nodes.
						</p><p class="simpara">
							Do not add the file system to the <code class="literal">/etc/fstab</code> file because it will be managed as a Pacemaker cluster resource. Mount options can be specified as part of the resource configuration with <code class="literal">options=<span class="emphasis"><em>options</em></span></code>. Run the <code class="literal">pcs resource describe Filesystem</code> command for full configuration options.
						</p><p class="simpara">
							The following command creates the file system resource. This command adds the resource to the resource group that includes the logical volume resource for that file system.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/mapper/luks_lv1" directory="/mnt/gfs1" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</strong></span></pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that the GFS2 file system is mounted on both nodes of the cluster.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>mount | grep gfs2</strong></span>
/dev/mapper/luks_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)

[root@z2 ~]# <span class="strong strong"><strong>mount | grep gfs2</strong></span>
/dev/mapper/luks_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)</pre></li><li class="listitem"><p class="simpara">
							Check the status of the cluster.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status --full</strong></span>
Cluster name: my_cluster
[...]

Full list of resources:

  smoke-apc      (stonith:fence_apc):    Started z1.example.com
  Clone Set: locking-clone [locking]
      Resource Group: locking:0
          dlm    (ocf::pacemaker:controld):      Started z2.example.com
          lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
      Resource Group: locking:1
          dlm    (ocf::pacemaker:controld):      Started z1.example.com
          lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
     Started: [ z1.example.com z2.example.com ]
  Clone Set: shared_vg1-clone [shared_vg1]
     Resource Group: shared_vg1:0
             sharedlv1      (ocf::heartbeat:LVM-activate):  Started z2.example.com
             crypt       (ocf::heartbeat:crypt) Started z2.example.com
             sharedfs1      (ocf::heartbeat:Filesystem):    Started z2.example.com
    Resource Group: shared_vg1:1
             sharedlv1      (ocf::heartbeat:LVM-activate):  Started z1.example.com
             crypt      (ocf::heartbeat:crypt)  Started z1.example.com
             sharedfs1      (ocf::heartbeat:Filesystem):    Started z1.example.com
          Started:  [z1.example.com z2.example.com ]
...</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/index">Configuring GFS2 file systems</a>
						</li></ul></div></section></section></section><section class="chapter" id="con_gfs2-tracepoints-configuring-gfs2-file-systems"><div class="titlepage"><div><div><h2 class="title">Chapter 9. GFS2 tracepoints and the glock debugfs interface</h2></div></div></div><p class="_abstract _abstract">
			This documentation on both the GFS2 tracepoints and the glock <code class="literal">debugfs</code> interface is intended for advanced users who are familiar with file system internals and who would like to learn more about the design of GFS2 and how to debug GFS2-specific issues.
		</p><p>
			The following sections describe GFS2 tracepoints and the GFS2 <code class="literal">glocks</code> file.
		</p><section class="section" id="gfs2_tracepoint_types"><div class="titlepage"><div><div><h3 class="title">9.1. GFS2 tracepoint types</h3></div></div></div><p>
				There are currently three types of GFS2 tracepoints: <span class="emphasis"><em>glock</em></span> (pronounced "gee-lock") tracepoints, <span class="emphasis"><em>bmap</em></span> tracepoints and <span class="emphasis"><em>log</em></span> tracepoints. These can be used to monitor a running GFS2 file system. Tracepoints are particularly useful when a problem, such as a hang or performance issue, is reproducible and thus the tracepoint output can be obtained during the problematic operation. In GFS2, glocks are the primary cache control mechanism and they are the key to understanding the performance of the core of GFS2. The bmap (block map) tracepoints can be used to monitor block allocations and block mapping (lookup of already allocated blocks in the on-disk metadata tree) as they happen and check for any issues relating to locality of access. The log tracepoints keep track of the data being written to and released from the journal and can provide useful information about that part of GFS2.
			</p><p>
				The tracepoints are designed to be as generic as possible. This should mean that it will not be necessary to change the API during the course of Red Hat Enterprise Linux 9. On the other hand, users of this interface should be aware that this is a debugging interface and not part of the normal Red Hat Enterprise Linux 9 API set, and as such Red Hat makes no guarantees that changes in the GFS2 tracepoints interface will not occur.
			</p><p>
				Tracepoints are a generic feature of Red Hat Enterprise Linux and their scope goes well beyond GFS2. In particular they are used to implement the <code class="literal">blktrace</code> infrastructure and the <code class="literal">blktrace</code> tracepoints can be used in combination with those of GFS2 to gain a fuller picture of the system performance. Due to the level at which the tracepoints operate, they can produce large volumes of data in a very short period of time. They are designed to put a minimum load on the system when they are enabled, but it is inevitable that they will have some effect. Filtering events by a variety of means can help reduce the volume of data and help focus on obtaining just the information which is useful for understanding any particular situation.
			</p></section><section class="section" id="ap-tracepoints-gfs2"><div class="titlepage"><div><div><h3 class="title">9.2. Tracepoints</h3></div></div></div><p>
				The tracepoints can be found under the <code class="literal">/sys/kernel/debug/tracing/</code> directory assuming that <code class="literal">debugfs</code> is mounted in the standard place at the <code class="literal">/sys/kernel/debug</code> directory. The <code class="literal">events</code> subdirectory contains all the tracing events that may be specified and, provided the <code class="literal">gfs2</code> module is loaded, there will be a <code class="literal">gfs2</code> subdirectory containing further subdirectories, one for each GFS2 event. The contents of the <code class="literal">/sys/kernel/debug/tracing/events/gfs2</code> directory should look roughly like the following:
			</p><pre class="literallayout">[root@chywoon gfs2]# <span class="strong strong"><strong>ls</strong></span>
enable            gfs2_bmap       gfs2_glock_queue         gfs2_log_flush
filter            gfs2_demote_rq  gfs2_glock_state_change  gfs2_pin
gfs2_block_alloc  gfs2_glock_put  gfs2_log_blocks          gfs2_promote</pre><p>
				To enable all the GFS2 tracepoints, enter the following command:
			</p><pre class="literallayout">[root@chywoon gfs2]# <span class="strong strong"><strong>echo -n 1 &gt;/sys/kernel/debug/tracing/events/gfs2/enable</strong></span></pre><p>
				To enable a specific tracepoint, there is an <code class="literal">enable</code> file in each of the individual event subdirectories. The same is true of the <code class="literal">filter</code> file which can be used to set an event filter for each event or set of events. The meaning of the individual events is explained in more detail below.
			</p><p>
				The output from the tracepoints is available in ASCII or binary format. This appendix does not currently cover the binary interface. The ASCII interface is available in two ways. To list the current content of the ring buffer, you can enter the following command:
			</p><pre class="literallayout">[root@chywoon gfs2]# <span class="strong strong"><strong>cat /sys/kernel/debug/tracing/trace</strong></span></pre><p>
				This interface is useful in cases where you are using a long-running process for a certain period of time and, after some event, want to look back at the latest captured information in the buffer. An alternative interface, <code class="literal">/sys/kernel/debug/tracing/trace_pipe</code>, can be used when all the output is required. Events are read from this file as they occur; there is no historical information available through this interface. The format of the output is the same from both interfaces and is described for each of the GFS2 events in the later sections of this appendix.
			</p><p>
				A utility called <code class="literal command">trace-cmd</code> is available for reading tracepoint data. For more information about this utility, see <a class="link" href="http://lwn.net/Articles/341902/"> http://lwn.net/Articles/341902/</a>. The <code class="literal command">trace-cmd</code> utility can be used in a similar way to the <code class="literal command">strace</code> utility, for example to run a command while gathering trace data from various sources.
			</p></section><section class="section" id="ap-glocks-gfs2"><div class="titlepage"><div><div><h3 class="title">9.3. Glocks</h3></div></div></div><p>
				To understand GFS2, the most important concept to understand, and the one which sets it aside from other file systems, is the concept of glocks. In terms of the source code, a glock is a data structure that brings together the DLM and caching into a single state machine. Each glock has a 1:1 relationship with a single DLM lock, and provides caching for that lock state so that repetitive operations carried out from a single node of the file system do not have to repeatedly call the DLM, and thus they help avoid unnecessary network traffic. There are two broad categories of glocks, those which cache metadata and those which do not. The inode glocks and the resource group glocks both cache metadata, other types of glocks do not cache metadata. The inode glock is also involved in the caching of data in addition to metadata and has the most complex logic of all glocks.
			</p><rh-table id="tb-table-glock-dlm-modes"><table class="lt-4-cols lt-7-rows"><caption>Table 9.1. Glock Modes and DLM Lock Modes</caption><colgroup><col style="width: 33%; " class="col_1"><!--Empty--><col style="width: 33%; " class="col_2"><!--Empty--><col style="width: 33%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm139688928243344" scope="col">Glock mode</th><th align="left" valign="top" id="idm139688928242256" scope="col">DLM lock mode</th><th align="left" valign="top" id="idm139688931515104" scope="col">Notes</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139688928243344"> <p>
								UN
							</p>
							 </td><td align="left" valign="top" headers="idm139688928242256"> <p>
								IV/NL
							</p>
							 </td><td align="left" valign="top" headers="idm139688931515104"> <p>
								Unlocked (no DLM lock associated with glock or NL lock depending on I flag)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928243344"> <p>
								SH
							</p>
							 </td><td align="left" valign="top" headers="idm139688928242256"> <p>
								PR
							</p>
							 </td><td align="left" valign="top" headers="idm139688931515104"> <p>
								Shared (protected read) lock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928243344"> <p>
								EX
							</p>
							 </td><td align="left" valign="top" headers="idm139688928242256"> <p>
								EX
							</p>
							 </td><td align="left" valign="top" headers="idm139688931515104"> <p>
								Exclusive lock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928243344"> <p>
								DF
							</p>
							 </td><td align="left" valign="top" headers="idm139688928242256"> <p>
								CW
							</p>
							 </td><td align="left" valign="top" headers="idm139688931515104"> <p>
								Deferred (concurrent write) used for Direct I/O and file system freeze
							</p>
							 </td></tr></tbody></table></rh-table><p>
				Glocks remain in memory until either they are unlocked (at the request of another node or at the request of the VM) and there are no local users. At that point they are removed from the glock hash table and freed. When a glock is created, the DLM lock is not associated with the glock immediately. The DLM lock becomes associated with the glock upon the first request to the DLM, and if this request is successful then the 'I' (initial) flag will be set on the glock. The "Glock Flags" table in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/con_gfs2-tracepoints-configuring-gfs2-file-systems#ap-glock-debugfs-gfs2">The glock debugfs interface</a> shows the meanings of the different glock flags. Once the DLM has been associated with the glock, the DLM lock will always remain at least at NL (Null) lock mode until the glock is to be freed. A demotion of the DLM lock from NL to unlocked is always the last operation in the life of a glock.
			</p><p>
				Each glock can have a number of "holders" associated with it, each of which represents one lock request from the higher layers. System calls relating to GFS2 queue and dequeue holders from the glock to protect the critical section of code.
			</p><p>
				The glock state machine is based on a work queue. For performance reasons, tasklets would be preferable; however, in the current implementation we need to submit I/O from that context which prohibits their use.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					Workqueues have their own tracepoints which can be used in combination with the GFS2 tracepoints.
				</p></div></rh-alert><p>
				The following table shows what state may be cached under each of the glock modes and whether that cached state may be dirty. This applies to both inode and resource group locks, although there is no data component for the resource group locks, only metadata.
			</p><rh-table id="tb-table-glockmode-data"><table class="gt-4-cols lt-7-rows"><caption>Table 9.2. Glock Modes and Data Types</caption><colgroup><col style="width: 20%; " class="col_1"><!--Empty--><col style="width: 20%; " class="col_2"><!--Empty--><col style="width: 20%; " class="col_3"><!--Empty--><col style="width: 20%; " class="col_4"><!--Empty--><col style="width: 20%; " class="col_5"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm139688929965744" scope="col">Glock mode</th><th align="left" valign="top" id="idm139688929964656" scope="col">Cache Data</th><th align="left" valign="top" id="idm139688929963568" scope="col">Cache Metadata</th><th align="left" valign="top" id="idm139688929962480" scope="col">Dirty Data</th><th align="left" valign="top" id="idm139688929961392" scope="col">Dirty Metadata</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139688929965744"> <p>
								UN
							</p>
							 </td><td align="left" valign="top" headers="idm139688929964656"> <p>
								No
							</p>
							 </td><td align="left" valign="top" headers="idm139688929963568"> <p>
								No
							</p>
							 </td><td align="left" valign="top" headers="idm139688929962480"> <p>
								No
							</p>
							 </td><td align="left" valign="top" headers="idm139688929961392"> <p>
								No
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688929965744"> <p>
								SH
							</p>
							 </td><td align="left" valign="top" headers="idm139688929964656"> <p>
								Yes
							</p>
							 </td><td align="left" valign="top" headers="idm139688929963568"> <p>
								Yes
							</p>
							 </td><td align="left" valign="top" headers="idm139688929962480"> <p>
								No
							</p>
							 </td><td align="left" valign="top" headers="idm139688929961392"> <p>
								No
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688929965744"> <p>
								DF
							</p>
							 </td><td align="left" valign="top" headers="idm139688929964656"> <p>
								No
							</p>
							 </td><td align="left" valign="top" headers="idm139688929963568"> <p>
								Yes
							</p>
							 </td><td align="left" valign="top" headers="idm139688929962480"> <p>
								No
							</p>
							 </td><td align="left" valign="top" headers="idm139688929961392"> <p>
								No
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688929965744"> <p>
								EX
							</p>
							 </td><td align="left" valign="top" headers="idm139688929964656"> <p>
								Yes
							</p>
							 </td><td align="left" valign="top" headers="idm139688929963568"> <p>
								Yes
							</p>
							 </td><td align="left" valign="top" headers="idm139688929962480"> <p>
								Yes
							</p>
							 </td><td align="left" valign="top" headers="idm139688929961392"> <p>
								Yes
							</p>
							 </td></tr></tbody></table></rh-table></section><section class="section" id="ap-glock-debugfs-gfs2"><div class="titlepage"><div><div><h3 class="title">9.4. The glock debugfs interface</h3></div></div></div><p>
				The glock <code class="literal">debugfs</code> interface allows the visualization of the internal state of the glocks and the holders and it also includes some summary details of the objects being locked in some cases. Each line of the file either begins G: with no indentation (which refers to the glock itself) or it begins with a different letter, indented with a single space, and refers to the structures associated with the glock immediately above it in the file (H: is a holder, I: an inode, and R: a resource group). Here is an example of what the content of this file might look like:
			</p><pre class="literallayout">G:  s:SH n:5/75320 f:I t:SH d:EX/0 a:0 r:3
 H: s:SH f:EH e:0 p:4466 [postmark] gfs2_inode_lookup+0x14e/0x260 [gfs2]
G:  s:EX n:3/258028 f:yI t:EX d:EX/0 a:3 r:4
 H: s:EX f:tH e:0 p:4466 [postmark] gfs2_inplace_reserve_i+0x177/0x780 [gfs2]
 R: n:258028 f:05 b:22256/22256 i:16800
G:  s:EX n:2/219916 f:yfI t:EX d:EX/0 a:0 r:3
 I: n:75661/219916 t:8 f:0x10 d:0x00000000 s:7522/7522
G:  s:SH n:5/127205 f:I t:SH d:EX/0 a:0 r:3
 H: s:SH f:EH e:0 p:4466 [postmark] gfs2_inode_lookup+0x14e/0x260 [gfs2]
G:  s:EX n:2/50382 f:yfI t:EX d:EX/0 a:0 r:2
G:  s:SH n:5/302519 f:I t:SH d:EX/0 a:0 r:3
 H: s:SH f:EH e:0 p:4466 [postmark] gfs2_inode_lookup+0x14e/0x260 [gfs2]
G:  s:SH n:5/313874 f:I t:SH d:EX/0 a:0 r:3
 H: s:SH f:EH e:0 p:4466 [postmark] gfs2_inode_lookup+0x14e/0x260 [gfs2]
G:  s:SH n:5/271916 f:I t:SH d:EX/0 a:0 r:3
 H: s:SH f:EH e:0 p:4466 [postmark] gfs2_inode_lookup+0x14e/0x260 [gfs2]
G:  s:SH n:5/312732 f:I t:SH d:EX/0 a:0 r:3
 H: s:SH f:EH e:0 p:4466 [postmark] gfs2_inode_lookup+0x14e/0x260 [gfs2]</pre><p>
				The above example is a series of excerpts (from an approximately 18MB file) generated by the command <code class="literal command">cat /sys/kernel/debug/gfs2/unity:myfs/glocks &gt;my.lock</code> during a run of the postmark benchmark on a single node GFS2 file system. The glocks in the figure have been selected in order to show some of the more interesting features of the glock dumps.
			</p><p>
				The glock states are either EX (exclusive), DF (deferred), SH (shared) or UN (unlocked). These states correspond directly with DLM lock modes except for UN which may represent either the DLM null lock state, or that GFS2 does not hold a DLM lock (depending on the I flag as explained above). The s: field of the glock indicates the current state of the lock and the same field in the holder indicates the requested mode. If the lock is granted, the holder will have the H bit set in its flags (f: field). Otherwise, it will have the W wait bit set.
			</p><p>
				The n: field (number) indicates the number associated with each item. For glocks, that is the type number followed by the glock number so that in the above example, the first glock is n:5/75320; which indicates an <code class="literal">iopen</code> glock which relates to inode 75320. In the case of inode and <code class="literal">iopen</code> glocks, the glock number is always identical to the inode’s disk block number.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					The glock numbers (n: field) in the debugfs glocks file are in hexadecimal, whereas the tracepoints output lists them in decimal. This is for historical reasons; glock numbers were always written in hex, but decimal was chosen for the tracepoints so that the numbers could easily be compared with the other tracepoint output (from <code class="literal command">blktrace</code> for example) and with output from <code class="literal command">stat</code>(1).
				</p></div></rh-alert><p>
				The full listing of all the flags for both the holder and the glock are set out in the "Glock Flags" table, below, and the "Glock Holder Flags" table in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/con_gfs2-tracepoints-configuring-gfs2-file-systems#ap-glock-holders-gfs2">Glock holders</a>. The content of lock value blocks is not currently available through the glock <code class="literal">debugfs</code> interface. The following table shows the meanings of the different glock types.
			</p><rh-table id="tb-glock-types-ap"><table class="lt-4-cols lt-7-rows"><caption>Table 9.3. Glock Types</caption><colgroup><col style="width: 33%; " class="col_1"><!--Empty--><col style="width: 33%; " class="col_2"><!--Empty--><col style="width: 33%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm139688927578800" scope="col">Type number</th><th align="left" valign="top" id="idm139688927577712" scope="col">Lock type</th><th align="left" valign="top" id="idm139688927576624" scope="col">Use</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139688927578800"> <p>
								1
							</p>
							 </td><td align="left" valign="top" headers="idm139688927577712"> <p>
								trans
							</p>
							 </td><td align="left" valign="top" headers="idm139688927576624"> <p>
								Transaction lock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927578800"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm139688927577712"> <p>
								inode
							</p>
							 </td><td align="left" valign="top" headers="idm139688927576624"> <p>
								Inode metadata and data
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927578800"> <p>
								3
							</p>
							 </td><td align="left" valign="top" headers="idm139688927577712"> <p>
								rgrp
							</p>
							 </td><td align="left" valign="top" headers="idm139688927576624"> <p>
								Resource group metadata
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927578800"> <p>
								4
							</p>
							 </td><td align="left" valign="top" headers="idm139688927577712"> <p>
								meta
							</p>
							 </td><td align="left" valign="top" headers="idm139688927576624"> <p>
								The superblock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927578800"> <p>
								5
							</p>
							 </td><td align="left" valign="top" headers="idm139688927577712"> <p>
								iopen
							</p>
							 </td><td align="left" valign="top" headers="idm139688927576624"> <p>
								Inode last closer detection
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927578800"> <p>
								6
							</p>
							 </td><td align="left" valign="top" headers="idm139688927577712"> <p>
								flock
							</p>
							 </td><td align="left" valign="top" headers="idm139688927576624"> <p>
								<code class="literal command">flock</code>(2) syscall
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927578800"> <p>
								8
							</p>
							 </td><td align="left" valign="top" headers="idm139688927577712"> <p>
								quota
							</p>
							 </td><td align="left" valign="top" headers="idm139688927576624"> <p>
								Quota operations
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927578800"> <p>
								9
							</p>
							 </td><td align="left" valign="top" headers="idm139688927577712"> <p>
								journal
							</p>
							 </td><td align="left" valign="top" headers="idm139688927576624"> <p>
								Journal mutex
							</p>
							 </td></tr></tbody></table></rh-table><p>
				One of the more important glock flags is the l (locked) flag. This is the bit lock that is used to arbitrate access to the glock state when a state change is to be performed. It is set when the state machine is about to send a remote lock request through the DLM, and only cleared when the complete operation has been performed. Sometimes this can mean that more than one lock request will have been sent, with various invalidations occurring between times.
			</p><p>
				The following table shows the meanings of the different glock flags.
			</p><rh-table id="tb-glock-flags-ap"><table class="lt-4-cols lt-7-rows"><caption>Table 9.4. Glock Flags</caption><colgroup><col style="width: 33%; " class="col_1"><!--Empty--><col style="width: 33%; " class="col_2"><!--Empty--><col style="width: 33%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm139688927620736" scope="col">Flag</th><th align="left" valign="top" id="idm139688927619648" scope="col">Name</th><th align="left" valign="top" id="idm139688927618560" scope="col">Meaning</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139688927620736"> <p>
								d
							</p>
							 </td><td align="left" valign="top" headers="idm139688927619648"> <p>
								Pending demote
							</p>
							 </td><td align="left" valign="top" headers="idm139688927618560"> <p>
								A deferred (remote) demote request
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927620736"> <p>
								D
							</p>
							 </td><td align="left" valign="top" headers="idm139688927619648"> <p>
								Demote
							</p>
							 </td><td align="left" valign="top" headers="idm139688927618560"> <p>
								A demote request (local or remote)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927620736"> <p>
								f
							</p>
							 </td><td align="left" valign="top" headers="idm139688927619648"> <p>
								Log flush
							</p>
							 </td><td align="left" valign="top" headers="idm139688927618560"> <p>
								The log needs to be committed before releasing this glock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927620736"> <p>
								F
							</p>
							 </td><td align="left" valign="top" headers="idm139688927619648"> <p>
								Frozen
							</p>
							 </td><td align="left" valign="top" headers="idm139688927618560"> <p>
								Replies from remote nodes ignored - recovery is in progress.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927620736"> <p>
								i
							</p>
							 </td><td align="left" valign="top" headers="idm139688927619648"> <p>
								Invalidate in progress
							</p>
							 </td><td align="left" valign="top" headers="idm139688927618560"> <p>
								In the process of invalidating pages under this glock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927620736"> <p>
								I
							</p>
							 </td><td align="left" valign="top" headers="idm139688927619648"> <p>
								Initial
							</p>
							 </td><td align="left" valign="top" headers="idm139688927618560"> <p>
								Set when DLM lock is associated with this glock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927620736"> <p>
								l
							</p>
							 </td><td align="left" valign="top" headers="idm139688927619648"> <p>
								Locked
							</p>
							 </td><td align="left" valign="top" headers="idm139688927618560"> <p>
								The glock is in the process of changing state
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927620736"> <p>
								L
							</p>
							 </td><td align="left" valign="top" headers="idm139688927619648"> <p>
								LRU
							</p>
							 </td><td align="left" valign="top" headers="idm139688927618560"> <p>
								Set when the glock is on the LRU list`
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927620736"> <p>
								o
							</p>
							 </td><td align="left" valign="top" headers="idm139688927619648"> <p>
								Object
							</p>
							 </td><td align="left" valign="top" headers="idm139688927618560"> <p>
								Set when the glock is associated with an object (that is, an inode for type 2 glocks, and a resource group for type 3 glocks)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927620736"> <p>
								p
							</p>
							 </td><td align="left" valign="top" headers="idm139688927619648"> <p>
								Demote in progress
							</p>
							 </td><td align="left" valign="top" headers="idm139688927618560"> <p>
								The glock is in the process of responding to a demote request
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927620736"> <p>
								q
							</p>
							 </td><td align="left" valign="top" headers="idm139688927619648"> <p>
								Queued
							</p>
							 </td><td align="left" valign="top" headers="idm139688927618560"> <p>
								Set when a holder is queued to a glock, and cleared when the glock is held, but there are no remaining holders. Used as part of the algorithm the calculates the minimum hold time for a glock.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927620736"> <p>
								r
							</p>
							 </td><td align="left" valign="top" headers="idm139688927619648"> <p>
								Reply pending
							</p>
							 </td><td align="left" valign="top" headers="idm139688927618560"> <p>
								Reply received from remote node is awaiting processing
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688927620736"> <p>
								y
							</p>
							 </td><td align="left" valign="top" headers="idm139688927619648"> <p>
								Dirty
							</p>
							 </td><td align="left" valign="top" headers="idm139688927618560"> <p>
								Data needs flushing to disk before releasing this glock
							</p>
							 </td></tr></tbody></table></rh-table><p>
				When a remote callback is received from a node that wants to get a lock in a mode that conflicts with that being held on the local node, then one or other of the two flags D (demote) or d (demote pending) is set. In order to prevent starvation conditions when there is contention on a particular lock, each lock is assigned a minimum hold time. A node which has not yet had the lock for the minimum hold time is allowed to retain that lock until the time interval has expired.
			</p><p>
				If the time interval has expired, then the D (demote) flag will be set and the state required will be recorded. In that case the next time there are no granted locks on the holders queue, the lock will be demoted. If the time interval has not expired, then the d (demote pending) flag is set instead. This also schedules the state machine to clear d (demote pending) and set D (demote) when the minimum hold time has expired.
			</p><p>
				The I (initial) flag is set when the glock has been assigned a DLM lock. This happens when the glock is first used and the I flag will then remain set until the glock is finally freed (which the DLM lock is unlocked).
			</p></section><section class="section" id="ap-glock-holders-gfs2"><div class="titlepage"><div><div><h3 class="title">9.5. Glock holders</h3></div></div></div><p>
				The following table shows the meanings of the different glock holder flags.
			</p><rh-table id="tb-glock-holderflags-ap"><table class="lt-4-cols lt-7-rows"><caption>Table 9.5. Glock Holder Flags</caption><colgroup><col style="width: 33%; " class="col_1"><!--Empty--><col style="width: 33%; " class="col_2"><!--Empty--><col style="width: 33%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm139688928276128" scope="col">Flag</th><th align="left" valign="top" id="idm139688928275040" scope="col">Name</th><th align="left" valign="top" id="idm139688928273952" scope="col">Meaning</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139688928276128"> <p>
								a
							</p>
							 </td><td align="left" valign="top" headers="idm139688928275040"> <p>
								Async
							</p>
							 </td><td align="left" valign="top" headers="idm139688928273952"> <p>
								Do not wait for glock result (will poll for result later)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928276128"> <p>
								A
							</p>
							 </td><td align="left" valign="top" headers="idm139688928275040"> <p>
								Any
							</p>
							 </td><td align="left" valign="top" headers="idm139688928273952"> <p>
								Any compatible lock mode is acceptable
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928276128"> <p>
								c
							</p>
							 </td><td align="left" valign="top" headers="idm139688928275040"> <p>
								No cache
							</p>
							 </td><td align="left" valign="top" headers="idm139688928273952"> <p>
								When unlocked, demote DLM lock immediately
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928276128"> <p>
								e
							</p>
							 </td><td align="left" valign="top" headers="idm139688928275040"> <p>
								No expire
							</p>
							 </td><td align="left" valign="top" headers="idm139688928273952"> <p>
								Ignore subsequent lock cancel requests
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928276128"> <p>
								E
							</p>
							 </td><td align="left" valign="top" headers="idm139688928275040"> <p>
								Exact
							</p>
							 </td><td align="left" valign="top" headers="idm139688928273952"> <p>
								Must have exact lock mode
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928276128"> <p>
								F
							</p>
							 </td><td align="left" valign="top" headers="idm139688928275040"> <p>
								First
							</p>
							 </td><td align="left" valign="top" headers="idm139688928273952"> <p>
								Set when holder is the first to be granted for this lock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928276128"> <p>
								H
							</p>
							 </td><td align="left" valign="top" headers="idm139688928275040"> <p>
								Holder
							</p>
							 </td><td align="left" valign="top" headers="idm139688928273952"> <p>
								Indicates that requested lock is granted
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928276128"> <p>
								p
							</p>
							 </td><td align="left" valign="top" headers="idm139688928275040"> <p>
								Priority
							</p>
							 </td><td align="left" valign="top" headers="idm139688928273952"> <p>
								Enqueue holder at the head of the queue
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928276128"> <p>
								t
							</p>
							 </td><td align="left" valign="top" headers="idm139688928275040"> <p>
								Try
							</p>
							 </td><td align="left" valign="top" headers="idm139688928273952"> <p>
								A "try" lock
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928276128"> <p>
								T
							</p>
							 </td><td align="left" valign="top" headers="idm139688928275040"> <p>
								Try 1CB
							</p>
							 </td><td align="left" valign="top" headers="idm139688928273952"> <p>
								A "try" lock that sends a callback
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928276128"> <p>
								W
							</p>
							 </td><td align="left" valign="top" headers="idm139688928275040"> <p>
								Wait
							</p>
							 </td><td align="left" valign="top" headers="idm139688928273952"> <p>
								Set while waiting for request to complete
							</p>
							 </td></tr></tbody></table></rh-table><p>
				The most important holder flags are H (holder) and W (wait) as mentioned earlier, since they are set on granted lock requests and queued lock requests respectively. The ordering of the holders in the list is important. If there are any granted holders, they will always be at the head of the queue, followed by any queued holders.
			</p><p>
				If there are no granted holders, then the first holder in the list will be the one that triggers the next state change. Since demote requests are always onsidered higher priority than requests from the file system, that might not always directly result in a change to the state requested.
			</p><p>
				The glock subsystem supports two kinds of "try" lock. These are useful both because they allow the taking of locks out of the normal order (with suitable back-off and retry) and because they can be used to help avoid resources in use by other nodes. The normal t (try) lock is just what its name indicates; it is a "try" lock that does not do anything special. The T (<code class="literal">try 1CB</code>) lock, on the other hand, is identical to the t lock except that the DLM will send a single callback to current incompatible lock holders. One use of the T (<code class="literal">try 1CB</code>) lock is with the <code class="literal">iopen</code> locks, which are used to arbitrate among the nodes when an inode’s <code class="literal">i_nlink</code> count is zero, and determine which of the nodes will be responsible for deallocating the inode. The <code class="literal">iopen</code> glock is normally held in the shared state, but when the <code class="literal">i_nlink</code> count becomes zero and <code class="literal command">→evict_inode</code>() is called, it will request an exclusive lock with T (<code class="literal">try 1CB</code>) set. It will continue to deallocate the inode if the lock is granted. If the lock is not granted it will result in the node(s) which were preventing the grant of the lock marking their glock(s) with the D (demote) flag, which is checked at <code class="literal command">→drop_inode</code>() time in order to ensure that the deallocation is not forgotten.
			</p><p>
				This means that inodes that have zero link count but are still open will be deallocated by the node on which the final <code class="literal command">close</code>() occurs. Also, at the same time as the inode’s link count is decremented to zero the inode is marked as being in the special state of having zero link count but still in use in the resource group bitmap. This functions like the ext3 file system3’s orphan list in that it allows any subsequent reader of the bitmap to know that there is potentially space that might be reclaimed, and to attempt to reclaim it.
			</p></section><section class="section" id="ap-glock-tracepoints-gfs2"><div class="titlepage"><div><div><h3 class="title">9.6. Glock tracepoints</h3></div></div></div><p>
				The tracepoints are also designed to be able to confirm the correctness of the cache control by combining them with the <code class="literal command">blktrace</code> output and with knowledge of the on-disk layout. It is then possible to check that any given I/O has been issued and completed under the correct lock, and that no races are present.
			</p><p>
				The <code class="literal">gfs2_glock_state_change</code> tracepoint is the most important one to understand. It tracks every state change of the glock from initial creation right through to the final demotion which ends with <code class="literal">gfs2_glock_put</code> and the final NL to unlocked transition. The l (locked) glock flag is always set before a state change occurs and will not be cleared until after it has finished. There are never any granted holders (the H glock holder flag) during a state change. If there are any queued holders, they will always be in the W (waiting) state. When the state change is complete then the holders may be granted which is the final operation before the l glock flag is cleared.
			</p><p>
				The <code class="literal">gfs2_demote_rq</code> tracepoint keeps track of demote requests, both local and remote. Assuming that there is enough memory on the node, the local demote requests will rarely be seen, and most often they will be created by <code class="literal command">umount</code> or by occasional memory reclaim. The number of remote demote requests is a measure of the contention between nodes for a particular inode or resource group.
			</p><p>
				The <code class="literal">gfs2_glock_lock_time</code> tracepoint provides information about the time taken by requests to the DLM. The blocking (<code class="literal">b</code>) flag was introduced into the glock specifically to be used in combination with this tracepoint.
			</p><p>
				When a holder is granted a lock, <code class="literal command">gfs2_promote</code> is called, this occurs as the final stages of a state change or when a lock is requested which can be granted immediately due to the glock state already caching a lock of a suitable mode. If the holder is the first one to be granted for this glock, then the f (first) flag is set on that holder. This is currently used only by resource groups.
			</p></section><section class="section" id="ap-bmap-tracepoints-gfs2"><div class="titlepage"><div><div><h3 class="title">9.7. Bmap tracepoints</h3></div></div></div><p>
				Block mapping is a task central to any file system. GFS2 uses a traditional bitmap-based system with two bits per block. The main purpose of the tracepoints in this subsystem is to allow monitoring of the time taken to allocate and map blocks.
			</p><p>
				The <code class="literal">gfs2_bmap</code> tracepoint is called twice for each bmap operation: once at the start to display the bmap request, and once at the end to display the result. This makes it easy to match the requests and results together and measure the time taken to map blocks in different parts of the file system, different file offsets, or even of different files. It is also possible to see what the average extent sizes being returned are in comparison to those being requested.
			</p><p>
				The <code class="literal">gfs2_rs</code> tracepoint traces block reservations as they are created, used, and destroyed in the block allocator.
			</p><p>
				To keep track of allocated blocks, <code class="literal">gfs2_block_alloc</code> is called not only on allocations, but also on freeing of blocks. Since the allocations are all referenced according to the inode for which the block is intended, this can be used to track which physical blocks belong to which files in a live file system. This is particularly useful when combined with <code class="literal">blktrace</code>, which will show problematic I/O patterns that may then be referred back to the relevant inodes using the mapping gained by means this tracepoint.
			</p><p>
				Direct I/O (<code class="literal">iomap</code>) is an alternative cache policy which allows file data transfers to happen directly between disk and the user’s buffer. This has benefits in situations where cache hit rate is expected to be low. Both <code class="literal">gfs2_iomap_start</code> and <code class="literal">gfs2_iomap_end</code> tracepoints trace these operations and can be used to keep track of mapping using Direct I/O, the positions on the file system of the Direct I/O along with the operation type.
			</p></section><section class="section" id="ap-log-gracepoints-gfs2"><div class="titlepage"><div><div><h3 class="title">9.8. Log tracepoints</h3></div></div></div><p>
				The tracepoints in this subsystem track blocks being added to and removed from the journal (<code class="literal">gfs2_pin</code>), as well as the time taken to commit the transactions to the log (<code class="literal">gfs2_log_flush</code>). This can be very useful when trying to debug journaling performance issues.
			</p><p>
				The <code class="literal">gfs2_log_blocks</code> tracepoint keeps track of the reserved blocks in the log, which can help show if the log is too small for the workload, for example.
			</p><p>
				The <code class="literal">gfs2_ail_flush</code> tracepoint is similar to the <code class="literal">gfs2_log_flush</code> tracepoint in that it keeps track of the start and end of flushes of the AIL list. The AIL list contains buffers which have been through the log, but have not yet been written back in place and this is periodically flushed in order to release more log space for use by the file system, or when a process requests a <code class="literal command">sync</code> or <code class="literal command">fsync</code>.
			</p></section><section class="section" id="ap-glockstats-gfs2"><div class="titlepage"><div><div><h3 class="title">9.9. Glock statistics</h3></div></div></div><p>
				GFS2 maintains statistics that can help track what is going on within the file system. This allows you to spot performance issues.
			</p><p>
				GFS2 maintains two counters:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">dcount</code>, which counts the number of DLM operations requested. This shows how much data has gone into the mean/variance calculations.
					</li><li class="listitem">
						<code class="literal">qcount</code>, which counts the number of <code class="literal">syscall</code> level operations requested. Generally <code class="literal">qcount</code> will be equal to or greater than <code class="literal">dcount</code>.
					</li></ul></div><p>
				In addition, GFS2 maintains three mean/variance pairs. The mean/variance pairs are smoothed exponential estimates and the algorithm used is the one used to calculate round trip times in network code.
			</p><p>
				The mean and variance pairs maintained in GFS2 are not scaled, but are in units of integer nanoseconds.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						srtt/srttvar: Smoothed round trip time for non-blocking operations
					</li><li class="listitem">
						srttb/srttvarb: Smoothed round trip time for blocking operations
					</li><li class="listitem">
						irtt/irttvar: Inter-request time (for example, time between DLM requests)
					</li></ul></div><p>
				A non-blocking request is one which will complete right away, whatever the state of the DLM lock in question. That currently means any requests when (a) the current state of the lock is exclusive (b) the requested state is either null or unlocked or (c) the "try lock" flag is set. A blocking request covers all the other lock requests.
			</p><p>
				Larger times are better for IRTTs, whereas smaller times are better for the RTTs.
			</p><p>
				Statistics are kept in two <code class="literal">sysfs</code> files:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The <code class="literal">glstats</code> file. This file is similar to the <code class="literal">glocks</code> file, except that it contains statistics, with one glock per line. The data is initialized from "per cpu" data for that glock type for which the glock is created (aside from counters, which are zeroed). This file may be very large.
					</li><li class="listitem">
						The <code class="literal">lkstats</code> file. This contains "per cpu" stats for each glock type. It contains one statistic per line, in which each column is a cpu core. There are eight lines per glock type, with types following on from each other.
					</li></ul></div></section><section class="section" id="ap-references-gfs2"><div class="titlepage"><div><div><h3 class="title">9.10. References</h3></div></div></div><p>
				For more information about tracepoints and the GFS2 <code class="literal">glocks</code> file, see the following resources:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						For information about glock internal locking rules, see <a class="link" href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/filesystems/gfs2-glocks.rst"> https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/filesystems/gfs2-glocks.rst</a>.
					</li><li class="listitem">
						For information about event tracing, see <a class="link" href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/trace/events.rst"> https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/trace/events.rst</a>.
					</li><li class="listitem">
						For information about the <code class="literal command">trace-cmd</code> utility, see <a class="link" href="http://lwn.net/Articles/341902/"> http://lwn.net/Articles/341902/</a>.
					</li></ul></div></section></section><section class="chapter" id="assembly_analyzing-gfs2-with-pcp-configuring-gfs2-file-systems"><div class="titlepage"><div><div><h2 class="title">Chapter 10. Monitoring and analyzing GFS2 file systems using Performance Co-Pilot (PCP)</h2></div></div></div><p class="_abstract _abstract">
			Performance Co-Pilot (PCP) can help with monitoring and analyzing GFS2 file systems. Monitoring of GFS2 file systems in PCP is provided by the GFS2 PMDA module in Red Hat Enterprise Linux which is available through the <code class="literal">pcp-pmda-gfs2</code> package.
		</p><p>
			The GFS2 PMDA provides a number of metrics given by the GFS2 statistics provided in the <code class="literal">debugfs</code> subsystem. When installed, the PMDA exposes values given in the <code class="literal">glocks</code>, <code class="literal">glstats</code>, and <code class="literal">sbstats</code> files. These report sets of statistics on each mounted GFS2 filesystem. The PMDA also makes use of the GFS2 kernel tracepoints exposed by the Kernel Function Tracer (<code class="literal">ftrace</code>).
		</p><section class="section" id="proc_installing-gfs2-pdma-analyzing-gfs2-with-pcp"><div class="titlepage"><div><div><h3 class="title">10.1. Installing the GFS2 PMDA</h3></div></div></div><p class="_abstract _abstract">
				In order to operate correctly, The GFS2 PMDA requires that the <code class="literal">debugfs</code> file system is mounted. If the <code class="literal">debugfs</code> file system is not mounted, run the following commands before installing the GFS2 PMDA:
			</p><pre class="literallayout"># <span class="strong strong"><strong>mkdir /sys/kernel/debug</strong></span>
# <span class="strong strong"><strong>mount -t debugfs none /sys/kernel/debug</strong></span></pre><p>
				The GFS2 PMDA is not enabled as part of the default installation. In order to make use of GFS2 metric monitoring through PCP you must enable it after installation.
			</p><p>
				Run the following commands to install PCP and enable the GFS2 PMDA. Note that the PMDA install script must be run as root.
			</p><pre class="literallayout"># <span class="strong strong"><strong>dnf install pcp pcp-pmda-gfs2</strong></span>
# <span class="strong strong"><strong>cd /var/lib/pcp/pmdas/gfs2</strong></span>
# <span class="strong strong"><strong>./Install</strong></span>
Updating the Performance Metrics Name Space (PMNS) ...
Terminate PMDA if already installed ...
Updating the PMCD control file, and notifying PMCD ...
Check gfs2 metrics have appeared ... 346 metrics and 255 values</pre></section><section class="section" id="proc_examining-number-of-glocks-analyzing-gfs2-with-pcp"><div class="titlepage"><div><div><h3 class="title">10.2. Displaying information about the available performance metrics with the pminfo tool</h3></div></div></div><p class="_abstract _abstract">
				The <code class="literal">pminfo</code> tool displays information about the available performance metrics. The following examples show different GFS2 metrics you can display with this tool.
			</p><section class="section" id="examining_the_number_of_glock_structures_that_currently_exist_per_file_system"><div class="titlepage"><div><div><h4 class="title">10.2.1. Examining the number of glock structures that currently exist per file system</h4></div></div></div><p>
					The GFS2 glock metrics give insights to the number of glock structures currently incore for each mounted GFS2 file system and their locking states. In GFS2, a glock is a data structure that brings together the DLM and caching into a single state machine. Each glock has a 1:1 mapping with a single DLM lock and provides caching for the lock states so that repetitive operations carried out on a single node do not have to repeatedly call the DLM, reducing unnecessary network traffic.
				</p><p>
					The following <code class="literal">pminfo</code> command displays a list of the number of glocks per mounted GFS2 file system by their lock mode.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pminfo -f gfs2.glocks</strong></span>

gfs2.glocks.total
    inst [0 or "afc_cluster:data"] value 43680
    inst [1 or "afc_cluster:bin"] value 2091

gfs2.glocks.shared
    inst [0 or "afc_cluster:data"] value 25
    inst [1 or "afc_cluster:bin"] value 25

gfs2.glocks.unlocked
    inst [0 or "afc_cluster:data"] value 43652
    inst [1 or "afc_cluster:bin"] value 2063

gfs2.glocks.deferred
    inst [0 or "afc_cluster:data"] value 0
    inst [1 or "afc_cluster:bin"] value 0

gfs2.glocks.exclusive
    inst [0 or "afc_cluster:data"] value 3
    inst [1 or "afc_cluster:bin"] value 3</pre></section><section class="section" id="examining_the_number_of_glock_structures_that_exist_per_file_system_by_type"><div class="titlepage"><div><div><h4 class="title">10.2.2. Examining the number of glock structures that exist per file system by type</h4></div></div></div><p>
					The GFS2 glstats metrics give counts of each type of glock which exist for each files ystem, a large number of these will normally be of either the inode (inode and metadata) or resource group (resource group metadata) type.
				</p><p>
					The following <code class="literal">pminfo</code> command displays a list of the number of each type of Glock per mounted GFS2 file system.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pminfo -f gfs2.glstats</strong></span>

gfs2.glstats.total
    inst [0 or "afc_cluster:data"] value 43680
    inst [1 or "afc_cluster:bin"] value 2091

gfs2.glstats.trans
    inst [0 or "afc_cluster:data"] value 3
    inst [1 or "afc_cluster:bin"] value 3

gfs2.glstats.inode
    inst [0 or "afc_cluster:data"] value 17
    inst [1 or "afc_cluster:bin"] value 17

gfs2.glstats.rgrp
    inst [0 or "afc_cluster:data"] value 43642
    inst [1 or "afc_cluster:bin"] value 2053

gfs2.glstats.meta
    inst [0 or "afc_cluster:data"] value 1
    inst [1 or "afc_cluster:bin"] value 1

gfs2.glstats.iopen
    inst [0 or "afc_cluster:data"] value 16
    inst [1 or "afc_cluster:bin"] value 16

gfs2.glstats.flock
    inst [0 or "afc_cluster:data"] value 0
    inst [1 or "afc_cluster:bin"] value 0

gfs2.glstats.quota
    inst [0 or "afc_cluster:data"] value 0
    inst [1 or "afc_cluster:bin"] value 0

gfs2.glstats.journal
    inst [0 or "afc_cluster:data"] value 1
    inst [1 or "afc_cluster:bin"] value 1</pre></section><section class="section" id="checking_the_number_of_glock_structures_that_are_in_a_wait_state"><div class="titlepage"><div><div><h4 class="title">10.2.3. Checking the number of glock structures that are in a wait state</h4></div></div></div><p>
					The most important holder flags are H (holder: indicates that requested lock is granted) and W (wait: set while waiting for request to complete). These flags are set on granted lock requests and queued lock requests, respectively.
				</p><p>
					The following <code class="literal">pminfo</code> command displays a list of the number of glocks with the Wait (W) holder flag for each mounted GFS2 file system.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pminfo -f gfs2.holders.flags.wait</strong></span>

gfs2.holders.flags.wait
    inst [0 or "afc_cluster:data"] value 0
    inst [1 or "afc_cluster:bin"] value 0</pre><p>
					If you do see a number of waiting requests queued on a resource group lock there may be a number of reasons for this. One is that there are a large number of nodes compared to the number of resource groups in the file system. Another is that the file system may be very nearly full (requiring, on average, longer searches for free blocks). The situation in both cases can be improved by adding more storage and using the <code class="literal">gfs2_grow</code> command to expand the file system.
				</p></section><section class="section" id="checking_file_system_operation_latency_using_the_kernel_tracepoint_based_metrics"><div class="titlepage"><div><div><h4 class="title">10.2.4. Checking file system operation latency using the kernel tracepoint based metrics</h4></div></div></div><p>
					The GFS2 PMDA supports collecting of metrics from the GFS2 kernel tracepoints. By default the reading of these metrics is disabled. Activating these metrics turns on the GFS2 kernel tracepoints when the metrics are collected in order to populate the metric values. This could have a small effect on performance throughput when these Kernel Tracepoint metrics are enabled.
				</p><p>
					PCP provides the <code class="literal">pmstore</code> tool, which allows you to modify PMDA settings based on metric values. The <code class="literal">gfs2.control.*</code> metrics allow the toggling of GFS2 kernel tracepoints. The following example uses the <code class="literal">pmstore</code> command to enable all of the GFS2 kernel tracepoints.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pmstore gfs2.control.tracepoints.all 1</strong></span>
gfs2.control.tracepoints.all old value=0 new value=1</pre><p>
					When this command is run, the PMDA switches on all of the GFS2 tracepoints in the <code class="literal">debugfs</code> file system. The "Complete Metric List" table in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_analyzing-gfs2-with-pcp-configuring-gfs2-file-systems#ref_available-gfs2-PCP-metrics-analyzing-gfs2-with-pcp">Complete listing of available metrics for GFS2 in PCP</a> explains each of the control tracepoints and their usage, An explanation on the effect of each control tracepoint and its available options is also available through the help switch in <code class="literal">pminfo</code>.
				</p><p>
					The GFS2 promote metrics count the number of promote requests on the file system. These requests are separated by the number of requests that have occurred on the first attempt and “others” which are granted after their initial promote request. A drop in the number of first time promotes with a rise in “other” promotes can indicate issues with file contention.
				</p><p>
					The GFS2 demote request metrics, like the promote request metrics, count the number of demote requests which occur on the file system. These, however, are also split between requests that have come from the current node and requests that have come from other nodes on the system. A large number of demote requests from remote nodes can indicate contention between two nodes for a given resource group.
				</p><p>
					The <code class="literal">pminfo</code> tool displays information about the available performance metrics. This procedure displays a list of the number of glocks with the Wait (W) holder flag for each mounted GFS2 file system. The following <code class="literal">pminfo</code> command displays a list of the number of glocks with the Wait (W) holder flag for each mounted GFS2 file system.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pminfo -f gfs2.latency.grant.all gfs2.latency.demote.all</strong></span>

gfs2.latency.grant.all
    inst [0 or "afc_cluster:data"] value 0
    inst [1 or "afc_cluster:bin"] value 0

gfs2.latency.demote.all
    inst [0 or "afc_cluster:data"] value 0
    inst [1 or "afc_cluster:bin"] value 0</pre><p>
					It is a good idea to determine the general values observed when the workload is running without issues to be able to notice changes in performance when these values differ from their normal range.
				</p><p>
					For example, you might notice a change in the number of promote requests waiting to complete rather than completing on first attempt, which the output from following command would allow you to determine.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pminfo -f gfs2.latency.grant.all gfs2.latency.demote.all</strong></span>

gfs2.tracepoints.promote.other.null_lock
     inst [0 or "afc_cluster:data"] value 0
     inst [1 or "afc_cluster:bin"] value 0

gfs2.tracepoints.promote.other.concurrent_read
     inst [0 or "afc_cluster:data"] value 0
     inst [1 or "afc_cluster:bin"] value 0

gfs2.tracepoints.promote.other.concurrent_write
     inst [0 or "afc_cluster:data"] value 0
     inst [1 or "afc_cluster:bin"] value 0

gfs2.tracepoints.promote.other.protected_read
     inst [0 or "afc_cluster:data"] value 0
     inst [1 or "afc_cluster:bin"] value 0

gfs2.tracepoints.promote.other.protected_write
     inst [0 or "afc_cluster:data"] value 0
     inst [1 or "afc_cluster:bin"] value 0

gfs2.tracepoints.promote.other.exclusive
     inst [0 or "afc_cluster:data"] value 0
     inst [1 or "afc_cluster:bin"] value 0</pre><p>
					The output from following command would allow you to determine a large increase in remote demote requests (especially if from other cluster nodes).
				</p><pre class="literallayout"># <span class="strong strong"><strong>pminfo -f gfs2.tracepoints.demote_rq.requested</strong></span>

gfs2.tracepoints.demote_rq.requested.remote
     inst [0 or "afc_cluster:data"] value 0
     inst [1 or "afc_cluster:bin"] value 0

gfs2.tracepoints.demote_rq.requested.local
     inst [0 or "afc_cluster:data"] value 0
     inst [1 or "afc_cluster:bin"] value 0</pre><p>
					The output from the following command could indicate an unexplained increase in log flushes.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pminfo -f gfs2.tracepoints.log_flush.total</strong></span>]

gfs2.tracepoints.log_flush.total
     inst [0 or "afc_cluster:data"] value 0
     inst [1 or "afc_cluster:bin"] value 0</pre></section></section><section class="section" id="ref_available-gfs2-PCP-metrics-analyzing-gfs2-with-pcp"><div class="titlepage"><div><div><h3 class="title">10.3. Complete listing of available metrics for GFS2 in PCP</h3></div></div></div><p class="_abstract _abstract">
				The following table describes the full list of performance metrics given by the <code class="literal">pcp-pmda-gfs2</code> package for GFS2 file systems.
			</p><rh-table id="tb-pcpgfs2metricgroups"><table class="lt-4-cols lt-7-rows"><caption>Table 10.1. Complete Metric List</caption><colgroup><col style="width: 29%; " class="col_1"><!--Empty--><col style="width: 71%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm139688928044944" scope="col">Metric Name</th><th align="left" valign="top" id="idm139688928043856" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139688928044944"> <p>
								<code class="literal">gfs2.glocks.*</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139688928043856"> <p>
								Metrics regarding the information collected from the glock stats file (<code class="literal">glocks</code>) which count the number of glocks in each state that currently exists for each GFS2 file system currently mounted on the system.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928044944"> <p>
								<code class="literal">gfs2.glocks.flags.*</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139688928043856"> <p>
								Range of metrics counting the number of glocks that exist with the given glocks flags
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928044944"> <p>
								<code class="literal">gfs2.holders.*</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139688928043856"> <p>
								Metrics regarding the information collected from the glock stats file (<code class="literal">glocks</code>) which counts the number of glocks with holders in each lock state that currently exists for each GFS2 file system currently mounted on the system.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928044944"> <p>
								<code class="literal">gfs2.holders.flags.*</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139688928043856"> <p>
								Range of metrics counting the number of glocks holders with the given holder flags
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928044944"> <p>
								<code class="literal">gfs2.sbstats.*</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139688928043856"> <p>
								Timing metrics regarding the information collected from the superblock stats file (<code class="literal">sbstats</code>) for each GFS2 file system currently mounted on the system.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928044944"> <p>
								<code class="literal">gfs2.glstats.*</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139688928043856"> <p>
								Metrics regarding the information collected from the glock stats file (<code class="literal">glstats</code>) which count the number of each type of glock that currently exists for each GFS2 file system currently mounted on the system.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928044944"> <p>
								<code class="literal">gfs2.latency.grant.*</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139688928043856"> <p>
								A derived metric making use of the data from both the <code class="literal">gfs2_glock_queue</code> and <code class="literal">gfs2_glock_state_change</code> tracepoints to calculate an average latency in microseconds for glock grant requests to be completed for each mounted file system. This metric is useful for discovering potential slowdowns on the file system when the grant latency increases.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928044944"> <p>
								<code class="literal">gfs2.latency.demote.*</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139688928043856"> <p>
								A derived metric making use of the data from both the <code class="literal">gfs2_glock_state_change</code> and <code class="literal">gfs2_demote_rq</code> tracepoints to calculate an average latency in microseconds for glock demote requests to be completed for each mounted file system. This metric is useful for discovering potential slowdowns on the file system when the demote latency increases.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928044944"> <p>
								<code class="literal">gfs2.latency.queue.*</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139688928043856"> <p>
								A derived metric making use of the data from the <code class="literal">gfs2_glock_queue</code> tracepoint to calculate an average latency in microseconds for glock queue requests to be completed for each mounted file system.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928044944"> <p>
								<code class="literal">gfs2.worst_glock.*</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139688928043856"> <p>
								A derived metric making use of the data from the <code class="literal">gfs2_glock_lock_time</code> tracepoint to calculate a perceived “current worst glock” for each mounted file system. This metric is useful for discovering potential lock contention and file system slowdown if the same lock is suggested multiple times.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928044944"> <p>
								<code class="literal">gfs2.tracepoints.*</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139688928043856"> <p>
								Metrics regarding the output from the GFS2 <code class="literal">debugfs</code> tracepoints for each file system currently mounted on the system. Each sub-type of these metrics (one of each GFS2 tracepoint) can be individually controlled whether on or off using the control metrics.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139688928044944"> <p>
								<code class="literal">gfs2.control.*</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139688928043856"> <p>
								Configuration metrics which are used to switch on or off metric recording in the PMDA. Conrol metricsare toggled by means of the <code class="literal command">pmstore</code> tool.
							</p>
							 </td></tr></tbody></table></rh-table></section><section class="section" id="proc_installing-minimal-PCP-setup-analyzing-gfs2-with-pcp"><div class="titlepage"><div><div><h3 class="title">10.4. Performing minimal PCP setup to gather file system data</h3></div></div></div><p class="_abstract _abstract">
				This procedure outlines instructions on how to install a minimal PCP setup to collect statistics on Red Hat Enterprise Linux. This setup involves adding the minimum number of packages on a production system needed to gather data for further analysis.
			</p><p>
				The resulting <code class="literal">tar.gz</code> archive of the <code class="literal">pmlogger</code> output can be analyzed by using further PCP tools and can be compared with other sources of performance information.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Install the required PCP packages.
					</p><pre class="literallayout"># <span class="strong strong"><strong>dnf install pcp pcp-pmda-gfs2</strong></span></pre></li><li class="listitem"><p class="simpara">
						Activate the GFS2 module for PCP.
					</p><pre class="literallayout"># <span class="strong strong"><strong>cd /var/lib/pcp/pmdas/gfs2</strong></span>
# <span class="strong strong"><strong>./Install</strong></span></pre></li><li class="listitem"><p class="simpara">
						Start both the <code class="literal">pmcd</code> and <code class="literal">pmlogger</code> services.
					</p><pre class="literallayout"># <span class="strong strong"><strong>systemctl start pmcd.service</strong></span>
# <span class="strong strong"><strong>systemctl start pmlogger.service</strong></span></pre></li><li class="listitem">
						Perform operations on the GFS2 file system.
					</li><li class="listitem"><p class="simpara">
						Stop both the <code class="literal">pmcd</code> and <code class="literal">pmlogger</code> services.
					</p><pre class="literallayout"># <span class="strong strong"><strong>systemctl stop pmcd.service</strong></span>
# <span class="strong strong"><strong>systemctl stop pmlogger.service</strong></span></pre></li><li class="listitem"><p class="simpara">
						Collect the output and save it to a <code class="literal">tar.gz</code> file named based on the host name and the current date and time.
					</p><pre class="literallayout"># <span class="strong strong"><strong>cd /var/log/pcp/pmlogger</strong></span>
# <span class="strong strong"><strong>tar -czf $(hostname).$(date+%F-%Hh%M).pcp.tar.gz $(hostname)</strong></span></pre></li></ol></div></section><section class="section _additional-resources" id="additional_resources"><div class="titlepage"><div><div><h3 class="title">10.5. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/con_gfs2-tracepoints-configuring-gfs2-file-systems">GFS2 tracepoints and the glock debugfs interface</a>.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/monitoring_and_managing_system_status_and_performance/monitoring-performance-with-performance-co-pilot_monitoring-and-managing-system-status-and-performance">Monitoring performance with Performance Co-Pilot</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/articles/1145953">Index of Performance Co-Pilot(PCP) articles, solutions, tutorials and white papers</a>
					</li></ul></div></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm139688929828816"><h2 class="legalnotice">Legal Notice</h2><div class="para">
		Copyright <span class="trademark"><!--Empty--></span>© 2024 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></body></section><!----></div></article><aside id="layout" class="span-xs-12 span-sm-2 span-md-2 content-format-selectors" aria-label="Select page format" data-v-8589d091><div class="sticky-top page-layout-options" data-v-8589d091><label for="page-format" data-v-8589d091>Format</label><select id="page-format" class="page-format-dropdown" data-v-8589d091><option class="page-type" value="html" data-v-8589d091>Multi-page</option><option selected class="page-type" value="html-single" data-v-8589d091>Single-page</option><option class="page-type" value="pdf" data-v-8589d091>View full doc as PDF</option></select></div><!----></aside></div><div class="btn-container hidden" data-v-8589d091><pf-button class="top-scroll-btn" icon="angle-up" icon-set="fas" icon-position="right" data-v-8589d091>Back to top</pf-button></div><!--]--><!--]--></main><rh-footer data-analytics-region="page-footer" data-v-97dd2752><a slot="logo" href="/en" data-analytics-category="Footer" data-analytics-text="Logo" data-v-97dd2752><img alt="Red Hat logo" src="/Logo-Red_Hat-Documentation-A-Reverse-RGB.svg" loading="lazy" width="222" height="40" data-v-97dd2752></a><rh-footer-social-link slot="social-links" icon="github" data-v-97dd2752><a href="https://github.com/redhat-documentation" data-analytics-region="social-links-exit" data-analytics-category="Footer|social-links" data-analytics-text="LinkedIn" data-v-97dd2752>Github</a></rh-footer-social-link><rh-footer-social-link slot="social-links" icon="reddit" data-v-97dd2752><a href="https://www.reddit.com/r/redhat/" data-analytics-region="social-links-exit" data-analytics-category="Footer|social-links" data-analytics-text="YouTube" data-v-97dd2752>Reddit</a></rh-footer-social-link><rh-footer-social-link slot="social-links" icon="youtube" data-v-97dd2752><a href="https://www.youtube.com/@redhat" data-analytics-region="social-links-exit" data-analytics-category="Footer|social-links" data-analytics-text="Facebook" data-v-97dd2752>Youtube</a></rh-footer-social-link><rh-footer-social-link slot="social-links" icon="twitter" data-v-97dd2752><a href="https://twitter.com/RedHat" data-analytics-region="social-links-exit" data-analytics-category="Footer|social-links" data-analytics-text="Twitter" data-v-97dd2752>Twitter</a></rh-footer-social-link><h3 slot="links" data-analytics-text="Learn" data-v-97dd2752>Learn</h3><ul slot="links" data-v-97dd2752><li data-v-97dd2752><a href="https://developers.redhat.com/learn" data-analytics-category="Footer|Learn" data-analytics-text="Developer resources" data-v-97dd2752>Developer resources</a></li><li data-v-97dd2752><a href="https://cloud.redhat.com/learn" data-analytics-category="Footer|Learn" data-analytics-text="Cloud learning hub" data-v-97dd2752>Cloud learning hub</a></li><li data-v-97dd2752><a href="https://www.redhat.com/en/interactive-labs" data-analytics-category="Footer|Learn" data-analytics-text="Interactive labs" data-v-97dd2752>Interactive labs</a></li><li data-v-97dd2752><a href="https://www.redhat.com/services/training-and-certification" data-analytics-category="Footer|Learn" data-analytics-text="Training and certification" data-v-97dd2752>Training and certification</a></li><li data-v-97dd2752><a href="https://access.redhat.com/support" data-analytics-category="Footer|Learn" data-analytics-text="Customer support" data-v-97dd2752>Customer support</a></li><li data-v-97dd2752><a href="/products" data-analytics-category="Footer|Learn" data-analytics-text="See all documentation" data-v-97dd2752>See all documentation</a></li></ul><h3 slot="links" data-analytics-text="Try buy sell" data-v-97dd2752>Try, buy, &amp; sell</h3><ul slot="links" data-v-97dd2752><li data-v-97dd2752><a href="https://redhat.com/en/products/trials" data-analytics-category="Footer|Try buy sell" data-analytics-text="Product trial center" data-v-97dd2752>Product trial center</a></li><li data-v-97dd2752><a href="https://marketplace.redhat.com" data-analytics-category="Footer|Try buy sell" data-analytics-text="Red Hat Marketplace" data-v-97dd2752>Red Hat Marketplace</a></li><li data-v-97dd2752><a href="https://catalog.redhat.com/" data-analytics-category="Footer|Try buy sell" data-analytics-text="Red Hat Ecosystem Catalog" data-v-97dd2752>Red Hat Ecosystem Catalog</a></li><li data-v-97dd2752><a href="https://www.redhat.com/en/store" data-analytics-category="Footer|Try buy sell" data-analytics-text="Red Hat Store" data-v-97dd2752>Red Hat Store</a></li><li data-v-97dd2752><a href="https://www.redhat.com/about/japan-buy" data-analytics-category="Footer|Try buy sell" data-analytics-text="Buy online (Japan)" data-v-97dd2752>Buy online (Japan)</a></li></ul><h3 slot="links" data-analytics-text="Communities" data-v-97dd2752>Communities</h3><ul slot="links" data-v-97dd2752><li data-v-97dd2752><a href="https://access.redhat.com/community" data-analytics-category="Footer|Communities" data-analytics-text="Customer Portal Community" data-v-97dd2752>Customer Portal Community</a></li><li data-v-97dd2752><a href="https://www.redhat.com/events" data-analytics-category="Footer|Communities" data-analytics-text="Events" data-v-97dd2752>Events</a></li><li data-v-97dd2752><a href="https://www.redhat.com/about/our-community-contributions" data-analytics-category="Footer|Communities" data-analytics-text="How we contribute" data-v-97dd2752>How we contribute</a></li></ul><rh-footer-block slot="main-secondary" data-v-97dd2752><h3 slot="header" data-analytics-text="About Red Hat Documentation" data-v-97dd2752>About Red Hat Documentation</h3><p data-v-97dd2752>We help Red Hat users innovate and achieve their goals with our products and services with content they can trust.</p></rh-footer-block><rh-footer-block slot="main-secondary" data-v-97dd2752><h3 slot="header" data-analytics-text="Making open source more inclusive" data-v-97dd2752>Making open source more inclusive</h3><p data-v-97dd2752>Red Hat is committed to replacing problematic language in our code, documentation, and web properties. For more details, see the <a href=" https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language" data-analytics-category="Footer|Making open source more inclusive" data-analytics-text="Red Hat Blog" data-v-97dd2752>Red Hat Blog</a>.</p></rh-footer-block><rh-footer-block slot="main-secondary" data-v-97dd2752><h3 slot="header" data-analytics-text="About Red Hat" data-v-97dd2752>About Red Hat</h3><p data-v-97dd2752>We deliver hardened solutions that make it easier for enterprises to work across platforms and environments, from the core datacenter to the network edge.</p></rh-footer-block><rh-footer-universal slot="universal" data-v-97dd2752><h3 slot="links-primary" data-analytics-text="Red Hat legal and privacy links" hidden data-v-97dd2752>Red Hat legal and privacy links</h3><ul slot="links-primary" data-analytics-region="page-footer-bottom-primary" data-v-97dd2752><li data-v-97dd2752><a href="https://redhat.com/en/about/company" data-analytics-category="Footer|Corporate" data-analytics-text="About Red Hat" data-v-97dd2752>About Red Hat</a></li><li data-v-97dd2752><a href="https://redhat.com/en/jobs" data-analytics-category="Footer|Corporate" data-analytics-text="Jobs" data-v-97dd2752>Jobs</a></li><li data-v-97dd2752><a href="https://redhat.com/en/events" data-analytics-category="Footer|Corporate" data-analytics-text="Events" data-v-97dd2752>Events</a></li><li data-v-97dd2752><a href="https://redhat.com/en/about/office-locations" data-analytics-category="Footer|Corporate" data-analytics-text="Locations" data-v-97dd2752>Locations</a></li><li data-v-97dd2752><a href="https://redhat.com/en/contact" data-analytics-category="Footer|Corporate" data-analytics-text="Contact Red Hat" data-v-97dd2752>Contact Red Hat</a></li><li data-v-97dd2752><a href="https://redhat.com/en/blog" data-analytics-category="Footer|Corporate" data-analytics-text="Red Hat Blog" data-v-97dd2752>Red Hat Blog</a></li><li data-v-97dd2752><a href="https://redhat.com/en/about/our-culture/diversity-equity-inclusion" data-analytics-category="Footer|Corporate" data-analytics-text="Diversity equity and inclusion" data-v-97dd2752>Diversity, equity, and inclusion</a></li><li data-v-97dd2752><a href="https://coolstuff.redhat.com/" data-analytics-category="Footer|Corporate" data-analytics-text="Cool Stuff Store" data-v-97dd2752>Cool Stuff Store</a></li><li data-v-97dd2752><a href="https://www.redhat.com/en/summit" data-analytics-category="Footer|Corporate" data-analytics-text="Red Hat Summit" data-v-97dd2752>Red Hat Summit</a></li></ul><span data-v-97dd2752 data-v-5f538988></span><rh-footer-copyright slot="links-secondary" data-v-97dd2752>© 2024 Red Hat, Inc.</rh-footer-copyright><h3 slot="links-secondary" data-analytics-text="Red Hat legal and privacy links" hidden data-v-97dd2752>Red Hat legal and privacy links</h3><ul slot="links-secondary" data-analytics-region="page-footer-bottom-secondary" data-v-97dd2752><li data-v-97dd2752><a href="https://redhat.com/en/about/privacy-policy" data-analytics-category="Footer|Red Hat legal and privacy links" data-analytics-text="Privacy statement" data-v-97dd2752>Privacy statement</a></li><li data-v-97dd2752><a href="https://redhat.com/en/about/terms-use" data-analytics-category="Footer|Red Hat legal and privacy links" data-analytics-text="Terms of use" data-v-97dd2752>Terms of use</a></li><li data-v-97dd2752><a href="https://redhat.com/en/about/all-policies-guidelines" data-analytics-category="Footer|Red Hat legal and privacy links" data-analytics-text="All policies and guidelines" data-v-97dd2752>All policies and guidelines</a></li><li data-v-97dd2752><a href="https://redhat.com/en/about/digital-accessibility" data-analytics-category="Footer|Red Hat legal and privacy links" data-analytics-text="Digital accessibility" class="active" data-v-97dd2752>Digital accessibility</a></li><li data-v-97dd2752><span id="teconsent" data-v-97dd2752></span></li></ul></rh-footer-universal></rh-footer><div id="consent_blackbar" style="position:fixed;bottom:0;width:100%;z-index:5;padding:10px;"></div><!--]--><!--]--></div><div id="teleports"></div><script type="application/json" id="__NUXT_DATA__" data-ssr="true">[["ShallowReactive",1],{"data":2,"state":554,"once":558,"_errors":559,"serverRendered":15,"path":561},["ShallowReactive",3],{"s8LoCEfG4A":4,"uUstF4AIyn":10,"Pn02PlJOas":482,"rFVLKcOK8e":553},[5,6,7,8,9],"fr-fr","en-us","ko-kr","zh-cn","ja-jp",{"name":11,"html":12,"type":-1,"toc":13,"breadcrumbs":403,"error":18,"title":411,"productName":405,"productVersions":420,"pagination":449,"redirect":463,"canonicalLinks":464,"openShiftProducts":466,"tocFromVolume":-1,"jumpLinks":481},"Configuring GFS2 file systems","\u003Cbody>\u003Cdiv xml:lang=\"en-US\" class=\"book\" id=\"idm139688935047696\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv class=\"producttitle\">\u003Cspan class=\"productname\">Red Hat Enterprise Linux\u003C/span> \u003Cspan class=\"productnumber\">9\u003C/span>\u003C/div>\u003Cdiv>\u003Ch3 class=\"subtitle\">Planning, administering, troubleshooting, and configuring GFS2 file systems in a high availability cluster\u003C/h3>\u003C/div>\u003Cdiv>\u003Cdiv xml:lang=\"en-US\" class=\"authorgroup\">\u003Cspan class=\"orgname\">Red Hat\u003C/span> \u003Cspan class=\"orgdiv\">Customer Content Services\u003C/span>\u003C/div>\u003C/div>\u003Cdiv>\u003Ca href=\"#idm139688929828816\">Legal Notice\u003C/a>\u003C/div>\u003Cdiv>\u003Cdiv class=\"abstract\">\u003Cp class=\"title\">\u003Cstrong>Abstract\u003C/strong>\u003C/p>\u003Cdiv class=\"para\">\n\t\t\t\tThe Red Hat Enterprise Linux (RHEL) Resilient Storage Add-On provides the Red Hat Global File System 2 (GFS2), a cluster file system that manages coherency between multiple nodes sharing a common block device. This title provides information about planning a GFS2 file system deployment as well as procedures for configuring, troubleshooting, and tuning GFS2 file systems.\n\t\t\t\u003C/div>\u003C/div>\u003C/div>\u003C/div>\u003Chr/>\u003C/div>\u003Csection class=\"preface\" id=\"proc_providing-feedback-on-red-hat-documentation_configuring-gfs2-file-systems\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Providing feedback on Red Hat documentation\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tWe appreciate your feedback on our documentation. Let us know how we can improve it.\n\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Submitting feedback through Jira (account required)\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\tLog in to the \u003Ca class=\"link\" href=\"https://issues.redhat.com/projects/RHELDOCS/issues\">Jira\u003C/a> website.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tClick \u003Cspan class=\"strong strong\">\u003Cstrong>Create\u003C/strong>\u003C/span> in the top navigation bar\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tEnter a descriptive title in the \u003Cspan class=\"strong strong\">\u003Cstrong>Summary\u003C/strong>\u003C/span> field.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tEnter your suggestion for improvement in the \u003Cspan class=\"strong strong\">\u003Cstrong>Description\u003C/strong>\u003C/span> field. Include links to the relevant parts of the documentation.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tClick \u003Cspan class=\"strong strong\">\u003Cstrong>Create\u003C/strong>\u003C/span> at the bottom of the dialogue.\n\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_planning-gfs2-deployment-configuring-gfs2-file-systems\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 1. Planning a GFS2 file system deployment\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tThe Red Hat Global File System 2 (GFS2) file system is a 64-bit symmetric cluster file system which provides a shared name space and manages coherency between multiple nodes sharing a common block device. A GFS2 file system is intended to provide a feature set which is as close as possible to a local file system, while at the same time enforcing full cluster coherency between nodes. To achieve this, the nodes employ a cluster-wide locking scheme for file system resources. This locking scheme uses communication protocols such as TCP/IP to exchange locking information.\n\t\t\u003C/p>\u003Cp>\n\t\t\tIn a few cases, the Linux file system API does not allow the clustered nature of GFS2 to be totally transparent; for example, programs using POSIX locks in GFS2 should avoid using the \u003Ccode class=\"literal\">GETLK\u003C/code> function since, in a clustered environment, the process ID may be for a different node in the cluster. In most cases however, the functionality of a GFS2 file system is identical to that of a local file system.\n\t\t\u003C/p>\u003Cp>\n\t\t\tThe Red Hat Enterprise Linux (RHEL) Resilient Storage Add-On provides GFS2, and it depends on the RHEL High Availability Add-On to provide the cluster management required by GFS2.\n\t\t\u003C/p>\u003Cp>\n\t\t\tThe \u003Ccode class=\"literal\">gfs2.ko\u003C/code> kernel module implements the GFS2 file system and is loaded on GFS2 cluster nodes.\n\t\t\u003C/p>\u003Cp>\n\t\t\tTo get the best performance from GFS2, it is important to take into account the performance considerations which stem from the underlying design. Just like a local file system, GFS2 relies on the page cache in order to improve performance by local caching of frequently used data. In order to maintain coherency across the nodes in the cluster, cache control is provided by the \u003Cspan class=\"emphasis\">\u003Cem>glock\u003C/em>\u003C/span> state machine.\n\t\t\u003C/p>\u003Crh-alert class=\"admonition important\" state=\"warning\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Important\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\tMake sure that your deployment of the Red Hat High Availability Add-On meets your needs and can be supported. Consult with an authorized Red Hat representative to verify your configuration prior to deployment.\n\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Csection class=\"section\" id=\"con_gfs2-filesystem-format-planning-gfs2-deployment\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">1.1. GFS2 file system format version 1802\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tAs of Red Hat Enterprise Linux 9, GFS2 file systems are created with format version 1802.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFormat version 1802 enables the following features:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tExtended attributes in the \u003Ccode class=\"literal\">trusted\u003C/code> namespace (\"trusted.* xattrs\") are recognized by \u003Ccode class=\"literal\">gfs2\u003C/code> and \u003Ccode class=\"literal\">gfs2-utils\u003C/code>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe \u003Ccode class=\"literal\">rgrplvb\u003C/code> option is active by default. This allows \u003Ccode class=\"literal\">gfs2\u003C/code> to attach updated resource group data to DLM lock requests, so the node acquiring the lock does not need to update the resource group information from disk. This improves performance in some cases.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tFilesystems created with the new format version will not be able to be mounted under earlier RHEL versions and older versions of the \u003Ccode class=\"literal\">fsck.gfs2\u003C/code> utility will not be able to check them.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tUsers can create a file system with the older format version by running the \u003Ccode class=\"literal\">mkfs.gfs2\u003C/code> command with the option \u003Ccode class=\"literal\">-o format=1801\u003C/code>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tUsers can upgrade the format version of an older file system running \u003Ccode class=\"literal\">tunegfs2 -r 1802 \u003Cspan class=\"emphasis\">\u003Cem>device\u003C/em>\u003C/span>\u003C/code> on an unmounted file system. Downgrading the format version is not supported.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"con_basic-gfs2-parameters-planning-gfs2-deployment\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">1.2. Key GFS2 parameters to determine\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThere are a number of key GFS2 parameters you should plan for before you install and configure a GFS2 file system.\n\t\t\t\u003C/p>\u003Cdiv class=\"variablelist\">\u003Cdl class=\"variablelist\">\u003Cdt>\u003Cspan class=\"term\">GFS2 nodes\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tDetermine which nodes in the cluster will mount the GFS2 file systems.\n\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">Number of file systems\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tDetermine how many GFS2 file systems to create initially. More file systems can be added later.\n\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">File system name\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tEach GFS2 file system should have a unique name. This name is usually the same as the LVM logical volume name and is used as the DLM lock table name when a GFS2 file system is mounted. For example, this guide uses file system names \u003Ccode class=\"literal\">mydata1\u003C/code> and \u003Ccode class=\"literal\">mydata2\u003C/code> in some example procedures.\n\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">Journals\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tDetermine the number of journals for your GFS2 file systems. GFS2 requires one journal for each node in the cluster that needs to mount the file system. For example, if you have a 16-node cluster but need to mount only the file system from two nodes, you need only two journals. GFS2 allows you to add journals dynamically at a later point with the \u003Ccode class=\"literal\">gfs2_jadd\u003C/code> utility as additional servers mount a file system.\n\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">Storage devices and partitions\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tDetermine the storage devices and partitions to be used for creating logical volumes (using \u003Ccode class=\"literal\">lvmlockd\u003C/code>) in the file systems.\n\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">Time protocol\u003C/span>\u003C/dt>\u003Cdd>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tMake sure that the clocks on the GFS2 nodes are synchronized. It is recommended that you use the Precision Time Protocol (PTP) or, if necessary for your configuration, the Network Time Protocol (NTP) software provided with your Red Hat Enterprise Linux distribution.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tThe system clocks in GFS2 nodes must be within a few minutes of each other to prevent unnecessary inode time stamp updating. Unnecessary inode time stamp updating severely impacts cluster performance.\n\t\t\t\t\t\t\u003C/p>\u003C/dd>\u003C/dl>\u003C/div>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tYou may see performance problems with GFS2 when many create and delete operations are issued from more than one node in the same directory at the same time. If this causes performance problems in your system, you should localize file creation and deletions by a node to directories specific to that node as much as possible.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/section>\u003Csection class=\"section\" id=\"con_gfs2-support-limits-planning-gfs2-deployment\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">1.3. GFS2 support considerations\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tTo be eligible for support from Red Hat for a cluster running a GFS2 file system, you must take into account the support policies for GFS2 file systems.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tFor full information about Red Hat’s support policies, requirements, and limitations for RHEL High Availability clusters, see \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/2912891/\">Support Policies for RHEL High Availability Clusters\u003C/a>.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Csection class=\"section\" id=\"maximum_file_system_and_cluster_size\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">1.3.1. Maximum file system and cluster size\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following table summarizes the current maximum file system size and number of nodes that GFS2 supports.\n\t\t\t\t\u003C/p>\u003Crh-table id=\"tb-table-gfs2-max\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 1.1. GFS2 Support Limits\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 50%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 50%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688932456128\" scope=\"col\">Parameter\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688929152064\" scope=\"col\">Maximum\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688932456128\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tNumber of nodes\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929152064\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t16 (x86, Power8 on PowerVM)\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t4 (s390x under z/VM)\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688932456128\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tFile system size\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929152064\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t100TB on all supported architectures\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\t\tGFS2 is based on a 64-bit architecture, which can theoretically accommodate an 8 EB file system. If your system requires larger GFS2 file systems than are currently supported, contact your Red Hat service representative.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tWhen determining the size of your file system, you should consider your recovery needs. Running the \u003Ccode class=\"literal command\">fsck.gfs2\u003C/code> command on a very large file system can take a long time and consume a large amount of memory. Additionally, in the event of a disk or disk subsystem failure, recovery time is limited by the speed of your backup media. For information about the amount of memory the \u003Ccode class=\"literal command\">fsck.gfs2\u003C/code> command requires, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_gfs2-filesystem-repair-configuring-gfs2-file-systems#proc_determining-needed-memory-for-fsckgfs2-gfs2-filesystem-repair\">Determining required memory for running fsck.gfs2\u003C/a>.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"minimum_cluster_size\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">1.3.2. Minimum cluster size\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tAlthough a GFS2 file system can be implemented in a standalone system or as part of a cluster configuration, Red Hat does not support the use of GFS2 as a single-node file system, with the following exceptions:\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tRed Hat supports single-node GFS2 file systems for mounting snapshots of cluster file systems as might be needed, for example, for backup purposes.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tA single-node cluster mounting GFS2 file systems (which uses DLM) is supported for the purposes of a secondary-site Disaster Recovery (DR) node. This exception is for DR purposes only and not for transferring the main cluster workload to the secondary site.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tFor example, copying off the data from the filesystem mounted on the secondary site while the primary site is offline is supported. However, migrating a workload from the primary site directly to a single-node cluster secondary site is unsupported. If the full work load needs to be migrated to the single-node secondary site then the secondary site must be the same size as the primary site.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tRed Hat recommends that when you mount a GFS2 file system in a single-node cluster you specify the \u003Ccode class=\"literal\">errors=panic\u003C/code> mount option so that the single-node cluster will panic when a GFS2 withdraw occurs since the single-node cluster will not be able to fence itself when encountering file system errors.\n\t\t\t\t\t\t\u003C/p>\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\t\tRed Hat supports a number of high-performance single-node file systems that are optimized for single node and thus have generally lower overhead than a cluster file system. Red Hat recommends using these file systems in preference to GFS2 in cases where only a single node needs to mount the file system. For information about the file systems that Red Hat Enterprise Linux 9 supports, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_file_systems/index\">Managing file systems\u003C/a>.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"shared_storage_considerations\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">1.3.3. Shared storage considerations\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tWhile a GFS2 file system may be used outside of LVM, Red Hat supports only GFS2 file systems that are created on a shared LVM logical volume.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tWhen you configure a GFS2 file system as a cluster file system, you must ensure that all nodes in the cluster have access to the shared storage. Asymmetric cluster configurations in which some nodes have access to the shared storage and others do not are not supported. This does not require that all nodes actually mount the GFS2 file system itself.\n\t\t\t\t\u003C/p>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"con_gfs2-formattiing-considerations-planning-gfs2-deployment\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">1.4. GFS2 formatting considerations\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tTo format your GFS2 file system to optimize performance, you should take these recommendations into account.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition important\" state=\"warning\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Important\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tMake sure that your deployment of the Red Hat High Availability Add-On meets your needs and can be supported. Consult with an authorized Red Hat representative to verify your configuration prior to deployment.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Ch5 id=\"file_system_size_smaller_is_better\">File System Size: Smaller Is Better\u003C/h5>\u003Cp>\n\t\t\t\tGFS2 is based on a 64-bit architecture, which can theoretically accommodate an 8 EB file system. However, the current supported maximum size of a GFS2 file system for 64-bit hardware is 100TB.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tNote that even though GFS2 large file systems are possible, that does not mean they are recommended. The rule of thumb with GFS2 is that smaller is better: it is better to have 10 1TB file systems than one 10TB file system.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThere are several reasons why you should keep your GFS2 file systems small:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tLess time is required to back up each file system.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tLess time is required if you need to check the file system with the \u003Ccode class=\"literal command\">fsck.gfs2\u003C/code> command.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tLess memory is required if you need to check the file system with the \u003Ccode class=\"literal command\">fsck.gfs2\u003C/code> command.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tIn addition, fewer resource groups to maintain mean better performance.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tOf course, if you make your GFS2 file system too small, you might run out of space, and that has its own consequences. You should consider your own use cases before deciding on a size.\n\t\t\t\u003C/p>\u003Ch5 id=\"block_size_default_4k_blocks_are_preferred\">Block Size: Default (4K) Blocks Are Preferred\u003C/h5>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal command\">mkfs.gfs2\u003C/code> command attempts to estimate an optimal block size based on device topology. In general, 4K blocks are the preferred block size because 4K is the default page size (memory) for Red Hat Enterprise Linux. Unlike some other file systems, GFS2 does most of its operations using 4K kernel buffers. If your block size is 4K, the kernel has to do less work to manipulate the buffers.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIt is recommended that you use the default block size, which should yield the highest performance. You may need to use a different block size only if you require efficient storage of many very small files.\n\t\t\t\u003C/p>\u003Ch5 id=\"journal_size_default_128mb_is_usually_optimal\">Journal Size: Default (128MB) Is Usually Optimal\u003C/h5>\u003Cp>\n\t\t\t\tWhen you run the \u003Ccode class=\"literal command\">mkfs.gfs2\u003C/code> command to create a GFS2 file system, you may specify the size of the journals. If you do not specify a size, it will default to 128MB, which should be optimal for most applications.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tSome system administrators might think that 128MB is excessive and be tempted to reduce the size of the journal to the minimum of 8MB or a more conservative 32MB. While that might work, it can severely impact performance. Like many journaling file systems, every time GFS2 writes metadata, the metadata is committed to the journal before it is put into place. This ensures that if the system crashes or loses power, you will recover all of the metadata when the journal is automatically replayed at mount time. However, it does not take much file system activity to fill an 8MB journal, and when the journal is full, performance slows because GFS2 has to wait for writes to the storage.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIt is generally recommended to use the default journal size of 128MB. If your file system is very small (for example, 5GB), having a 128MB journal might be impractical. If you have a larger file system and can afford the space, using 256MB journals might improve performance.\n\t\t\t\u003C/p>\u003Ch5 id=\"size_and_number_of_resource_groups\">Size and Number of Resource Groups\u003C/h5>\u003Cp>\n\t\t\t\tWhen a GFS2 file system is created with the \u003Ccode class=\"literal command\">mkfs.gfs2\u003C/code> command, it divides the storage into uniform slices known as resource groups. It attempts to estimate an optimal resource group size (ranging from 32MB to 2GB). You can override the default with the \u003Ccode class=\"literal command\">-r\u003C/code> option of the \u003Ccode class=\"literal command\">mkfs.gfs2\u003C/code> command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYour optimal resource group size depends on how you will use the file system. Consider how full it will be and whether or not it will be severely fragmented.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou should experiment with different resource group sizes to see which results in optimal performance. It is a best practice to experiment with a test cluster before deploying GFS2 into full production.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf your file system has too many resource groups, each of which is too small, block allocations can waste too much time searching tens of thousands of resource groups for a free block. The more full your file system, the more resource groups that will be searched, and every one of them requires a cluster-wide lock. This leads to slow performance.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf, however, your file system has too few resource groups, each of which is too big, block allocations might contend more often for the same resource group lock, which also impacts performance. For example, if you have a 10GB file system that is carved up into five resource groups of 2GB, the nodes in your cluster will fight over those five resource groups more often than if the same file system were carved into 320 resource groups of 32MB. The problem is exacerbated if your file system is nearly full because every block allocation might have to look through several resource groups before it finds one with a free block. GFS2 tries to mitigate this problem in two ways:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tFirst, when a resource group is completely full, it remembers that and tries to avoid checking it for future allocations until a block is freed from it. If you never delete files, contention will be less severe. However, if your application is constantly deleting blocks and allocating new blocks on a file system that is mostly full, contention will be very high and this will severely impact performance.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tSecond, when new blocks are added to an existing file (for example, by appending) GFS2 will attempt to group the new blocks together in the same resource group as the file. This is done to increase performance: on a spinning disk, seek operations take less time when they are physically close together.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tThe worst case scenario is when there is a central directory in which all the nodes create files because all of the nodes will constantly fight to lock the same resource group.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"con_gfs2-cluster-considerations-planning-gfs2-deployment\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">1.5. Considerations for GFS2 in a cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen determining the number of nodes that your system will contain, note that there is a trade-off between high availability and performance. With a larger number of nodes, it becomes increasingly difficult to make workloads scale. For that reason, Red Hat does not support using GFS2 for cluster file system deployments greater than 16 nodes.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tDeploying a cluster file system is not a \"drop in\" replacement for a single node deployment. Red Hat recommends that you allow a period of around 8-12 weeks of testing on new installations in order to test the system and ensure that it is working at the required performance level. During this period, any performance or functional issues can be worked out and any queries should be directed to the Red Hat support team.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tRed Hat recommends that customers considering deploying clusters have their configurations reviewed by Red Hat support before deployment to avoid any possible support issues later on.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"con_basic-gfs2-hardware-considerations-planning-gfs2-deployment\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">1.6. Hardware considerations\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tTake the following hardware considerations into account when deploying a GFS2 file system.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tUse higher quality storage options\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tGFS2 can operate on cheaper shared storage options, such as iSCSI or Fibre Channel over Ethernet (FCoE), but you will get better performance if you buy higher quality storage with larger caching capacity. Red Hat performs most quality, sanity, and performance tests on SAN storage with Fibre Channel interconnect. As a general rule, it is always better to deploy something that has been tested first.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tTest network equipment before deploying\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tHigher quality, faster network equipment makes cluster communications and GFS2 run faster with better reliability. However, you do not have to purchase the most expensive hardware. Some of the most expensive network switches have problems passing multicast packets, which are used for passing \u003Ccode class=\"literal\">fcntl\u003C/code> locks (flocks), whereas cheaper commodity network switches are sometimes faster and more reliable. Red Hat recommends trying equipment before deploying it into full production.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_gfs2-usage-considerations-configuring-gfs2-file-systems\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 2. Recommendations for GFS2 usage\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tWhen deploying a GFS2 file system, there are a variety of general recommendations you should take into account.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_configuring-atime-gfs2-usage-considerations\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">2.1. Configuring \u003Ccode class=\"literal\">atime\u003C/code> updates\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tEach file inode and directory inode has three time stamps associated with it:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal command\">ctime\u003C/code> — The last time the inode status was changed\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal command\">mtime\u003C/code> — The last time the file (or directory) data was modified\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal command\">atime\u003C/code> — The last time the file (or directory) data was accessed\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tIf \u003Ccode class=\"literal command\">atime\u003C/code> updates are enabled as they are by default on GFS2 and other Linux file systems, then every time a file is read its inode needs to be updated.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tBecause few applications use the information provided by \u003Ccode class=\"literal command\">atime\u003C/code>, those updates can require a significant amount of unnecessary write traffic and file locking traffic. That traffic can degrade performance; therefore, it may be preferable to turn off or reduce the frequency of \u003Ccode class=\"literal command\">atime\u003C/code> updates.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following methods of reducing the effects of \u003Ccode class=\"literal command\">atime\u003C/code> updating are available:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tMount with \u003Ccode class=\"literal\">relatime\u003C/code> (relative atime), which updates the \u003Ccode class=\"literal command\">atime\u003C/code> if the previous \u003Ccode class=\"literal command\">atime\u003C/code> update is older than the \u003Ccode class=\"literal command\">mtime\u003C/code> or \u003Ccode class=\"literal command\">ctime\u003C/code> update. This is the default mount option for GFS2 file systems.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tMount with \u003Ccode class=\"literal command\">noatime\u003C/code> or \u003Ccode class=\"literal command\">nodiratime\u003C/code>. Mounting with \u003Ccode class=\"literal command\">noatime\u003C/code> disables \u003Ccode class=\"literal command\">atime\u003C/code> updates for both files and directories on that file system, while mounting with \u003Ccode class=\"literal command\">nodiratime\u003C/code> disables \u003Ccode class=\"literal command\">atime\u003C/code> updates only for directories on that file system, It is generally recommended that you mount GFS2 file systems with the \u003Ccode class=\"literal\">noatime\u003C/code> or \u003Ccode class=\"literal\">nodiratime\u003C/code> mount option whenever possible, with the preference for \u003Ccode class=\"literal\">noatime\u003C/code> where the application allows for this. For more information about the effect of these arguments on GFS2 file system performance, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_gfs2-performance-configuring-gfs2-file-systems#con_gfs2-node-locking-gfs2-performance\">GFS2 Node Locking\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tUse the following command to mount a GFS2 file system with the \u003Ccode class=\"literal option\">noatime\u003C/code> Linux mount option.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">mount \u003Cspan class=\"emphasis\">\u003Cem>BlockDevice\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>MountPoint\u003C/em>\u003C/span> -o noatime\u003C/pre>\u003Cdiv class=\"variablelist\">\u003Cdl class=\"variablelist\">\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">BlockDevice\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tSpecifies the block device where the GFS2 file system resides.\n\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">MountPoint\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tSpecifies the directory where the GFS2 file system should be mounted.\n\t\t\t\t\t\t\u003C/dd>\u003C/dl>\u003C/div>\u003Cp>\n\t\t\t\tIn this example, the GFS2 file system resides on \u003Ccode class=\"literal\">/dev/vg01/lvol0\u003C/code> and is mounted on directory \u003Ccode class=\"literal\">/mygfs2\u003C/code> with \u003Ccode class=\"literal command\">atime\u003C/code> updates turned off.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>mount /dev/vg01/lvol0 /mygfs2 -o noatime\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"con_vfs-tuning-options-gfs2-usage-considerations\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">2.2. VFS tuning options: research and experiment\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tLike all Linux file systems, GFS2 sits on top of a layer called the virtual file system (VFS). The VFS provides good defaults for the cache settings for most workloads and should not need changing in most cases. If, however, you have a workload that is not running efficiently (for example, cache is too large or too small) then you may be able to improve the performance by using the \u003Ccode class=\"literal command\">sysctl\u003C/code>(8) command to adjust the values of the \u003Ccode class=\"literal\">sysctl\u003C/code> files in the \u003Ccode class=\"literal\">/proc/sys/vm\u003C/code> directory. Documentation for these files can be found in the kernel source tree \u003Ccode class=\"literal\">Documentation/sysctl/vm.txt\u003C/code>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFor example, the values for \u003Ccode class=\"literal\">dirty_background_ratio\u003C/code> and \u003Ccode class=\"literal\">vfs_cache_pressure\u003C/code> may be adjusted depending on your situation. To fetch the current values, use the following commands:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>sysctl -n vm.dirty_background_ratio\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>sysctl -n vm.vfs_cache_pressure\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe following commands adjust the values:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>sysctl -w vm.dirty_background_ratio=20\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>sysctl -w vm.vfs_cache_pressure=500\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tYou can permanently change the values of these parameters by editing the \u003Ccode class=\"literal\">/etc/sysctl.conf\u003C/code> file.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tTo find the optimal values for your use cases, research the various VFS options and experiment on a test cluster before deploying into full production.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"con_selinux-on-gfs2-gfs2-usage-considerations\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">2.3. SELinux on GFS2\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tUse of Security Enhanced Linux (SELinux) with GFS2 incurs a small performance penalty. To avoid this overhead, you may choose not to use SELinux with GFS2 even on a system with SELinux in enforcing mode. When mounting a GFS2 file system, you can ensure that SELinux will not attempt to read the \u003Ccode class=\"literal\">seclabel\u003C/code> element on each file system object by using one of the \u003Ccode class=\"literal\">context\u003C/code> options as described on the \u003Ccode class=\"literal\">mount\u003C/code>(8) man page; SELinux will assume that all content in the file system is labeled with the \u003Ccode class=\"literal\">seclabel\u003C/code> element provided in the \u003Ccode class=\"literal\">context\u003C/code> mount options. This will also speed up processing as it avoids another disk read of the extended attribute block that could contain \u003Ccode class=\"literal\">seclabel\u003C/code> elements.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFor example, on a system with SELinux in enforcing mode, you can use the following \u003Ccode class=\"literal command\">mount\u003C/code> command to mount the GFS2 file system if the file system is going to contain Apache content. This label will apply to the entire file system; it remains in memory and is not written to disk.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>mount -t gfs2 -o context=system_u:object_r:httpd_sys_content_t:s0 /dev/mapper/xyz/mnt/gfs2\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tIf you are not sure whether the file system will contain Apache content, you can use the labels \u003Ccode class=\"literal\">public_content_rw_t\u003C/code> or \u003Ccode class=\"literal\">public_content_t\u003C/code>, or you could define a new label altogether and define a policy around it.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tNote that in a Pacemaker cluster you should always use Pacemaker to manage a GFS2 file system. You can specify the mount options when you create a GFS2 file system resource.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"con_nfs-over-gfs2-gfs2-usage-considerations\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">2.4. Setting up NFS over GFS2\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tDue to the added complexity of the GFS2 locking subsystem and its clustered nature, setting up NFS over GFS2 requires taking many precautions.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tIf the GFS2 file system is NFS exported, then you must mount the file system with the \u003Ccode class=\"literal\">localflocks\u003C/code> option. Because utilizing the \u003Ccode class=\"literal\">localflocks\u003C/code> option prevents you from safely accessing the GFS2 filesystem from multiple locations, and it is not viable to export GFS2 from multiple nodes simultaneously, it is a support requirement that the GFS2 file system be mounted on only one node at a time when using this configuration. The intended effect of this is to force POSIX locks from each server to be local: non-clustered, independent of each other. This is because a number of problems exist if GFS2 attempts to implement POSIX locks from NFS across the nodes of a cluster. For applications running on NFS clients, localized POSIX locks means that two clients can hold the same lock concurrently if the two clients are mounting from different servers, which could cause data corruption. If all clients mount NFS from one server, then the problem of separate servers granting the same locks independently goes away. If you are not sure whether to mount your file system with the \u003Ccode class=\"literal\">localflocks\u003C/code> option, you should not use the option. Contact Red Hat support immediately to discuss the appropriate configuration to avoid data loss. Exporting GFS2 via NFS, while technically supported in some circumstances, is not recommended.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tFor all other (non-NFS) GFS2 applications, do not mount your file system using \u003Ccode class=\"literal\">localflocks\u003C/code>, so that GFS2 will manage the POSIX locks and flocks between all the nodes in the cluster (on a cluster-wide basis). If you specify \u003Ccode class=\"literal\">localflocks\u003C/code> and do not use NFS, the other nodes in the cluster will not have knowledge of each other’s POSIX locks and flocks, thus making them unsafe in a clustered environment\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tIn addition to the locking considerations, you should take the following into account when configuring an NFS service over a GFS2 file system.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tRed Hat supports only Red Hat High Availability Add-On configurations using NFSv3 with locking in an active/passive configuration with the following characteristics. This configuration provides High Availability (HA) for the file system and reduces system downtime since a failed node does not result in the requirement to execute the \u003Ccode class=\"literal command\">fsck\u003C/code> command when failing the NFS server from one node to another.\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"circle\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\tThe back-end file system is a GFS2 file system running on a 2 to 16 node cluster.\n\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\tAn NFSv3 server is defined as a service exporting the entire GFS2 file system from a single cluster node at a time.\n\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\tThe NFS server can fail over from one cluster node to another (active/passive configuration).\n\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\tNo access to the GFS2 file system is allowed \u003Cspan class=\"emphasis\">\u003Cem>except\u003C/em>\u003C/span> through the NFS server. This includes both local GFS2 file system access as well as access through Samba or Clustered Samba. Accessing the file system locally via the cluster node from which it is mounted may result in data corruption.\n\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\tThere is no NFS quota support on the system.\n\t\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe \u003Ccode class=\"literal\">fsid=\u003C/code> NFS option is mandatory for NFS exports of GFS2.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tIf problems arise with your cluster (for example, the cluster becomes inquorate and fencing is not successful), the clustered logical volumes and the GFS2 file system will be frozen and no access is possible until the cluster is quorate. You should consider this possibility when determining whether a simple failover solution such as the one defined in this procedure is the most appropriate for your system.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"con_samba-over-gfs2-gfs2-usage-considerations\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">2.5. Samba (SMB or Windows) file serving over GFS2\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can use Samba (SMB or Windows) file serving from a GFS2 file system with CTDB, which allows active/active configurations.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tSimultaneous access to the data in the Samba share from outside of Samba is not supported. There is currently no support for GFS2 cluster leases, which slows Samba file serving. For further information about support policies for Samba, see \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/3278591\">Support Policies for RHEL Resilient Storage - ctdb General Policies\u003C/a> and \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/3252211\">Support Policies for RHEL Resilient Storage - Exporting gfs2 contents via other protocols\u003C/a>.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"con_vms-for-gfs2-gfs2-usage-considerations\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">2.6. Configuring virtual machines for GFS2\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen using a GFS2 file system with a virtual machine, it is important that your VM storage settings on each node be configured properly in order to force the cache off. For example, including these settings for \u003Ccode class=\"literal\">cache\u003C/code> and \u003Ccode class=\"literal\">io\u003C/code> in the \u003Ccode class=\"literal\">libvirt\u003C/code> domain should allow GFS2 to behave as expected.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">&lt;driver name='qemu' type='raw' cache='none' io='native'/&gt;\u003C/pre>\u003Cp>\n\t\t\t\tAlternately, you can configure the \u003Ccode class=\"literal\">shareable\u003C/code> attribute within the device element. This indicates that the device is expected to be shared between domains (as long as hypervisor and OS support this). If \u003Ccode class=\"literal\">shareable\u003C/code> is used, \u003Ccode class=\"literal\">cache='no'\u003C/code> should be used for that device.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"con_gfs2-block-allocation-issues-gfs2-usage-considerations\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">2.7. Block allocation\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tEven though applications that only write data typically do not care how or where a block is allocated, some knowledge of how block allocation works can help you optimize performance.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"leave_free_space_in_the_file_system\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">2.7.1. Leave free space in the file system\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tWhen a GFS2 file system is nearly full, the block allocator starts to have a difficult time finding space for new blocks to be allocated. As a result, blocks given out by the allocator tend to be squeezed into the end of a resource group or in tiny slices where file fragmentation is much more likely. This file fragmentation can cause performance problems. In addition, when a GFS2 file system is nearly full, the GFS2 block allocator spends more time searching through multiple resource groups, and that adds lock contention that would not necessarily be there on a file system that has ample free space. This also can cause performance problems.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tFor these reasons, it is recommended that you not run a file system that is more than 85 percent full, although this figure may vary depending on workload.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"have_each_node_allocate_its_own_files_if_possible\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">2.7.2. Have each node allocate its own files, if possible\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tWhen developing applications for use with GFS2 file systems, it is recommended that you have each node allocate it own files, if possible. Due to the way the distributed lock manager (DLM) works, there will be more lock contention if all files are allocated by one node and other nodes need to add blocks to those files.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe term \"lock master\" has been used historically to denote a node which is currently the coordinator of lock requests, which originate locally or from a remote node in the cluster. This term for the lock request coordinator is slightly misleading because it is really a resource (in DLM terminology) in relation to which lock requests are either queued, granted or declined. In the sense in which the term is used in the DLM, it should be taken to refer to \"first among equals\", since the DLM is a peer-to-peer system.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tIn the Linux kernel DLM implementation, the node on which the lock is first used becomes the coordinator of lock requests, and after that point it does not change. This is an implementation detail of the Linux kernel DLM and not a property of DLMs in general. It is possible that a future update may allow the coordination of lock requests for a particular lock to move between nodes.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe location where lock requests are coordinated is transparent to the initiator of the lock request, except by the effect on the latency of the request. One consequence of the current implementation is that if there is an imbalance of the initial workload (for example, one node scans through the whole filesystem before others perform any I/O commands) this can result in higher lock latencies for other nodes in the cluster compared with the node that performed the initial scan of the filesystem.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tAs in many file systems, the GFS2 allocator tries to keep blocks in the same file close to one another to reduce the movement of disk heads and boost performance. A node that allocates blocks to a file will likely need to use and lock the same resource groups for the new blocks (unless all the blocks in that resource group are in use). The file system will run faster if the lock request coordinator for the resource group containing the file allocates its data blocks (it is faster to have the node that first opened the file do all the writing of new blocks).\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"preallocate_if_possible\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">2.7.3. Preallocate, if possible\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tIf files are preallocated, block allocations can be avoided altogether and the file system can run more efficiently. GFS2 includes the \u003Ccode class=\"literal command\">fallocate(1)\u003C/code> system call, which you can use to preallocate blocks of data.\n\t\t\t\t\u003C/p>\u003C/section>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_creating-mounting-gfs2-configuring-gfs2-file-systems\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 3. Administering GFS2 file systems\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tThere are a variety of commands and options that you use to create, mount, grow, and manage GFS2 file systems.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_creating-gfs2-creating-mounting-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">3.1. GFS2 file system creation\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou create a GFS2 file system with the \u003Ccode class=\"literal command\">mkfs.gfs2\u003C/code> command. A file system is created on an activated LVM volume.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"the_gfs2_mkfs_command\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">3.1.1. The GFS2 mkfs command\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following information is required to run the \u003Ccode class=\"literal command\">mkfs.gfs2\u003C/code> command to create a clustered GFS2 file system:\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tLock protocol/module name, which is \u003Ccode class=\"literal\">lock_dlm\u003C/code> for a cluster\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tCluster name\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tNumber of journals (one journal required for each node that may be mounting the file system)\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\tOnce you have created a GFS2 file system with the \u003Ccode class=\"literal command\">mkfs.gfs2\u003C/code> command, you cannot decrease the size of the file system. You can, however, increase the size of an existing file system with the \u003Ccode class=\"literal command\">gfs2_grow\u003C/code> command.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\t\tThe format for creating a clustered GFS2 file system is as follows. Note that Red Hat does not support the use of GFS2 as a single-node file system.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">mkfs.gfs2 -p lock_dlm -t \u003Cspan class=\"emphasis\">\u003Cem>ClusterName:FSName\u003C/em>\u003C/span> -j \u003Cspan class=\"emphasis\">\u003Cem>NumberJournals\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>BlockDevice\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tIf you prefer, you can create a GFS2 file system by using the \u003Ccode class=\"literal command\">mkfs\u003C/code> command with the \u003Ccode class=\"literal\">-t\u003C/code> parameter specifying a file system of type \u003Ccode class=\"literal\">gfs2\u003C/code>, followed by the GFS2 file system options.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">mkfs -t gfs2 -p lock_dlm -t \u003Cspan class=\"emphasis\">\u003Cem>ClusterName:FSName\u003C/em>\u003C/span> -j \u003Cspan class=\"emphasis\">\u003Cem>NumberJournals\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>BlockDevice\u003C/em>\u003C/span>\u003C/pre>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\tImproperly specifying the \u003Cspan class=\"emphasis\">\u003Cem>ClusterName:FSName\u003C/em>\u003C/span> parameter may cause file system or lock space corruption.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cdiv class=\"variablelist\">\u003Cdl class=\"variablelist\">\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">ClusterName\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\t\tThe name of the cluster for which the GFS2 file system is being created.\n\t\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">FSName\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\t\tThe file system name, which can be 1 to 16 characters long. The name must be unique for all \u003Ccode class=\"literal\">lock_dlm\u003C/code> file systems over the cluster.\n\t\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">NumberJournals\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\t\tSpecifies the number of journals to be created by the \u003Ccode class=\"literal command\">mkfs.gfs2\u003C/code> command. One journal is required for each node that mounts the file system. For GFS2 file systems, more journals can be added later without growing the file system.\n\t\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">BlockDevice\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\t\tSpecifies a logical or other block device\n\t\t\t\t\t\t\t\u003C/dd>\u003C/dl>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following table describes the \u003Ccode class=\"literal command\">mkfs.gfs2\u003C/code> command options (flags and parameters).\n\t\t\t\t\u003C/p>\u003Crh-table id=\"tb-table-gfs2-mkfs\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 3.1. Command Options: mkfs.gfs2\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 33%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688937186224\" scope=\"col\">Flag\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688937185136\" scope=\"col\">Parameter\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688930182128\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937186224\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal option\">-c\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937185136\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">Megabytes\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688930182128\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tSets the initial size of each journal’s quota change file to \u003Ccode class=\"literal\">Megabytes\u003C/code>.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937186224\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal option\">-D\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937185136\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688930182128\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tEnables debugging output.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937186224\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal option\">-h\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937185136\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688930182128\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tHelp. Displays available options.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937186224\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal option\">-J\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937185136\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">Megabytes\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688930182128\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tSpecifies the size of the journal in megabytes. Default journal size is 128 megabytes. The minimum size is 8 megabytes. Larger journals improve performance, although they use more memory than smaller journals.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937186224\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal option\">-j\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937185136\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">Number\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688930182128\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tSpecifies the number of journals to be created by the \u003Ccode class=\"literal command\">mkfs.gfs2\u003C/code> command. One journal is required for each node that mounts the file system. If this option is not specified, one journal will be created. For GFS2 file systems, you can add additional journals at a later time without growing the file system.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937186224\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal option\">-O\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937185136\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688930182128\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tPrevents the \u003Ccode class=\"literal command\">mkfs.gfs2\u003C/code> command from asking for confirmation before writing the file system.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937186224\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal option\">-p\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937185136\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">LockProtoName\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688930182128\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t* Specifies the name of the locking protocol to use. Recognized locking protocols include:\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">lock_dlm\u003C/code> — The standard locking module, required for a clustered file system.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">lock_nolock\u003C/code> — Used when GFS2 is acting as a local file system (one node only). Red Hat does not support the use of GFS2 as a single-node file system in a production environment. \u003Ccode class=\"literal\">lock_nolock\u003C/code> should be used only for the purposes of backup or for a secondary-site Disaster Recovery node, as described in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/configuring_gfs2_file_systems/index#minimum_cluster_size\">Minimum cluster size\u003C/a>. When using \u003Ccode class=\"literal\">lock_nolock\u003C/code>, you must ensure that the GFS2 file system is being used by only one system at a time.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937186224\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal option\">-q\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937185136\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688930182128\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tQuiet. Do not display anything.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937186224\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal option\">-r\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937185136\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">Megabytes\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688930182128\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tSpecifies the size of the resource groups in megabytes. The minimum resource group size is 32 megabytes. The maximum resource group size is 2048 megabytes. A large resource group size may increase performance on very large file systems. If this is not specified, \u003Ccode class=\"literal command\">mkfs.gfs2\u003C/code> chooses the resource group size based on the size of the file system: average size file systems will have 256 megabyte resource groups, and bigger file systems will have bigger resource groups for better performance.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937186224\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal option\">-t\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937185136\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">LockTableName\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688930182128\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t* A unique identifier that specifies the lock table field when you use the \u003Ccode class=\"literal\">lock_dlm\u003C/code> protocol; the \u003Ccode class=\"literal\">lock_nolock\u003C/code> protocol does not use this parameter.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* This parameter has two parts separated by a colon (no spaces) as follows: \u003Ccode class=\"literal\">ClusterName:FSName\u003C/code>.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">ClusterName\u003C/code> is the name of the cluster for which the GFS2 file system is being created; only members of this cluster are permitted to use this file system.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">FSName\u003C/code>, the file system name, can be 1 to 16 characters in length, and the name must be unique among all file systems in the cluster.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937186224\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal option\">-V\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688937185136\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688930182128\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tDisplays command version information.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003C/section>\u003Csection class=\"section\" id=\"creating_a_gfs2_file_system\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">3.1.2. Creating a GFS2 file system\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following example creates two GFS2 file systems. For both of these file systems, lock_dlm` is the locking protocol that the file system uses, since this is a clustered file system. Both file systems can be used in the cluster named \u003Ccode class=\"literal\">alpha\u003C/code>.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tFor the first file system, file system name is \u003Ccode class=\"literal\">mydata1\u003C/code>. it contains eight journals and is created on \u003Ccode class=\"literal\">/dev/vg01/lvol0\u003C/code>. For the second file system, the file system name is \u003Ccode class=\"literal\">mydata2\u003C/code>. It contains eight journals and is created on \u003Ccode class=\"literal\">/dev/vg01/lvol1\u003C/code>.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>mkfs.gfs2 -p lock_dlm -t alpha:mydata1 -j 8 /dev/vg01/lvol0\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>mkfs.gfs2 -p lock_dlm -t alpha:mydata2 -j 8 /dev/vg01/lvol1\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"proc_mounting-gfs2-filesystem_creating-mounting-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">3.2. Mounting a GFS2 file system\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tBefore you can mount a GFS2 file system, the file system must exist, the volume where the file system exists must be activated, and the supporting clustering and locking systems must be started. After those requirements have been met, you can mount the GFS2 file system as you would any Linux file system.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tYou should always use Pacemaker to manage the GFS2 file system in a production environment rather than manually mounting the file system with a \u003Ccode class=\"literal command\">mount\u003C/code> command, as this may cause issues at system shutdown.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tTo manipulate file ACLs, you must mount the file system with the \u003Ccode class=\"literal command\">-o acl\u003C/code> mount option. If a file system is mounted without the \u003Ccode class=\"literal command\">-o acl\u003C/code> mount option, users are allowed to view ACLs (with \u003Ccode class=\"literal command\">getfacl\u003C/code>), but are not allowed to set them (with \u003Ccode class=\"literal command\">setfacl\u003C/code>).\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"mounting_a_gfs2_file_system_with_no_options_specified\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">3.2.1. Mounting a GFS2 file system with no options specified\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tIn this example, the GFS2 file system on \u003Ccode class=\"literal\">/dev/vg01/lvol0\u003C/code> is mounted on the \u003Ccode class=\"literal\">/mygfs2\u003C/code> directory.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>mount /dev/vg01/lvol0 /mygfs2\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"mounting_a_gfs2_file_system_that_specifies_mount_options\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">3.2.2. Mounting a GFS2 file system that specifies mount options\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following is the format for the command to mount a GFS2 file system that specifies mount options.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">mount \u003Cspan class=\"emphasis\">\u003Cem>BlockDevice\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>MountPoint\u003C/em>\u003C/span> -o \u003Cspan class=\"emphasis\">\u003Cem>option\u003C/em>\u003C/span>\u003C/pre>\u003Cdiv class=\"variablelist\">\u003Cdl class=\"variablelist\">\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">BlockDevice\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\t\tSpecifies the block device where the GFS2 file system resides.\n\t\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">MountPoint\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\t\tSpecifies the directory where the GFS2 file system should be mounted.\n\t\t\t\t\t\t\t\u003C/dd>\u003C/dl>\u003C/div>\u003Cp>\n\t\t\t\t\tThe \u003Ccode class=\"literal option\">-o option\u003C/code> argument consists of GFS2-specific options or acceptable standard Linux \u003Ccode class=\"literal command\">mount -o\u003C/code> options, or a combination of both. Multiple \u003Ccode class=\"literal\">option\u003C/code> parameters are separated by a comma and no spaces.\n\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\tThe \u003Ccode class=\"literal command\">mount\u003C/code> command is a Linux system command. In addition to using these GFS2-specific options, you can use other, standard, \u003Ccode class=\"literal command\">mount\u003C/code> command options (for example, \u003Ccode class=\"literal option\">-r\u003C/code>). For information about other Linux \u003Ccode class=\"literal command\">mount\u003C/code> command options, see the Linux \u003Ccode class=\"literal command\">mount\u003C/code> man page.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\t\tThe following table describes the available GFS2-specific \u003Ccode class=\"literal option\">-o option\u003C/code> values that can be passed to GFS2 at mount time.\n\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\tThis table includes descriptions of options that are used with local file systems only. Note, however, that Red Hat does not support the use of GFS2 as a single-node file system. Red Hat will continue to support single-node GFS2 file systems for mounting snapshots of cluster file systems (for example, for backup purposes).\n\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Crh-table id=\"tb-table-gfs2-mount\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 3.2. GFS2-Specific Mount Options\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 50%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 50%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688951536048\" scope=\"col\">Option\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688951534960\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951536048\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">acl\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951534960\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tAllows manipulating file ACLs. If a file system is mounted without the \u003Ccode class=\"literal command\">acl\u003C/code> mount option, users are allowed to view ACLs (with \u003Ccode class=\"literal command\">getfacl\u003C/code>), but are not allowed to set them (with \u003Ccode class=\"literal command\">setfacl\u003C/code>).\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951536048\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">data=[ordered|writeback]\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951534960\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tWhen \u003Ccode class=\"literal\">data=ordered\u003C/code> is set, the user data modified by a transaction is flushed to the disk before the transaction is committed to disk. This should prevent the user from seeing uninitialized blocks in a file after a crash. When \u003Ccode class=\"literal\">data=writeback\u003C/code> mode is set, the user data is written to the disk at any time after it is dirtied; this does not provide the same consistency guarantee as \u003Ccode class=\"literal\">ordered\u003C/code> mode, but it should be slightly faster for some workloads. The default value is \u003Ccode class=\"literal\">ordered\u003C/code> mode.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951536048\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">ignore_local_fs\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">Caution:\u003C/code> This option should not be used when GFS2 file systems are shared.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951534960\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tForces GFS2 to treat the file system as a multi-host file system. By default, using \u003Ccode class=\"literal\">lock_nolock\u003C/code> automatically turns on the \u003Ccode class=\"literal\">localflocks\u003C/code> flag.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951536048\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">localflocks\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">Caution:\u003C/code> This option should not be used when GFS2 file systems are shared.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951534960\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tTells GFS2 to let the VFS (virtual file system) layer do all flock and fcntl. The \u003Ccode class=\"literal\">localflocks\u003C/code> flag is automatically turned on by \u003Ccode class=\"literal\">lock_nolock\u003C/code>.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951536048\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">lockproto=\u003C/code>\u003Ccode class=\"literal\">LockModuleName\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951534960\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tAllows the user to specify which locking protocol to use with the file system. If \u003Ccode class=\"literal\">LockModuleName\u003C/code> is not specified, the locking protocol name is read from the file system superblock.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951536048\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">locktable=\u003C/code>\u003Ccode class=\"literal\">LockTableName\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951534960\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tAllows the user to specify which locking table to use with the file system.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951536048\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">quota=[off/account/on]\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951534960\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tTurns quotas on or off for a file system. Setting the quotas to be in the \u003Ccode class=\"literal\">account\u003C/code> state causes the per UID/GID usage statistics to be correctly maintained by the file system; limit and warn values are ignored. The default value is \u003Ccode class=\"literal\">off\u003C/code>.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951536048\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal command\">errors=panic|withdraw\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951534960\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tWhen \u003Ccode class=\"literal\">errors=panic\u003C/code> is specified, file system errors will cause a kernel panic. When \u003Ccode class=\"literal\">errors=withdraw\u003C/code> is specified, which is the default behavior, file system errors will cause the system to withdraw from the file system and make it inaccessible until the next reboot; in some cases the system may remain running.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951536048\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">discard/nodiscard\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951534960\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tCauses GFS2 to generate \"discard\" I/O requests for blocks that have been freed. These can be used by suitable hardware to implement thin provisioning and similar schemes.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951536048\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">barrier/nobarrier\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951534960\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tCauses GFS2 to send I/O barriers when flushing the journal. The default value is \u003Ccode class=\"literal\">on\u003C/code>. This option is automatically turned \u003Ccode class=\"literal\">off\u003C/code> if the underlying device does not support I/O barriers. Use of I/O barriers with GFS2 is highly recommended at all times unless the block device is designed so that it cannot lose its write cache content (for example, if it is on a UPS or it does not have a write cache).\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951536048\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">quota_quantum=\u003Cspan class=\"emphasis\">\u003Cem>secs\u003C/em>\u003C/span>\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951534960\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tSets the number of seconds for which a change in the quota information may sit on one node before being written to the quota file. This is the preferred way to set this parameter. The value is an integer number of seconds greater than zero. The default is 60 seconds. Shorter settings result in faster updates of the lazy quota information and less likelihood of someone exceeding their quota. Longer settings make file system operations involving quotas faster and more efficient.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951536048\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">statfs_quantum=\u003Cspan class=\"emphasis\">\u003Cem>secs\u003C/em>\u003C/span>\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951534960\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tSetting \u003Ccode class=\"literal\">statfs_quantum\u003C/code> to 0 is the preferred way to set the slow version of \u003Ccode class=\"literal command\">statfs\u003C/code>. The default value is 30 secs which sets the maximum time period before \u003Ccode class=\"literal\">statfs\u003C/code> changes will be synced to the master \u003Ccode class=\"literal\">statfs\u003C/code> file. This can be adjusted to allow for faster, less accurate \u003Ccode class=\"literal\">statfs\u003C/code> values or slower more accurate values. When this option is set to 0, \u003Ccode class=\"literal\">statfs\u003C/code> will always report the true values.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951536048\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">statfs_percent=\u003Cspan class=\"emphasis\">\u003Cem>value\u003C/em>\u003C/span>\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688951534960\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tProvides a bound on the maximum percentage change in the \u003Ccode class=\"literal\">statfs\u003C/code> information about a local basis before it is synced back to the master \u003Ccode class=\"literal\">statfs\u003C/code> file, even if the time period has not expired. If the setting of \u003Ccode class=\"literal\">statfs_quantum\u003C/code> is 0, then this setting is ignored.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003C/section>\u003Csection class=\"section\" id=\"unmounting_a_gfs2_file_system\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">3.2.3. Unmounting a GFS2 file system\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tGFS2 file systems that have been mounted manually rather than automatically through Pacemaker will not be known to the system when file systems are unmounted at system shutdown. As a result, the GFS2 resource agent will not unmount the GFS2 file system. After the GFS2 resource agent is shut down, the standard shutdown process kills off all remaining user processes, including the cluster infrastructure, and tries to unmount the file system. This unmount will fail without the cluster infrastructure and the system will hang.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tTo prevent the system from hanging when the GFS2 file systems are unmounted, you should do one of the following:\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tAlways use Pacemaker to manage the GFS2 file system.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tIf a GFS2 file system has been mounted manually with the \u003Ccode class=\"literal command\">mount\u003C/code> command, be sure to unmount the file system manually with the \u003Ccode class=\"literal command\">umount\u003C/code> command before rebooting or shutting down the system.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\t\tIf your file system hangs while it is being unmounted during system shutdown under these circumstances, perform a hardware reboot. It is unlikely that any data will be lost since the file system is synced earlier in the shutdown process.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe GFS2 file system can be unmounted the same way as any Linux file system, by using the \u003Ccode class=\"literal command\">umount\u003C/code> command.\n\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\tThe \u003Ccode class=\"literal command\">umount\u003C/code> command is a Linux system command. Information about this command can be found in the Linux \u003Ccode class=\"literal command\">umount\u003C/code> command man pages.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\t\tUsage\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">umount \u003Cspan class=\"emphasis\">\u003Cem>MountPoint\u003C/em>\u003C/span>\u003C/pre>\u003Cdiv class=\"variablelist\">\u003Cdl class=\"variablelist\">\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">MountPoint\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\t\tSpecifies the directory where the GFS2 file system is currently mounted.\n\t\t\t\t\t\t\t\u003C/dd>\u003C/dl>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"proc_backing-up-a-gfs2-filesystem-creating-mounting-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">3.3. Backing up a GFS2 file system\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIt is important to make regular backups of your GFS2 file system in case of emergency, regardless of the size of your file system. Many system administrators feel safe because they are protected by RAID, multipath, mirroring, snapshots, and other forms of redundancy, but there is no such thing as safe enough.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIt can be a problem to create a backup since the process of backing up a node or set of nodes usually involves reading the entire file system in sequence. If this is done from a single node, that node will retain all the information in cache until other nodes in the cluster start requesting locks. Running this type of backup program while the cluster is in operation will negatively impact performance.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tDropping the caches once the backup is complete reduces the time required by other nodes to regain ownership of their cluster locks and caches. This is still not ideal, however, because the other nodes will have stopped caching the data that they were caching before the backup process began. You can drop caches using the following command after the backup is complete:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">echo -n 3 &gt; /proc/sys/vm/drop_caches\u003C/pre>\u003Cp>\n\t\t\t\tIt is faster if each node in the cluster backs up its own files so that the task is split between the nodes. You might be able to accomplish this with a script that uses the \u003Ccode class=\"literal command\">rsync\u003C/code> command on node-specific directories.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tRed Hat recommends making a GFS2 backup by creating a hardware snapshot on the SAN, presenting the snapshot to another system, and backing it up there. The backup system should mount the snapshot with \u003Ccode class=\"literal command\">-o lockproto=lock_nolock\u003C/code> since it will not be in a cluster. Note, however, that Red Hat does not support the use of GFS2 as a single-node file system in a production environment. This option should be used only for the purposes of backup or for a secondary-site Disaster Recovery node, as described in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/configuring_gfs2_file_systems/index#minimum_cluster_size\">Minimum cluster size\u003C/a>. When using this option, you must ensure that the GFS2 file system is being used by only one system at a time.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_suspending-activity-on-a-gfs2-filesystem-creating-mounting-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">3.4. Suspending activity on a GFS2 file system\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can suspend write activity to a file system by using the \u003Ccode class=\"literal command\">dmsetup suspend\u003C/code> command. Suspending write activity allows hardware-based device snapshots to be used to capture the file system in a consistent state. The \u003Ccode class=\"literal command\">dmsetup resume\u003C/code> command ends the suspension.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe format for the command to suspend activity on a GFS2 file system is as follows.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">dmsetup suspend \u003Cspan class=\"emphasis\">\u003Cem>MountPoint\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThis example suspends writes to file system \u003Ccode class=\"literal\">/mygfs2\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dmsetup suspend /mygfs2\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe format for the command to end suspension of activity on a GFS2 file system is as follows.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">dmsetup resume \u003Cspan class=\"emphasis\">\u003Cem>MountPoint\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThis example ends suspension of writes to file system \u003Ccode class=\"literal\">/mygfs2\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dmsetup resume /mygfs2\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_growing-gfs2-filesystem-creating-mounting-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">3.5. Growing a GFS2 file system\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe \u003Ccode class=\"literal command\">gfs2_grow\u003C/code> command is used to expand a GFS2 file system after the device where the file system resides has been expanded. Running the \u003Ccode class=\"literal command\">gfs2_grow\u003C/code> command on an existing GFS2 file system fills all spare space between the current end of the file system and the end of the device with a newly initialized GFS2 file system extension. All nodes in the cluster can then use the extra storage space that has been added.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tYou cannot decrease the size of a GFS2 file system.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal command\">gfs2_grow\u003C/code> command must be run on a mounted file system. The following procedure increases the size of the GFS2 file system in a cluster that is mounted on the logical volume \u003Ccode class=\"literal\">shared_vg/shared_lv1\u003C/code> with a mount point of \u003Ccode class=\"literal\">/mnt/gfs2\u003C/code>.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tPerform a backup of the data on the file system.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf you do not know the logical volume that is used by the file system to be expanded, you can determine this by running the \u003Ccode class=\"literal command\">df \u003Cspan class=\"emphasis\">\u003Cem>mountpoint\u003C/em>\u003C/span>\u003C/code> command. This will display the device name in the following format:\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">/dev/mapper/\u003Cspan class=\"emphasis\">\u003Cem>vg\u003C/em>\u003C/span>-\u003Cspan class=\"emphasis\">\u003Cem>lv\u003C/em>\u003C/span>\u003C/code>\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tFor example, the device name \u003Ccode class=\"literal\">/dev/mapper/shared_vg-shared_lv1\u003C/code> indicates that the logical volume is \u003Ccode class=\"literal\">shared_vg/shared_lv1\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn one node of the cluster, expand the underlying cluster volume with the \u003Ccode class=\"literal command\">lvextend\u003C/code> command.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>lvextend -L+1G shared_vg/shared_lv1\u003C/strong>\u003C/span>\nSize of logical volume shared_vg/shared_lv1 changed from 5.00 GiB (1280 extents) to 6.00 GiB (1536 extents).\nWARNING: extending LV with a shared lock, other hosts may require LV refresh.\nLogical volume shared_vg/shared_lv1 successfully resized.\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOne one node of the cluster, increase the size of the GFS2 file system. Do not extend the file system if the logical volume was not refreshed on all of the nodes, otherwise the file system data may become unavailable throughout the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>gfs2_grow /mnt/gfs2\u003C/strong>\u003C/span>\nFS: Mount point:             /mnt/gfs2\nFS: Device:                  /dev/mapper/shared_vg-shared_lv1\nFS: Size:                    1310719 (0x13ffff)\nDEV: Length:                 1572864 (0x180000)\nThe file system will grow by 1024MB.\ngfs2_grow complete.\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tRun the \u003Ccode class=\"literal command\">df\u003C/code> command on all nodes to check that the new space is now available in the file system. Note that it may take up to 30 seconds for the \u003Ccode class=\"literal command\">df\u003C/code> command on all nodes to show the same file system size\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>df -h /mnt/gfs2\u003C/strong>\u003C/span>]\nFilesystem                        Size  Used Avail Use% Mounted on\n/dev/mapper/shared_vg-shared_lv1  6.0G  4.5G  1.6G  75% /mnt/gfs2\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_adding-gfs2-journal-creating-mounting-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">3.6. Adding journals to a GFS2 file system\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tGFS2 requires one journal for each node in a cluster that needs to mount the file system. If you add additional nodes to the cluster, you can add journals to a GFS2 file system with the \u003Ccode class=\"literal command\">gfs2_jadd\u003C/code> command. You can add journals to a GFS2 file system dynamically at any point without expanding the underlying logical volume. The \u003Ccode class=\"literal command\">gfs2_jadd\u003C/code> command must be run on a mounted file system, but it needs to be run on only one node in the cluster. All the other nodes sense that the expansion has occurred.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tIf a GFS2 file system is full, the \u003Ccode class=\"literal command\">gfs2_jadd\u003C/code> command will fail, even if the logical volume containing the file system has been extended and is larger than the file system. This is because in a GFS2 file system, journals are plain files rather than embedded metadata, so simply extending the underlying logical volume will not provide space for the journals.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tBefore adding journals to a GFS2 file system, you can find out how many journals the GFS2 file system currently contains with the \u003Ccode class=\"literal command\">gfs2_edit -p jindex\u003C/code> command, as in the following example:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>gfs2_edit -p jindex /dev/sasdrives/scratch|grep journal\u003C/strong>\u003C/span>\n   3/3 [fc7745eb] 4/25 (0x4/0x19): File    journal0\n   4/4 [8b70757d] 5/32859 (0x5/0x805b): File    journal1\n   5/5 [127924c7] 6/65701 (0x6/0x100a5): File    journal2\u003C/pre>\u003Cp>\n\t\t\t\tThe format for the basic command to add journals to a GFS2 file system is as follows.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">gfs2_jadd -j \u003Cspan class=\"emphasis\">\u003Cem>Number\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>MountPoint\u003C/em>\u003C/span>\u003C/pre>\u003Cdiv class=\"variablelist\">\u003Cdl class=\"variablelist\">\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">Number\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tSpecifies the number of new journals to be added.\n\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">MountPoint\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tSpecifies the directory where the GFS2 file system is mounted.\n\t\t\t\t\t\t\u003C/dd>\u003C/dl>\u003C/div>\u003Cp>\n\t\t\t\tIn this example, one journal is added to the file system on the \u003Ccode class=\"literal\">/mygfs2\u003C/code> directory.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>gfs2_jadd -j 1 /mygfs2\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_gfs2-disk-quota-administration-configuring-gfs2-file-systems\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 4. GFS2 quota management\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tFile system quotas are used to limit the amount of file system space a user or group can use. A user or group does not have a quota limit until one is set. When a GFS2 file system is mounted with the \u003Ccode class=\"literal\">quota=on\u003C/code> or \u003Ccode class=\"literal\">quota=account\u003C/code> option, GFS2 keeps track of the space used by each user and group even when there are no limits in place. GFS2 updates quota information in a transactional way so system crashes do not require quota usages to be reconstructed. To prevent a performance slowdown, a GFS2 node synchronizes updates to the quota file only periodically. The fuzzy quota accounting can allow users or groups to slightly exceed the set limit. To minimize this, GFS2 dynamically reduces the synchronization period as a hard quota limit is approached.\n\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\tGFS2 supports the standard Linux quota facilities. In order to use this you will need to install the \u003Cspan class=\"strong strong\">\u003Cstrong>\u003Cspan class=\"application application\">quota\u003C/span>\u003C/strong>\u003C/span> RPM. This is the preferred way to administer quotas on GFS2 and should be used for all new deployments of GFS2 using quotas.\n\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\tFor more information about disk quotas, see the \u003Ccode class=\"literal command\">man\u003C/code> pages of the following commands:\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\u003Ccode class=\"literal command\">quotacheck\u003C/code>\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\u003Ccode class=\"literal command\">edquota\u003C/code>\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\u003Ccode class=\"literal command\">repquota\u003C/code>\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\u003Ccode class=\"literal command\">quota\u003C/code>\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Csection class=\"section\" id=\"proc_configuring-gfs2-disk-quotas-gfs2-disk-quota-administration\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">4.1. Configuring GFS2 disk quotas\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tTo implement disk quotas for GFS2 file systems, there are three steps to perform.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe steps to perform to implement disk quotas are as follows:\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tSet up quotas in enforcement or accounting mode.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tInitialize the quota database file with current block usage information.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tAssign quota policies. (In accounting mode, these policies are not enforced.)\n\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\tEach of these steps is discussed in detail in the following sections.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"setting_up_quotas_in_enforcement_or_accounting_mode\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">4.1.1. Setting up quotas in enforcement or accounting mode\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tIn GFS2 file systems, quotas are disabled by default. To enable quotas for a file system, mount the file system with the \u003Ccode class=\"literal\">quota=on\u003C/code> option specified.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tTo mount a file system with quotas enabled, specify \u003Ccode class=\"literal\">quota=on\u003C/code> for the \u003Ccode class=\"literal\">options\u003C/code> argument when creating the GFS2 file system resource in a cluster. For example, the following command specifies that the GFS2 \u003Ccode class=\"literal\">Filesystem\u003C/code> resource being created will be mounted with quotas enabled.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create gfs2mount Filesystem options=\"quota=on\" device=BLOCKDEVICE directory=MOUNTPOINT fstype=gfs2 clone\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tIt is possible to keep track of disk usage and maintain quota accounting for every user and group without enforcing the limit and warn values. To do this, mount the file system with the \u003Ccode class=\"literal\">quota=account\u003C/code> option specified.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tTo mount a file system with quotas disabled, specify \u003Ccode class=\"literal\">quota=off\u003C/code> for the \u003Ccode class=\"literal\">options\u003C/code> argument when creating the GFS2 file system resource in a cluster.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"creating_the_quota_database_files\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">4.1.2. Creating the quota database files\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tAfter each quota-enabled file system is mounted, the system is capable of working with disk quotas. However, the file system itself is not yet ready to support quotas. The next step is to run the \u003Ccode class=\"literal command\">quotacheck\u003C/code> command.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe \u003Ccode class=\"literal command\">quotacheck\u003C/code> command examines quota-enabled file systems and builds a table of the current disk usage per file system. The table is then used to update the operating system’s copy of disk usage. In addition, the file system’s disk quota files are updated.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tTo create the quota files on the file system, use the \u003Ccode class=\"literal option\">-u\u003C/code> and the \u003Ccode class=\"literal option\">-g\u003C/code> options of the \u003Ccode class=\"literal command\">quotacheck\u003C/code> command; both of these options must be specified for user and group quotas to be initialized. For example, if quotas are enabled for the \u003Ccode class=\"literal\">/home\u003C/code> file system, create the files in the \u003Ccode class=\"literal\">/home\u003C/code> directory:\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>quotacheck -ug /home\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"assigning_quotas_per_user\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">4.1.3. Assigning quotas per user\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe last step is assigning the disk quotas with the \u003Ccode class=\"literal command\">edquota\u003C/code> command. Note that if you have mounted your file system in accounting mode (with the \u003Ccode class=\"literal\">quota=account\u003C/code> option specified), the quotas are not enforced.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tTo configure the quota for a user, as root in a shell prompt, execute the command:\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>edquota username\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tPerform this step for each user who needs a quota. For example, if a quota is enabled for the \u003Ccode class=\"literal\">/home\u003C/code> partition (\u003Ccode class=\"literal\">/dev/VolGroup00/LogVol02\u003C/code> in the example below) and the command \u003Ccode class=\"literal command\">edquota testuser\u003C/code> is executed, the following is shown in the editor configured as the default for the system:\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">Disk quotas for user testuser (uid 501):\nFilesystem                blocks     soft     hard    inodes   soft   hard\n/dev/VolGroup00/LogVol02  440436        0        0\u003C/pre>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\tThe text editor defined by the \u003Ccode class=\"literal\">EDITOR\u003C/code> environment variable is used by \u003Ccode class=\"literal command\">edquota\u003C/code>. To change the editor, set the \u003Ccode class=\"literal\">EDITOR\u003C/code> environment variable in your \u003Ccode class=\"literal\">~/.bash_profile\u003C/code> file to the full path of the editor of your choice.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\t\tThe first column is the name of the file system that has a quota enabled for it. The second column shows how many blocks the user is currently using. The next two columns are used to set soft and hard block limits for the user on the file system.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe soft block limit defines the maximum amount of disk space that can be used.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe hard block limit is the absolute maximum amount of disk space that a user or group can use. Once this limit is reached, no further disk space can be used.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe GFS2 file system does not maintain quotas for inodes, so these columns do not apply to GFS2 file systems and will be blank.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tIf any of the values are set to 0, that limit is not set. In the text editor, change the limits. For example:\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">Disk quotas for user testuser (uid 501):\nFilesystem                blocks     soft     hard    inodes   soft   hard\n/dev/VolGroup00/LogVol02  440436   500000   550000\u003C/pre>\u003Cp>\n\t\t\t\t\tTo verify that the quota for the user has been set, use the following command:\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>quota testuser\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tYou can also set quotas from the command line with the \u003Ccode class=\"literal\">setquota\u003C/code> command. For information about the \u003Ccode class=\"literal\">setquota\u003C/code> command, see the \u003Ccode class=\"literal\">setquota\u003C/code>(8) man page.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"assigning_quotas_per_group\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">4.1.4. Assigning quotas per group\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tQuotas can also be assigned on a per-group basis. Note that if you have mounted your file system in accounting mode (with the \u003Ccode class=\"literal\">account=on\u003C/code> option specified), the quotas are not enforced.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tTo set a group quota for the \u003Ccode class=\"literal\">devel\u003C/code> group (the group must exist prior to setting the group quota), use the following command:\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>edquota -g devel\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tThis command displays the existing quota for the group in the text editor:\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">Disk quotas for group devel (gid 505):\nFilesystem                blocks    soft     hard    inodes   soft   hard\n/dev/VolGroup00/LogVol02  440400       0        0\u003C/pre>\u003Cp>\n\t\t\t\t\tThe GFS2 file system does not maintain quotas for inodes, so these columns do not apply to GFS2 file systems and will be blank. Modify the limits, then save the file.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tTo verify that the group quota has been set, use the following command:\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">$ \u003Cspan class=\"strong strong\">\u003Cstrong>quota -g devel\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"proc_managing-gfs2-disk-quotas-gfs2-disk-quota-administration\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">4.2. Managing GFS2 disk Quotas\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf quotas are implemented, they need some maintenance, mostly in the form of watching to see if the quotas are exceeded and making sure the quotas are accurate.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf users repeatedly exceed their quotas or consistently reach their soft limits, a system administrator has a few choices to make depending on what type of users they are and how much disk space impacts their work. The administrator can either help the user determine how to use less disk space or increase the user’s disk quota.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can create a disk usage report by running the \u003Ccode class=\"literal command\">repquota\u003C/code> utility. For example, the command \u003Ccode class=\"literal command\">repquota /home\u003C/code> produces this output:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">*** Report for user quotas on device /dev/mapper/VolGroup00-LogVol02\nBlock grace time: 7days; Inode grace time: 7days\n\t\t\tBlock limits\t\t\tFile limits\nUser\t\tused\tsoft\thard\tgrace\tused\tsoft\thard\tgrace\n----------------------------------------------------------------------\nroot      --      36       0       0              4     0     0\nkristin   --     540       0       0            125     0     0\ntestuser  --  440400  500000  550000          37418     0     0\u003C/pre>\u003Cp>\n\t\t\t\tTo view the disk usage report for all (option \u003Ccode class=\"literal option\">-a\u003C/code>) quota-enabled file systems, use the command:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>repquota -a\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal\">--\u003C/code> displayed after each user is a quick way to determine whether the block limits have been exceeded. If the block soft limit is exceeded, a \u003Ccode class=\"literal\">+\u003C/code> appears in place of the first \u003Ccode class=\"literal\">-\u003C/code> in the output. The second \u003Ccode class=\"literal\">-\u003C/code> indicates the inode limit, but GFS2 file systems do not support inode limits so that character will remain as \u003Ccode class=\"literal\">-\u003C/code>. GFS2 file systems do not support a grace period, so the \u003Ccode class=\"literal\">grace\u003C/code> column will remain blank.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tNote that the \u003Ccode class=\"literal command\">repquota\u003C/code> command is not supported over NFS, irrespective of the underlying file system.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_keeping-gfs2-quotas-accurate-with-quotacheck-gfs2-disk-quota-administration\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">4.3. Keeping GFS2 disk quotas accurate with the quotacheck command\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf you enable quotas on your file system after a period of time when you have been running with quotas disabled, you should run the \u003Ccode class=\"literal\">quotacheck\u003C/code> command to create, check, and repair quota files. Additionally, you may want to run the \u003Ccode class=\"literal\">quotacheck\u003C/code> command if you think your quota files may not be accurate, as may occur when a file system is not unmounted cleanly after a system crash.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFor more information about the \u003Ccode class=\"literal\">quotacheck\u003C/code> command, see the \u003Ccode class=\"literal\">quotacheck(8)\u003C/code> man page.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tRun \u003Ccode class=\"literal command\">quotacheck\u003C/code> when the file system is relatively idle on all nodes because disk activity may affect the computed quota values.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/section>\u003Csection class=\"section\" id=\"proc_synchronizing-gfs2-quotas-gfs2-disk-quota-administration\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">4.4. Synchronizing quotas with the quotasync Command\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tGFS2 stores all quota information in its own internal file on disk. A GFS2 node does not update this quota file for every file system write; rather, by default it updates the quota file once every 60 seconds. This is necessary to avoid contention among nodes writing to the quota file, which would cause a slowdown in performance.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tAs a user or group approaches their quota limit, GFS2 dynamically reduces the time between its quota-file updates to prevent the limit from being exceeded. The normal time period between quota synchronizations is a tunable parameter, \u003Ccode class=\"literal\">quota_quantum\u003C/code>. You can change this from its default value of 60 seconds using the \u003Ccode class=\"literal\">quota_quantum=\u003C/code> mount option, as described in the \"GFS2-Specific Mount Options\" table in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_creating-mounting-gfs2-configuring-gfs2-file-systems#mounting_a_gfs2_file_system_that_specifies_mount_options\">Mounting a GFS2 file system that specifies mount options\u003C/a>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal\">quota_quantum\u003C/code> parameter must be set on each node and each time the file system is mounted. Changes to the \u003Ccode class=\"literal command\">quota_quantum\u003C/code> parameter are not persistent across unmounts. You can update the \u003Ccode class=\"literal\">quota_quantum\u003C/code> value with the \u003Ccode class=\"literal command\">mount -o remount\u003C/code>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can use the \u003Ccode class=\"literal command\">quotasync\u003C/code> command to synchronize the quota information from a node to the on-disk quota file between the automatic updates performed by GFS2. Usage \u003Cspan class=\"strong strong\">\u003Cstrong>\u003Cspan class=\"application application\">Synchronizing Quota Information\u003C/span>\u003C/strong>\u003C/span>\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">quotasync [-ug] -a|\u003Cspan class=\"emphasis\">\u003Cem>mountpoint\u003C/em>\u003C/span>...\u003C/pre>\u003Cdiv class=\"variablelist\">\u003Cdl class=\"variablelist\">\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">u\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tSync the user quota files.\n\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">g\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tSync the group quota files\n\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">a\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tSync all file systems that are currently quota-enabled and support sync. When -a is absent, a file system mountpoint should be specified.\n\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">mountpoint\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tSpecifies the GFS2 file system to which the actions apply.\n\t\t\t\t\t\t\u003C/dd>\u003C/dl>\u003C/div>\u003Cp>\n\t\t\t\tYou can tune the time between synchronizations by specifying a \u003Ccode class=\"literal\">quota-quantum\u003C/code> mount option.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>mount -o quota_quantum=\u003Cspan class=\"emphasis\">\u003Cem>secs\u003C/em>\u003C/span>,remount \u003Cspan class=\"emphasis\">\u003Cem>BlockDevice\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>MountPoint\u003C/em>\u003C/span>\u003C/strong>\u003C/span>\u003C/pre>\u003Cdiv class=\"variablelist\">\u003Cdl class=\"variablelist\">\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">MountPoint\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tSpecifies the GFS2 file system to which the actions apply.\n\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">secs\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tSpecifies the new time period between regular quota-file synchronizations by GFS2. Smaller values may increase contention and slow down performance.\n\t\t\t\t\t\t\u003C/dd>\u003C/dl>\u003C/div>\u003Cp>\n\t\t\t\tThe following example synchronizes all the cached dirty quotas from the node it is run on to the on-disk quota file for the file system \u003Ccode class=\"literal\">/mnt/mygfs2\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>quotasync -ug /mnt/mygfs2\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThis following example changes the default time period between regular quota-file updates to one hour (3600 seconds) for file system \u003Ccode class=\"literal\">/mnt/mygfs2\u003C/code> when remounting that file system on logical volume \u003Ccode class=\"literal\">/dev/volgroup/logical_volume\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>mount -o quota_quantum=3600,remount /dev/volgroup/logical_volume /mnt/mygfs2\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_gfs2-filesystem-repair-configuring-gfs2-file-systems\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 5. GFS2 file system repair\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tWhen nodes fail with the file system mounted, file system journaling allows fast recovery. However, if a storage device loses power or is physically disconnected, file system corruption may occur. (Journaling cannot be used to recover from storage subsystem failures.) When that type of corruption occurs, you can recover the GFS2 file system by using the \u003Ccode class=\"literal command\">fsck.gfs2\u003C/code> command.\n\t\t\u003C/p>\u003Crh-alert class=\"admonition important\" state=\"warning\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Important\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal command\">fsck.gfs2\u003C/code> command must be run only on a file system that is unmounted from all nodes. When the file system is being managed as a Pacemaker cluster resource, you can disable the file system resource, which unmounts the file system. After running the \u003Ccode class=\"literal command\">fsck.gfs2\u003C/code> command, you enable the file system resource again. The \u003Cspan class=\"emphasis\">\u003Cem>timeout\u003C/em>\u003C/span> value specified with the \u003Ccode class=\"literal\">--wait\u003C/code> option of the \u003Ccode class=\"literal command\">pcs resource disable\u003C/code> indicates a value in seconds.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource disable --wait=\u003Cspan class=\"emphasis\">\u003Cem>timeoutvalue\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>\n[fsck.gfs2]\npcs resource enable \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tNote that even if a file system is part of a resource group, as in an encrypted file system deployment, you need to disable only the file system resource in order to run the fsck command on the file system. You must not disable the entire resource group.\n\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\tTo ensure that \u003Ccode class=\"literal command\">fsck.gfs2\u003C/code> command does not run on a GFS2 file system at boot time, you can set the \u003Ccode class=\"literal\">run_fsck\u003C/code> parameter of the \u003Ccode class=\"literal\">options\u003C/code> argument when creating the GFS2 file system resource in a cluster. Specifying \u003Ccode class=\"literal\">\"run_fsck=no\"\u003C/code> will indicate that you should not run the \u003Ccode class=\"literal command\">fsck\u003C/code> command.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_determining-needed-memory-for-fsckgfs2-gfs2-filesystem-repair\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">5.1. Determining required memory for running fsck.gfs2\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tRunning the \u003Ccode class=\"literal command\">fsck.gfs2\u003C/code> command may require system memory above and beyond the memory used for the operating system and kernel. Larger file systems in particular may require additional memory to run this command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following table shows approximate values of memory that may be required to run \u003Ccode class=\"literal\">fsck.gfs2\u003C/code> file systems on GFS2 file systems that are 1TB, 10TB, and 100TB in size with a block size of 4K.\n\t\t\t\u003C/p>\u003Crh-table>\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccolgroup>\u003Ccol style=\"width: 33%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 67%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688927449616\" scope=\"col\">GFS2 file system size\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688927448528\" scope=\"col\">Approximate memory required to run \u003Ccode class=\"literal\">fsck.gfs2\u003C/code>\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927449616\"> \u003Cp>\n\t\t\t\t\t\t\t\t1 TB\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927448528\"> \u003Cp>\n\t\t\t\t\t\t\t\t0.16 GB\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927449616\"> \u003Cp>\n\t\t\t\t\t\t\t\t10 TB\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927448528\"> \u003Cp>\n\t\t\t\t\t\t\t\t1.6 GB\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927449616\"> \u003Cp>\n\t\t\t\t\t\t\t\t100 TB\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927448528\"> \u003Cp>\n\t\t\t\t\t\t\t\t16 GB\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tNote that a smaller block size for the file system would require a larger amount of memory. For example, GFS2 file systems with a block size of 1K would require four times the amount of memory indicated in this table.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_repairing-a-gfs2-filesystem-gfs2-filesystem-repair\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">5.2. Repairing a gfs2 filesystem\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe format of the \u003Ccode class=\"literal\">fsck.gfs2\u003C/code> command to repair a GFS2 filesystem is as follows:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">fsck.gfs2 -y \u003Cspan class=\"emphasis\">\u003Cem>BlockDevice\u003C/em>\u003C/span>\u003C/pre>\u003Cdiv class=\"variablelist\">\u003Cdl class=\"variablelist\">\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal option\">-y\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tThe \u003Ccode class=\"literal option\">-y\u003C/code> flag causes all questions to be answered with \u003Ccode class=\"literal\">yes\u003C/code>. With the \u003Ccode class=\"literal option\">-y\u003C/code> flag specified, the \u003Ccode class=\"literal command\">fsck.gfs2\u003C/code> command does not prompt you for an answer before making changes.\n\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">BlockDevice\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\tSpecifies the block device where the GFS2 file system resides.\n\t\t\t\t\t\t\u003C/dd>\u003C/dl>\u003C/div>\u003Cp>\n\t\t\t\tIn this example, the GFS2 file system residing on block device \u003Ccode class=\"literal\">/dev/testvg/testlv\u003C/code> is repaired. All queries to repair are automatically answered with \u003Ccode class=\"literal\">yes\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>fsck.gfs2 -y /dev/testvg/testlv\u003C/strong>\u003C/span>\nInitializing fsck\nValidating Resource Group index.\nLevel 1 RG check.\n(level 1 passed)\nClearing journals (this may take a while)...\nJournals cleared.\nStarting pass1\nPass1 complete\nStarting pass1b\nPass1b complete\nStarting pass1c\nPass1c complete\nStarting pass2\nPass2 complete\nStarting pass3\nPass3 complete\nStarting pass4\nPass4 complete\nStarting pass5\nPass5 complete\nWriting changes to disk\nfsck.gfs2 complete\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_gfs2-performance-configuring-gfs2-file-systems\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 6. Improving GFS2 performance\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tThere are many aspects of GFS2 configuration that you can analyze to improve file system performance.\n\t\t\u003C/p>\u003Cp>\n\t\t\tFor general recommendations for deploying and upgrading Red Hat Enterprise Linux clusters using the High Availability Add-On and Red Hat Global File System 2 (GFS2) see the article \u003Ca class=\"link\" href=\"https://access.redhat.com/kb/docs/DOC-40821\">Red Hat Enterprise Linux Cluster, High Availability, and GFS Deployment Best Practices\u003C/a> on the Red Hat Customer Portal.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_gfs2-defragment-gfs2-performance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">6.1. GFS2 file system defragmentation\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhile there is no defragmentation tool for GFS2 on Red Hat Enterprise Linux, you can defragment individual files by identifying them with the \u003Ccode class=\"literal\">filefrag\u003C/code> tool, copying them to temporary files, and renaming the temporary files to replace the originals.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"con_gfs2-node-locking-gfs2-performance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">6.2. GFS2 node locking\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIn order to get the best performance from a GFS2 file system, it is important to understand some of the basic theory of its operation. A single node file system is implemented alongside a cache, the purpose of which is to eliminate latency of disk accesses when using frequently requested data. In Linux the page cache (and historically the buffer cache) provide this caching function.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tWith GFS2, each node has its own page cache which may contain some portion of the on-disk data. GFS2 uses a locking mechanism called \u003Cspan class=\"emphasis\">\u003Cem>glocks\u003C/em>\u003C/span> (pronounced gee-locks) to maintain the integrity of the cache between nodes. The glock subsystem provides a cache management function which is implemented using the \u003Cspan class=\"emphasis\">\u003Cem>distributed lock manager\u003C/em>\u003C/span> (DLM) as the underlying communication layer.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe glocks provide protection for the cache on a per-inode basis, so there is one lock per inode which is used for controlling the caching layer. If that glock is granted in shared mode (DLM lock mode: PR) then the data under that glock may be cached upon one or more nodes at the same time, so that all the nodes may have local access to the data.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf the glock is granted in exclusive mode (DLM lock mode: EX) then only a single node may cache the data under that glock. This mode is used by all operations which modify the data (such as the \u003Ccode class=\"literal command\">write\u003C/code> system call).\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf another node requests a glock which cannot be granted immediately, then the DLM sends a message to the node or nodes which currently hold the glocks blocking the new request to ask them to drop their locks. Dropping glocks can be (by the standards of most file system operations) a long process. Dropping a shared glock requires only that the cache be invalidated, which is relatively quick and proportional to the amount of cached data.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tDropping an exclusive glock requires a log flush, and writing back any changed data to disk, followed by the invalidation as per the shared glock.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe difference between a single node file system and GFS2, then, is that a single node file system has a single cache and GFS2 has a separate cache on each node. In both cases, latency to access cached data is of a similar order of magnitude, but the latency to access uncached data is much greater in GFS2 if another node has previously cached that same data.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tOperations such as \u003Ccode class=\"literal\">read\u003C/code> (buffered), \u003Ccode class=\"literal\">stat,\u003C/code> and \u003Ccode class=\"literal\">readdir\u003C/code> only require a shared glock. Operations such as \u003Ccode class=\"literal\">write\u003C/code> (buffered), \u003Ccode class=\"literal\">mkdir\u003C/code>, \u003Ccode class=\"literal\">rmdir\u003C/code>, and \u003Ccode class=\"literal\">unlink\u003C/code> require an exclusive glock. Direct I/O read/write operations require a deferred glock if no allocation is taking place, or an exclusive glock if the write requires an allocation (that is, extending the file, or hole filling).\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThere are two main performance considerations which follow from this. First, read-only operations parallelize extremely well across a cluster, since they can run independently on every node. Second, operations requiring an exclusive glock can reduce performance, if there are multiple nodes contending for access to the same inode(s). Consideration of the working set on each node is thus an important factor in GFS2 file system performance such as when, for example, you perform a file system backup, as described in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_creating-mounting-gfs2-configuring-gfs2-file-systems#proc_backing-up-a-gfs2-filesystem-creating-mounting-gfs2\">Backing up a GFS2 file system\u003C/a>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tA further consequence of this is that we recommend the use of the \u003Ccode class=\"literal\">noatime\u003C/code> or \u003Ccode class=\"literal\">nodiratime\u003C/code> mount option with GFS2 whenever possible, with the preference for \u003Ccode class=\"literal\">noatime\u003C/code> where the application allows for this. This prevents reads from requiring exclusive locks to update the \u003Ccode class=\"literal\">atime\u003C/code> timestamp.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFor users who are concerned about the working set or caching efficiency, GFS2 provides tools that allow you to monitor the performance of a GFS2 file system: Performance Co-Pilot and GFS2 tracepoints.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tDue to the way in which GFS2’s caching is implemented the best performance is obtained when either of the following takes place:\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tAn inode is used in a read-only fashion across all nodes.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tAn inode is written or modified from a single node only.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\t\tNote that inserting and removing entries from a directory during file creation and deletion counts as writing to the directory inode.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tIt is possible to break this rule provided that it is broken relatively infrequently. Ignoring this rule too often will result in a severe performance penalty.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tIf you \u003Ccode class=\"literal command\">mmap\u003C/code>() a file on GFS2 with a read/write mapping, but only read from it, this only counts as a read.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tIf you do not set the \u003Ccode class=\"literal\">noatime\u003C/code> \u003Ccode class=\"literal command\">mount\u003C/code> parameter, then reads will also result in writes to update the file timestamps. We recommend that all GFS2 users should mount with \u003Ccode class=\"literal\">noatime\u003C/code> unless they have a specific requirement for \u003Ccode class=\"literal\">atime\u003C/code>.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/section>\u003Csection class=\"section\" id=\"con_posix-locking-issues-gfs2-performance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">6.3. Issues with Posix locking\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen using Posix locking, you should take the following into account:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tUse of Flocks will yield faster processing than use of Posix locks.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tPrograms using Posix locks in GFS2 should avoid using the \u003Ccode class=\"literal\">GETLK\u003C/code> function since, in a clustered environment, the process ID may be for a different node in the cluster.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"con_troubleshooting-gfs2-performance-gfs2-performance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">6.4. Performance tuning with GFS2\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIt is usually possible to alter the way in which a troublesome application stores its data in order to gain a considerable performance advantage.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tA typical example of a troublesome application is an email server. These are often laid out with a spool directory containing files for each user (\u003Ccode class=\"literal\">mbox\u003C/code>), or with a directory for each user containing a file for each message (\u003Ccode class=\"literal\">maildir\u003C/code>). When requests arrive over IMAP, the ideal arrangement is to give each user an affinity to a particular node. That way their requests to view and delete email messages will tend to be served from the cache on that one node. Obviously if that node fails, then the session can be restarted on a different node.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tWhen mail arrives by means of SMTP, then again the individual nodes can be set up so as to pass a certain user’s mail to a particular node by default. If the default node is not up, then the message can be saved directly into the user’s mail spool by the receiving node. Again this design is intended to keep particular sets of files cached on just one node in the normal case, but to allow direct access in the case of node failure.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThis setup allows the best use of GFS2’s page cache and also makes failures transparent to the application, whether \u003Ccode class=\"literal\">imap\u003C/code> or \u003Ccode class=\"literal\">smtp\u003C/code>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tBackup is often another tricky area. Again, if it is possible it is greatly preferable to back up the working set of each node directly from the node which is caching that particular set of inodes. If you have a backup script which runs at a regular point in time, and that seems to coincide with a spike in the response time of an application running on GFS2, then there is a good chance that the cluster may not be making the most efficient use of the page cache.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tObviously, if you are in the position of being able to stop the application in order to perform a backup, then this will not be a problem. On the other hand, if a backup is run from just one node, then after it has completed a large portion of the file system will be cached on that node, with a performance penalty for subsequent accesses from other nodes. This can be mitigated to a certain extent by dropping the VFS page cache on the backup node after the backup has completed with following command:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">echo -n 3 &gt;/proc/sys/vm/drop_caches\u003C/pre>\u003Cp>\n\t\t\t\tHowever this is not as good a solution as taking care to ensure the working set on each node is either shared, mostly read-only across the cluster, or accessed largely from a single node.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"con_gfs2-lockdump-gfs2-performance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">6.5. Troubleshooting GFS2 performance with the GFS2 lock dump\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf your cluster performance is suffering because of inefficient use of GFS2 caching, you may see large and increasing I/O wait times. You can make use of GFS2’s lock dump information to determine the cause of the problem.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe GFS2 lock dump information can be gathered from the \u003Ccode class=\"literal\">debugfs\u003C/code> file which can be found at the following path name, assuming that \u003Ccode class=\"literal\">debugfs\u003C/code> is mounted on \u003Ccode class=\"literal\">/sys/kernel/debug/\u003C/code>:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">/sys/kernel/debug/gfs2/\u003Cspan class=\"emphasis\">\u003Cem>fsname\u003C/em>\u003C/span>/glocks\u003C/pre>\u003Cp>\n\t\t\t\tThe content of the file is a series of lines. Each line starting with G: represents one glock, and the following lines, indented by a single space, represent an item of information relating to the glock immediately before them in the file.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe best way to use the \u003Ccode class=\"literal\">debugfs\u003C/code> file is to use the \u003Ccode class=\"literal command\">cat\u003C/code> command to take a copy of the complete content of the file (it might take a long time if you have a large amount of RAM and a lot of cached inodes) while the application is experiencing problems, and then looking through the resulting data at a later date.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tIt can be useful to make two copies of the \u003Ccode class=\"literal\">debugfs\u003C/code> file, one a few seconds or even a minute or two after the other. By comparing the holder information in the two traces relating to the same glock number, you can tell whether the workload is making progress (it is just slow) or whether it has become stuck (which is always a bug and should be reported to Red Hat support immediately).\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tLines in the \u003Ccode class=\"literal\">debugfs\u003C/code> file starting with H: (holders) represent lock requests either granted or waiting to be granted. The flags field on the holders line f: shows which: The 'W' flag refers to a waiting request, the 'H' flag refers to a granted request. The glocks which have large numbers of waiting requests are likely to be those which are experiencing particular contention.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following tables show the meanings of the glock flags and glock holder flags.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-glock-flags\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 6.1. Glock flags\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 33%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688931730816\" scope=\"col\">Flag\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688931729728\" scope=\"col\">Name\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688931728640\" scope=\"col\">Meaning\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931730816\"> \u003Cp>\n\t\t\t\t\t\t\t\tb\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931729728\"> \u003Cp>\n\t\t\t\t\t\t\t\tBlocking\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931728640\"> \u003Cp>\n\t\t\t\t\t\t\t\tValid when the locked flag is set, and indicates that the operation that has been requested from the DLM may block. This flag is cleared for demotion operations and for \"try\" locks. The purpose of this flag is to allow gathering of stats of the DLM response time independent from the time taken by other nodes to demote locks.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931730816\"> \u003Cp>\n\t\t\t\t\t\t\t\td\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931729728\"> \u003Cp>\n\t\t\t\t\t\t\t\tPending demote\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931728640\"> \u003Cp>\n\t\t\t\t\t\t\t\tA deferred (remote) demote request\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931730816\"> \u003Cp>\n\t\t\t\t\t\t\t\tD\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931729728\"> \u003Cp>\n\t\t\t\t\t\t\t\tDemote\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931728640\"> \u003Cp>\n\t\t\t\t\t\t\t\tA demote request (local or remote)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931730816\"> \u003Cp>\n\t\t\t\t\t\t\t\tf\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931729728\"> \u003Cp>\n\t\t\t\t\t\t\t\tLog flush\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931728640\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe log needs to be committed before releasing this glock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931730816\"> \u003Cp>\n\t\t\t\t\t\t\t\tF\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931729728\"> \u003Cp>\n\t\t\t\t\t\t\t\tFrozen\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931728640\"> \u003Cp>\n\t\t\t\t\t\t\t\tReplies from remote nodes ignored - recovery is in progress. This flag is not related to file system freeze, which uses a different mechanism, but is used only in recovery.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931730816\"> \u003Cp>\n\t\t\t\t\t\t\t\ti\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931729728\"> \u003Cp>\n\t\t\t\t\t\t\t\tInvalidate in progress\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931728640\"> \u003Cp>\n\t\t\t\t\t\t\t\tIn the process of invalidating pages under this glock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931730816\"> \u003Cp>\n\t\t\t\t\t\t\t\tI\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931729728\"> \u003Cp>\n\t\t\t\t\t\t\t\tInitial\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931728640\"> \u003Cp>\n\t\t\t\t\t\t\t\tSet when DLM lock is associated with this glock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931730816\"> \u003Cp>\n\t\t\t\t\t\t\t\tl\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931729728\"> \u003Cp>\n\t\t\t\t\t\t\t\tLocked\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931728640\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe glock is in the process of changing state\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931730816\"> \u003Cp>\n\t\t\t\t\t\t\t\tL\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931729728\"> \u003Cp>\n\t\t\t\t\t\t\t\tLRU\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931728640\"> \u003Cp>\n\t\t\t\t\t\t\t\tSet when the glock is on the LRU list\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931730816\"> \u003Cp>\n\t\t\t\t\t\t\t\to\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931729728\"> \u003Cp>\n\t\t\t\t\t\t\t\tObject\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931728640\"> \u003Cp>\n\t\t\t\t\t\t\t\tSet when the glock is associated with an object (that is, an inode for type 2 glocks, and a resource group for type 3 glocks)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931730816\"> \u003Cp>\n\t\t\t\t\t\t\t\tp\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931729728\"> \u003Cp>\n\t\t\t\t\t\t\t\tDemote in progress\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931728640\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe glock is in the process of responding to a demote request\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931730816\"> \u003Cp>\n\t\t\t\t\t\t\t\tq\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931729728\"> \u003Cp>\n\t\t\t\t\t\t\t\tQueued\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931728640\"> \u003Cp>\n\t\t\t\t\t\t\t\tSet when a holder is queued to a glock, and cleared when the glock is held, but there are no remaining holders. Used as part of the algorithm the calculates the minimum hold time for a glock.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931730816\"> \u003Cp>\n\t\t\t\t\t\t\t\tr\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931729728\"> \u003Cp>\n\t\t\t\t\t\t\t\tReply pending\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931728640\"> \u003Cp>\n\t\t\t\t\t\t\t\tReply received from remote node is awaiting processing\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931730816\"> \u003Cp>\n\t\t\t\t\t\t\t\ty\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931729728\"> \u003Cp>\n\t\t\t\t\t\t\t\tDirty\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931728640\"> \u003Cp>\n\t\t\t\t\t\t\t\tData needs flushing to disk before releasing this glock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Crh-table id=\"tb-glock-holderflags\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 6.2. Glock holder flags\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 33%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688931006304\" scope=\"col\">Flag\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688931005216\" scope=\"col\">Name\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688931004128\" scope=\"col\">Meaning\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931006304\"> \u003Cp>\n\t\t\t\t\t\t\t\ta\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931005216\"> \u003Cp>\n\t\t\t\t\t\t\t\tAsync\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931004128\"> \u003Cp>\n\t\t\t\t\t\t\t\tDo not wait for glock result (will poll for result later)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931006304\"> \u003Cp>\n\t\t\t\t\t\t\t\tA\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931005216\"> \u003Cp>\n\t\t\t\t\t\t\t\tAny\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931004128\"> \u003Cp>\n\t\t\t\t\t\t\t\tAny compatible lock mode is acceptable\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931006304\"> \u003Cp>\n\t\t\t\t\t\t\t\tc\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931005216\"> \u003Cp>\n\t\t\t\t\t\t\t\tNo cache\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931004128\"> \u003Cp>\n\t\t\t\t\t\t\t\tWhen unlocked, demote DLM lock immediately\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931006304\"> \u003Cp>\n\t\t\t\t\t\t\t\te\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931005216\"> \u003Cp>\n\t\t\t\t\t\t\t\tNo expire\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931004128\"> \u003Cp>\n\t\t\t\t\t\t\t\tIgnore subsequent lock cancel requests\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931006304\"> \u003Cp>\n\t\t\t\t\t\t\t\tE\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931005216\"> \u003Cp>\n\t\t\t\t\t\t\t\texact\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931004128\"> \u003Cp>\n\t\t\t\t\t\t\t\tMust have exact lock mode\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931006304\"> \u003Cp>\n\t\t\t\t\t\t\t\tF\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931005216\"> \u003Cp>\n\t\t\t\t\t\t\t\tFirst\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931004128\"> \u003Cp>\n\t\t\t\t\t\t\t\tSet when holder is the first to be granted for this lock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931006304\"> \u003Cp>\n\t\t\t\t\t\t\t\tH\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931005216\"> \u003Cp>\n\t\t\t\t\t\t\t\tHolder\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931004128\"> \u003Cp>\n\t\t\t\t\t\t\t\tIndicates that requested lock is granted\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931006304\"> \u003Cp>\n\t\t\t\t\t\t\t\tp\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931005216\"> \u003Cp>\n\t\t\t\t\t\t\t\tPriority\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931004128\"> \u003Cp>\n\t\t\t\t\t\t\t\tEnqueue holder at the head of the queue\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931006304\"> \u003Cp>\n\t\t\t\t\t\t\t\tt\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931005216\"> \u003Cp>\n\t\t\t\t\t\t\t\tTry\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931004128\"> \u003Cp>\n\t\t\t\t\t\t\t\tA \"try\" lock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931006304\"> \u003Cp>\n\t\t\t\t\t\t\t\tT\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931005216\"> \u003Cp>\n\t\t\t\t\t\t\t\tTry 1CB\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931004128\"> \u003Cp>\n\t\t\t\t\t\t\t\tA \"try\" lock that sends a callback\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931006304\"> \u003Cp>\n\t\t\t\t\t\t\t\tW\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931005216\"> \u003Cp>\n\t\t\t\t\t\t\t\tWait\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931004128\"> \u003Cp>\n\t\t\t\t\t\t\t\tSet while waiting for request to complete\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tHaving identified a glock which is causing a problem, the next step is to find out which inode it relates to. The glock number (n: on the G: line) indicates this. It is of the form \u003Cspan class=\"emphasis\">\u003Cem>type\u003C/em>\u003C/span>/\u003Cspan class=\"emphasis\">\u003Cem>number\u003C/em>\u003C/span> and if \u003Cspan class=\"emphasis\">\u003Cem>type\u003C/em>\u003C/span> is 2, then the glock is an inode glock and the \u003Cspan class=\"emphasis\">\u003Cem>number\u003C/em>\u003C/span> is an inode number. To track down the inode, you can then run \u003Ccode class=\"literal command\">find -inum \u003Cspan class=\"emphasis\">\u003Cem>number\u003C/em>\u003C/span>\u003C/code> where \u003Cspan class=\"emphasis\">\u003Cem>number\u003C/em>\u003C/span> is the inode number converted from the hex format in the glocks file into decimal.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tIf you run the \u003Ccode class=\"literal command\">find\u003C/code> command on a file system when it is experiencing lock contention, you are likely to make the problem worse. It is a good idea to stop the application before running the \u003Ccode class=\"literal command\">find\u003C/code> command when you are looking for contended inodes.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tThe following table shows the meanings of the different glock types.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-glock-types\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 6.3. Glock types\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 33%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688929069632\" scope=\"col\">Type number\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688929068544\" scope=\"col\">Lock type\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688929067456\" scope=\"col\">Use\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929069632\"> \u003Cp>\n\t\t\t\t\t\t\t\t1\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929068544\"> \u003Cp>\n\t\t\t\t\t\t\t\tTrans\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929067456\"> \u003Cp>\n\t\t\t\t\t\t\t\tTransaction lock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929069632\"> \u003Cp>\n\t\t\t\t\t\t\t\t2\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929068544\"> \u003Cp>\n\t\t\t\t\t\t\t\tInode\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929067456\"> \u003Cp>\n\t\t\t\t\t\t\t\tInode metadata and data\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929069632\"> \u003Cp>\n\t\t\t\t\t\t\t\t3\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929068544\"> \u003Cp>\n\t\t\t\t\t\t\t\tRgrp\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929067456\"> \u003Cp>\n\t\t\t\t\t\t\t\tResource group metadata\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929069632\"> \u003Cp>\n\t\t\t\t\t\t\t\t4\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929068544\"> \u003Cp>\n\t\t\t\t\t\t\t\tMeta\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929067456\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe superblock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929069632\"> \u003Cp>\n\t\t\t\t\t\t\t\t5\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929068544\"> \u003Cp>\n\t\t\t\t\t\t\t\tIopen\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929067456\"> \u003Cp>\n\t\t\t\t\t\t\t\tInode last closer detection\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929069632\"> \u003Cp>\n\t\t\t\t\t\t\t\t6\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929068544\"> \u003Cp>\n\t\t\t\t\t\t\t\tFlock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929067456\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal command\">flock\u003C/code>(2) syscall\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929069632\"> \u003Cp>\n\t\t\t\t\t\t\t\t8\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929068544\"> \u003Cp>\n\t\t\t\t\t\t\t\tQuota\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929067456\"> \u003Cp>\n\t\t\t\t\t\t\t\tQuota operations\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929069632\"> \u003Cp>\n\t\t\t\t\t\t\t\t9\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929068544\"> \u003Cp>\n\t\t\t\t\t\t\t\tJournal\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929067456\"> \u003Cp>\n\t\t\t\t\t\t\t\tJournal mutex\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tIf the glock that was identified was of a different type, then it is most likely to be of type 3: (resource group). If you see significant numbers of processes waiting for other types of glock under normal loads, report this to Red Hat support.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf you do see a number of waiting requests queued on a resource group lock there may be a number of reasons for this. One is that there are a large number of nodes compared to the number of resource groups in the file system. Another is that the file system may be very nearly full (requiring, on average, longer searches for free blocks). The situation in both cases can be improved by adding more storage and using the \u003Ccode class=\"literal command\">gfs2_grow\u003C/code> command to expand the file system.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"enabling-data-journaling-gfs2-performance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">6.6. Enabling data journaling\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tOrdinarily, GFS2 writes only metadata to its journal. File contents are subsequently written to disk by the kernel’s periodic sync that flushes file system buffers. An \u003Ccode class=\"literal command\">fsync()\u003C/code> call on a file causes the file’s data to be written to disk immediately. The call returns when the disk reports that all data is safely written.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tData journaling can result in a reduced \u003Ccode class=\"literal command\">fsync()\u003C/code> time for very small files because the file data is written to the journal in addition to the metadata. This advantage rapidly reduces as the file size increases. Writing to medium and larger files will be much slower with data journaling turned on.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tApplications that rely on \u003Ccode class=\"literal command\">fsync()\u003C/code> to sync file data may see improved performance by using data journaling. Data journaling can be enabled automatically for any GFS2 files created in a flagged directory (and all its subdirectories). Existing files with zero length can also have data journaling turned on or off.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tEnabling data journaling on a directory sets the directory to \"inherit jdata\", which indicates that all files and directories subsequently created in that directory are journaled. You can enable and disable data journaling on a file with the \u003Ccode class=\"literal command\">chattr\u003C/code> command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following commands enable data journaling on the \u003Ccode class=\"literal\">/mnt/gfs2/gfs2_dir/newfile\u003C/code> file and then check whether the flag has been set properly.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>chattr +j /mnt/gfs2/gfs2_dir/newfile\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>lsattr /mnt/gfs2/gfs2_dir\u003C/strong>\u003C/span>\n---------j--- /mnt/gfs2/gfs2_dir/newfile\u003C/pre>\u003Cp>\n\t\t\t\tThe following commands disable data journaling on the \u003Ccode class=\"literal\">/mnt/gfs2/gfs2_dir/newfile\u003C/code> file and then check whether the flag has been set properly.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>chattr -j /mnt/gfs2/gfs2_dir/newfile\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>lsattr /mnt/gfs2/gfs2_dir\u003C/strong>\u003C/span>\n------------- /mnt/gfs2/gfs2_dir/newfile\u003C/pre>\u003Cp>\n\t\t\t\tYou can also use the \u003Ccode class=\"literal command\">chattr\u003C/code> command to set the \u003Ccode class=\"literal\">j\u003C/code> flag on a directory. When you set this flag for a directory, all files and directories subsequently created in that directory are journaled. The following set of commands sets the \u003Ccode class=\"literal\">j\u003C/code> flag on the \u003Ccode class=\"literal\">gfs2_dir\u003C/code> directory, then checks whether the flag has been set properly. After this, the commands create a new file called \u003Ccode class=\"literal\">newfile\u003C/code> in the \u003Ccode class=\"literal\">/mnt/gfs2/gfs2_dir\u003C/code> directory and then check whether the \u003Ccode class=\"literal\">j\u003C/code> flag has been set for the file. Since the \u003Ccode class=\"literal\">j\u003C/code> flag is set for the directory, then \u003Ccode class=\"literal\">newfile\u003C/code> should also have journaling enabled.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>chattr -j /mnt/gfs2/gfs2_dir\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>lsattr /mnt/gfs2\u003C/strong>\u003C/span>\n---------j--- /mnt/gfs2/gfs2_dir\n# \u003Cspan class=\"strong strong\">\u003Cstrong>touch /mnt/gfs2/gfs2_dir/newfile\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>lsattr /mnt/gfs2/gfs2_dir\u003C/strong>\u003C/span>\n---------j--- /mnt/gfs2/gfs2_dir/newfile\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_troubleshooting-gfs2-configuring-gfs2-file-systems\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 7. Diagnosing and correcting problems with GFS2 file systems\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tThe following procedures describe some common GFS2 issues and provide information on how to address them.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"ref_gfs2-filesystem-unavailable-troubleshooting-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">7.1. GFS2 file system unavailable to a node (the GFS2 withdraw function)\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe GFS2 \u003Cspan class=\"emphasis\">\u003Cem>withdraw\u003C/em>\u003C/span> function is a data integrity feature of the GFS2 file system that prevents potential file system damage due to faulty hardware or kernel software. If the GFS2 kernel module detects an inconsistency while using a GFS2 file system on any given cluster node, it withdraws from the file system, leaving it unavailable to that node until it is unmounted and remounted (or the machine detecting the problem is rebooted). All other mounted GFS2 file systems remain fully functional on that node. (The GFS2 withdraw function is less severe than a kernel panic, which causes the node to be fenced.)\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe main categories of inconsistency that can cause a GFS2 withdraw are as follows:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tInode consistency error\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tResource group consistency error\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tJournal consistency error\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tMagic number metadata consistency error\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tMetadata type consistency error\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tAn example of an inconsistency that would cause a GFS2 withdraw is an incorrect block count for a file’s inode. When GFS2 deletes a file, it systematically removes all the data and metadata blocks referenced by that file. When done, it checks the inode’s block count. If the block count is not 1 (meaning all that is left is the disk inode itself), that indicates a file system inconsistency, since the inode’s block count did not match the actual blocks used for the file. In many cases, the problem may have been caused by faulty hardware (faulty memory, motherboard, HBA, disk drives, cables, and so forth). It may also have been caused by a kernel bug (another kernel module accidentally overwriting GFS2’s memory), or actual file system damage (caused by a GFS2 bug).\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIn most cases, the best way to recover from a withdrawn GFS2 file system is to reboot or fence the node. The withdrawn GFS2 file system will give you an opportunity to relocate services to another node in the cluster. After services are relocated you can reboot the node or force a fence with this command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs stonith fence \u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span>\u003C/pre>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tDo not try to unmount and remount the file system manually with the \u003Ccode class=\"literal command\">umount\u003C/code> and \u003Ccode class=\"literal command\">mount\u003C/code> commands. You must use the \u003Ccode class=\"literal command\">pcs\u003C/code> command, otherwise Pacemaker will detect the file system service has disappeared and fence the node.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tThe consistency problem that caused the withdraw may make stopping the file system service impossible as it may cause the system to hang.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf the problem persists after a remount, you should stop the file system service to unmount the file system from all nodes in the cluster, then perform a file system check with the \u003Ccode class=\"literal\">fsck.gfs2\u003C/code> command before restarting the service with the following procedure.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tReboot the affected node.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tDisable the non-clone file system service in Pacemaker to unmount the file system from every node in the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource disable --wait=100 mydata_fs\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tFrom one node of the cluster, run the \u003Ccode class=\"literal command\">fsck.gfs2\u003C/code> command on the file system device to check for and repair any file system damage.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>fsck.gfs2 -y /dev/vg_mydata/mydata &gt; /tmp/fsck.out\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tRemount the GFS2 file system from all nodes by re-enabling the file system service:\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource enable --wait=100 mydata_fs\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\tYou can override the GFS2 withdraw function by mounting the file system with the \u003Ccode class=\"literal\">-o errors=panic\u003C/code> option specified in the file system service.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource update mydata_fs “options=noatime,errors=panic”\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tWhen this option is specified, any errors that would normally cause the system to withdraw force a kernel panic instead. This stops the node’s communications, which causes the node to be fenced. This is especially useful for clusters that are left unattended for long periods of time without monitoring or intervention.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tInternally, the GFS2 withdraw function works by disconnecting the locking protocol to ensure that all further file system operations result in I/O errors. As a result, when the withdraw occurs, it is normal to see a number of I/O errors from the device mapper device reported in the system logs.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"ref_gfs2-filesystem-hangs-one-node-troubleshooting-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">7.2. GFS2 file system hangs and requires reboot of one node\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf your GFS2 file system hangs and does not return commands run against it, but rebooting one specific node returns the system to normal, this may be indicative of a locking problem or bug. Should this occur, gather GFS2 data during one of these occurences and open a support ticket with Red Hat Support, as described in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_troubleshooting-gfs2-configuring-gfs2-file-systems#proc_gathering-gfs2-data-troubleshooting-gfs2\">Gathering GFS2 data for troubleshooting\u003C/a>.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"ref_gfs2-filesystem-hangs-all-nodes-troubleshooting-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">7.3. GFS2 file system hangs and requires reboot of all nodes\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf your GFS2 file system hangs and does not return commands run against it, requiring that you reboot all nodes in the cluster before using it, check for the following issues.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tYou may have had a failed fence. GFS2 file systems will freeze to ensure data integrity in the event of a failed fence. Check the messages logs to see if there are any failed fences at the time of the hang. Ensure that fencing is configured correctly.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe GFS2 file system may have withdrawn. Check through the messages logs for the word \u003Ccode class=\"literal\">withdraw\u003C/code> and check for any messages and call traces from GFS2 indicating that the file system has been withdrawn. A withdraw is indicative of file system corruption, a storage failure, or a bug. At the earliest time when it is convenient to unmount the file system, you should perform the following procedure:\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tReboot the node on which the withdraw occurred.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>/sbin/reboot\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tStop the file system resource to unmount the GFS2 file system on all nodes.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource disable --wait=100 mydata_fs\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCapture the metadata with the \u003Ccode class=\"literal command\">gfs2_edit savemeta…​\u003C/code> command. You should ensure that there is sufficient space for the file, which in some cases may be large. In this example, the metadata is saved to a file in the \u003Ccode class=\"literal\">/root\u003C/code> directory.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>gfs2_edit savemeta /dev/vg_mydata/mydata /root/gfs2metadata.gz\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tUpdate the \u003Ccode class=\"literal\">gfs2-utils\u003C/code> package.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>sudo dnf update gfs2-utils\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tOn one node, run the \u003Ccode class=\"literal command\">fsck.gfs2\u003C/code> command on the file system to ensure file system integrity and repair any damage.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>fsck.gfs2 -y /dev/vg_mydata/mydata &gt; /tmp/fsck.out\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tAfter the \u003Ccode class=\"literal command\">fsck.gfs2\u003C/code> command has completed, re-enable the file system resource to return it to service:\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource enable --wait=100 mydata_fs\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tOpen a support ticket with Red Hat Support. Inform them you experienced a GFS2 withdraw and provide logs and the debugging information generated by the \u003Ccode class=\"literal command\">sosreports\u003C/code> and \u003Ccode class=\"literal command\">gfs2_edit savemeta\u003C/code> commands.\n\t\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tIn some instances of a GFS2 withdraw, commands can hang that are trying to access the file system or its block device. In these cases a hard reboot is required to reboot the cluster.\n\t\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tFor information about the GFS2 withdraw function, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_troubleshooting-gfs2-configuring-gfs2-file-systems#ref_gfs2-filesystem-unavailable-troubleshooting-gfs2\">GFS2 filesystem unavailable to a node (the GFS2 withdraw function)\u003C/a>.\n\t\t\t\t\t\t\t\u003C/p>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThis error may be indicative of a locking problem or bug. Gather data during one of these occurrences and open a support ticket with Red Hat Support, as described in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_troubleshooting-gfs2-configuring-gfs2-file-systems#proc_gathering-gfs2-data-troubleshooting-gfs2\">Gathering GFS2 data for troubleshooting\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"ref_gfs2-nomount-new-cluster-node-troubleshooting-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">7.4. GFS2 file system does not mount on newly added cluster node\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf you add a new node to a cluster and find that you cannot mount your GFS2 file system on that node, you may have fewer journals on the GFS2 file system than nodes attempting to access the GFS2 file system. You must have one journal per GFS2 host you intend to mount the file system on (with the exception of GFS2 file systems mounted with the \u003Ccode class=\"literal\">spectator\u003C/code> mount option set, since these do not require a journal). You can add journals to a GFS2 file system with the \u003Ccode class=\"literal command\">gfs2_jadd\u003C/code> command, as described in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_creating-mounting-gfs2-configuring-gfs2-file-systems#proc_adding-gfs2-journal-creating-mounting-gfs2\">Adding journals to a GFS2 file system\u003C/a>.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"ref_gfs2-used-space-empty-filesystem-troubleshooting-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">7.5. Space indicated as used in empty file system\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf you have an empty GFS2 file system, the \u003Ccode class=\"literal command\">df\u003C/code> command will show that there is space being taken up. This is because GFS2 file system journals consume space (number of journals * journal size) on disk. f you created a GFS2 file system with a large number of journals or specified a large journal size then you will be see (number of journals * journal size) as already in use when you execute the \u003Ccode class=\"literal command\">df\u003C/code> command. Even if you did not specify a large number of journals or large journals, small GFS2 file systems (in the 1GB or less range) will show a large amount of space as being in use with the default GFS2 journal size.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_gathering-gfs2-data-troubleshooting-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">7.6. Gathering GFS2 data for troubleshooting\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf your GFS2 file system hangs and does not return commands run against it and you find that you need to open a ticket with Red Hat Support, you should first gather the following data:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe GFS2 lock dump for the file system on each node:\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">cat /sys/kernel/debug/gfs2/\u003Cspan class=\"emphasis\">\u003Cem>fsname\u003C/em>\u003C/span>/glocks &gt;glocks.\u003Cspan class=\"emphasis\">\u003Cem>fsname\u003C/em>\u003C/span>.\u003Cspan class=\"emphasis\">\u003Cem>nodename\u003C/em>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe DLM lock dump for the file system on each node: You can get this information with the \u003Ccode class=\"literal command\">dlm_tool\u003C/code>:\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">dlm_tool lockdebug -sv \u003Cspan class=\"emphasis\">\u003Cem>lsname\u003C/em>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIn this command, \u003Cspan class=\"emphasis\">\u003Cem>lsname\u003C/em>\u003C/span> is the lockspace name used by DLM for the file system in question. You can find this value in the output from the \u003Ccode class=\"literal command\">group_tool\u003C/code> command.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe output from the \u003Ccode class=\"literal command\">sysrq -t\u003C/code> command.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe contents of the \u003Ccode class=\"literal\">/var/log/messages\u003C/code> file.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tOnce you have gathered that data, you can open a ticket with Red Hat Support and provide the data you have collected.\n\t\t\t\u003C/p>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-gfs2-in-a-cluster-configuring-gfs2-file-systems\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 8. GFS2 file systems in a cluster\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tUse the following administrative procedures to configure GFS2 file systems in a Red Hat high availability cluster.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">8.1. Configuring a GFS2 file system in a cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can set up a Pacemaker cluster that includes GFS2 file systems with the following procedure. In this example, you create three GFS2 file systems on three logical volumes in a two-node cluster.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Prerequisites\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tInstall and start the cluster software on both cluster nodes and create a basic two-node cluster.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tConfigure fencing for the cluster.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tFor information about creating a Pacemaker cluster and configuring fencing for the cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters\">Creating a Red Hat High-Availability cluster with Pacemaker\u003C/a>.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes in the cluster, enable the repository for Resilient Storage that corresponds to your system architecture. For example, to enable the Resilient Storage repository for an x86_64 system, you can enter the following \u003Ccode class=\"literal\">subscription-manager\u003C/code> command:\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>subscription-manager repos --enable=rhel-9-for-x86_64-resilientstorage-rpms\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tNote that the Resilient Storage repository is a superset of the High Availability repository. If you enable the Resilient Storage repository you do not also need to enable the High Availability repository.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes of the cluster, install the \u003Ccode class=\"literal\">lvm2-lockd\u003C/code>, \u003Ccode class=\"literal\">gfs2-utils\u003C/code>, and \u003Ccode class=\"literal\">dlm\u003C/code> packages. To support these packages, you must be subscribed to the AppStream channel and the Resilient Storage channel.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install lvm2-lockd gfs2-utils dlm\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes of the cluster, set the \u003Ccode class=\"literal\">use_lvmlockd\u003C/code> configuration option in the \u003Ccode class=\"literal\">/etc/lvm/lvm.conf\u003C/code> file to \u003Ccode class=\"literal\">use_lvmlockd=1\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">...\nuse_lvmlockd = 1\n...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSet the global Pacemaker parameter \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> to \u003Ccode class=\"literal\">freeze\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tBy default, the value of \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> is set to \u003Ccode class=\"literal\">stop\u003C/code>, indicating that once quorum is lost, all the resources on the remaining partition will immediately be stopped. Typically this default is the safest and most optimal option, but unlike most resources, GFS2 requires quorum to function. When quorum is lost both the applications using the GFS2 mounts and the GFS2 mount itself cannot be correctly stopped. Any attempts to stop these resources without quorum will fail which will ultimately result in the entire cluster being fenced every time quorum is lost.\n\t\t\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\t\t\tTo address this situation, set \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> to \u003Ccode class=\"literal\">freeze\u003C/code> when GFS2 is in use. This means that when quorum is lost, the remaining partition will do nothing until quorum is regained.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set no-quorum-policy=freeze\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSet up a \u003Ccode class=\"literal\">dlm\u003C/code> resource. This is a required dependency for configuring a GFS2 file system in a cluster. This example creates the \u003Ccode class=\"literal\">dlm\u003C/code> resource as part of a resource group named \u003Ccode class=\"literal\">locking\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tClone the \u003Ccode class=\"literal\">locking\u003C/code> resource group so that the resource group can be active on both nodes of the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource clone locking interleave=true\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSet up an \u003Ccode class=\"literal\">lvmlockd\u003C/code> resource as part of the \u003Ccode class=\"literal\">locking\u003C/code> resource group.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCheck the status of the cluster to ensure that the \u003Ccode class=\"literal\">locking\u003C/code> resource group has started on both nodes of the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status --full\u003C/strong>\u003C/span>\nCluster name: my_cluster\n[...]\n\nOnline: [ z1.example.com (1) z2.example.com (2) ]\n\nFull list of resources:\n\n smoke-apc      (stonith:fence_apc):    Started z1.example.com\n Clone Set: locking-clone [locking]\n     Resource Group: locking:0\n         dlm    (ocf::pacemaker:controld):      Started z1.example.com\n         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com\n     Resource Group: locking:1\n         dlm    (ocf::pacemaker:controld):      Started z2.example.com\n         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com\n     Started: [ z1.example.com z2.example.com ]\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn one node of the cluster, create two shared volume groups. One volume group will contain two GFS2 file systems, and the other volume group will contain one GFS2 file system.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tIf your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order\">Configuring startup order for resource dependencies not managed by Pacemaker\u003C/a>.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following command creates the shared volume group \u003Ccode class=\"literal\">shared_vg1\u003C/code> on \u003Ccode class=\"literal\">/dev/vdb\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgcreate --shared shared_vg1 /dev/vdb\u003C/strong>\u003C/span>\n  Physical volume \"/dev/vdb\" successfully created.\n  Volume group \"shared_vg1\" successfully created\n  VG shared_vg1 starting dlm lockspace\n  Starting locking.  Waiting until locks are ready...\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following command creates the shared volume group \u003Ccode class=\"literal\">shared_vg2\u003C/code> on \u003Ccode class=\"literal\">/dev/vdc\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgcreate --shared shared_vg2 /dev/vdc\u003C/strong>\u003C/span>\n  Physical volume \"/dev/vdc\" successfully created.\n  Volume group \"shared_vg2\" successfully created\n  VG shared_vg2 starting dlm lockspace\n  Starting locking.  Waiting until locks are ready...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn the second node in the cluster:\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tIf the use of a devices file is enabled with the \u003Ccode class=\"literal\">use_devicesfile = 1\u003C/code> parameter in the \u003Ccode class=\"literal\">lvm.conf\u003C/code> file, add the shared devices to the devices file This feature is enabled by default.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvmdevices --adddev /dev/vdb\u003C/strong>\u003C/span>\n[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvmdevices --adddev /dev/vdc\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tStart the lock manager for each of the shared volume groups.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgchange --lockstart shared_vg1\u003C/strong>\u003C/span>\n  VG shared_vg1 starting dlm lockspace\n  Starting locking.  Waiting until locks are ready...\n[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgchange --lockstart shared_vg2\u003C/strong>\u003C/span>\n  VG shared_vg2 starting dlm lockspace\n  Starting locking.  Waiting until locks are ready...\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn one node in the cluster, create the shared logical volumes and format the volumes with a GFS2 file system. One journal is required for each node that mounts the file system. Ensure that you create enough journals for each of the nodes in your cluster. The format of the lock table name is \u003Cspan class=\"emphasis\">\u003Cem>ClusterName:FSName\u003C/em>\u003C/span> where \u003Cspan class=\"emphasis\">\u003Cem>ClusterName\u003C/em>\u003C/span> is the name of the cluster for which the GFS2 file system is being created and \u003Cspan class=\"emphasis\">\u003Cem>FSName\u003C/em>\u003C/span> is the file system name, which must be unique for all \u003Ccode class=\"literal\">lock_dlm\u003C/code> file systems over the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvcreate --activate sy -L5G -n shared_lv1 shared_vg1\u003C/strong>\u003C/span>\n  Logical volume \"shared_lv1\" created.\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvcreate --activate sy -L5G -n shared_lv2 shared_vg1\u003C/strong>\u003C/span>\n  Logical volume \"shared_lv2\" created.\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvcreate --activate sy -L5G -n shared_lv1 shared_vg2\u003C/strong>\u003C/span>\n  Logical volume \"shared_lv1\" created.\n\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo1 /dev/shared_vg1/shared_lv1\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo2 /dev/shared_vg1/shared_lv2\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo3 /dev/shared_vg2/shared_lv1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource for each logical volume to automatically activate that logical volume on all nodes.\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource named \u003Ccode class=\"literal\">sharedlv1\u003C/code> for the logical volume \u003Ccode class=\"literal\">shared_lv1\u003C/code> in volume group \u003Ccode class=\"literal\">shared_vg1\u003C/code>. This command also creates the resource group \u003Ccode class=\"literal\">shared_vg1\u003C/code> that includes the resource. In this example, the resource group has the same name as the shared volume group that includes the logical volume.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedlv1 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource named \u003Ccode class=\"literal\">sharedlv2\u003C/code> for the logical volume \u003Ccode class=\"literal\">shared_lv2\u003C/code> in volume group \u003Ccode class=\"literal\">shared_vg1\u003C/code>. This resource will also be part of the resource group \u003Ccode class=\"literal\">shared_vg1\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedlv2 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv2 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource named \u003Ccode class=\"literal\">sharedlv3\u003C/code> for the logical volume \u003Ccode class=\"literal\">shared_lv1\u003C/code> in volume group \u003Ccode class=\"literal\">shared_vg2\u003C/code>. This command also creates the resource group \u003Ccode class=\"literal\">shared_vg2\u003C/code> that includes the resource.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedlv3 --group shared_vg2 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg2 activation_mode=shared vg_access_mode=lvmlockd\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tClone the two new resource groups.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource clone shared_vg1 interleave=true\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource clone shared_vg2 interleave=true\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tConfigure ordering constraints to ensure that the \u003Ccode class=\"literal\">locking\u003C/code> resource group that includes the \u003Ccode class=\"literal\">dlm\u003C/code> and \u003Ccode class=\"literal\">lvmlockd\u003C/code> resources starts first.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order start locking-clone then shared_vg1-clone\u003C/strong>\u003C/span>\nAdding locking-clone shared_vg1-clone (kind: Mandatory) (Options: first-action=start then-action=start)\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order start locking-clone then shared_vg2-clone\u003C/strong>\u003C/span>\nAdding locking-clone shared_vg2-clone (kind: Mandatory) (Options: first-action=start then-action=start)\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tConfigure colocation constraints to ensure that the \u003Ccode class=\"literal\">vg1\u003C/code> and \u003Ccode class=\"literal\">vg2\u003C/code> resource groups start on the same node as the \u003Ccode class=\"literal\">locking\u003C/code> resource group.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint colocation add shared_vg1-clone with locking-clone\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint colocation add shared_vg2-clone with locking-clone\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes in the cluster, verify that the logical volumes are active. There may be a delay of a few seconds.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvs\u003C/strong>\u003C/span>\n  LV         VG          Attr       LSize\n  shared_lv1 shared_vg1  -wi-a----- 5.00g\n  shared_lv2 shared_vg1  -wi-a----- 5.00g\n  shared_lv1 shared_vg2  -wi-a----- 5.00g\n\n[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvs\u003C/strong>\u003C/span>\n  LV         VG          Attr       LSize\n  shared_lv1 shared_vg1  -wi-a----- 5.00g\n  shared_lv2 shared_vg1  -wi-a----- 5.00g\n  shared_lv1 shared_vg2  -wi-a----- 5.00g\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate a file system resource to automatically mount each GFS2 file system on all nodes.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tYou should not add the file system to the \u003Ccode class=\"literal\">/etc/fstab\u003C/code> file because it will be managed as a Pacemaker cluster resource. Mount options can be specified as part of the resource configuration with \u003Ccode class=\"literal\">options=\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>\u003C/code>. Run the \u003Ccode class=\"literal command\">pcs resource describe Filesystem\u003C/code> command to display the full configuration options.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following commands create the file system resources. These commands add each resource to the resource group that includes the logical volume resource for that file system.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device=\"/dev/shared_vg1/shared_lv1\" directory=\"/mnt/gfs1\" fstype=\"gfs2\" options=noatime op monitor interval=10s on-fail=fence\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedfs2 --group shared_vg1 ocf:heartbeat:Filesystem device=\"/dev/shared_vg1/shared_lv2\" directory=\"/mnt/gfs2\" fstype=\"gfs2\" options=noatime op monitor interval=10s on-fail=fence\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedfs3 --group shared_vg2 ocf:heartbeat:Filesystem device=\"/dev/shared_vg2/shared_lv1\" directory=\"/mnt/gfs3\" fstype=\"gfs2\" options=noatime op monitor interval=10s on-fail=fence\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Verification\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tVerify that the GFS2 file systems are mounted on both nodes of the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mount | grep gfs2\u003C/strong>\u003C/span>\n/dev/mapper/shared_vg1-shared_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)\n/dev/mapper/shared_vg1-shared_lv2 on /mnt/gfs2 type gfs2 (rw,noatime,seclabel)\n/dev/mapper/shared_vg2-shared_lv1 on /mnt/gfs3 type gfs2 (rw,noatime,seclabel)\n\n[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mount | grep gfs2\u003C/strong>\u003C/span>\n/dev/mapper/shared_vg1-shared_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)\n/dev/mapper/shared_vg1-shared_lv2 on /mnt/gfs2 type gfs2 (rw,noatime,seclabel)\n/dev/mapper/shared_vg2-shared_lv1 on /mnt/gfs3 type gfs2 (rw,noatime,seclabel)\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCheck the status of the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status --full\u003C/strong>\u003C/span>\nCluster name: my_cluster\n[...]\n\nFull list of resources:\n\n smoke-apc      (stonith:fence_apc):    Started z1.example.com\n Clone Set: locking-clone [locking]\n     Resource Group: locking:0\n         dlm    (ocf::pacemaker:controld):      Started z2.example.com\n         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com\n     Resource Group: locking:1\n         dlm    (ocf::pacemaker:controld):      Started z1.example.com\n         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com\n     Started: [ z1.example.com z2.example.com ]\n Clone Set: shared_vg1-clone [shared_vg1]\n     Resource Group: shared_vg1:0\n         sharedlv1      (ocf::heartbeat:LVM-activate):  Started z2.example.com\n         sharedlv2      (ocf::heartbeat:LVM-activate):  Started z2.example.com\n         sharedfs1      (ocf::heartbeat:Filesystem):    Started z2.example.com\n         sharedfs2      (ocf::heartbeat:Filesystem):    Started z2.example.com\n     Resource Group: shared_vg1:1\n         sharedlv1      (ocf::heartbeat:LVM-activate):  Started z1.example.com\n         sharedlv2      (ocf::heartbeat:LVM-activate):  Started z1.example.com\n         sharedfs1      (ocf::heartbeat:Filesystem):    Started z1.example.com\n         sharedfs2      (ocf::heartbeat:Filesystem):    Started z1.example.com\n     Started: [ z1.example.com z2.example.com ]\n Clone Set: shared_vg2-clone [shared_vg2]\n     Resource Group: shared_vg2:0\n         sharedlv3      (ocf::heartbeat:LVM-activate):  Started z2.example.com\n         sharedfs3      (ocf::heartbeat:Filesystem):    Started z2.example.com\n     Resource Group: shared_vg2:1\n         sharedlv3      (ocf::heartbeat:LVM-activate):  Started z1.example.com\n         sharedfs3      (ocf::heartbeat:Filesystem):    Started z1.example.com\n     Started: [ z1.example.com z2.example.com ]\n\n...\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv class=\"itemizedlist _additional-resources\">\u003Cp class=\"title\">\u003Cstrong>Additional resources\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist _additional-resources\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/index\">Configuring GFS2 file systems\u003C/a>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ca class=\"link\" href=\"https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/deploying_rhel_9_on_microsoft_azure/index#configuring-rhel-high-availability-on-azure_cloud-content-azure\">Configuring a Red Hat High Availability cluster on Microsoft Azure\u003C/a>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ca class=\"link\" href=\"https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/deploying_rhel_9_on_amazon_web_services/index#configuring-a-red-hat-high-availability-cluster-on-aws_deploying-a-virtual-machine-on-aws\">Configuring a Red Hat High Availability cluster on AWS\u003C/a>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ca class=\"link\" href=\"https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/deploying_rhel_9_on_google_cloud_platform/index#configuring-rhel-ha-on-gcp_cloud-content-gcp\">Configuring a Red Hat High Availability Cluster on Google Cloud Platform\u003C/a>\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">8.2. Configuring an encrypted GFS2 file system in a cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can create a Pacemaker cluster that includes a LUKS encrypted GFS2 file system with the following procedure. In this example, you create one GFS2 file systems on a logical volume and encrypt the file system. Encrypted GFS2 file systems are supported using the \u003Ccode class=\"literal\">crypt\u003C/code> resource agent, which provides support for LUKS encryption.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThere are three parts to this procedure:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tConfiguring a shared logical volume in a Pacemaker cluster\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tEncrypting the logical volume and creating a \u003Ccode class=\"literal\">crypt\u003C/code> resource\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tFormatting the encrypted logical volume with a GFS2 file system and creating a file system resource for the cluster\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Csection class=\"section\" id=\"configure_a_shared_logical_volume_in_a_pacemaker_cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">8.2.1. Configure a shared logical volume in a Pacemaker cluster\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Prerequisites\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tInstall and start the cluster software on two cluster nodes and create a basic two-node cluster.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tConfigure fencing for the cluster.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\t\tFor information about creating a Pacemaker cluster and configuring fencing for the cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters\">Creating a Red Hat High-Availability cluster with Pacemaker\u003C/a>.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn both nodes in the cluster, enable the repository for Resilient Storage that corresponds to your system architecture. For example, to enable the Resilient Storage repository for an x86_64 system, you can enter the following \u003Ccode class=\"literal\">subscription-manager\u003C/code> command:\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>subscription-manager repos --enable=rhel-9-for-x86_64-resilientstorage-rpms\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tNote that the Resilient Storage repository is a superset of the High Availability repository. If you enable the Resilient Storage repository you do not also need to enable the High Availability repository.\n\t\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn both nodes of the cluster, install the \u003Ccode class=\"literal\">lvm2-lockd\u003C/code>, \u003Ccode class=\"literal\">gfs2-utils\u003C/code>, and \u003Ccode class=\"literal\">dlm\u003C/code> packages. To support these packages, you must be subscribed to the AppStream channel and the Resilient Storage channel.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install lvm2-lockd gfs2-utils dlm\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn both nodes of the cluster, set the \u003Ccode class=\"literal\">use_lvmlockd\u003C/code> configuration option in the \u003Ccode class=\"literal\">/etc/lvm/lvm.conf\u003C/code> file to \u003Ccode class=\"literal\">use_lvmlockd=1\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">...\nuse_lvmlockd = 1\n...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tSet the global Pacemaker parameter \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> to \u003Ccode class=\"literal\">freeze\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\tBy default, the value of \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> is set to \u003Ccode class=\"literal\">stop\u003C/code>, indicating that when quorum is lost, all the resources on the remaining partition will immediately be stopped. Typically this default is the safest and most optimal option, but unlike most resources, GFS2 requires quorum to function. When quorum is lost both the applications using the GFS2 mounts and the GFS2 mount itself cannot be correctly stopped. Any attempts to stop these resources without quorum will fail which will ultimately result in the entire cluster being fenced every time quorum is lost.\n\t\t\t\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\t\t\t\tTo address this situation, set \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> to \u003Ccode class=\"literal\">freeze\u003C/code> when GFS2 is in use. This means that when quorum is lost, the remaining partition will do nothing until quorum is regained.\n\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set no-quorum-policy=freeze\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tSet up a \u003Ccode class=\"literal\">dlm\u003C/code> resource. This is a required dependency for configuring a GFS2 file system in a cluster. This example creates the \u003Ccode class=\"literal\">dlm\u003C/code> resource as part of a resource group named \u003Ccode class=\"literal\">locking\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tClone the \u003Ccode class=\"literal\">locking\u003C/code> resource group so that the resource group can be active on both nodes of the cluster.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource clone locking interleave=true\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tSet up an \u003Ccode class=\"literal\">lvmlockd\u003C/code> resource as part of the group \u003Ccode class=\"literal\">locking\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCheck the status of the cluster to ensure that the \u003Ccode class=\"literal\">locking\u003C/code> resource group has started on both nodes of the cluster.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status --full\u003C/strong>\u003C/span>\nCluster name: my_cluster\n[...]\n\nOnline: [ z1.example.com (1) z2.example.com (2) ]\n\nFull list of resources:\n\n smoke-apc      (stonith:fence_apc):    Started z1.example.com\n Clone Set: locking-clone [locking]\n     Resource Group: locking:0\n         dlm    (ocf::pacemaker:controld):      Started z1.example.com\n         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com\n     Resource Group: locking:1\n         dlm    (ocf::pacemaker:controld):      Started z2.example.com\n         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com\n     Started: [ z1.example.com z2.example.com ]\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn one node of the cluster, create a shared volume group.\n\t\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\tIf your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order\">Configuring startup order for resource dependencies not managed by Pacemaker\u003C/a>.\n\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tThe following command creates the shared volume group \u003Ccode class=\"literal\">shared_vg1\u003C/code> on \u003Ccode class=\"literal\">/dev/sda1\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgcreate --shared shared_vg1 /dev/sda1\u003C/strong>\u003C/span>\n  Physical volume \"/dev/sda1\" successfully created.\n  Volume group \"shared_vg1\" successfully created\n  VG shared_vg1 starting dlm lockspace\n  Starting locking.  Waiting until locks are ready...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn the second node in the cluster:\n\t\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\t\tIf the use of a devices file is enabled with the \u003Ccode class=\"literal\">use_devicesfile = 1\u003C/code> parameter in the \u003Ccode class=\"literal\">lvm.conf\u003C/code> file, add the shared device to the devices file on the second node in the cluster. This feature is enabled by default.\n\t\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvmdevices --adddev /dev/sda1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\t\tStart the lock manager for the shared volume group.\n\t\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgchange --lockstart shared_vg1\u003C/strong>\u003C/span>\n  VG shared_vg1 starting dlm lockspace\n  Starting locking.  Waiting until locks are ready...\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn one node in the cluster, create the shared logical volume.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvcreate --activate sy -L5G -n shared_lv1 shared_vg1\u003C/strong>\u003C/span>\n  Logical volume \"shared_lv1\" created.\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCreate an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource for the logical volume to automatically activate the logical volume on all nodes.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tThe following command creates an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource named \u003Ccode class=\"literal\">sharedlv1\u003C/code> for the logical volume \u003Ccode class=\"literal\">shared_lv1\u003C/code> in volume group \u003Ccode class=\"literal\">shared_vg1\u003C/code>. This command also creates the resource group \u003Ccode class=\"literal\">shared_vg1\u003C/code> that includes the resource. In this example, the resource group has the same name as the shared volume group that includes the logical volume.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedlv1 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tClone the new resource group.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource clone shared_vg1 interleave=true\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tConfigure an ordering constraints to ensure that the \u003Ccode class=\"literal\">locking\u003C/code> resource group that includes the \u003Ccode class=\"literal\">dlm\u003C/code> and \u003Ccode class=\"literal\">lvmlockd\u003C/code> resources starts first.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order start locking-clone then shared_vg1-clone\u003C/strong>\u003C/span>\nAdding locking-clone shared_vg1-clone (kind: Mandatory) (Options: first-action=start then-action=start)\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tConfigure a colocation constraints to ensure that the \u003Ccode class=\"literal\">vg1\u003C/code> and \u003Ccode class=\"literal\">vg2\u003C/code> resource groups start on the same node as the \u003Ccode class=\"literal\">locking\u003C/code> resource group.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint colocation add shared_vg1-clone with locking-clone\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Verification\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\t\tOn both nodes in the cluster, verify that the logical volume is active. There may be a delay of a few seconds.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvs\u003C/strong>\u003C/span>\n  LV         VG          Attr       LSize\n  shared_lv1 shared_vg1  -wi-a----- 5.00g\n\n[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvs\u003C/strong>\u003C/span>\n  LV         VG          Attr       LSize\n  shared_lv1 shared_vg1  -wi-a----- 5.00g\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"encrypt_the_logical_volume_and_create_a_crypt_resource\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">8.2.2. Encrypt the logical volume and create a crypt resource\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Prerequisites\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tYou have configured a shared logical volume in a Pacemaker cluster.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn one node in the cluster, create a new file that will contain the crypt key and set the permissions on the file so that it is readable only by root.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>touch /etc/crypt_keyfile\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>chmod 600 /etc/crypt_keyfile\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCreate the crypt key.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>dd if=/dev/urandom bs=4K count=1 of=/etc/crypt_keyfile\u003C/strong>\u003C/span>\n1+0 records in\n1+0 records out\n4096 bytes (4.1 kB, 4.0 KiB) copied, 0.000306202 s, 13.4 MB/s\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>scp /etc/crypt_keyfile root@z2.example.com:/etc/\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tDistribute the crypt keyfile to the other nodes in the cluster, using the \u003Ccode class=\"literal\">-p\u003C/code> parameter to preserve the permissions you set.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>scp -p /etc/crypt_keyfile root@z2.example.com:/etc/\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCreate the encrypted device on the LVM volume where you will configure the encrypted GFS2 file system.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>cryptsetup luksFormat /dev/shared_vg1/shared_lv1 --type luks2 --key-file=/etc/crypt_keyfile\u003C/strong>\u003C/span>\nWARNING!\n========\nThis will overwrite data on /dev/shared_vg1/shared_lv1 irrevocably.\n\nAre you sure? (Type 'yes' in capital letters): YES\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCreate the crypt resource as part of the \u003Ccode class=\"literal\">shared_vg1\u003C/code> volume group.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create crypt --group shared_vg1 ocf:heartbeat:crypt crypt_dev=\"luks_lv1\" crypt_type=luks2 key_file=/etc/crypt_keyfile encrypted_dev=\"/dev/shared_vg1/shared_lv1\"\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Verification\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\t\tEnsure that the crypt resource has created the crypt device, which in this example is \u003Ccode class=\"literal\">/dev/mapper/luks_lv1\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>ls -l /dev/mapper/\u003C/strong>\u003C/span>\n...\nlrwxrwxrwx 1 root root 7 Mar 4 09:52 luks_lv1 -&gt; ../dm-3\n...\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"format_the_encrypted_logical_volume_with_a_gfs2_file_system_and_create_a_file_system_resource_for_the_cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">8.2.3. Format the encrypted logical volume with a GFS2 file system and create a file system resource for the cluster\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Prerequisites\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tYou have encrypted the logical volume and created a crypt resource.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn one node in the cluster, format the volume with a GFS2 file system. One journal is required for each node that mounts the file system. Ensure that you create enough journals for each of the nodes in your cluster. The format of the lock table name is \u003Cspan class=\"emphasis\">\u003Cem>ClusterName:FSName\u003C/em>\u003C/span> where \u003Cspan class=\"emphasis\">\u003Cem>ClusterName\u003C/em>\u003C/span> is the name of the cluster for which the GFS2 file system is being created and \u003Cspan class=\"emphasis\">\u003Cem>FSName\u003C/em>\u003C/span> is the file system name, which must be unique for all \u003Ccode class=\"literal\">lock_dlm\u003C/code> file systems over the cluster.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkfs.gfs2 -j3 -p lock_dlm -t my_cluster:gfs2-demo1 /dev/mapper/luks_lv1\u003C/strong>\u003C/span>\n/dev/mapper/luks_lv1 is a symbolic link to /dev/dm-3\nThis will destroy any data on /dev/dm-3\nAre you sure you want to proceed? [y/n] y\nDiscarding device contents (may take a while on large devices): Done\nAdding journals: Done\nBuilding resource groups: Done\nCreating quota file: Done\nWriting superblock and syncing: Done\nDevice:                    /dev/mapper/luks_lv1\nBlock size:                4096\nDevice size:               4.98 GB (1306624 blocks)\nFilesystem size:           4.98 GB (1306622 blocks)\nJournals:                  3\nJournal size:              16MB\nResource groups:           23\nLocking protocol:          \"lock_dlm\"\nLock table:                \"my_cluster:gfs2-demo1\"\nUUID:                      de263f7b-0f12-4d02-bbb2-56642fade293\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCreate a file system resource to automatically mount the GFS2 file system on all nodes.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tDo not add the file system to the \u003Ccode class=\"literal\">/etc/fstab\u003C/code> file because it will be managed as a Pacemaker cluster resource. Mount options can be specified as part of the resource configuration with \u003Ccode class=\"literal\">options=\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>\u003C/code>. Run the \u003Ccode class=\"literal\">pcs resource describe Filesystem\u003C/code> command for full configuration options.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tThe following command creates the file system resource. This command adds the resource to the resource group that includes the logical volume resource for that file system.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device=\"/dev/mapper/luks_lv1\" directory=\"/mnt/gfs1\" fstype=\"gfs2\" options=noatime op monitor interval=10s on-fail=fence\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Verification\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tVerify that the GFS2 file system is mounted on both nodes of the cluster.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mount | grep gfs2\u003C/strong>\u003C/span>\n/dev/mapper/luks_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)\n\n[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mount | grep gfs2\u003C/strong>\u003C/span>\n/dev/mapper/luks_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCheck the status of the cluster.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status --full\u003C/strong>\u003C/span>\nCluster name: my_cluster\n[...]\n\nFull list of resources:\n\n  smoke-apc      (stonith:fence_apc):    Started z1.example.com\n  Clone Set: locking-clone [locking]\n      Resource Group: locking:0\n          dlm    (ocf::pacemaker:controld):      Started z2.example.com\n          lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com\n      Resource Group: locking:1\n          dlm    (ocf::pacemaker:controld):      Started z1.example.com\n          lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com\n     Started: [ z1.example.com z2.example.com ]\n  Clone Set: shared_vg1-clone [shared_vg1]\n     Resource Group: shared_vg1:0\n             sharedlv1      (ocf::heartbeat:LVM-activate):  Started z2.example.com\n             crypt       (ocf::heartbeat:crypt) Started z2.example.com\n             sharedfs1      (ocf::heartbeat:Filesystem):    Started z2.example.com\n    Resource Group: shared_vg1:1\n             sharedlv1      (ocf::heartbeat:LVM-activate):  Started z1.example.com\n             crypt      (ocf::heartbeat:crypt)  Started z1.example.com\n             sharedfs1      (ocf::heartbeat:Filesystem):    Started z1.example.com\n          Started:  [z1.example.com z2.example.com ]\n...\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv class=\"itemizedlist _additional-resources\">\u003Cp class=\"title\">\u003Cstrong>Additional resources\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist _additional-resources\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/index\">Configuring GFS2 file systems\u003C/a>\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"con_gfs2-tracepoints-configuring-gfs2-file-systems\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 9. GFS2 tracepoints and the glock debugfs interface\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tThis documentation on both the GFS2 tracepoints and the glock \u003Ccode class=\"literal\">debugfs\u003C/code> interface is intended for advanced users who are familiar with file system internals and who would like to learn more about the design of GFS2 and how to debug GFS2-specific issues.\n\t\t\u003C/p>\u003Cp>\n\t\t\tThe following sections describe GFS2 tracepoints and the GFS2 \u003Ccode class=\"literal\">glocks\u003C/code> file.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"gfs2_tracepoint_types\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">9.1. GFS2 tracepoint types\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tThere are currently three types of GFS2 tracepoints: \u003Cspan class=\"emphasis\">\u003Cem>glock\u003C/em>\u003C/span> (pronounced \"gee-lock\") tracepoints, \u003Cspan class=\"emphasis\">\u003Cem>bmap\u003C/em>\u003C/span> tracepoints and \u003Cspan class=\"emphasis\">\u003Cem>log\u003C/em>\u003C/span> tracepoints. These can be used to monitor a running GFS2 file system. Tracepoints are particularly useful when a problem, such as a hang or performance issue, is reproducible and thus the tracepoint output can be obtained during the problematic operation. In GFS2, glocks are the primary cache control mechanism and they are the key to understanding the performance of the core of GFS2. The bmap (block map) tracepoints can be used to monitor block allocations and block mapping (lookup of already allocated blocks in the on-disk metadata tree) as they happen and check for any issues relating to locality of access. The log tracepoints keep track of the data being written to and released from the journal and can provide useful information about that part of GFS2.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe tracepoints are designed to be as generic as possible. This should mean that it will not be necessary to change the API during the course of Red Hat Enterprise Linux 9. On the other hand, users of this interface should be aware that this is a debugging interface and not part of the normal Red Hat Enterprise Linux 9 API set, and as such Red Hat makes no guarantees that changes in the GFS2 tracepoints interface will not occur.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tTracepoints are a generic feature of Red Hat Enterprise Linux and their scope goes well beyond GFS2. In particular they are used to implement the \u003Ccode class=\"literal\">blktrace\u003C/code> infrastructure and the \u003Ccode class=\"literal\">blktrace\u003C/code> tracepoints can be used in combination with those of GFS2 to gain a fuller picture of the system performance. Due to the level at which the tracepoints operate, they can produce large volumes of data in a very short period of time. They are designed to put a minimum load on the system when they are enabled, but it is inevitable that they will have some effect. Filtering events by a variety of means can help reduce the volume of data and help focus on obtaining just the information which is useful for understanding any particular situation.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"ap-tracepoints-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">9.2. Tracepoints\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tThe tracepoints can be found under the \u003Ccode class=\"literal\">/sys/kernel/debug/tracing/\u003C/code> directory assuming that \u003Ccode class=\"literal\">debugfs\u003C/code> is mounted in the standard place at the \u003Ccode class=\"literal\">/sys/kernel/debug\u003C/code> directory. The \u003Ccode class=\"literal\">events\u003C/code> subdirectory contains all the tracing events that may be specified and, provided the \u003Ccode class=\"literal\">gfs2\u003C/code> module is loaded, there will be a \u003Ccode class=\"literal\">gfs2\u003C/code> subdirectory containing further subdirectories, one for each GFS2 event. The contents of the \u003Ccode class=\"literal\">/sys/kernel/debug/tracing/events/gfs2\u003C/code> directory should look roughly like the following:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@chywoon gfs2]# \u003Cspan class=\"strong strong\">\u003Cstrong>ls\u003C/strong>\u003C/span>\nenable            gfs2_bmap       gfs2_glock_queue         gfs2_log_flush\nfilter            gfs2_demote_rq  gfs2_glock_state_change  gfs2_pin\ngfs2_block_alloc  gfs2_glock_put  gfs2_log_blocks          gfs2_promote\u003C/pre>\u003Cp>\n\t\t\t\tTo enable all the GFS2 tracepoints, enter the following command:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@chywoon gfs2]# \u003Cspan class=\"strong strong\">\u003Cstrong>echo -n 1 &gt;/sys/kernel/debug/tracing/events/gfs2/enable\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tTo enable a specific tracepoint, there is an \u003Ccode class=\"literal\">enable\u003C/code> file in each of the individual event subdirectories. The same is true of the \u003Ccode class=\"literal\">filter\u003C/code> file which can be used to set an event filter for each event or set of events. The meaning of the individual events is explained in more detail below.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe output from the tracepoints is available in ASCII or binary format. This appendix does not currently cover the binary interface. The ASCII interface is available in two ways. To list the current content of the ring buffer, you can enter the following command:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@chywoon gfs2]# \u003Cspan class=\"strong strong\">\u003Cstrong>cat /sys/kernel/debug/tracing/trace\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThis interface is useful in cases where you are using a long-running process for a certain period of time and, after some event, want to look back at the latest captured information in the buffer. An alternative interface, \u003Ccode class=\"literal\">/sys/kernel/debug/tracing/trace_pipe\u003C/code>, can be used when all the output is required. Events are read from this file as they occur; there is no historical information available through this interface. The format of the output is the same from both interfaces and is described for each of the GFS2 events in the later sections of this appendix.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tA utility called \u003Ccode class=\"literal command\">trace-cmd\u003C/code> is available for reading tracepoint data. For more information about this utility, see \u003Ca class=\"link\" href=\"http://lwn.net/Articles/341902/\"> http://lwn.net/Articles/341902/\u003C/a>. The \u003Ccode class=\"literal command\">trace-cmd\u003C/code> utility can be used in a similar way to the \u003Ccode class=\"literal command\">strace\u003C/code> utility, for example to run a command while gathering trace data from various sources.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"ap-glocks-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">9.3. Glocks\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tTo understand GFS2, the most important concept to understand, and the one which sets it aside from other file systems, is the concept of glocks. In terms of the source code, a glock is a data structure that brings together the DLM and caching into a single state machine. Each glock has a 1:1 relationship with a single DLM lock, and provides caching for that lock state so that repetitive operations carried out from a single node of the file system do not have to repeatedly call the DLM, and thus they help avoid unnecessary network traffic. There are two broad categories of glocks, those which cache metadata and those which do not. The inode glocks and the resource group glocks both cache metadata, other types of glocks do not cache metadata. The inode glock is also involved in the caching of data in addition to metadata and has the most complex logic of all glocks.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-table-glock-dlm-modes\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 9.1. Glock Modes and DLM Lock Modes\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 33%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688928243344\" scope=\"col\">Glock mode\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688928242256\" scope=\"col\">DLM lock mode\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688931515104\" scope=\"col\">Notes\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928243344\"> \u003Cp>\n\t\t\t\t\t\t\t\tUN\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928242256\"> \u003Cp>\n\t\t\t\t\t\t\t\tIV/NL\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931515104\"> \u003Cp>\n\t\t\t\t\t\t\t\tUnlocked (no DLM lock associated with glock or NL lock depending on I flag)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928243344\"> \u003Cp>\n\t\t\t\t\t\t\t\tSH\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928242256\"> \u003Cp>\n\t\t\t\t\t\t\t\tPR\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931515104\"> \u003Cp>\n\t\t\t\t\t\t\t\tShared (protected read) lock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928243344\"> \u003Cp>\n\t\t\t\t\t\t\t\tEX\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928242256\"> \u003Cp>\n\t\t\t\t\t\t\t\tEX\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931515104\"> \u003Cp>\n\t\t\t\t\t\t\t\tExclusive lock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928243344\"> \u003Cp>\n\t\t\t\t\t\t\t\tDF\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928242256\"> \u003Cp>\n\t\t\t\t\t\t\t\tCW\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688931515104\"> \u003Cp>\n\t\t\t\t\t\t\t\tDeferred (concurrent write) used for Direct I/O and file system freeze\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tGlocks remain in memory until either they are unlocked (at the request of another node or at the request of the VM) and there are no local users. At that point they are removed from the glock hash table and freed. When a glock is created, the DLM lock is not associated with the glock immediately. The DLM lock becomes associated with the glock upon the first request to the DLM, and if this request is successful then the 'I' (initial) flag will be set on the glock. The \"Glock Flags\" table in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/con_gfs2-tracepoints-configuring-gfs2-file-systems#ap-glock-debugfs-gfs2\">The glock debugfs interface\u003C/a> shows the meanings of the different glock flags. Once the DLM has been associated with the glock, the DLM lock will always remain at least at NL (Null) lock mode until the glock is to be freed. A demotion of the DLM lock from NL to unlocked is always the last operation in the life of a glock.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tEach glock can have a number of \"holders\" associated with it, each of which represents one lock request from the higher layers. System calls relating to GFS2 queue and dequeue holders from the glock to protect the critical section of code.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe glock state machine is based on a work queue. For performance reasons, tasklets would be preferable; however, in the current implementation we need to submit I/O from that context which prohibits their use.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tWorkqueues have their own tracepoints which can be used in combination with the GFS2 tracepoints.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tThe following table shows what state may be cached under each of the glock modes and whether that cached state may be dirty. This applies to both inode and resource group locks, although there is no data component for the resource group locks, only metadata.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-table-glockmode-data\">\u003Ctable class=\"gt-4-cols lt-7-rows\">\u003Ccaption>Table 9.2. Glock Modes and Data Types\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 20%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 20%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 20%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 20%; \" class=\"col_4\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 20%; \" class=\"col_5\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688929965744\" scope=\"col\">Glock mode\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688929964656\" scope=\"col\">Cache Data\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688929963568\" scope=\"col\">Cache Metadata\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688929962480\" scope=\"col\">Dirty Data\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688929961392\" scope=\"col\">Dirty Metadata\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929965744\"> \u003Cp>\n\t\t\t\t\t\t\t\tUN\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929964656\"> \u003Cp>\n\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929963568\"> \u003Cp>\n\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929962480\"> \u003Cp>\n\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929961392\"> \u003Cp>\n\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929965744\"> \u003Cp>\n\t\t\t\t\t\t\t\tSH\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929964656\"> \u003Cp>\n\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929963568\"> \u003Cp>\n\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929962480\"> \u003Cp>\n\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929961392\"> \u003Cp>\n\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929965744\"> \u003Cp>\n\t\t\t\t\t\t\t\tDF\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929964656\"> \u003Cp>\n\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929963568\"> \u003Cp>\n\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929962480\"> \u003Cp>\n\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929961392\"> \u003Cp>\n\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929965744\"> \u003Cp>\n\t\t\t\t\t\t\t\tEX\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929964656\"> \u003Cp>\n\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929963568\"> \u003Cp>\n\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929962480\"> \u003Cp>\n\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688929961392\"> \u003Cp>\n\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003C/section>\u003Csection class=\"section\" id=\"ap-glock-debugfs-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">9.4. The glock debugfs interface\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tThe glock \u003Ccode class=\"literal\">debugfs\u003C/code> interface allows the visualization of the internal state of the glocks and the holders and it also includes some summary details of the objects being locked in some cases. Each line of the file either begins G: with no indentation (which refers to the glock itself) or it begins with a different letter, indented with a single space, and refers to the structures associated with the glock immediately above it in the file (H: is a holder, I: an inode, and R: a resource group). Here is an example of what the content of this file might look like:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">G:  s:SH n:5/75320 f:I t:SH d:EX/0 a:0 r:3\n H: s:SH f:EH e:0 p:4466 [postmark] gfs2_inode_lookup+0x14e/0x260 [gfs2]\nG:  s:EX n:3/258028 f:yI t:EX d:EX/0 a:3 r:4\n H: s:EX f:tH e:0 p:4466 [postmark] gfs2_inplace_reserve_i+0x177/0x780 [gfs2]\n R: n:258028 f:05 b:22256/22256 i:16800\nG:  s:EX n:2/219916 f:yfI t:EX d:EX/0 a:0 r:3\n I: n:75661/219916 t:8 f:0x10 d:0x00000000 s:7522/7522\nG:  s:SH n:5/127205 f:I t:SH d:EX/0 a:0 r:3\n H: s:SH f:EH e:0 p:4466 [postmark] gfs2_inode_lookup+0x14e/0x260 [gfs2]\nG:  s:EX n:2/50382 f:yfI t:EX d:EX/0 a:0 r:2\nG:  s:SH n:5/302519 f:I t:SH d:EX/0 a:0 r:3\n H: s:SH f:EH e:0 p:4466 [postmark] gfs2_inode_lookup+0x14e/0x260 [gfs2]\nG:  s:SH n:5/313874 f:I t:SH d:EX/0 a:0 r:3\n H: s:SH f:EH e:0 p:4466 [postmark] gfs2_inode_lookup+0x14e/0x260 [gfs2]\nG:  s:SH n:5/271916 f:I t:SH d:EX/0 a:0 r:3\n H: s:SH f:EH e:0 p:4466 [postmark] gfs2_inode_lookup+0x14e/0x260 [gfs2]\nG:  s:SH n:5/312732 f:I t:SH d:EX/0 a:0 r:3\n H: s:SH f:EH e:0 p:4466 [postmark] gfs2_inode_lookup+0x14e/0x260 [gfs2]\u003C/pre>\u003Cp>\n\t\t\t\tThe above example is a series of excerpts (from an approximately 18MB file) generated by the command \u003Ccode class=\"literal command\">cat /sys/kernel/debug/gfs2/unity:myfs/glocks &gt;my.lock\u003C/code> during a run of the postmark benchmark on a single node GFS2 file system. The glocks in the figure have been selected in order to show some of the more interesting features of the glock dumps.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe glock states are either EX (exclusive), DF (deferred), SH (shared) or UN (unlocked). These states correspond directly with DLM lock modes except for UN which may represent either the DLM null lock state, or that GFS2 does not hold a DLM lock (depending on the I flag as explained above). The s: field of the glock indicates the current state of the lock and the same field in the holder indicates the requested mode. If the lock is granted, the holder will have the H bit set in its flags (f: field). Otherwise, it will have the W wait bit set.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe n: field (number) indicates the number associated with each item. For glocks, that is the type number followed by the glock number so that in the above example, the first glock is n:5/75320; which indicates an \u003Ccode class=\"literal\">iopen\u003C/code> glock which relates to inode 75320. In the case of inode and \u003Ccode class=\"literal\">iopen\u003C/code> glocks, the glock number is always identical to the inode’s disk block number.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tThe glock numbers (n: field) in the debugfs glocks file are in hexadecimal, whereas the tracepoints output lists them in decimal. This is for historical reasons; glock numbers were always written in hex, but decimal was chosen for the tracepoints so that the numbers could easily be compared with the other tracepoint output (from \u003Ccode class=\"literal command\">blktrace\u003C/code> for example) and with output from \u003Ccode class=\"literal command\">stat\u003C/code>(1).\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tThe full listing of all the flags for both the holder and the glock are set out in the \"Glock Flags\" table, below, and the \"Glock Holder Flags\" table in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/con_gfs2-tracepoints-configuring-gfs2-file-systems#ap-glock-holders-gfs2\">Glock holders\u003C/a>. The content of lock value blocks is not currently available through the glock \u003Ccode class=\"literal\">debugfs\u003C/code> interface. The following table shows the meanings of the different glock types.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-glock-types-ap\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 9.3. Glock Types\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 33%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688927578800\" scope=\"col\">Type number\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688927577712\" scope=\"col\">Lock type\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688927576624\" scope=\"col\">Use\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927578800\"> \u003Cp>\n\t\t\t\t\t\t\t\t1\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927577712\"> \u003Cp>\n\t\t\t\t\t\t\t\ttrans\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927576624\"> \u003Cp>\n\t\t\t\t\t\t\t\tTransaction lock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927578800\"> \u003Cp>\n\t\t\t\t\t\t\t\t2\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927577712\"> \u003Cp>\n\t\t\t\t\t\t\t\tinode\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927576624\"> \u003Cp>\n\t\t\t\t\t\t\t\tInode metadata and data\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927578800\"> \u003Cp>\n\t\t\t\t\t\t\t\t3\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927577712\"> \u003Cp>\n\t\t\t\t\t\t\t\trgrp\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927576624\"> \u003Cp>\n\t\t\t\t\t\t\t\tResource group metadata\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927578800\"> \u003Cp>\n\t\t\t\t\t\t\t\t4\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927577712\"> \u003Cp>\n\t\t\t\t\t\t\t\tmeta\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927576624\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe superblock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927578800\"> \u003Cp>\n\t\t\t\t\t\t\t\t5\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927577712\"> \u003Cp>\n\t\t\t\t\t\t\t\tiopen\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927576624\"> \u003Cp>\n\t\t\t\t\t\t\t\tInode last closer detection\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927578800\"> \u003Cp>\n\t\t\t\t\t\t\t\t6\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927577712\"> \u003Cp>\n\t\t\t\t\t\t\t\tflock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927576624\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal command\">flock\u003C/code>(2) syscall\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927578800\"> \u003Cp>\n\t\t\t\t\t\t\t\t8\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927577712\"> \u003Cp>\n\t\t\t\t\t\t\t\tquota\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927576624\"> \u003Cp>\n\t\t\t\t\t\t\t\tQuota operations\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927578800\"> \u003Cp>\n\t\t\t\t\t\t\t\t9\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927577712\"> \u003Cp>\n\t\t\t\t\t\t\t\tjournal\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927576624\"> \u003Cp>\n\t\t\t\t\t\t\t\tJournal mutex\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tOne of the more important glock flags is the l (locked) flag. This is the bit lock that is used to arbitrate access to the glock state when a state change is to be performed. It is set when the state machine is about to send a remote lock request through the DLM, and only cleared when the complete operation has been performed. Sometimes this can mean that more than one lock request will have been sent, with various invalidations occurring between times.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following table shows the meanings of the different glock flags.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-glock-flags-ap\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 9.4. Glock Flags\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 33%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688927620736\" scope=\"col\">Flag\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688927619648\" scope=\"col\">Name\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688927618560\" scope=\"col\">Meaning\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927620736\"> \u003Cp>\n\t\t\t\t\t\t\t\td\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927619648\"> \u003Cp>\n\t\t\t\t\t\t\t\tPending demote\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927618560\"> \u003Cp>\n\t\t\t\t\t\t\t\tA deferred (remote) demote request\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927620736\"> \u003Cp>\n\t\t\t\t\t\t\t\tD\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927619648\"> \u003Cp>\n\t\t\t\t\t\t\t\tDemote\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927618560\"> \u003Cp>\n\t\t\t\t\t\t\t\tA demote request (local or remote)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927620736\"> \u003Cp>\n\t\t\t\t\t\t\t\tf\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927619648\"> \u003Cp>\n\t\t\t\t\t\t\t\tLog flush\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927618560\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe log needs to be committed before releasing this glock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927620736\"> \u003Cp>\n\t\t\t\t\t\t\t\tF\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927619648\"> \u003Cp>\n\t\t\t\t\t\t\t\tFrozen\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927618560\"> \u003Cp>\n\t\t\t\t\t\t\t\tReplies from remote nodes ignored - recovery is in progress.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927620736\"> \u003Cp>\n\t\t\t\t\t\t\t\ti\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927619648\"> \u003Cp>\n\t\t\t\t\t\t\t\tInvalidate in progress\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927618560\"> \u003Cp>\n\t\t\t\t\t\t\t\tIn the process of invalidating pages under this glock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927620736\"> \u003Cp>\n\t\t\t\t\t\t\t\tI\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927619648\"> \u003Cp>\n\t\t\t\t\t\t\t\tInitial\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927618560\"> \u003Cp>\n\t\t\t\t\t\t\t\tSet when DLM lock is associated with this glock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927620736\"> \u003Cp>\n\t\t\t\t\t\t\t\tl\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927619648\"> \u003Cp>\n\t\t\t\t\t\t\t\tLocked\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927618560\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe glock is in the process of changing state\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927620736\"> \u003Cp>\n\t\t\t\t\t\t\t\tL\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927619648\"> \u003Cp>\n\t\t\t\t\t\t\t\tLRU\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927618560\"> \u003Cp>\n\t\t\t\t\t\t\t\tSet when the glock is on the LRU list`\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927620736\"> \u003Cp>\n\t\t\t\t\t\t\t\to\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927619648\"> \u003Cp>\n\t\t\t\t\t\t\t\tObject\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927618560\"> \u003Cp>\n\t\t\t\t\t\t\t\tSet when the glock is associated with an object (that is, an inode for type 2 glocks, and a resource group for type 3 glocks)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927620736\"> \u003Cp>\n\t\t\t\t\t\t\t\tp\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927619648\"> \u003Cp>\n\t\t\t\t\t\t\t\tDemote in progress\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927618560\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe glock is in the process of responding to a demote request\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927620736\"> \u003Cp>\n\t\t\t\t\t\t\t\tq\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927619648\"> \u003Cp>\n\t\t\t\t\t\t\t\tQueued\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927618560\"> \u003Cp>\n\t\t\t\t\t\t\t\tSet when a holder is queued to a glock, and cleared when the glock is held, but there are no remaining holders. Used as part of the algorithm the calculates the minimum hold time for a glock.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927620736\"> \u003Cp>\n\t\t\t\t\t\t\t\tr\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927619648\"> \u003Cp>\n\t\t\t\t\t\t\t\tReply pending\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927618560\"> \u003Cp>\n\t\t\t\t\t\t\t\tReply received from remote node is awaiting processing\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927620736\"> \u003Cp>\n\t\t\t\t\t\t\t\ty\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927619648\"> \u003Cp>\n\t\t\t\t\t\t\t\tDirty\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688927618560\"> \u003Cp>\n\t\t\t\t\t\t\t\tData needs flushing to disk before releasing this glock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tWhen a remote callback is received from a node that wants to get a lock in a mode that conflicts with that being held on the local node, then one or other of the two flags D (demote) or d (demote pending) is set. In order to prevent starvation conditions when there is contention on a particular lock, each lock is assigned a minimum hold time. A node which has not yet had the lock for the minimum hold time is allowed to retain that lock until the time interval has expired.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf the time interval has expired, then the D (demote) flag will be set and the state required will be recorded. In that case the next time there are no granted locks on the holders queue, the lock will be demoted. If the time interval has not expired, then the d (demote pending) flag is set instead. This also schedules the state machine to clear d (demote pending) and set D (demote) when the minimum hold time has expired.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe I (initial) flag is set when the glock has been assigned a DLM lock. This happens when the glock is first used and the I flag will then remain set until the glock is finally freed (which the DLM lock is unlocked).\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"ap-glock-holders-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">9.5. Glock holders\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tThe following table shows the meanings of the different glock holder flags.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-glock-holderflags-ap\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 9.5. Glock Holder Flags\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 33%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688928276128\" scope=\"col\">Flag\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688928275040\" scope=\"col\">Name\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688928273952\" scope=\"col\">Meaning\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928276128\"> \u003Cp>\n\t\t\t\t\t\t\t\ta\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928275040\"> \u003Cp>\n\t\t\t\t\t\t\t\tAsync\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928273952\"> \u003Cp>\n\t\t\t\t\t\t\t\tDo not wait for glock result (will poll for result later)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928276128\"> \u003Cp>\n\t\t\t\t\t\t\t\tA\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928275040\"> \u003Cp>\n\t\t\t\t\t\t\t\tAny\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928273952\"> \u003Cp>\n\t\t\t\t\t\t\t\tAny compatible lock mode is acceptable\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928276128\"> \u003Cp>\n\t\t\t\t\t\t\t\tc\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928275040\"> \u003Cp>\n\t\t\t\t\t\t\t\tNo cache\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928273952\"> \u003Cp>\n\t\t\t\t\t\t\t\tWhen unlocked, demote DLM lock immediately\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928276128\"> \u003Cp>\n\t\t\t\t\t\t\t\te\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928275040\"> \u003Cp>\n\t\t\t\t\t\t\t\tNo expire\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928273952\"> \u003Cp>\n\t\t\t\t\t\t\t\tIgnore subsequent lock cancel requests\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928276128\"> \u003Cp>\n\t\t\t\t\t\t\t\tE\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928275040\"> \u003Cp>\n\t\t\t\t\t\t\t\tExact\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928273952\"> \u003Cp>\n\t\t\t\t\t\t\t\tMust have exact lock mode\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928276128\"> \u003Cp>\n\t\t\t\t\t\t\t\tF\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928275040\"> \u003Cp>\n\t\t\t\t\t\t\t\tFirst\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928273952\"> \u003Cp>\n\t\t\t\t\t\t\t\tSet when holder is the first to be granted for this lock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928276128\"> \u003Cp>\n\t\t\t\t\t\t\t\tH\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928275040\"> \u003Cp>\n\t\t\t\t\t\t\t\tHolder\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928273952\"> \u003Cp>\n\t\t\t\t\t\t\t\tIndicates that requested lock is granted\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928276128\"> \u003Cp>\n\t\t\t\t\t\t\t\tp\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928275040\"> \u003Cp>\n\t\t\t\t\t\t\t\tPriority\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928273952\"> \u003Cp>\n\t\t\t\t\t\t\t\tEnqueue holder at the head of the queue\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928276128\"> \u003Cp>\n\t\t\t\t\t\t\t\tt\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928275040\"> \u003Cp>\n\t\t\t\t\t\t\t\tTry\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928273952\"> \u003Cp>\n\t\t\t\t\t\t\t\tA \"try\" lock\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928276128\"> \u003Cp>\n\t\t\t\t\t\t\t\tT\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928275040\"> \u003Cp>\n\t\t\t\t\t\t\t\tTry 1CB\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928273952\"> \u003Cp>\n\t\t\t\t\t\t\t\tA \"try\" lock that sends a callback\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928276128\"> \u003Cp>\n\t\t\t\t\t\t\t\tW\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928275040\"> \u003Cp>\n\t\t\t\t\t\t\t\tWait\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928273952\"> \u003Cp>\n\t\t\t\t\t\t\t\tSet while waiting for request to complete\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tThe most important holder flags are H (holder) and W (wait) as mentioned earlier, since they are set on granted lock requests and queued lock requests respectively. The ordering of the holders in the list is important. If there are any granted holders, they will always be at the head of the queue, followed by any queued holders.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf there are no granted holders, then the first holder in the list will be the one that triggers the next state change. Since demote requests are always onsidered higher priority than requests from the file system, that might not always directly result in a change to the state requested.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe glock subsystem supports two kinds of \"try\" lock. These are useful both because they allow the taking of locks out of the normal order (with suitable back-off and retry) and because they can be used to help avoid resources in use by other nodes. The normal t (try) lock is just what its name indicates; it is a \"try\" lock that does not do anything special. The T (\u003Ccode class=\"literal\">try 1CB\u003C/code>) lock, on the other hand, is identical to the t lock except that the DLM will send a single callback to current incompatible lock holders. One use of the T (\u003Ccode class=\"literal\">try 1CB\u003C/code>) lock is with the \u003Ccode class=\"literal\">iopen\u003C/code> locks, which are used to arbitrate among the nodes when an inode’s \u003Ccode class=\"literal\">i_nlink\u003C/code> count is zero, and determine which of the nodes will be responsible for deallocating the inode. The \u003Ccode class=\"literal\">iopen\u003C/code> glock is normally held in the shared state, but when the \u003Ccode class=\"literal\">i_nlink\u003C/code> count becomes zero and \u003Ccode class=\"literal command\">→evict_inode\u003C/code>() is called, it will request an exclusive lock with T (\u003Ccode class=\"literal\">try 1CB\u003C/code>) set. It will continue to deallocate the inode if the lock is granted. If the lock is not granted it will result in the node(s) which were preventing the grant of the lock marking their glock(s) with the D (demote) flag, which is checked at \u003Ccode class=\"literal command\">→drop_inode\u003C/code>() time in order to ensure that the deallocation is not forgotten.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThis means that inodes that have zero link count but are still open will be deallocated by the node on which the final \u003Ccode class=\"literal command\">close\u003C/code>() occurs. Also, at the same time as the inode’s link count is decremented to zero the inode is marked as being in the special state of having zero link count but still in use in the resource group bitmap. This functions like the ext3 file system3’s orphan list in that it allows any subsequent reader of the bitmap to know that there is potentially space that might be reclaimed, and to attempt to reclaim it.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"ap-glock-tracepoints-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">9.6. Glock tracepoints\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tThe tracepoints are also designed to be able to confirm the correctness of the cache control by combining them with the \u003Ccode class=\"literal command\">blktrace\u003C/code> output and with knowledge of the on-disk layout. It is then possible to check that any given I/O has been issued and completed under the correct lock, and that no races are present.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal\">gfs2_glock_state_change\u003C/code> tracepoint is the most important one to understand. It tracks every state change of the glock from initial creation right through to the final demotion which ends with \u003Ccode class=\"literal\">gfs2_glock_put\u003C/code> and the final NL to unlocked transition. The l (locked) glock flag is always set before a state change occurs and will not be cleared until after it has finished. There are never any granted holders (the H glock holder flag) during a state change. If there are any queued holders, they will always be in the W (waiting) state. When the state change is complete then the holders may be granted which is the final operation before the l glock flag is cleared.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal\">gfs2_demote_rq\u003C/code> tracepoint keeps track of demote requests, both local and remote. Assuming that there is enough memory on the node, the local demote requests will rarely be seen, and most often they will be created by \u003Ccode class=\"literal command\">umount\u003C/code> or by occasional memory reclaim. The number of remote demote requests is a measure of the contention between nodes for a particular inode or resource group.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal\">gfs2_glock_lock_time\u003C/code> tracepoint provides information about the time taken by requests to the DLM. The blocking (\u003Ccode class=\"literal\">b\u003C/code>) flag was introduced into the glock specifically to be used in combination with this tracepoint.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tWhen a holder is granted a lock, \u003Ccode class=\"literal command\">gfs2_promote\u003C/code> is called, this occurs as the final stages of a state change or when a lock is requested which can be granted immediately due to the glock state already caching a lock of a suitable mode. If the holder is the first one to be granted for this glock, then the f (first) flag is set on that holder. This is currently used only by resource groups.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"ap-bmap-tracepoints-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">9.7. Bmap tracepoints\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tBlock mapping is a task central to any file system. GFS2 uses a traditional bitmap-based system with two bits per block. The main purpose of the tracepoints in this subsystem is to allow monitoring of the time taken to allocate and map blocks.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal\">gfs2_bmap\u003C/code> tracepoint is called twice for each bmap operation: once at the start to display the bmap request, and once at the end to display the result. This makes it easy to match the requests and results together and measure the time taken to map blocks in different parts of the file system, different file offsets, or even of different files. It is also possible to see what the average extent sizes being returned are in comparison to those being requested.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal\">gfs2_rs\u003C/code> tracepoint traces block reservations as they are created, used, and destroyed in the block allocator.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tTo keep track of allocated blocks, \u003Ccode class=\"literal\">gfs2_block_alloc\u003C/code> is called not only on allocations, but also on freeing of blocks. Since the allocations are all referenced according to the inode for which the block is intended, this can be used to track which physical blocks belong to which files in a live file system. This is particularly useful when combined with \u003Ccode class=\"literal\">blktrace\u003C/code>, which will show problematic I/O patterns that may then be referred back to the relevant inodes using the mapping gained by means this tracepoint.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tDirect I/O (\u003Ccode class=\"literal\">iomap\u003C/code>) is an alternative cache policy which allows file data transfers to happen directly between disk and the user’s buffer. This has benefits in situations where cache hit rate is expected to be low. Both \u003Ccode class=\"literal\">gfs2_iomap_start\u003C/code> and \u003Ccode class=\"literal\">gfs2_iomap_end\u003C/code> tracepoints trace these operations and can be used to keep track of mapping using Direct I/O, the positions on the file system of the Direct I/O along with the operation type.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"ap-log-gracepoints-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">9.8. Log tracepoints\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tThe tracepoints in this subsystem track blocks being added to and removed from the journal (\u003Ccode class=\"literal\">gfs2_pin\u003C/code>), as well as the time taken to commit the transactions to the log (\u003Ccode class=\"literal\">gfs2_log_flush\u003C/code>). This can be very useful when trying to debug journaling performance issues.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal\">gfs2_log_blocks\u003C/code> tracepoint keeps track of the reserved blocks in the log, which can help show if the log is too small for the workload, for example.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal\">gfs2_ail_flush\u003C/code> tracepoint is similar to the \u003Ccode class=\"literal\">gfs2_log_flush\u003C/code> tracepoint in that it keeps track of the start and end of flushes of the AIL list. The AIL list contains buffers which have been through the log, but have not yet been written back in place and this is periodically flushed in order to release more log space for use by the file system, or when a process requests a \u003Ccode class=\"literal command\">sync\u003C/code> or \u003Ccode class=\"literal command\">fsync\u003C/code>.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"ap-glockstats-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">9.9. Glock statistics\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tGFS2 maintains statistics that can help track what is going on within the file system. This allows you to spot performance issues.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tGFS2 maintains two counters:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">dcount\u003C/code>, which counts the number of DLM operations requested. This shows how much data has gone into the mean/variance calculations.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">qcount\u003C/code>, which counts the number of \u003Ccode class=\"literal\">syscall\u003C/code> level operations requested. Generally \u003Ccode class=\"literal\">qcount\u003C/code> will be equal to or greater than \u003Ccode class=\"literal\">dcount\u003C/code>.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tIn addition, GFS2 maintains three mean/variance pairs. The mean/variance pairs are smoothed exponential estimates and the algorithm used is the one used to calculate round trip times in network code.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe mean and variance pairs maintained in GFS2 are not scaled, but are in units of integer nanoseconds.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tsrtt/srttvar: Smoothed round trip time for non-blocking operations\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tsrttb/srttvarb: Smoothed round trip time for blocking operations\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tirtt/irttvar: Inter-request time (for example, time between DLM requests)\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tA non-blocking request is one which will complete right away, whatever the state of the DLM lock in question. That currently means any requests when (a) the current state of the lock is exclusive (b) the requested state is either null or unlocked or (c) the \"try lock\" flag is set. A blocking request covers all the other lock requests.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tLarger times are better for IRTTs, whereas smaller times are better for the RTTs.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tStatistics are kept in two \u003Ccode class=\"literal\">sysfs\u003C/code> files:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe \u003Ccode class=\"literal\">glstats\u003C/code> file. This file is similar to the \u003Ccode class=\"literal\">glocks\u003C/code> file, except that it contains statistics, with one glock per line. The data is initialized from \"per cpu\" data for that glock type for which the glock is created (aside from counters, which are zeroed). This file may be very large.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe \u003Ccode class=\"literal\">lkstats\u003C/code> file. This contains \"per cpu\" stats for each glock type. It contains one statistic per line, in which each column is a cpu core. There are eight lines per glock type, with types following on from each other.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"ap-references-gfs2\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">9.10. References\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tFor more information about tracepoints and the GFS2 \u003Ccode class=\"literal\">glocks\u003C/code> file, see the following resources:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tFor information about glock internal locking rules, see \u003Ca class=\"link\" href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/filesystems/gfs2-glocks.rst\"> https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/filesystems/gfs2-glocks.rst\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tFor information about event tracing, see \u003Ca class=\"link\" href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/trace/events.rst\"> https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/trace/events.rst\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tFor information about the \u003Ccode class=\"literal command\">trace-cmd\u003C/code> utility, see \u003Ca class=\"link\" href=\"http://lwn.net/Articles/341902/\"> http://lwn.net/Articles/341902/\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_analyzing-gfs2-with-pcp-configuring-gfs2-file-systems\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 10. Monitoring and analyzing GFS2 file systems using Performance Co-Pilot (PCP)\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tPerformance Co-Pilot (PCP) can help with monitoring and analyzing GFS2 file systems. Monitoring of GFS2 file systems in PCP is provided by the GFS2 PMDA module in Red Hat Enterprise Linux which is available through the \u003Ccode class=\"literal\">pcp-pmda-gfs2\u003C/code> package.\n\t\t\u003C/p>\u003Cp>\n\t\t\tThe GFS2 PMDA provides a number of metrics given by the GFS2 statistics provided in the \u003Ccode class=\"literal\">debugfs\u003C/code> subsystem. When installed, the PMDA exposes values given in the \u003Ccode class=\"literal\">glocks\u003C/code>, \u003Ccode class=\"literal\">glstats\u003C/code>, and \u003Ccode class=\"literal\">sbstats\u003C/code> files. These report sets of statistics on each mounted GFS2 filesystem. The PMDA also makes use of the GFS2 kernel tracepoints exposed by the Kernel Function Tracer (\u003Ccode class=\"literal\">ftrace\u003C/code>).\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_installing-gfs2-pdma-analyzing-gfs2-with-pcp\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.1. Installing the GFS2 PMDA\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIn order to operate correctly, The GFS2 PMDA requires that the \u003Ccode class=\"literal\">debugfs\u003C/code> file system is mounted. If the \u003Ccode class=\"literal\">debugfs\u003C/code> file system is not mounted, run the following commands before installing the GFS2 PMDA:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>mkdir /sys/kernel/debug\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>mount -t debugfs none /sys/kernel/debug\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe GFS2 PMDA is not enabled as part of the default installation. In order to make use of GFS2 metric monitoring through PCP you must enable it after installation.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tRun the following commands to install PCP and enable the GFS2 PMDA. Note that the PMDA install script must be run as root.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install pcp pcp-pmda-gfs2\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>cd /var/lib/pcp/pmdas/gfs2\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>./Install\u003C/strong>\u003C/span>\nUpdating the Performance Metrics Name Space (PMNS) ...\nTerminate PMDA if already installed ...\nUpdating the PMCD control file, and notifying PMCD ...\nCheck gfs2 metrics have appeared ... 346 metrics and 255 values\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_examining-number-of-glocks-analyzing-gfs2-with-pcp\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.2. Displaying information about the available performance metrics with the pminfo tool\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe \u003Ccode class=\"literal\">pminfo\u003C/code> tool displays information about the available performance metrics. The following examples show different GFS2 metrics you can display with this tool.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"examining_the_number_of_glock_structures_that_currently_exist_per_file_system\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">10.2.1. Examining the number of glock structures that currently exist per file system\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe GFS2 glock metrics give insights to the number of glock structures currently incore for each mounted GFS2 file system and their locking states. In GFS2, a glock is a data structure that brings together the DLM and caching into a single state machine. Each glock has a 1:1 mapping with a single DLM lock and provides caching for the lock states so that repetitive operations carried out on a single node do not have to repeatedly call the DLM, reducing unnecessary network traffic.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe following \u003Ccode class=\"literal\">pminfo\u003C/code> command displays a list of the number of glocks per mounted GFS2 file system by their lock mode.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pminfo -f gfs2.glocks\u003C/strong>\u003C/span>\n\ngfs2.glocks.total\n    inst [0 or \"afc_cluster:data\"] value 43680\n    inst [1 or \"afc_cluster:bin\"] value 2091\n\ngfs2.glocks.shared\n    inst [0 or \"afc_cluster:data\"] value 25\n    inst [1 or \"afc_cluster:bin\"] value 25\n\ngfs2.glocks.unlocked\n    inst [0 or \"afc_cluster:data\"] value 43652\n    inst [1 or \"afc_cluster:bin\"] value 2063\n\ngfs2.glocks.deferred\n    inst [0 or \"afc_cluster:data\"] value 0\n    inst [1 or \"afc_cluster:bin\"] value 0\n\ngfs2.glocks.exclusive\n    inst [0 or \"afc_cluster:data\"] value 3\n    inst [1 or \"afc_cluster:bin\"] value 3\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"examining_the_number_of_glock_structures_that_exist_per_file_system_by_type\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">10.2.2. Examining the number of glock structures that exist per file system by type\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe GFS2 glstats metrics give counts of each type of glock which exist for each files ystem, a large number of these will normally be of either the inode (inode and metadata) or resource group (resource group metadata) type.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe following \u003Ccode class=\"literal\">pminfo\u003C/code> command displays a list of the number of each type of Glock per mounted GFS2 file system.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pminfo -f gfs2.glstats\u003C/strong>\u003C/span>\n\ngfs2.glstats.total\n    inst [0 or \"afc_cluster:data\"] value 43680\n    inst [1 or \"afc_cluster:bin\"] value 2091\n\ngfs2.glstats.trans\n    inst [0 or \"afc_cluster:data\"] value 3\n    inst [1 or \"afc_cluster:bin\"] value 3\n\ngfs2.glstats.inode\n    inst [0 or \"afc_cluster:data\"] value 17\n    inst [1 or \"afc_cluster:bin\"] value 17\n\ngfs2.glstats.rgrp\n    inst [0 or \"afc_cluster:data\"] value 43642\n    inst [1 or \"afc_cluster:bin\"] value 2053\n\ngfs2.glstats.meta\n    inst [0 or \"afc_cluster:data\"] value 1\n    inst [1 or \"afc_cluster:bin\"] value 1\n\ngfs2.glstats.iopen\n    inst [0 or \"afc_cluster:data\"] value 16\n    inst [1 or \"afc_cluster:bin\"] value 16\n\ngfs2.glstats.flock\n    inst [0 or \"afc_cluster:data\"] value 0\n    inst [1 or \"afc_cluster:bin\"] value 0\n\ngfs2.glstats.quota\n    inst [0 or \"afc_cluster:data\"] value 0\n    inst [1 or \"afc_cluster:bin\"] value 0\n\ngfs2.glstats.journal\n    inst [0 or \"afc_cluster:data\"] value 1\n    inst [1 or \"afc_cluster:bin\"] value 1\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"checking_the_number_of_glock_structures_that_are_in_a_wait_state\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">10.2.3. Checking the number of glock structures that are in a wait state\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe most important holder flags are H (holder: indicates that requested lock is granted) and W (wait: set while waiting for request to complete). These flags are set on granted lock requests and queued lock requests, respectively.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe following \u003Ccode class=\"literal\">pminfo\u003C/code> command displays a list of the number of glocks with the Wait (W) holder flag for each mounted GFS2 file system.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pminfo -f gfs2.holders.flags.wait\u003C/strong>\u003C/span>\n\ngfs2.holders.flags.wait\n    inst [0 or \"afc_cluster:data\"] value 0\n    inst [1 or \"afc_cluster:bin\"] value 0\u003C/pre>\u003Cp>\n\t\t\t\t\tIf you do see a number of waiting requests queued on a resource group lock there may be a number of reasons for this. One is that there are a large number of nodes compared to the number of resource groups in the file system. Another is that the file system may be very nearly full (requiring, on average, longer searches for free blocks). The situation in both cases can be improved by adding more storage and using the \u003Ccode class=\"literal\">gfs2_grow\u003C/code> command to expand the file system.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"checking_file_system_operation_latency_using_the_kernel_tracepoint_based_metrics\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">10.2.4. Checking file system operation latency using the kernel tracepoint based metrics\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe GFS2 PMDA supports collecting of metrics from the GFS2 kernel tracepoints. By default the reading of these metrics is disabled. Activating these metrics turns on the GFS2 kernel tracepoints when the metrics are collected in order to populate the metric values. This could have a small effect on performance throughput when these Kernel Tracepoint metrics are enabled.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tPCP provides the \u003Ccode class=\"literal\">pmstore\u003C/code> tool, which allows you to modify PMDA settings based on metric values. The \u003Ccode class=\"literal\">gfs2.control.*\u003C/code> metrics allow the toggling of GFS2 kernel tracepoints. The following example uses the \u003Ccode class=\"literal\">pmstore\u003C/code> command to enable all of the GFS2 kernel tracepoints.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pmstore gfs2.control.tracepoints.all 1\u003C/strong>\u003C/span>\ngfs2.control.tracepoints.all old value=0 new value=1\u003C/pre>\u003Cp>\n\t\t\t\t\tWhen this command is run, the PMDA switches on all of the GFS2 tracepoints in the \u003Ccode class=\"literal\">debugfs\u003C/code> file system. The \"Complete Metric List\" table in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/assembly_analyzing-gfs2-with-pcp-configuring-gfs2-file-systems#ref_available-gfs2-PCP-metrics-analyzing-gfs2-with-pcp\">Complete listing of available metrics for GFS2 in PCP\u003C/a> explains each of the control tracepoints and their usage, An explanation on the effect of each control tracepoint and its available options is also available through the help switch in \u003Ccode class=\"literal\">pminfo\u003C/code>.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe GFS2 promote metrics count the number of promote requests on the file system. These requests are separated by the number of requests that have occurred on the first attempt and “others” which are granted after their initial promote request. A drop in the number of first time promotes with a rise in “other” promotes can indicate issues with file contention.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe GFS2 demote request metrics, like the promote request metrics, count the number of demote requests which occur on the file system. These, however, are also split between requests that have come from the current node and requests that have come from other nodes on the system. A large number of demote requests from remote nodes can indicate contention between two nodes for a given resource group.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe \u003Ccode class=\"literal\">pminfo\u003C/code> tool displays information about the available performance metrics. This procedure displays a list of the number of glocks with the Wait (W) holder flag for each mounted GFS2 file system. The following \u003Ccode class=\"literal\">pminfo\u003C/code> command displays a list of the number of glocks with the Wait (W) holder flag for each mounted GFS2 file system.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pminfo -f gfs2.latency.grant.all gfs2.latency.demote.all\u003C/strong>\u003C/span>\n\ngfs2.latency.grant.all\n    inst [0 or \"afc_cluster:data\"] value 0\n    inst [1 or \"afc_cluster:bin\"] value 0\n\ngfs2.latency.demote.all\n    inst [0 or \"afc_cluster:data\"] value 0\n    inst [1 or \"afc_cluster:bin\"] value 0\u003C/pre>\u003Cp>\n\t\t\t\t\tIt is a good idea to determine the general values observed when the workload is running without issues to be able to notice changes in performance when these values differ from their normal range.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tFor example, you might notice a change in the number of promote requests waiting to complete rather than completing on first attempt, which the output from following command would allow you to determine.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pminfo -f gfs2.latency.grant.all gfs2.latency.demote.all\u003C/strong>\u003C/span>\n\ngfs2.tracepoints.promote.other.null_lock\n     inst [0 or \"afc_cluster:data\"] value 0\n     inst [1 or \"afc_cluster:bin\"] value 0\n\ngfs2.tracepoints.promote.other.concurrent_read\n     inst [0 or \"afc_cluster:data\"] value 0\n     inst [1 or \"afc_cluster:bin\"] value 0\n\ngfs2.tracepoints.promote.other.concurrent_write\n     inst [0 or \"afc_cluster:data\"] value 0\n     inst [1 or \"afc_cluster:bin\"] value 0\n\ngfs2.tracepoints.promote.other.protected_read\n     inst [0 or \"afc_cluster:data\"] value 0\n     inst [1 or \"afc_cluster:bin\"] value 0\n\ngfs2.tracepoints.promote.other.protected_write\n     inst [0 or \"afc_cluster:data\"] value 0\n     inst [1 or \"afc_cluster:bin\"] value 0\n\ngfs2.tracepoints.promote.other.exclusive\n     inst [0 or \"afc_cluster:data\"] value 0\n     inst [1 or \"afc_cluster:bin\"] value 0\u003C/pre>\u003Cp>\n\t\t\t\t\tThe output from following command would allow you to determine a large increase in remote demote requests (especially if from other cluster nodes).\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pminfo -f gfs2.tracepoints.demote_rq.requested\u003C/strong>\u003C/span>\n\ngfs2.tracepoints.demote_rq.requested.remote\n     inst [0 or \"afc_cluster:data\"] value 0\n     inst [1 or \"afc_cluster:bin\"] value 0\n\ngfs2.tracepoints.demote_rq.requested.local\n     inst [0 or \"afc_cluster:data\"] value 0\n     inst [1 or \"afc_cluster:bin\"] value 0\u003C/pre>\u003Cp>\n\t\t\t\t\tThe output from the following command could indicate an unexplained increase in log flushes.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pminfo -f gfs2.tracepoints.log_flush.total\u003C/strong>\u003C/span>]\n\ngfs2.tracepoints.log_flush.total\n     inst [0 or \"afc_cluster:data\"] value 0\n     inst [1 or \"afc_cluster:bin\"] value 0\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"ref_available-gfs2-PCP-metrics-analyzing-gfs2-with-pcp\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.3. Complete listing of available metrics for GFS2 in PCP\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe following table describes the full list of performance metrics given by the \u003Ccode class=\"literal\">pcp-pmda-gfs2\u003C/code> package for GFS2 file systems.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-pcpgfs2metricgroups\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 10.1. Complete Metric List\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 29%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 71%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688928044944\" scope=\"col\">Metric Name\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm139688928043856\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928044944\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">gfs2.glocks.*\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928043856\"> \u003Cp>\n\t\t\t\t\t\t\t\tMetrics regarding the information collected from the glock stats file (\u003Ccode class=\"literal\">glocks\u003C/code>) which count the number of glocks in each state that currently exists for each GFS2 file system currently mounted on the system.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928044944\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">gfs2.glocks.flags.*\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928043856\"> \u003Cp>\n\t\t\t\t\t\t\t\tRange of metrics counting the number of glocks that exist with the given glocks flags\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928044944\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">gfs2.holders.*\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928043856\"> \u003Cp>\n\t\t\t\t\t\t\t\tMetrics regarding the information collected from the glock stats file (\u003Ccode class=\"literal\">glocks\u003C/code>) which counts the number of glocks with holders in each lock state that currently exists for each GFS2 file system currently mounted on the system.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928044944\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">gfs2.holders.flags.*\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928043856\"> \u003Cp>\n\t\t\t\t\t\t\t\tRange of metrics counting the number of glocks holders with the given holder flags\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928044944\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">gfs2.sbstats.*\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928043856\"> \u003Cp>\n\t\t\t\t\t\t\t\tTiming metrics regarding the information collected from the superblock stats file (\u003Ccode class=\"literal\">sbstats\u003C/code>) for each GFS2 file system currently mounted on the system.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928044944\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">gfs2.glstats.*\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928043856\"> \u003Cp>\n\t\t\t\t\t\t\t\tMetrics regarding the information collected from the glock stats file (\u003Ccode class=\"literal\">glstats\u003C/code>) which count the number of each type of glock that currently exists for each GFS2 file system currently mounted on the system.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928044944\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">gfs2.latency.grant.*\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928043856\"> \u003Cp>\n\t\t\t\t\t\t\t\tA derived metric making use of the data from both the \u003Ccode class=\"literal\">gfs2_glock_queue\u003C/code> and \u003Ccode class=\"literal\">gfs2_glock_state_change\u003C/code> tracepoints to calculate an average latency in microseconds for glock grant requests to be completed for each mounted file system. This metric is useful for discovering potential slowdowns on the file system when the grant latency increases.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928044944\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">gfs2.latency.demote.*\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928043856\"> \u003Cp>\n\t\t\t\t\t\t\t\tA derived metric making use of the data from both the \u003Ccode class=\"literal\">gfs2_glock_state_change\u003C/code> and \u003Ccode class=\"literal\">gfs2_demote_rq\u003C/code> tracepoints to calculate an average latency in microseconds for glock demote requests to be completed for each mounted file system. This metric is useful for discovering potential slowdowns on the file system when the demote latency increases.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928044944\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">gfs2.latency.queue.*\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928043856\"> \u003Cp>\n\t\t\t\t\t\t\t\tA derived metric making use of the data from the \u003Ccode class=\"literal\">gfs2_glock_queue\u003C/code> tracepoint to calculate an average latency in microseconds for glock queue requests to be completed for each mounted file system.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928044944\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">gfs2.worst_glock.*\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928043856\"> \u003Cp>\n\t\t\t\t\t\t\t\tA derived metric making use of the data from the \u003Ccode class=\"literal\">gfs2_glock_lock_time\u003C/code> tracepoint to calculate a perceived “current worst glock” for each mounted file system. This metric is useful for discovering potential lock contention and file system slowdown if the same lock is suggested multiple times.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928044944\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">gfs2.tracepoints.*\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928043856\"> \u003Cp>\n\t\t\t\t\t\t\t\tMetrics regarding the output from the GFS2 \u003Ccode class=\"literal\">debugfs\u003C/code> tracepoints for each file system currently mounted on the system. Each sub-type of these metrics (one of each GFS2 tracepoint) can be individually controlled whether on or off using the control metrics.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928044944\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">gfs2.control.*\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm139688928043856\"> \u003Cp>\n\t\t\t\t\t\t\t\tConfiguration metrics which are used to switch on or off metric recording in the PMDA. Conrol metricsare toggled by means of the \u003Ccode class=\"literal command\">pmstore\u003C/code> tool.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003C/section>\u003Csection class=\"section\" id=\"proc_installing-minimal-PCP-setup-analyzing-gfs2-with-pcp\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.4. Performing minimal PCP setup to gather file system data\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThis procedure outlines instructions on how to install a minimal PCP setup to collect statistics on Red Hat Enterprise Linux. This setup involves adding the minimum number of packages on a production system needed to gather data for further analysis.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe resulting \u003Ccode class=\"literal\">tar.gz\u003C/code> archive of the \u003Ccode class=\"literal\">pmlogger\u003C/code> output can be analyzed by using further PCP tools and can be compared with other sources of performance information.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tInstall the required PCP packages.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install pcp pcp-pmda-gfs2\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tActivate the GFS2 module for PCP.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>cd /var/lib/pcp/pmdas/gfs2\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>./Install\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tStart both the \u003Ccode class=\"literal\">pmcd\u003C/code> and \u003Ccode class=\"literal\">pmlogger\u003C/code> services.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl start pmcd.service\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl start pmlogger.service\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tPerform operations on the GFS2 file system.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tStop both the \u003Ccode class=\"literal\">pmcd\u003C/code> and \u003Ccode class=\"literal\">pmlogger\u003C/code> services.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl stop pmcd.service\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl stop pmlogger.service\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCollect the output and save it to a \u003Ccode class=\"literal\">tar.gz\u003C/code> file named based on the host name and the current date and time.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>cd /var/log/pcp/pmlogger\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>tar -czf $(hostname).$(date+%F-%Hh%M).pcp.tar.gz $(hostname)\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section _additional-resources\" id=\"additional_resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.5. Additional resources\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/con_gfs2-tracepoints-configuring-gfs2-file-systems\">GFS2 tracepoints and the glock debugfs interface\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/monitoring_and_managing_system_status_and_performance/monitoring-performance-with-performance-co-pilot_monitoring-and-managing-system-status-and-performance\">Monitoring performance with Performance Co-Pilot\u003C/a>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ca class=\"link\" href=\"https://access.redhat.com/articles/1145953\">Index of Performance Co-Pilot(PCP) articles, solutions, tutorials and white papers\u003C/a>\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003C/section>\u003Cdiv>\u003Cdiv xml:lang=\"en-US\" class=\"legalnotice\" id=\"idm139688929828816\">\u003Ch2 class=\"legalnotice\">Legal Notice\u003C/h2>\u003Cdiv class=\"para\">\n\t\tCopyright \u003Cspan class=\"trademark\">\u003C!--Empty-->\u003C/span>© 2024 Red Hat, Inc.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\tThe text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license (\"CC-BY-SA\"). An explanation of CC-BY-SA is available at \u003Ca class=\"uri\" href=\"http://creativecommons.org/licenses/by-sa/3.0/\">http://creativecommons.org/licenses/by-sa/3.0/\u003C/a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\tRed Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\tRed Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\t\u003Cspan class=\"trademark\">Linux\u003C/span>® is the registered trademark of Linus Torvalds in the United States and other countries.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\t\u003Cspan class=\"trademark\">Java\u003C/span>® is a registered trademark of Oracle and/or its affiliates.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\t\u003Cspan class=\"trademark\">XFS\u003C/span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\t\u003Cspan class=\"trademark\">MySQL\u003C/span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\t\u003Cspan class=\"trademark\">Node.js\u003C/span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\tThe \u003Cspan class=\"trademark\">OpenStack\u003C/span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\tAll other trademarks are the property of their respective owners.\n\t\u003C/div>\u003C/div>\u003C/div>\u003C/div>\u003C/body>",[14,21,26,72,119,170,208,221,251,281,308,354,397],{"title":11,"visible":15,"weight":16,"urlFragment":17,"anchor":18,"singlePageAnchor":18,"docTitle":19,"url":20},true,1,"index",null,"configuring_gfs2_file_systems","#",{"title":22,"visible":15,"weight":23,"urlFragment":24,"anchor":18,"singlePageAnchor":24,"docTitle":19,"url":25},"Providing feedback on Red Hat documentation",2,"proc_providing-feedback-on-red-hat-documentation_configuring-gfs2-file-systems","#proc_providing-feedback-on-red-hat-documentation_configuring-gfs2-file-systems",{"title":27,"visible":15,"weight":28,"urlFragment":29,"anchor":18,"singlePageAnchor":29,"sections":30,"docTitle":19,"url":71},"1. Planning a GFS2 file system deployment",3,"assembly_planning-gfs2-deployment-configuring-gfs2-file-systems",[31,35,39,56,61,66],{"title":32,"visible":15,"weight":16,"urlFragment":29,"anchor":33,"singlePageAnchor":33,"docTitle":19,"url":34},"1.1. GFS2 file system format version 1802","con_gfs2-filesystem-format-planning-gfs2-deployment","#con_gfs2-filesystem-format-planning-gfs2-deployment",{"title":36,"visible":15,"weight":23,"urlFragment":29,"anchor":37,"singlePageAnchor":37,"docTitle":19,"url":38},"1.2. Key GFS2 parameters to determine","con_basic-gfs2-parameters-planning-gfs2-deployment","#con_basic-gfs2-parameters-planning-gfs2-deployment",{"title":40,"visible":15,"weight":28,"urlFragment":29,"anchor":41,"singlePageAnchor":41,"sections":42,"docTitle":19,"url":55},"1.3. GFS2 support considerations","con_gfs2-support-limits-planning-gfs2-deployment",[43,47,51],{"title":44,"visible":15,"weight":16,"urlFragment":29,"anchor":45,"singlePageAnchor":45,"docTitle":19,"url":46},"1.3.1. Maximum file system and cluster size","maximum_file_system_and_cluster_size","#maximum_file_system_and_cluster_size",{"title":48,"visible":15,"weight":23,"urlFragment":29,"anchor":49,"singlePageAnchor":49,"docTitle":19,"url":50},"1.3.2. Minimum cluster size","minimum_cluster_size","#minimum_cluster_size",{"title":52,"visible":15,"weight":28,"urlFragment":29,"anchor":53,"singlePageAnchor":53,"docTitle":19,"url":54},"1.3.3. Shared storage considerations","shared_storage_considerations","#shared_storage_considerations","#con_gfs2-support-limits-planning-gfs2-deployment",{"title":57,"visible":15,"weight":58,"urlFragment":29,"anchor":59,"singlePageAnchor":59,"docTitle":19,"url":60},"1.4. GFS2 formatting considerations",4,"con_gfs2-formattiing-considerations-planning-gfs2-deployment","#con_gfs2-formattiing-considerations-planning-gfs2-deployment",{"title":62,"visible":15,"weight":63,"urlFragment":29,"anchor":64,"singlePageAnchor":64,"docTitle":19,"url":65},"1.5. Considerations for GFS2 in a cluster",5,"con_gfs2-cluster-considerations-planning-gfs2-deployment","#con_gfs2-cluster-considerations-planning-gfs2-deployment",{"title":67,"visible":15,"weight":68,"urlFragment":29,"anchor":69,"singlePageAnchor":69,"docTitle":19,"url":70},"1.6. Hardware considerations",6,"con_basic-gfs2-hardware-considerations-planning-gfs2-deployment","#con_basic-gfs2-hardware-considerations-planning-gfs2-deployment","#assembly_planning-gfs2-deployment-configuring-gfs2-file-systems",{"title":73,"visible":15,"weight":58,"urlFragment":74,"anchor":18,"singlePageAnchor":74,"sections":75,"docTitle":19,"url":118},"2. Recommendations for GFS2 usage","assembly_gfs2-usage-considerations-configuring-gfs2-file-systems",[76,80,84,88,92,96,100],{"title":77,"visible":15,"weight":16,"urlFragment":74,"anchor":78,"singlePageAnchor":78,"docTitle":19,"url":79},"2.1. Configuring atime updates","proc_configuring-atime-gfs2-usage-considerations","#proc_configuring-atime-gfs2-usage-considerations",{"title":81,"visible":15,"weight":23,"urlFragment":74,"anchor":82,"singlePageAnchor":82,"docTitle":19,"url":83},"2.2. VFS tuning options: research and experiment","con_vfs-tuning-options-gfs2-usage-considerations","#con_vfs-tuning-options-gfs2-usage-considerations",{"title":85,"visible":15,"weight":28,"urlFragment":74,"anchor":86,"singlePageAnchor":86,"docTitle":19,"url":87},"2.3. SELinux on GFS2","con_selinux-on-gfs2-gfs2-usage-considerations","#con_selinux-on-gfs2-gfs2-usage-considerations",{"title":89,"visible":15,"weight":58,"urlFragment":74,"anchor":90,"singlePageAnchor":90,"docTitle":19,"url":91},"2.4. Setting up NFS over GFS2","con_nfs-over-gfs2-gfs2-usage-considerations","#con_nfs-over-gfs2-gfs2-usage-considerations",{"title":93,"visible":15,"weight":63,"urlFragment":74,"anchor":94,"singlePageAnchor":94,"docTitle":19,"url":95},"2.5. Samba (SMB or Windows) file serving over GFS2","con_samba-over-gfs2-gfs2-usage-considerations","#con_samba-over-gfs2-gfs2-usage-considerations",{"title":97,"visible":15,"weight":68,"urlFragment":74,"anchor":98,"singlePageAnchor":98,"docTitle":19,"url":99},"2.6. Configuring virtual machines for GFS2","con_vms-for-gfs2-gfs2-usage-considerations","#con_vms-for-gfs2-gfs2-usage-considerations",{"title":101,"visible":15,"weight":102,"urlFragment":74,"anchor":103,"singlePageAnchor":103,"sections":104,"docTitle":19,"url":117},"2.7. Block allocation",7,"con_gfs2-block-allocation-issues-gfs2-usage-considerations",[105,109,113],{"title":106,"visible":15,"weight":16,"urlFragment":74,"anchor":107,"singlePageAnchor":107,"docTitle":19,"url":108},"2.7.1. Leave free space in the file system","leave_free_space_in_the_file_system","#leave_free_space_in_the_file_system",{"title":110,"visible":15,"weight":23,"urlFragment":74,"anchor":111,"singlePageAnchor":111,"docTitle":19,"url":112},"2.7.2. Have each node allocate its own files, if possible","have_each_node_allocate_its_own_files_if_possible","#have_each_node_allocate_its_own_files_if_possible",{"title":114,"visible":15,"weight":28,"urlFragment":74,"anchor":115,"singlePageAnchor":115,"docTitle":19,"url":116},"2.7.3. Preallocate, if possible","preallocate_if_possible","#preallocate_if_possible","#con_gfs2-block-allocation-issues-gfs2-usage-considerations","#assembly_gfs2-usage-considerations-configuring-gfs2-file-systems",{"title":120,"visible":15,"weight":63,"urlFragment":121,"anchor":18,"singlePageAnchor":121,"sections":122,"docTitle":19,"url":169},"3. Administering GFS2 file systems","assembly_creating-mounting-gfs2-configuring-gfs2-file-systems",[123,136,153,157,161,165],{"title":124,"visible":15,"weight":16,"urlFragment":121,"anchor":125,"singlePageAnchor":125,"sections":126,"docTitle":19,"url":135},"3.1. GFS2 file system creation","proc_creating-gfs2-creating-mounting-gfs2",[127,131],{"title":128,"visible":15,"weight":16,"urlFragment":121,"anchor":129,"singlePageAnchor":129,"docTitle":19,"url":130},"3.1.1. The GFS2 mkfs command","the_gfs2_mkfs_command","#the_gfs2_mkfs_command",{"title":132,"visible":15,"weight":23,"urlFragment":121,"anchor":133,"singlePageAnchor":133,"docTitle":19,"url":134},"3.1.2. Creating a GFS2 file system","creating_a_gfs2_file_system","#creating_a_gfs2_file_system","#proc_creating-gfs2-creating-mounting-gfs2",{"title":137,"visible":15,"weight":23,"urlFragment":121,"anchor":138,"singlePageAnchor":138,"sections":139,"docTitle":19,"url":152},"3.2. Mounting a GFS2 file system","proc_mounting-gfs2-filesystem_creating-mounting-gfs2",[140,144,148],{"title":141,"visible":15,"weight":16,"urlFragment":121,"anchor":142,"singlePageAnchor":142,"docTitle":19,"url":143},"3.2.1. Mounting a GFS2 file system with no options specified","mounting_a_gfs2_file_system_with_no_options_specified","#mounting_a_gfs2_file_system_with_no_options_specified",{"title":145,"visible":15,"weight":23,"urlFragment":121,"anchor":146,"singlePageAnchor":146,"docTitle":19,"url":147},"3.2.2. Mounting a GFS2 file system that specifies mount options","mounting_a_gfs2_file_system_that_specifies_mount_options","#mounting_a_gfs2_file_system_that_specifies_mount_options",{"title":149,"visible":15,"weight":28,"urlFragment":121,"anchor":150,"singlePageAnchor":150,"docTitle":19,"url":151},"3.2.3. Unmounting a GFS2 file system","unmounting_a_gfs2_file_system","#unmounting_a_gfs2_file_system","#proc_mounting-gfs2-filesystem_creating-mounting-gfs2",{"title":154,"visible":15,"weight":28,"urlFragment":121,"anchor":155,"singlePageAnchor":155,"docTitle":19,"url":156},"3.3. Backing up a GFS2 file system","proc_backing-up-a-gfs2-filesystem-creating-mounting-gfs2","#proc_backing-up-a-gfs2-filesystem-creating-mounting-gfs2",{"title":158,"visible":15,"weight":58,"urlFragment":121,"anchor":159,"singlePageAnchor":159,"docTitle":19,"url":160},"3.4. Suspending activity on a GFS2 file system","proc_suspending-activity-on-a-gfs2-filesystem-creating-mounting-gfs2","#proc_suspending-activity-on-a-gfs2-filesystem-creating-mounting-gfs2",{"title":162,"visible":15,"weight":63,"urlFragment":121,"anchor":163,"singlePageAnchor":163,"docTitle":19,"url":164},"3.5. Growing a GFS2 file system","proc_growing-gfs2-filesystem-creating-mounting-gfs2","#proc_growing-gfs2-filesystem-creating-mounting-gfs2",{"title":166,"visible":15,"weight":68,"urlFragment":121,"anchor":167,"singlePageAnchor":167,"docTitle":19,"url":168},"3.6. Adding journals to a GFS2 file system","proc_adding-gfs2-journal-creating-mounting-gfs2","#proc_adding-gfs2-journal-creating-mounting-gfs2","#assembly_creating-mounting-gfs2-configuring-gfs2-file-systems",{"title":171,"visible":15,"weight":68,"urlFragment":172,"anchor":18,"singlePageAnchor":172,"sections":173,"docTitle":19,"url":207},"4. GFS2 quota management","assembly_gfs2-disk-quota-administration-configuring-gfs2-file-systems",[174,195,199,203],{"title":175,"visible":15,"weight":16,"urlFragment":172,"anchor":176,"singlePageAnchor":176,"sections":177,"docTitle":19,"url":194},"4.1. Configuring GFS2 disk quotas","proc_configuring-gfs2-disk-quotas-gfs2-disk-quota-administration",[178,182,186,190],{"title":179,"visible":15,"weight":16,"urlFragment":172,"anchor":180,"singlePageAnchor":180,"docTitle":19,"url":181},"4.1.1. Setting up quotas in enforcement or accounting mode","setting_up_quotas_in_enforcement_or_accounting_mode","#setting_up_quotas_in_enforcement_or_accounting_mode",{"title":183,"visible":15,"weight":23,"urlFragment":172,"anchor":184,"singlePageAnchor":184,"docTitle":19,"url":185},"4.1.2. Creating the quota database files","creating_the_quota_database_files","#creating_the_quota_database_files",{"title":187,"visible":15,"weight":28,"urlFragment":172,"anchor":188,"singlePageAnchor":188,"docTitle":19,"url":189},"4.1.3. Assigning quotas per user","assigning_quotas_per_user","#assigning_quotas_per_user",{"title":191,"visible":15,"weight":58,"urlFragment":172,"anchor":192,"singlePageAnchor":192,"docTitle":19,"url":193},"4.1.4. Assigning quotas per group","assigning_quotas_per_group","#assigning_quotas_per_group","#proc_configuring-gfs2-disk-quotas-gfs2-disk-quota-administration",{"title":196,"visible":15,"weight":23,"urlFragment":172,"anchor":197,"singlePageAnchor":197,"docTitle":19,"url":198},"4.2. Managing GFS2 disk Quotas","proc_managing-gfs2-disk-quotas-gfs2-disk-quota-administration","#proc_managing-gfs2-disk-quotas-gfs2-disk-quota-administration",{"title":200,"visible":15,"weight":28,"urlFragment":172,"anchor":201,"singlePageAnchor":201,"docTitle":19,"url":202},"4.3. Keeping GFS2 disk quotas accurate with the quotacheck command","proc_keeping-gfs2-quotas-accurate-with-quotacheck-gfs2-disk-quota-administration","#proc_keeping-gfs2-quotas-accurate-with-quotacheck-gfs2-disk-quota-administration",{"title":204,"visible":15,"weight":58,"urlFragment":172,"anchor":205,"singlePageAnchor":205,"docTitle":19,"url":206},"4.4. Synchronizing quotas with the quotasync Command","proc_synchronizing-gfs2-quotas-gfs2-disk-quota-administration","#proc_synchronizing-gfs2-quotas-gfs2-disk-quota-administration","#assembly_gfs2-disk-quota-administration-configuring-gfs2-file-systems",{"title":209,"visible":15,"weight":102,"urlFragment":210,"anchor":18,"singlePageAnchor":210,"sections":211,"docTitle":19,"url":220},"5. GFS2 file system repair","assembly_gfs2-filesystem-repair-configuring-gfs2-file-systems",[212,216],{"title":213,"visible":15,"weight":16,"urlFragment":210,"anchor":214,"singlePageAnchor":214,"docTitle":19,"url":215},"5.1. Determining required memory for running fsck.gfs2","proc_determining-needed-memory-for-fsckgfs2-gfs2-filesystem-repair","#proc_determining-needed-memory-for-fsckgfs2-gfs2-filesystem-repair",{"title":217,"visible":15,"weight":23,"urlFragment":210,"anchor":218,"singlePageAnchor":218,"docTitle":19,"url":219},"5.2. Repairing a gfs2 filesystem","proc_repairing-a-gfs2-filesystem-gfs2-filesystem-repair","#proc_repairing-a-gfs2-filesystem-gfs2-filesystem-repair","#assembly_gfs2-filesystem-repair-configuring-gfs2-file-systems",{"title":222,"visible":15,"weight":223,"urlFragment":224,"anchor":18,"singlePageAnchor":224,"sections":225,"docTitle":19,"url":250},"6. Improving GFS2 performance",8,"assembly_gfs2-performance-configuring-gfs2-file-systems",[226,230,234,238,242,246],{"title":227,"visible":15,"weight":16,"urlFragment":224,"anchor":228,"singlePageAnchor":228,"docTitle":19,"url":229},"6.1. GFS2 file system defragmentation","proc_gfs2-defragment-gfs2-performance","#proc_gfs2-defragment-gfs2-performance",{"title":231,"visible":15,"weight":23,"urlFragment":224,"anchor":232,"singlePageAnchor":232,"docTitle":19,"url":233},"6.2. GFS2 node locking","con_gfs2-node-locking-gfs2-performance","#con_gfs2-node-locking-gfs2-performance",{"title":235,"visible":15,"weight":28,"urlFragment":224,"anchor":236,"singlePageAnchor":236,"docTitle":19,"url":237},"6.3. Issues with Posix locking","con_posix-locking-issues-gfs2-performance","#con_posix-locking-issues-gfs2-performance",{"title":239,"visible":15,"weight":58,"urlFragment":224,"anchor":240,"singlePageAnchor":240,"docTitle":19,"url":241},"6.4. Performance tuning with GFS2","con_troubleshooting-gfs2-performance-gfs2-performance","#con_troubleshooting-gfs2-performance-gfs2-performance",{"title":243,"visible":15,"weight":63,"urlFragment":224,"anchor":244,"singlePageAnchor":244,"docTitle":19,"url":245},"6.5. Troubleshooting GFS2 performance with the GFS2 lock dump","con_gfs2-lockdump-gfs2-performance","#con_gfs2-lockdump-gfs2-performance",{"title":247,"visible":15,"weight":68,"urlFragment":224,"anchor":248,"singlePageAnchor":248,"docTitle":19,"url":249},"6.6. Enabling data journaling","enabling-data-journaling-gfs2-performance","#enabling-data-journaling-gfs2-performance","#assembly_gfs2-performance-configuring-gfs2-file-systems",{"title":252,"visible":15,"weight":253,"urlFragment":254,"anchor":18,"singlePageAnchor":254,"sections":255,"docTitle":19,"url":280},"7. Diagnosing and correcting problems with GFS2 file systems",9,"assembly_troubleshooting-gfs2-configuring-gfs2-file-systems",[256,260,264,268,272,276],{"title":257,"visible":15,"weight":16,"urlFragment":254,"anchor":258,"singlePageAnchor":258,"docTitle":19,"url":259},"7.1. GFS2 file system unavailable to a node (the GFS2 withdraw function)","ref_gfs2-filesystem-unavailable-troubleshooting-gfs2","#ref_gfs2-filesystem-unavailable-troubleshooting-gfs2",{"title":261,"visible":15,"weight":23,"urlFragment":254,"anchor":262,"singlePageAnchor":262,"docTitle":19,"url":263},"7.2. GFS2 file system hangs and requires reboot of one node","ref_gfs2-filesystem-hangs-one-node-troubleshooting-gfs2","#ref_gfs2-filesystem-hangs-one-node-troubleshooting-gfs2",{"title":265,"visible":15,"weight":28,"urlFragment":254,"anchor":266,"singlePageAnchor":266,"docTitle":19,"url":267},"7.3. GFS2 file system hangs and requires reboot of all nodes","ref_gfs2-filesystem-hangs-all-nodes-troubleshooting-gfs2","#ref_gfs2-filesystem-hangs-all-nodes-troubleshooting-gfs2",{"title":269,"visible":15,"weight":58,"urlFragment":254,"anchor":270,"singlePageAnchor":270,"docTitle":19,"url":271},"7.4. GFS2 file system does not mount on newly added cluster node","ref_gfs2-nomount-new-cluster-node-troubleshooting-gfs2","#ref_gfs2-nomount-new-cluster-node-troubleshooting-gfs2",{"title":273,"visible":15,"weight":63,"urlFragment":254,"anchor":274,"singlePageAnchor":274,"docTitle":19,"url":275},"7.5. Space indicated as used in empty file system","ref_gfs2-used-space-empty-filesystem-troubleshooting-gfs2","#ref_gfs2-used-space-empty-filesystem-troubleshooting-gfs2",{"title":277,"visible":15,"weight":68,"urlFragment":254,"anchor":278,"singlePageAnchor":278,"docTitle":19,"url":279},"7.6. Gathering GFS2 data for troubleshooting","proc_gathering-gfs2-data-troubleshooting-gfs2","#proc_gathering-gfs2-data-troubleshooting-gfs2","#assembly_troubleshooting-gfs2-configuring-gfs2-file-systems",{"title":282,"visible":15,"weight":283,"urlFragment":284,"anchor":18,"singlePageAnchor":284,"sections":285,"docTitle":19,"url":307},"8. GFS2 file systems in a cluster",10,"assembly_configuring-gfs2-in-a-cluster-configuring-gfs2-file-systems",[286,290],{"title":287,"visible":15,"weight":16,"urlFragment":284,"anchor":288,"singlePageAnchor":288,"docTitle":19,"url":289},"8.1. Configuring a GFS2 file system in a cluster","proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster","#proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster",{"title":291,"visible":15,"weight":23,"urlFragment":284,"anchor":292,"singlePageAnchor":292,"sections":293,"docTitle":19,"url":306},"8.2. Configuring an encrypted GFS2 file system in a cluster","proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster",[294,298,302],{"title":295,"visible":15,"weight":16,"urlFragment":284,"anchor":296,"singlePageAnchor":296,"docTitle":19,"url":297},"8.2.1. Configure a shared logical volume in a Pacemaker cluster","configure_a_shared_logical_volume_in_a_pacemaker_cluster","#configure_a_shared_logical_volume_in_a_pacemaker_cluster",{"title":299,"visible":15,"weight":23,"urlFragment":284,"anchor":300,"singlePageAnchor":300,"docTitle":19,"url":301},"8.2.2. Encrypt the logical volume and create a crypt resource","encrypt_the_logical_volume_and_create_a_crypt_resource","#encrypt_the_logical_volume_and_create_a_crypt_resource",{"title":303,"visible":15,"weight":28,"urlFragment":284,"anchor":304,"singlePageAnchor":304,"docTitle":19,"url":305},"8.2.3. Format the encrypted logical volume with a GFS2 file system and create a file system resource for the cluster","format_the_encrypted_logical_volume_with_a_gfs2_file_system_and_create_a_file_system_resource_for_the_cluster","#format_the_encrypted_logical_volume_with_a_gfs2_file_system_and_create_a_file_system_resource_for_the_cluster","#proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster","#assembly_configuring-gfs2-in-a-cluster-configuring-gfs2-file-systems",{"title":309,"visible":15,"weight":310,"urlFragment":311,"anchor":18,"singlePageAnchor":311,"sections":312,"docTitle":19,"url":353},"9. GFS2 tracepoints and the glock debugfs interface",11,"con_gfs2-tracepoints-configuring-gfs2-file-systems",[313,317,321,325,329,333,337,341,345,349],{"title":314,"visible":15,"weight":16,"urlFragment":311,"anchor":315,"singlePageAnchor":315,"docTitle":19,"url":316},"9.1. GFS2 tracepoint types","gfs2_tracepoint_types","#gfs2_tracepoint_types",{"title":318,"visible":15,"weight":23,"urlFragment":311,"anchor":319,"singlePageAnchor":319,"docTitle":19,"url":320},"9.2. Tracepoints","ap-tracepoints-gfs2","#ap-tracepoints-gfs2",{"title":322,"visible":15,"weight":28,"urlFragment":311,"anchor":323,"singlePageAnchor":323,"docTitle":19,"url":324},"9.3. Glocks","ap-glocks-gfs2","#ap-glocks-gfs2",{"title":326,"visible":15,"weight":58,"urlFragment":311,"anchor":327,"singlePageAnchor":327,"docTitle":19,"url":328},"9.4. The glock debugfs interface","ap-glock-debugfs-gfs2","#ap-glock-debugfs-gfs2",{"title":330,"visible":15,"weight":63,"urlFragment":311,"anchor":331,"singlePageAnchor":331,"docTitle":19,"url":332},"9.5. Glock holders","ap-glock-holders-gfs2","#ap-glock-holders-gfs2",{"title":334,"visible":15,"weight":68,"urlFragment":311,"anchor":335,"singlePageAnchor":335,"docTitle":19,"url":336},"9.6. Glock tracepoints","ap-glock-tracepoints-gfs2","#ap-glock-tracepoints-gfs2",{"title":338,"visible":15,"weight":102,"urlFragment":311,"anchor":339,"singlePageAnchor":339,"docTitle":19,"url":340},"9.7. Bmap tracepoints","ap-bmap-tracepoints-gfs2","#ap-bmap-tracepoints-gfs2",{"title":342,"visible":15,"weight":223,"urlFragment":311,"anchor":343,"singlePageAnchor":343,"docTitle":19,"url":344},"9.8. Log tracepoints","ap-log-gracepoints-gfs2","#ap-log-gracepoints-gfs2",{"title":346,"visible":15,"weight":253,"urlFragment":311,"anchor":347,"singlePageAnchor":347,"docTitle":19,"url":348},"9.9. Glock statistics","ap-glockstats-gfs2","#ap-glockstats-gfs2",{"title":350,"visible":15,"weight":283,"urlFragment":311,"anchor":351,"singlePageAnchor":351,"docTitle":19,"url":352},"9.10. References","ap-references-gfs2","#ap-references-gfs2","#con_gfs2-tracepoints-configuring-gfs2-file-systems",{"title":355,"visible":15,"weight":356,"urlFragment":357,"anchor":18,"singlePageAnchor":357,"sections":358,"docTitle":19,"url":396},"10. Monitoring and analyzing GFS2 file systems using Performance Co-Pilot (PCP)",12,"assembly_analyzing-gfs2-with-pcp-configuring-gfs2-file-systems",[359,363,384,388,392],{"title":360,"visible":15,"weight":16,"urlFragment":357,"anchor":361,"singlePageAnchor":361,"docTitle":19,"url":362},"10.1. Installing the GFS2 PMDA","proc_installing-gfs2-pdma-analyzing-gfs2-with-pcp","#proc_installing-gfs2-pdma-analyzing-gfs2-with-pcp",{"title":364,"visible":15,"weight":23,"urlFragment":357,"anchor":365,"singlePageAnchor":365,"sections":366,"docTitle":19,"url":383},"10.2. Displaying information about the available performance metrics with the pminfo tool","proc_examining-number-of-glocks-analyzing-gfs2-with-pcp",[367,371,375,379],{"title":368,"visible":15,"weight":16,"urlFragment":357,"anchor":369,"singlePageAnchor":369,"docTitle":19,"url":370},"10.2.1. Examining the number of glock structures that currently exist per file system","examining_the_number_of_glock_structures_that_currently_exist_per_file_system","#examining_the_number_of_glock_structures_that_currently_exist_per_file_system",{"title":372,"visible":15,"weight":23,"urlFragment":357,"anchor":373,"singlePageAnchor":373,"docTitle":19,"url":374},"10.2.2. Examining the number of glock structures that exist per file system by type","examining_the_number_of_glock_structures_that_exist_per_file_system_by_type","#examining_the_number_of_glock_structures_that_exist_per_file_system_by_type",{"title":376,"visible":15,"weight":28,"urlFragment":357,"anchor":377,"singlePageAnchor":377,"docTitle":19,"url":378},"10.2.3. Checking the number of glock structures that are in a wait state","checking_the_number_of_glock_structures_that_are_in_a_wait_state","#checking_the_number_of_glock_structures_that_are_in_a_wait_state",{"title":380,"visible":15,"weight":58,"urlFragment":357,"anchor":381,"singlePageAnchor":381,"docTitle":19,"url":382},"10.2.4. Checking file system operation latency using the kernel tracepoint based metrics","checking_file_system_operation_latency_using_the_kernel_tracepoint_based_metrics","#checking_file_system_operation_latency_using_the_kernel_tracepoint_based_metrics","#proc_examining-number-of-glocks-analyzing-gfs2-with-pcp",{"title":385,"visible":15,"weight":28,"urlFragment":357,"anchor":386,"singlePageAnchor":386,"docTitle":19,"url":387},"10.3. Complete listing of available metrics for GFS2 in PCP","ref_available-gfs2-PCP-metrics-analyzing-gfs2-with-pcp","#ref_available-gfs2-PCP-metrics-analyzing-gfs2-with-pcp",{"title":389,"visible":15,"weight":58,"urlFragment":357,"anchor":390,"singlePageAnchor":390,"docTitle":19,"url":391},"10.4. Performing minimal PCP setup to gather file system data","proc_installing-minimal-PCP-setup-analyzing-gfs2-with-pcp","#proc_installing-minimal-PCP-setup-analyzing-gfs2-with-pcp",{"title":393,"visible":15,"weight":63,"urlFragment":357,"anchor":394,"singlePageAnchor":394,"docTitle":19,"url":395},"10.5. Additional resources","additional_resources","#additional_resources","#assembly_analyzing-gfs2-with-pcp-configuring-gfs2-file-systems",{"title":398,"visible":15,"weight":399,"urlFragment":400,"anchor":18,"singlePageAnchor":401,"docTitle":19,"url":402},"Legal Notice",13,"legal-notice","idm139688929828816","#idm139688929828816",[404,407,410],{"text":405,"link":406},"Red Hat Enterprise Linux","/documentation/red_hat_enterprise_linux/",{"text":408,"link":409},"9","/documentation/red_hat_enterprise_linux/9/",{"text":11},{"name":11,"translations":412,"productVersion":413,"singlePage":414,"pdf":417,"publishingStatus":419},[5,6,7,8,9],{"name":408},{"contentUrl":415,"name":11,"new":416,"url":19},"https://d2bhdhkti9t3uj.cloudfront.net/html/41d123ad-359f-408a-8d26-ce98ba9875f9/c916640cc2c0443b035a1d74f06bcf87.html","https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/configuring_gfs2_file_systems/index",{"url":418},"https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/pdf/configuring_gfs2_file_systems/Red_Hat_Enterprise_Linux-9-Configuring_GFS2_file_systems-en-US.pdf","PUBLISHED",[421,426,429,433,437,441,445],{"name":422,"new":423,"url":424,"urlAliases":425},"10.0-Beta","https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/10-beta","10-beta",[],{"name":408,"new":427,"url":408,"urlAliases":428},"https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9",[],{"name":430,"new":431,"url":430,"urlAliases":432},"8","https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8",[],{"name":434,"new":435,"url":434,"urlAliases":436},"7","https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7",[],{"name":438,"new":439,"url":438,"urlAliases":440},"6","https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6",[],{"name":442,"new":443,"url":442,"urlAliases":444},"5","https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/5",[],{"name":446,"new":447,"url":446,"urlAliases":448},"4","https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/4",[],{"configuring_gfs2_file_systems/index":450,"configuring_gfs2_file_systems/proc_providing-feedback-on-red-hat-documentation_configuring-gfs2-file-systems":451,"configuring_gfs2_file_systems/assembly_planning-gfs2-deployment-configuring-gfs2-file-systems":452,"configuring_gfs2_file_systems/assembly_gfs2-usage-considerations-configuring-gfs2-file-systems":453,"configuring_gfs2_file_systems/assembly_creating-mounting-gfs2-configuring-gfs2-file-systems":454,"configuring_gfs2_file_systems/assembly_gfs2-disk-quota-administration-configuring-gfs2-file-systems":455,"configuring_gfs2_file_systems/assembly_gfs2-filesystem-repair-configuring-gfs2-file-systems":456,"configuring_gfs2_file_systems/assembly_gfs2-performance-configuring-gfs2-file-systems":457,"configuring_gfs2_file_systems/assembly_troubleshooting-gfs2-configuring-gfs2-file-systems":458,"configuring_gfs2_file_systems/assembly_configuring-gfs2-in-a-cluster-configuring-gfs2-file-systems":459,"configuring_gfs2_file_systems/con_gfs2-tracepoints-configuring-gfs2-file-systems":460,"configuring_gfs2_file_systems/assembly_analyzing-gfs2-with-pcp-configuring-gfs2-file-systems":461,"configuring_gfs2_file_systems/legal-notice":462},{"prevt":18,"next":24},{"prevt":17,"next":29},{"prevt":18,"next":29},{"prevt":18,"next":74},{"prevt":18,"next":121},{"prevt":18,"next":172},{"prevt":18,"next":210},{"prevt":18,"next":224},{"prevt":18,"next":254},{"prevt":18,"next":284},{"prevt":18,"next":311},{"prevt":18,"next":357},{"prevt":357,"next":18},{"product":18,"version":18},{"configuring_gfs2_file_systems":465},[416],{"products":467},[468,469,470,471,472,473,474,475,476,477,478,479,480],"builds_for_red_hat_openshift","migration_toolkit_for_virtualization","openshift_container_platform","openshift_sandboxed_containers","red_hat_advanced_cluster_security_for_kubernetes","red_hat_advanced_cluster_management_for_kubernetes","red_hat_openshift_data_foundation","red_hat_openshift_dev_spaces","red_hat_openshift_gitops","red_hat_openshift_local","red_hat_openshift_pipelines","red_hat_openshift_serverless","workload_availability_for_red_hat_openshift",[],{"default":483},[484,493,499,508,516,524,531,539,546],{"nid":485,"type":486,"langcode":487,"Published":16,"title":488,"Created":489,"Updated":490,"body_value":18,"field_documentation banner_text_value":18,"field_documentation banner_text_format":18,"field_paths_value":491,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":492},580,"rebrand banner","en","OpenShift Container Storage is now OpenShift Data Foundation starting with version 4.9.","2023-01-11 15:38:32","2023-01-11 15:40:04","/documentation/red_hat_openshift_container_storage\r\n/documentation/red_hat_openshift_container_storage/\r\n/documentation/red_hat_openshift_container_storage/*","internal:/documentation/red_hat_openshift_data_foundation/",{"nid":494,"type":486,"langcode":487,"Published":16,"title":488,"Created":495,"Updated":496,"body_value":18,"field_documentation banner_text_value":18,"field_documentation banner_text_format":18,"field_paths_value":497,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":498},581,"2023-01-11 15:41:31","2023-01-11 15:42:04","/documentation/red_hat_openshift_data_foundation/4.9\r\n/documentation/red_hat_openshift_data_foundation/4.9/\r\n/documentation/red_hat_openshift_data_foundation/4.9/*\r\n/documentation/red_hat_openshift_data_foundation/4.10\r\n/documentation/red_hat_openshift_data_foundation/4.10/\r\n/documentation/red_hat_openshift_data_foundation/4.10/*\r\n/documentation/red_hat_openshift_data_foundation/4.11\r\n/documentation/red_hat_openshift_data_foundation/4.11/\r\n/documentation/red_hat_openshift_data_foundation/4.11/*","internal:/documentation/red_hat_openshift_container_storage/",{"nid":500,"type":501,"langcode":487,"Published":16,"title":502,"Created":503,"Updated":504,"body_value":18,"field_documentation banner_text_value":505,"field_documentation banner_text_format":506,"field_paths_value":507,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":18},582,"developer preview banner","MicroShift is Developer Preview software only","2023-01-11 15:50:24","2023-01-30 19:00:52","\u003Cp slot=header>MicroShift is Developer Preview software only.\u003C/p>For more information about the support scope of Red Hat Developer Preview software, see \u003Ca href=\"https://access.redhat.com/support/offerings/devpreview/\">Developer Preview Support Scope\u003C/a>.","documentation banner","/documentation/microshift/4.12\r\n/documentation/microshift/4.12/*\r\n/documentation/red_hat_build_of_microshift/4.12\r\n/documentation/red_hat_build_of_microshift/4.12/*",{"nid":509,"type":510,"langcode":487,"Published":16,"title":511,"Created":512,"Updated":513,"body_value":18,"field_documentation banner_text_value":514,"field_documentation banner_text_format":506,"field_paths_value":515,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":18},583,"obsolete documentation banner","RHACS EOL - DAT-3433","2023-01-23 16:36:43","2023-01-23 16:39:14","\u003Cp slot=header>You are viewing documentation for a release that is no longer maintained. To view the documentation for the most recent version, see the \u003Ca href=\"/documentation/red_hat_advanced_cluster_security_for_kubernetes/\">latest RHACS docs\u003C/a>.\u003C/p>","/documentation/red_hat_advanced_cluster_security_for_kubernetes/3.69\r\n/documentation/red_hat_advanced_cluster_security_for_kubernetes/3.69/*\r\n/documentation/red_hat_advanced_cluster_security_for_kubernetes/3.70\r\n/documentation/red_hat_advanced_cluster_security_for_kubernetes/3.70/*\r\n/documentation/red_hat_advanced_cluster_security_for_kubernetes/3.71\r\n/documentation/red_hat_advanced_cluster_security_for_kubernetes/3.71/*",{"nid":517,"type":518,"langcode":487,"Published":16,"title":519,"Created":520,"Updated":521,"body_value":18,"field_documentation banner_text_value":522,"field_documentation banner_text_format":506,"field_paths_value":523,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":18},584,"end of life banner","EOL banner for RHV","2023-05-23 14:58:05","2023-05-24 15:19:42","\u003Cp slot=header>The Red Hat Virtualization\u003C/p>Maintenance Phase runs until August 31, 2024, followed by the Extended Life Phase with no more software fixes through August 31, 2026. See \u003Ca href=\"https://access.redhat.com/articles/6975303\">Migration Paths for OpenShift Container Platform deployed on Red Hat Virtualization\u003C/a> for details.","/documentation/red_hat_virtualization/4.4",{"nid":525,"type":518,"langcode":487,"Published":16,"title":526,"Created":527,"Updated":528,"body_value":18,"field_documentation banner_text_value":529,"field_documentation banner_text_format":506,"field_paths_value":530,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":18},585,"RHHI-V EOL","2023-06-01 16:52:57","2023-06-01 17:03:44","\u003Cp slot=header>Red Hat Hyperconverged Infrastructure for Virtualization\u003C/p> is in the \u003Ca href=\"https://access.redhat.com/support/policy/updates/rhhiv\">Maintenance Support Phase\u003C/a> of its lifecycle until October 31, 2024. After that date, the product will be End of Life. See the \u003Ca href=\"https://access.redhat.com/announcements/6972521\">RHHI-V announcement\u003C/a> for next steps.","/documentation/red_hat_hyperconverged_infrastructure_for_virtualization/1.8",{"nid":532,"type":533,"langcode":487,"Published":16,"title":534,"Created":535,"Updated":536,"body_value":18,"field_documentation banner_text_value":537,"field_documentation banner_text_format":506,"field_paths_value":538,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":18},586,"preview banner","MicroShift is Technology Preview software only","2024-03-18 16:53:05","2024-03-18 16:54:56","\u003Cp slot=header>MicroShift is Technology Preview software only.\u003C/p>For more information about the support scope of Red Hat Technology Preview software, see \u003Ca href=\"https://access.redhat.com/support/offerings/techpreview/\">Technology Preview Support Scope\u003C/a>.","/documentation/red_hat_build_of_microshift/4.13\r\n/documentation/red_hat_build_of_microshift/4.13/*",{"nid":540,"type":533,"langcode":487,"Published":16,"title":541,"Created":542,"Updated":543,"body_value":18,"field_documentation banner_text_value":544,"field_documentation banner_text_format":506,"field_paths_value":545,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":18},588,"Ansible 2.5 upgrade limitation","2024-09-30 16:53:05","2024-10-28 16:54:56","\u003Cp slot=\"header\">Support added for upgrades from 2.4\u003C/p>Ansible Automation Platform 2.5-3, released on October 28, 2024, adds support for upgrades from 2.4. See the upgrade documentation for more information.","",{"nid":547,"alertType":548,"type":549,"langcode":487,"Published":16,"title":550,"Created":545,"Updated":545,"body_value":18,"field_documentation banner_text_value":551,"field_documentation banner_text_format":506,"field_paths_value":552,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":18},587,"warning","Warning banner","Red Hat build of Apache Camel K","\u003Cp slot=header>Red Hat Camel K is deprecated\u003C/p>Red Hat Camel K is deprecated and the End of Life date for this product is June 30, 2025. For help migrating to the current go-to solution, \u003Ca target=_blank href=\"https://docs.redhat.com/en/documentation/red_hat_build_of_apache_camel\">Red Hat build of Apache Camel\u003C/a>, see the \u003Ca target=_blank href=\"https://docs.redhat.com/en/documentation/red_hat_build_of_apache_camel_k/1.10.7/html/migration_guide_camel_k_to_camel_extensions_for_quarkus/index\">Migration Guide\u003C/a>.","/documentation/red_hat_build_of_apache_camel_k/*",{"product":405},["Reactive",555],{"$snuxt-i18n-meta":556,"$sisLoading":557,"$sisSinglePage":15,"$sisInFocusMode":557,"$smobileTocOpen":557,"$sisLargeTOC":557,"$scurrentChapter":17,"$scurrentSection":17,"$scurrentSubSection":545},{},false,["Set"],["ShallowReactive",560],{"s8LoCEfG4A":18,"rFVLKcOK8e":18,"uUstF4AIyn":18,"MdHNSZP4nR":18,"Pn02PlJOas":18},"/en/documentation/red_hat_enterprise_linux/9/html-single/configuring_gfs2_file_systems/index"]</script>
<script>window.__NUXT__={};window.__NUXT__.config={public:{contentEnv:"",collectFeedback:true,i18n:{baseUrl:"",defaultLocale:"en",defaultDirection:"ltr",strategy:"prefix",lazy:false,rootRedirect:"",routesNameSeparator:"___",defaultLocaleRouteNameSuffix:"default",skipSettingLocaleOnNavigate:false,differentDomains:false,trailingSlash:false,configLocales:[{code:"en",name:"English",iso:"en-US"},{code:"fr",name:"Français",iso:"fr-FR"},{code:"ko",name:"한국어",iso:"ko-KR"},{code:"ja",name:"日本語",iso:"ja-JP"},{code:"zh-cn",name:"中文 (中国)",iso:"zh-CN"},{code:"de",name:"Deutsch",iso:"de_DE"},{code:"it",name:"Italiano",iso:"it_IT"},{code:"pt-br",name:"Português",iso:"pt_BR"},{code:"es",name:"Español",iso:"es-ES"}],locales:{en:{domain:""},fr:{domain:""},ko:{domain:""},ja:{domain:""},"zh-cn":{domain:""},de:{domain:""},it:{domain:""},"pt-br":{domain:""},es:{domain:""}},detectBrowserLanguage:{alwaysRedirect:false,cookieCrossOrigin:false,cookieDomain:"",cookieKey:"i18n_redirected",cookieSecure:false,fallbackLocale:"",redirectOn:"root",useCookie:true},experimental:{localeDetector:"",switchLocalePathLinkSSR:false,autoImportTranslationFunctions:false}}},app:{baseURL:"/",buildId:"5ff82c51-785e-4a7d-aca6-77d6692e8c7a",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body></html>
