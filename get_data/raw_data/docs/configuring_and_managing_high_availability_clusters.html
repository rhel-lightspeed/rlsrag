<!DOCTYPE html><html  data-capo=""><head><script type="importmap">
  {
    "imports": {
      "@patternfly/elements/": "/scripts/v1/@patternfly/elements/",
      "@patternfly/pfe-clipboard/": "/scripts/v1/@patternfly/pfe-clipboard/",
      "@rhds/elements/": "/scripts/v1/@rhds/elements/elements/",
      "@cpelements/elements/": "/scripts/v1/@cpelements/elements/elements/"
    },
    "scopes": {
      "/": {
        "@floating-ui/core": "/scripts/v1/@floating-ui/core/dist/floating-ui.core.mjs",
        "@floating-ui/dom": "/scripts/v1/@floating-ui/dom/dist/floating-ui.dom.mjs",
        "@floating-ui/utils": "/scripts/v1/@floating-ui/utils/dist/floating-ui.utils.mjs",
        "@floating-ui/utils/dom": "/scripts/v1/@floating-ui/utils/dom/dist/floating-ui.utils.dom.mjs",
        "@lit/reactive-element": "/scripts/v1/@lit/reactive-element/reactive-element.js",
        "@lit/reactive-element/decorators/": "/scripts/v1/@lit/reactive-element/decorators/",
        "@patternfly/pfe-core": "/scripts/v1/@patternfly/pfe-core/core.js",
        "@patternfly/pfe-core/": "/scripts/v1/@patternfly/pfe-core/",
        "@rhds/tokens/media.js": "/scripts/v1/@rhds/tokens/js/media.js",
        "lit": "/scripts/v1/lit/index.js",
        "lit-element/lit-element.js": "/scripts/v1/lit-element/lit-element.js",
        "lit-html": "/scripts/v1/lit-html/lit-html.js",
        "lit-html/": "/scripts/v1/lit-html/",
        "lit/": "/scripts/v1/lit/",
        "tslib": "/scripts/v1/tslib/tslib.es6.mjs",
        "@cpelements/rh-table/dist/rh-table.js": "/scripts/v1/@cpelements/rh-table/dist/rh-table.js"
      }
    }
  }
</script><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Configuring and managing high availability clusters | Red Hat Product Documentation</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<script type="text/javascript" id="trustarc" src="//static.redhat.com/libs/redhat/marketing/latest/trustarc/trustarc.js"></script>
<script src="//www.redhat.com/dtm.js"></script>
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Red+Hat+Display:wght@400;500;700&family=Red+Hat+Text:wght@400;500;700&display=swap">
<link rel="stylesheet" href="/styles/rh-table--lightdom.min.css">
<style>.section .titlepage{gap:.75rem}.section .titlepage,div.edit{align-items:center;display:flex}div.edit{font-size:.9rem;margin-bottom:8px}div.edit>a{align-items:center;display:flex}.edit pf-icon{margin-right:4px}</style>
<style>#error[data-v-df31ff14]{align-items:center;display:flex;flex-direction:column;justify-content:center;min-height:80vh}h1[data-v-df31ff14]{font-size:calc(var(--rh-font-size-body-text-md, 1rem)*4);font-weight:700;margin-bottom:0}h1[data-v-df31ff14],h1 span[data-v-df31ff14]{line-height:var(--rh-line-height-heading,1.3)}h1 span[data-v-df31ff14]{color:var(--rh-color-text-brand-on-light,#e00);display:block;text-transform:uppercase}h1 span[data-v-df31ff14],p[data-v-df31ff14]{font-size:var(--rh-font-size-body-text-lg,1.125rem)}aside[data-v-df31ff14]{align-items:center;background:var(--rh-color-surface-lightest,#fff);border:var(--rh-border-width-sm,1px) solid var(--rh-color-border-subtle-on-light,#c7c7c7);border-radius:var(--rh-border-radius-default,3px);border-top:calc(var(--rh-border-width-md, 2px)*2) solid var(--rh-color-text-brand-on-light,#e00);box-shadow:var(--rh-box-shadow-sm,0 2px 4px 0 hsla(0,0%,8%,.2));display:flex;flex-direction:column;justify-content:space-between;margin-top:var(--rh-space-2xl,32px);padding:var(--rh-space-xl,24px)}aside[data-v-df31ff14],aside>div[data-v-df31ff14]{width:100%}.text-container[data-v-df31ff14]{margin:auto;max-width:442px;text-align:center}.desktop[data-v-df31ff14]{display:none}.sr-only[data-v-df31ff14]{height:1px;overflow:hidden;padding:0;position:absolute;width:1px;clip:rect(0,0,0,0);border:0;clip-path:inset(50%);white-space:nowrap}form[data-v-df31ff14]{display:flex}button[data-v-df31ff14],input[data-v-df31ff14]{border:1px solid var(--rh-color-black-500,#8a8d90);box-sizing:border-box;height:40px}input[data-v-df31ff14]{border-right:none;flex:1;font-family:var(--rh-font-family-heading,RedHatDisplay,"Red Hat Display","Noto Sans Arabic","Noto Sans Hebrew","Noto Sans JP","Noto Sans KR","Noto Sans Malayalam","Noto Sans SC","Noto Sans TC","Noto Sans Thai",Helvetica,Arial,sans-serif);font-size:var(--rh-font-size-body-text-md,1rem);padding-left:var(--rh-space-md,8px);width:100%}button[data-v-df31ff14]{-webkit-appearance:none;-moz-appearance:none;appearance:none;background:transparent;border-left:none;display:flex;width:var(--rh-size-icon-04,40px)}button[data-v-df31ff14]:before{background:var(--rh-context-light-color-text-link,#06c);content:"";cursor:pointer;display:block;height:28px;margin:auto;width:28px}.search-icon[data-v-df31ff14]{margin:auto}ul[data-v-df31ff14]{max-width:275px;padding:0}ul li[data-v-df31ff14]{display:inline-block;list-style:none;margin-right:var(--rh-space-xl,24px);padding:var(--rh-space-xs,4px) 0}ul li a[data-v-df31ff14]{color:var(--rh-context-light-color-text-link,#06c);text-decoration:none}@media (min-width:992px){aside[data-v-df31ff14]{flex-direction:row}.mobile[data-v-df31ff14]{display:none}.desktop[data-v-df31ff14]{display:block}input[data-v-df31ff14]{width:auto}}</style>
<style>@keyframes fade-in{0%{opacity:0;visibility:hidden}1%{visibility:visible}to{opacity:1;visibility:visible}}@media (min-height:48em){.rhdocs{--rh-table--maxHeight:calc(100vh - 12.5rem)}}*,.rhdocs *,.rhdocs :after,.rhdocs :before,:after,:before{box-sizing:border-box}.rhdocs img,.rhdocs object,.rhdocs svg,img,object,svg{display:inline-block;max-width:100%;vertical-align:middle}.rhdocs hr{border:0;border-top:.0625rem solid #d2d2d2;clear:both;margin:1rem 0}.rhdocs a{color:#06c;text-decoration:underline}.rhdocs a:focus,.rhdocs a:hover{color:#036}.rhdocs a.anchor-heading{color:#151515;cursor:pointer;text-decoration:none;word-break:break-all}.rhdocs p{margin:1.49963rem 0}.rhdocs li>p{margin:0}.rhdocs h1,.rhdocs h2,.rhdocs h3,.rhdocs h4,.rhdocs h5,.rhdocs h6{font-family:RedHatDisplay,Red Hat Display,Helvetica Neue,Arial,sans-serif;font-weight:500;margin:0 0 .625rem}.rhdocs h1{font-size:2.25rem;margin:2rem 0}.rhdocs h2{font-size:1.625rem;margin:2rem 0}.rhdocs h3{font-size:1.5rem;font-weight:400}.rhdocs h4,.rhdocs h5{font-size:1.25rem}.rhdocs h5{font-weight:400}.rhdocs h6{font-size:1.125rem;font-weight:500;line-height:1.6667}.rhdocs ol,.rhdocs ul{margin:1rem 0;padding:0 0 0 1.5rem}.rhdocs ol ::marker,.rhdocs ul ::marker{font:inherit}.rhdocs li{margin:0 0 .5em;padding:0}.rhdocs li>p{margin:.5rem 0}.rhdocs li>ol,.rhdocs li>ul{margin:0}.rhdocs dl dd{margin:.5rem 0 .5rem 1rem}.rhdocs dl dd>p{margin:.5rem 0}.rhdocs .informaltable,.rhdocs .table-contents,.rhdocs .table-wrapper{max-height:var(--rh-table--maxHeight);overflow:auto}.rhdocs table{border:0;font-size:1rem;line-height:1.6667;table-layout:fixed}.rhdocs table caption{color:#585858;margin-bottom:.5rem;margin-top:.5rem;text-align:left}.rhdocs table td,.rhdocs table th{border:0;border-bottom:.0625rem solid #d2d2d2;border-bottom:.0625rem solid var(--pfe-table--Border,#d2d2d2);padding:.5em 1rem}.rhdocs table td.halign-left,.rhdocs table th.halign-left{text-align:left}.rhdocs table td.halign-center,.rhdocs table th.halign-center,table td.halign-center,table th.halign-center{text-align:center}.rhdocs table td.halign-right,.rhdocs table th.halign-right{text-align:right}.rhdocs table td.valign-top,.rhdocs table th.valign-top{vertical-align:top}.rhdocs table td.valign-middle,.rhdocs table th.valign-middle{vertical-align:middle}.rhdocs table td.valign-bottom,.rhdocs table th.valign-bottom{vertical-align:bottom}.rhdocs table thead td,.rhdocs table thead th{background:#f5f5f5;font-weight:600}.rhdocs rh-table table,.rhdocs rh-table.rh-table--expanded-vertically{max-height:-moz-max-content;max-height:max-content}.rhdocs pre.nowrap{overflow:auto;overflow-wrap:normal;white-space:pre;word-break:normal}.rhdocs .codeblock__wrapper pre{background:transparent}.rh-table--full-screen code,.rhdocs .content--md code,.rhdocs .content--sm code,.rhdocs .rh-table--full-screen code{overflow-wrap:normal;word-break:normal}.rhdocs[class] pre code,[class] pre code{background:inherit;color:inherit;font-family:inherit;font-size:inherit;font-weight:inherit;line-height:inherit;padding:0}.rhdocs .keycap,.rhdocs kbd{background-color:#eee;background-image:linear-gradient(180deg,#ddd,#eee,#fff);border-radius:.1875rem;box-shadow:0 -.0625rem 0 0 #fff,0 .0625rem 0 .1875rem #aaa;font-family:RedHatMono,Red Hat Mono,Consolas,monospace;font-size:90%;font-weight:400;margin:0 .25rem;padding:.125rem .375rem}.keycap strong,.rhdocs .keycap strong{font-weight:inherit}.rhdocs kbd.keyseq,kbd.keyseq{background:transparent;border:0;box-shadow:none;padding:0}.rhdocs kbd.keyseq kbd,kbd.keyseq kbd{display:inline-block;margin:0 .375rem}.rhdocs kbd.keyseq kbd:first-child,kbd.keyseq kbd:first-child{margin-left:0}.rhdocs b.button{font-size:90%;font-weight:700;padding:.1875rem}.rhdocs b.button:before{content:"["}.rhdocs b.button:after{content:"]"}html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}.rhdocs audio,.rhdocs canvas,.rhdocs progress,.rhdocs video{display:inline-block;vertical-align:baseline}.rhdocs audio:not([controls]){display:none;height:0}[hidden],template{display:none}.rhdocs a{background:transparent}.rhdocs a:active,.rhdocs a:hover{outline:0}.rhdocs a.anchor-heading:hover:before{color:#151515;content:"#";margin-left:-1.6rem;position:absolute}.rhdocs a.anchor-heading:focus-visible{color:#151515}@media screen and (max-width:990px){.rhdocs a.anchor-heading:hover:before{font-size:16px;margin-left:-1rem;padding-top:8px}.rhdocs h1 a.anchor-heading:hover:before{padding-top:12px}.rhdocs h4 a.anchor-heading:hover:before,.rhdocs h5 a.anchor-heading:hover:before{padding-top:4px}.rhdocs h6 a.anchor-heading:hover:before{padding-top:2px}}.rhdocs abbr[title]{border-bottom:.0625rem dotted}.rhdocs dfn{font-style:italic}.rhdocs h1{font-size:2em;margin:.67em 0}.rhdocs mark{background:#ff0;color:#000}.rhdocs small{font-size:80%}.rhdocs sub,.rhdocs sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}.rhdocs sup{top:-.5em}.rhdocs sub{bottom:-.25em}.rhdocs img{border:0}.rhdocs svg:not(:root){overflow:hidden}.rhdocs figure{margin:1em 2.5rem}.rhdocs hr{box-sizing:content-box;height:0}.rhdocs code,.rhdocs kbd,.rhdocs pre,.rhdocs samp{font-family:monospace,monospace;font-size:1em}.rhdocs button,.rhdocs optgroup,.rhdocs select,.rhdocs textarea,.rhdocsinput{color:inherit;font:inherit;margin:0}.rhdocs button.copy-link-btn{background:none;border:2px solid #fff;font:1px Red Hat Text}.rhdocs button.copy-link-btn:hover{border-bottom:2px solid #06c}.rhdocs button.copy-link-btn .link-icon{padding-bottom:4px}.rhdocs button{overflow:visible}.rhdocs button,.rhdocs select{text-transform:none}.rhdocs button,.rhdocs html input[type=button],.rhdocs input[type=reset],.rhdocs input[type=submit]{-moz-appearance:button;appearance:button;-webkit-appearance:button;cursor:pointer}.rhdocs button[disabled],.rhdocs html input[disabled]{cursor:default}.rhdocs button::-moz-focus-inner,.rhdocs input::-moz-focus-inner{border:0;padding:0}.rhdocs input{line-height:normal}.rhdocs input[type=checkbox],.rhdocs input[type=radio]{box-sizing:border-box;padding:0}.rhdocs input[type=number]::-webkit-inner-spin-button,.rhdocs input[type=number]::-webkit-outer-spin-button{height:auto}.rhdocs input[type=search]{-moz-appearance:textfield;appearance:textfield;-webkit-appearance:textfield;box-sizing:content-box}.rhdocs input[type=search]::-webkit-search-cancel-button,.rhdocs input[type=search]::-webkit-search-decoration{-webkit-appearance:none}.rhdocs fieldset{border:.0625rem solid silver;margin:0 .125rem;padding:.35em .625em .75em}.rhdocs legend{border:0;padding:0}.rhdocs textarea{overflow:auto}.rhdocs optgroup{font-weight:700}.rhdocs table{border-collapse:collapse;border-spacing:0}.rhdocs td,.rhdocs th{padding:0}.rhdocs ._additional-resources[class][class][id]:last-child{margin-top:-2rem}.rhdocs ._additional-resources[class][class]:only-child{grid-column:1/-1}._additional-resources[class][class] .additional-resources__heading,._additional-resources[class][class] .heading,._additional-resources[class][class] h1,._additional-resources[class][class] h2,._additional-resources[class][class] h3,._additional-resources[class][class] h4,._additional-resources[class][class] h5,._additional-resources[class][class] h6,._additional-resources[class][class] p.title{display:block;font-family:RedHatDisplay,Red Hat Display,Helvetica Neue,Arial,sans-serif;font-size:1.125rem;font-weight:700;line-height:1.5rem;margin:0 0 .5rem;padding:0;text-transform:uppercase}._additional-resources[class][class] ul{border:0;list-style:none;margin:0;padding:0;position:relative}.related-topic-content__wrapper ._additional-resources[class][class] ul{display:block}._additional-resources[class][class] ul:after{background-color:#fff;bottom:0;content:"";display:block;height:.125rem;position:absolute;width:100%}._additional-resources[class][class] li{border-bottom:.0625rem solid #d2d2d2;box-sizing:content-box;margin:0;padding:1rem 1.5rem 1rem 0;-moz-column-break-inside:avoid;break-inside:avoid}._additional-resources[class][class] li:only-child{grid-column:1/-1}._additional-resources[class][class] li:last-child{border:0}@media (min-width:1100px){._additional-resources[class][class] li:last-child{border-bottom:.0625rem solid #d2d2d2}}._additional-resources[class][class] li p:only-child{margin:0;padding:0}._additional-resources[class][class] li a{text-decoration:none}._additional-resources[class][class] li a:focus,._additional-resources[class][class] li a:hover{text-decoration:underline}.rhdocs table .admonitionblock>div:nth-child(2),.rhdocs table .caution>div:nth-child(2),.rhdocs table .important>div:nth-child(2),.rhdocs table .note>div:nth-child(2),.rhdocs table .tip>div:nth-child(2),.rhdocs table .warning>div:nth-child(2){margin:.5rem 0}.rhdocs table .admonitionblock>div:nth-child(2)>:first-child,.rhdocs table .caution>div:nth-child(2)>:first-child,.rhdocs table .important>div:nth-child(2)>:first-child,.rhdocs table .note>div:nth-child(2)>:first-child,.rhdocs table .tip>div:nth-child(2)>:first-child,.rhdocs table .warning>div:nth-child(2)>:first-child{margin-top:0}.rhdocs table .admonitionblock>div:nth-child(2)>:last-child,.rhdocs table .caution>div:nth-child(2)>:last-child,.rhdocs table .important>div:nth-child(2)>:last-child,.rhdocs table .note>div:nth-child(2)>:last-child,.rhdocs table .tip>div:nth-child(2)>:last-child,.rhdocs table .warning>div:nth-child(2)>:last-child{margin-bottom:0}.rhdocs .codeblock__wrapper+.codeblock__wrapper,.rhdocs pre+pre,.rhdocs pre[class]+pre[class]{margin-top:2rem}.rhdocs .codeblock__wrapper{background:#f8f8f8;overflow:visible;position:relative;transform:translate(0);z-index:0}.codeblock__wrapper:before{background-repeat:no-repeat;background-size:6.25rem 100%;bottom:var(--scrollbar__height,1px);content:"";display:block;height:7.125rem;max-height:100%;max-height:calc(100% - var(--scrollbar__height, 2px));position:absolute;right:var(--scrollbar__width,6px);top:.0625rem;width:4.0625rem;z-index:1}.rhdocs .codeblock__inner-wrapper,.rhdocs pre{max-height:calc(100vh - 6.25rem)}@media (min-height:48em){.rhdocs .codeblock__inner-wrapper,.rhdocs pre{max-height:calc(100vh - 12.5rem)}}.rhdocs .codeblock__inner-wrapper{display:grid;grid-template-columns:1fr 4.375rem}.rhdocs .codeblock__wrapper--expanded .codeblock__inner-wrapper{max-height:-moz-max-content;max-height:max-content}.codeblock__copy span{display:block;height:0;position:absolute;visibility:hidden;width:0}.codeblock__copy:focus{outline:.0625rem dashed currentcolor}.codeblock__copy svg#icon--copy{height:1rem;width:1rem}.codeblock__expand{-webkit-appearance:none;-moz-appearance:none;appearance:none;background:#f0efef;border:0;cursor:pointer;height:1.75rem;left:calc(100% - 2.75rem - var(--scrollbar__width, 0px));position:absolute;text-indent:-9999em;top:3.25rem;width:1.75rem;z-index:2}.codeblock__expand:before{background:#6a6e73;content:"";height:100%;left:0;-webkit-mask-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 320 512'%3E%3C!--! Font Awesome Pro 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2022 Fonticons, Inc.--%3E%3Cpath d='M182.6 9.4c-12.5-12.5-32.8-12.5-45.3 0l-96 96c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l41.4-41.4v293.4l-41.4-41.3c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l96 96c12.5 12.5 32.8 12.5 45.3 0l96-96c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192 402.7V109.3l41.4 41.4c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3l-96-96z'/%3E%3C/svg%3E");mask-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 320 512'%3E%3C!--! Font Awesome Pro 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2022 Fonticons, Inc.--%3E%3Cpath d='M182.6 9.4c-12.5-12.5-32.8-12.5-45.3 0l-96 96c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l41.4-41.4v293.4l-41.4-41.3c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l96 96c12.5 12.5 32.8 12.5 45.3 0l96-96c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192 402.7V109.3l41.4 41.4c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3l-96-96z'/%3E%3C/svg%3E");-webkit-mask-position:center center;mask-position:center center;-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:auto 1rem;mask-size:auto 1rem;position:absolute;top:0;width:100%}.codeblock__wrapper--expanded .codeblock__expand{background:#2b9af3}.codeblock__wrapper--expanded .codeblock__expand:before{background:#fff}.codeblock__expand:focus:before,.codeblock__expand:hover:before{background:#06c}.codeblock__wrapper--expanded .codeblock__expand:focus:before,.codeblock__wrapper--expanded .codeblock__expand:hover:before{background:#fff}.codeblock__expand:focus{outline:.0625rem dashed currentcolor}.rhdocs .calloutlist>ol,.rhdocs .colist>ol{counter-reset:colist;list-style:none;margin:1rem 0 2rem;padding:0}.rhdocs .calloutlist>ol>li,.rhdocs .colist>ol>li{counter-increment:colist;font-size:1rem;margin:.5rem 0;padding-left:1.75rem;position:relative}.rhdocs .calloutlist>ol>li .colist-num,.rhdocs .colist>ol>li .colist-num{display:none}.calloutlist>ol>li:before,.colist>ol>li:before{content:counter(colist);left:0;position:absolute;top:.1875rem}.calloutlist dt{clear:left;float:left;margin:0;padding:0 .5rem 0 0}.included-in-guides[class],.included-in-guides[class][id]:last-child{background:#fff;border:.0625rem solid #d2d2d2;border-radius:.1875rem;margin:2em 0 4em;padding:2rem 2rem 1rem}.included-in-guides[class][id]:last-child{margin-top:-2rem}.included-in-guides[class]:only-child{grid-column:1/-1}.included-in-guides[class] .additional-resources__heading,.included-in-guides[class] .heading,.included-in-guides[class] h1,.included-in-guides[class] h2,.included-in-guides[class] h3,.included-in-guides[class] h4,.included-in-guides[class] h5,.included-in-guides[class] h6,.included-in-guides[class] p.title{display:block;font-family:RedHatDisplay,Red Hat Display,Helvetica Neue,Arial,sans-serif;font-size:1.125rem;font-weight:700;line-height:1.5rem;margin:0 0 .5rem;padding:0;text-transform:uppercase}.included-in-guides[class] ul{border:0;list-style:none;margin:0;padding:0;position:relative}.related-topic-content__wrapper .included-in-guides[class] ul{display:block}.included-in-guides[class] ul:after{background-color:#fff;bottom:0;content:"";display:block;height:.125rem;position:absolute;width:100%}.included-in-guides[class] li{border-bottom:.0625rem solid #d2d2d2;box-sizing:content-box;margin:0;padding:1rem 1.5rem 1rem 0;-moz-column-break-inside:avoid;break-inside:avoid}.included-in-guides[class] li:only-child{grid-column:1/-1}.included-in-guides[class] li:last-child{border:0}@media (min-width:1100px){.included-in-guides[class] li:last-child{border-bottom:.0625rem solid #d2d2d2}}.included-in-guides[class] li p:only-child{margin:0;padding:0}.included-in-guides[class] li a{text-decoration:none}.included-in-guides[class] li a:focus,.included-in-guides[class] li a:hover{text-decoration:underline}.menuseq{display:inline-flex;overflow:hidden;text-indent:-9999em}.menuseq .menu,.menuseq .menuitem,.menuseq .submenu{display:block;position:relative;text-indent:0}.menuseq .menu+.menu:before,.menuseq .menu+.menuitem:before,.menuseq .menu+.submenu:before,.menuseq .menuitem+.menu:before,.menuseq .menuitem+.menuitem:before,.menuseq .menuitem+.submenu:before,.menuseq .submenu+.menu:before,.menuseq .submenu+.menuitem:before,.menuseq .submenu+.submenu:before{content:">";display:inline-block;font-weight:700;padding:0 .25em}.related-topic-content__wrapper{margin:2em 0}.related-topic-content__wrapper--for-guide{margin-bottom:-2.5rem;padding-bottom:.0625rem;position:relative;z-index:1}.related-topic-content__wrapper--for-guide:before{background:#f0f0f0;content:"";display:block;height:100%;left:-3rem;position:absolute;right:-4.5rem;top:0;width:auto;z-index:-1}@media (min-width:1100px){.related-topic-content__wrapper--for-guide:before{left:-2.5rem;right:-3.625rem}}.related-topic-content__wrapper--for-guide summary{padding:1em 2em 1em 2.1875rem}@media (min-width:950px){.related-topic-content__inner-wrapper{display:grid;gap:2em;grid-template-columns:repeat(2,minmax(0,1fr))}}.local-render .rhdocs-content{margin:0 auto}.rhdocs cp-documentation{display:block;padding-bottom:2.5rem}.rhdocs cp-documentation.PFElement,.rhdocs cp-documentation[pfelement]{padding:0}rh-table{display:block}::-webkit-scrollbar,:host .rhdocs ::-webkit-scrollbar{height:.625rem;width:.625rem}::-webkit-scrollbar,::-webkit-scrollbar-track,:host .rhdocs ::-webkit-scrollbar,:host .rhdocs ::-webkit-scrollbar-track{background-color:#d6d6d6}::-webkit-scrollbar-thumb,:host .rhdocs ::-webkit-scrollbar-thumb{background-color:#8e8e8e}*,:host .rhdocs *{scrollbar-color:#8e8e8e #d6d6d6}.rhdocs p:empty,p:empty{display:none}.rhdocs[class] h1 code,.rhdocs[class] h2 code,.rhdocs[class] h3 code,.rhdocs[class] h4 code,.rhdocs[class] h5 code,.rhdocs[class] h6 code,[class] h1 code,[class] h2 code,[class] h3 code,[class] h4 code,[class] h5 code,[class] h6 code{background:transparent;border:0;color:inherit;font:inherit;margin:0;padding:0}.pane-page-title h1,.rhdocs__header__primary-wrapper h1{font-family:RedHatDisplay,Red Hat Display,Helvetica Neue,Arial,sans-serif;font-size:2.25rem;line-height:1.333}.rhdocs details[class]{list-style:none;margin:1rem 0 3rem;padding:0}.rhdocs-toc[class]{background:#f2f2f2;margin:1rem 0 2rem;padding:1rem}.rhdocs-toc[class]>:last-child{margin-bottom:0}.rhdocs-toc[class] .rhdocs-toctitle{font-size:1.25rem;font-weight:400;line-height:1.6667;margin-top:0;text-transform:none}.rhdocs-toc[class] li{margin-bottom:.25em;padding-left:.5em}.preamble{margin:0 0 2rem}.sect1{margin:2rem 0 1rem}:host .sect1,cp-documentation .sect1{margin:0 0 2rem;padding:.0625rem 0 0}:host(.cp-documentation--has-external-header) .sect1:first-child>h2:first-child,:host(.cp-documentation--has-external-header) .sect1:first-child>h3:first-child{margin-top:0}.listingblock,.literalblock{margin:1rem 0}.quoteblock,.verseblock{border-left:.25rem solid #d2d2d2;margin:1rem 0;padding:1rem 1rem 1rem 2rem}.quoteblock.pullleft,.verseblock.pullleft{float:left;margin-right:3rem;width:25rem}@media (min-width:768px){.quoteblock.pullleft,.verseblock.pullleft{margin-left:-1rem}}.quoteblock.pullright,.verseblock.pullright{float:right;margin-left:3rem;width:25rem}@media (min-width:768){.quoteblock.pullright,.verseblock.pullright{margin-right:-2rem}}@media (min-width:1100px){.quoteblock.pullright,.verseblock.pullright{margin-right:-10rem}}.quoteblock>:first-child,.verseblock>:first-child{margin-top:0}.quoteblock .content,.verseblock .content{font-family:RedHatText,Red Hat Text,Helvetica Neue,Arial,sans-serif;font-size:1.25rem;line-height:1.6667}.quoteblock .attribution,.verseblock .attribution{font-size:.875rem;font-style:italic;font-weight:600;line-height:1.6667;text-transform:uppercase}.quoteblock .attribution .citetitle,.verseblock .attribution .citetitle{color:#585858}.quoteblock .attribution cite,.verseblock .attribution cite{font-size:1em}.quoteblock blockquote{font-style:italic;margin:0;padding:0}.quoteblock blockquote .content>:first-child{margin-top:0}.quoteblock blockquote .content>:first-child:before{color:#e00;content:"â€œ";display:block;float:left;font-size:2.75rem;font-style:normal;line-height:1.125em;margin-right:.5rem}.quoteblock blockquote .content>:first-child .content>:first-child:before{content:none}.imageblock{margin:1rem 0}.imageblock.pullleft{float:left;margin-right:3rem;width:25rem}@media (min-width:768px){.imageblock.pullleft{margin-left:-1rem}}.imageblock.pullright{float:right;margin-left:3rem;width:25rem}@media (min-width:768){.imageblock.pullright{margin-right:-2rem}}@media (min-width:1100px){.imageblock.pullright{margin-right:-10rem}}.imageblock.interrupter{margin:2rem 0}@media (min-width:768px){.imageblock.interrupter{margin-left:-1rem;margin-right:-2rem}.imageblock.interrupter .caption{margin-left:1rem;margin-right:2rem}}@media (min-width:1100px){.imageblock.interrupter{margin-right:-10rem}.imageblock.interrupter .caption{margin-right:10rem}}.imageblock.interrupter img{max-width:100%}.imageblock .caption{color:#585858;display:block;font-size:.875rem;line-height:1.6667;margin:.5rem 0 0}.rhdocs-footnotes{border-top:.0625rem solid #d2d2d2;margin:3rem 0 1rem;padding:1rem 0 0}.rhdocs-footnotes>ol{margin:0;padding:0 0 0 1.5rem}@supports (counter-reset:footnotenum){.rhdocs-footnotes>ol{counter-reset:footnotenum;list-style:none;padding:0}.rhdocs-footnotes>ol>li{counter-increment:footnotenum}.rhdocs-footnotes>ol>li:before{color:#585858;content:"[" counter(footnotenum) "]";display:inline-block;margin-right:.25rem}}.rhdocs-footer{background:#ededed;color:#151515;font-size:.875rem;line-height:1.6667;margin:3rem 0 0;padding:1rem}.center{margin-left:auto;margin-right:auto}.stretch{width:100%}.visually-hidden{overflow:hidden;position:absolute;clip:rect(0,0,0,0);border:0;height:.0625rem;margin:-.0625rem;padding:0;width:.0625rem}.rh-docs-legal-notice{margin-top:4em}pre,pre[class]{margin:0;padding:1.25em 1em;position:relative}code[class*=language-],pre[class*=language-]{color:#151515;-moz-tab-size:4;-o-tab-size:4;tab-size:4}code.language-none,code.language-text,code.language-txt,pre.language-none,pre.language-text,pre.language-txt{color:#151515}code[class*=language-] ::-moz-selection,code[class*=language-]::-moz-selection,pre[class*=language-] ::-moz-selection,pre[class*=language-]::-moz-selection{background:#cceae7;color:#263238}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{background:#cceae7;color:#263238}:not(pre)>code[class*=language-]{border-radius:.2em;padding:.1em;white-space:normal}.token.atrule{color:#40199a}.token.attr-name{color:#06c}.token.attr-value,.token.attribute{color:#b300b3}.token.boolean{color:#40199a}.token.builtin,.token.cdata,.token.char,.token.class,.token.class-name{color:#06c}.token.comment{color:#6a6e73}.token.constant{color:#40199a}.token.deleted{color:#c9190b}.token.doctype{color:#6a6e73}.token.entity{color:#c9190b}.token.function{color:#40199a}.token.hexcode{color:#b300b3}.token.id,.token.important{color:#40199a;font-weight:700}.token.inserted{color:#06c}.token.keyword{color:#40199a}.token.number{color:#b300b3}.token.operator{color:#06c}.token.prolog{color:#6a6e73}.token.property{color:#06c}.token.pseudo-class,.token.pseudo-element{color:#b300b3}.token.punctuation,.token.regex{color:#06c}.token.selector{color:#c9190b}.token.string{color:#b300b3}.token.symbol{color:#40199a}.token.unit{color:#b300b3}.token.url,.token.variable{color:#c9190b}.rhdocs.local-render{margin:0 auto;max-width:45.8125rem;padding:0 1.5rem}@media print{.field code,.field pre,code[class*=language-],pre,pre[class*=language-]{white-space:pre-wrap!important;word-wrap:break-word!important;overflow-wrap:break-word!important;word-break:break-word!important}}.book-nav__list[class]{display:flex;justify-content:space-between;line-height:var(--jupiter__lineHeight--xs,1.3333);list-style:none;margin:5rem 0 0;padding:0}@media (min-width:1200px){.book-nav__list[class]{display:grid;gap:2rem;grid-template-columns:repeat(2,minmax(0,1fr))}}.book-nav__item a{display:inline-block;font-size:.875rem;font-weight:500;padding-left:1.25rem;position:relative;text-transform:uppercase}.book-nav__item a:before{background:url(/sites/dxp-docs/penumbra-dist/jupiter/images/arrow-down-solid.svg) no-repeat;background-size:contain;content:"";display:block;height:.875rem;left:0;position:absolute;top:.125rem;transform:rotate(90deg);width:.875rem}.book>.titlepage:not(:last-child),.rhdocs .chapter,section[id]{padding-bottom:3.75rem}.book>.titlepage .chapter:last-child,.book>.titlepage section[id]:last-child,.chapter .chapter:last-child,.chapter section[id]:last-child,section[id] .chapter:last-child,section[id] section[id]:last-child{margin-bottom:-3.75rem}.rhdocs .codeblock__wrapper+section[id],pre+section[id]{padding-top:3.75rem}.rhdocs .cta-link{font-size:inherit}.rhdocs a{word-wrap:break-word;overflow-wrap:break-word}.rhdocs .caution,.rhdocs .important,.rhdocs .note,.rhdocs .tip,.rhdocs .warning{padding:.8888888889em;position:relative}.rhdocs .QSIPopOver{bottom:18.75rem!important;top:auto!important}.rhdocs .alert{position:relative}.rhdocs button.dismiss-button{background:none;border:0;cursor:pointer;height:2.5rem;margin-top:-1.25rem;padding:0;position:absolute;right:.3125rem;text-align:center;top:50%;width:2.5rem;z-index:50}.rhdocs button.dismiss-button:after{content:"\f109";display:inline-block;filter:alpha(opacity=30);font-family:rh-web-iconfont;font-size:1.3125rem;font-style:normal;font-variant:normal;font-weight:400;line-height:1;line-height:2.5rem;opacity:.3;text-decoration:inherit;text-rendering:optimizeLegibility;text-transform:none!important;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;font-smoothing:antialiased}.rhdocs .book>.titlepage,.rhdocs .chapter,.rhdocs section[id]{padding-bottom:var(--rh-space-4xl,64px)}.rhdocs .alert{border:0;border-radius:0}.rhdocs .alert>h2:first-child,.rhdocs .alert>h3:first-child,.rhdocs .alert>h4:first-child,.rhdocs .alert>h5:first-child,.rhdocs .alert>h6:first-child,.rhdocs .alert>p:first-child{margin-top:0!important}.rhdocs .alert>p:last-child{margin-bottom:0!important}.rhdocs .alert-w-icon[class]{padding-left:2.8125rem}.rhdocs .alert-w-icon .alert-icon{float:left;font-size:1.125rem;margin-left:-1.875rem;margin-right:.625rem}.rhdocs .alert-w-icon .alert-icon[class*=" rh-icon-"],.rhdocs .alert-w-icon .alert-icon[class^=rh-icon-]{font-size:2.25rem;line-height:1em;margin-left:-2.5rem;margin-top:-.375rem}.rhdocs .alert-w-icon .alert-icon[class*=" icon-innov-prev"],.rhdocs .alert-w-icon .alert-icon[class^=icon-innov-prev]{font-size:1.3125rem;margin-top:.25rem}.rhdocs .alert-w-icon.alert-plain{background:none;color:#151515;padding-left:5rem}.rhdocs .alert-w-icon.alert-plain .alert-icon{font-size:3rem;margin-left:-4.375rem;margin-right:0}.rhdocs .alert-w-icon.alert-plain.alert-success .alert-icon{color:#3f9c35}.rhdocs .alert-w-icon.alert-plain.alert-info .alert-icon{color:#0088ce}.rhdocs .alert-w-icon.alert-plain.alert-warning .alert-icon{color:#f0ab00}.rhdocs .alert-w-icon.alert-plain.alert-danger .alert-icon{color:#e00}#target_banner .copy-url{float:right;margin-top:0}#target_banner .dropdown-menu{font-size:inherit}.titlepage .svg-img[data*="title_logo.svg"]{margin:1.5rem 0;width:15rem}.para{margin:1.49963rem 0}.para[class]{margin-bottom:1.49963rem}dd{margin-bottom:2.5rem}.rhdocs .card-light,.rhdocs .card-light-gray,.rhdocs .card-light-grey{background:#f0f0f0;border:.0625rem solid #f0f0f0;color:#151515}.rhdocs .card-light-gray.push-bottom:first-child,.rhdocs .card-light-grey.push-bottom:first-child,.rhdocs .card-light.push-bottom:first-child{margin-bottom:3.125rem!important}.rhdocs .card-light a.card-link,.rhdocs .card-light h1,.rhdocs .card-light h2,.rhdocs .card-light h3,.rhdocs .card-light h4,.rhdocs .card-light h5,.rhdocs .card-light h6,.rhdocs .card-light-gray a.card-link,.rhdocs .card-light-gray h1,.rhdocs .card-light-gray h2,.rhdocs .card-light-gray h3,.rhdocs .card-light-gray h4,.rhdocs .card-light-gray h5,.rhdocs .card-light-gray h6,.rhdocs .card-light-grey a.card-link,.rhdocs .card-light-grey h1,.rhdocs .card-light-grey h2,.rhdocs .card-light-grey h3,.rhdocs .card-light-grey h4,.rhdocs .card-light-grey h5,.rhdocs .card-light-grey h6{color:#151515}.rhdocs .card-light-gray.card-active:after,.rhdocs .card-light-grey.card-active:after,.rhdocs .card-light.card-active:after{border-top-color:#f0f0f0}.rhdocs .card-md,.rhdocs .card-narrow{display:block;padding:1.1875rem;white-space:normal;word-wrap:break-word}.rhdocs .card .card-heading.card-heading-sm,.rhdocs .card-sm .card .card-heading{font-size:1.0625em;font-weight:500;line-height:1.5}.rhdocs .card .card-heading.card-heading-flush{margin-bottom:.25rem}.rhdocs .card .card-heading.card-heading-red{color:#d10000}.rhdocs .card>p{margin-top:0}.rhdocs .card>p:last-child{margin-bottom:0}.rhdocs .new-experience{background-color:#e7f1fa;border:.0625rem solid #bee1f4;font-size:1rem;margin:1.5rem;padding:1.5rem;position:relative;z-index:1}@media (min-width:48rem){.new-experience{display:flex}.new-experience--contained{left:50%;position:relative;transform:translateX(-50%);width:calc(100vw - 2.5rem)}}.new-experience__primary-content{flex-grow:1}@media (min-width:48rem){.new-experience__primary-content{margin-right:1.25rem}}.new-experience__title{font-size:inherit;font-weight:inherit;line-height:1.6;margin:0;padding:0}.new-experience__title+a,.new-experience__title+pfe-cta{display:inline-block;margin-top:1.5em}.new-experience__secondary-content{min-width:12.5rem}@media (min-width:48rem){.new-experience__secondary-content{text-align:right}}.example{border-left:.3125rem solid #ccc;margin-bottom:2rem;padding:1rem 0 1rem 1rem}dl.calloutlist[class]{display:grid;gap:1.25em .75em;grid-template-columns:min-content 1fr}dl.calloutlist[class] dt{float:none;margin:0;padding:0}dl.calloutlist[class] dd{margin:0;padding:0}dl.calloutlist[class] dd>:first-child{margin-top:0}dl.calloutlist[class] dd>:last-child{margin-bottom:0}.toast{background-color:#000;background-color:rgba(0,0,0,.9);bottom:.9375rem;box-shadow:0 .125rem .3125rem 0 rgba(0,0,0,.26);color:#fff;left:.9375rem;max-width:32.8125rem;min-width:6.25rem;padding:.9375rem;position:fixed;right:.9375rem;transform:translate3d(0,150%,0);transition:transform .2s cubic-bezier(.465,.183,.153,.946);will-change:transform;z-index:999}.toast.show{transform:translateZ(0)}.toast a{color:#fff;text-decoration:underline}.toast a:focus,.toast a:hover{color:#2b9af3}.toast a.btn{text-decoration:none}.toast .btn.btn-link{color:#fff}.toast .close{color:#fff;opacity:.3;text-decoration:none}.toast .close:focus,.toast .close:hover{color:#fff;opacity:.5}.no-csstransforms3d.csstransitions .toast{transition:all .2s cubic-bezier(.465,.183,.153,.946)}.no-csstransforms3d .toast{opacity:0;visibility:hidden}.no-csstransforms3d .toast.show{opacity:1;visibility:visible}.annotator-outer[class][class]{display:none;flex-direction:column;flex-grow:1;height:auto;margin:0;position:static;width:auto}@media (min-width:1400px){.annotator-outer[class][class]{display:flex}}.annotator-frame[class] *{height:auto}@media (min-width:1400px){.annotator-frame .h-sidebar-iframe[class]{position:static;width:calc(100% + 1.5rem)}}.annotator-toolbar[class][class]{position:static;width:auto}.annotator-toolbar>ul,.annotator-toolbar>ul>li{display:block;height:auto;list-style:none;margin:0;padding:0;width:auto}.annotator-toolbar>ul>li{display:flex;justify-content:flex-end}.annotator-frame[class] .annotator-frame-button--sidebar_toggle,.annotator-outer .annotator-frame-button[class][class],.app-content-wrapper *{font-family:RedHatText,Red Hat Text,Helvetica Neue,Arial,sans-serif!important}.annotator-outer .annotator-frame-button[class][class]{font-size:.9375rem;font-weight:500;height:auto;line-height:1.333;margin-right:1.875rem;padding:.75em 1em;position:static}@media (min-width:1400px){.annotator-outer .annotator-frame-button[class][class]{margin-right:0}}.annotator-outer iframe{flex-grow:1;margin-bottom:1.25rem}@media (min-width:1400px){.annotator-outer iframe{min-height:37.5rem}}.producttitle{color:#000;font-size:1.25rem;text-transform:uppercase}.producttitle .productnumber{color:var(--jupiter__palette__red--50,#e00)}.cp-modal-open,.zoom-open{overflow:hidden}.cp-modal,.cp-video-modal,.zoom-modal{bottom:0;display:none;filter:alpha(opacity=0);left:0;opacity:0;outline:0;overflow:hidden;position:fixed;right:0;top:0;transition:all .2s cubic-bezier(.465,.183,.153,.946);z-index:1040;z-index:1050;-webkit-overflow-scrolling:touch}.rhdocs .in.cp-modal,.rhdocs .in.cp-video-modal,.rhdocs .in.zoom-modal{display:block;filter:alpha(opacity=100);opacity:1;overflow-x:hidden;overflow-y:auto}.rhdocs .cp-modal .close,.rhdocs .cp-video-modal .close,.rhdocs .zoom-modal .close{background-color:#fff;border-radius:50%;color:#1a1a1a;font-size:1.75rem;height:28px;height:1.75rem;line-height:1.75rem;margin-bottom:.375rem;margin-top:0;opacity:.9;position:absolute;right:-.5rem;text-shadow:none;top:0;width:28px;width:1.75rem}.cp-modal .close:after,.cp-video-modal .close:after,.zoom-modal .close:after{line-height:1.75rem}.cp-modal-wrap,.zoom-wrap{margin:.625rem;padding-top:.5rem;position:relative}@media (min-width:48rem){.rhdocs .cp-modal-wrap,.rhdocs .zoom-wrap{margin:2.8125rem auto;width:38.4375rem}}@media (min-width:62rem){.rhdocs .cp-modal-wrap,.rhdocs .zoom-wrap{width:49.8958rem}}@media (min-width:75rem){.rhdocs .cp-modal-wrap,.rhdocs .zoom-wrap{width:60.3125rem}}.rhdocs .cp-modal-body :last-child{margin-bottom:0}.rhdocs .cp-modal-backdrop,.rhdocs .zoom-backdrop{background-color:#000;bottom:0;display:none;filter:alpha(opacity=0);left:0;opacity:0;position:fixed;right:0;top:0;transition:opacity .2s cubic-bezier(.465,.183,.153,.946);z-index:1040}.rhdocs .in.cp-modal-backdrop,.rhdocs .in.zoom-backdrop{display:block;filter:alpha(opacity=80);opacity:.8}.rhdocs .cp-modal-body{background:#fff;padding:1.875rem}.rhdocs .cp-modal[data-cp-modal-video=true] .cp-modal-body,.rhdocs .cp-video-modal .cp-modal-body{padding:0}.rhdocs [data-action=zoom]{position:relative}.rhdocs [data-action=zoom]:after{background:rgba(0,0,0,.4);bottom:0;color:#fff;display:inline-block;font-family:rh-web-iconfont;font-style:normal;font-variant:normal;font-weight:400;line-height:1;padding:.375rem;position:absolute;right:0;text-decoration:inherit;text-decoration:none!important;text-rendering:optimizeLegibility;text-transform:none!important;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;font-smoothing:antialiased}.rhdocs [data-action=zoom]:focus:after,.rhdocs [data-action=zoom]:hover:after{background:rgba(0,0,0,.9)}.rhdocs .zoom-wrap .zoom-larger{text-align:center}.rhdocs .zoom-wrap .zoom-larger a{color:#fff}.rhdocs .zoom-wrap .zoom-larger a:focus,.rhdocs .zoom-wrap .zoom-larger a:hover{color:#fff;text-decoration:underline}.rhdocs .zoom-wrap .zoom-larger a:after{content:"â¿»";display:inline-block;margin-left:.25rem}.rhdocs .zoom-body{background:#fff;border-radius:.5rem;margin:0 0 1rem;padding:1rem;text-align:center}.rhdocs .zoom-body .video-wrapper{height:0;overflow:hidden;padding-bottom:56.25%;position:relative}.rhdocs .zoom-body .video-wrapper[data-aspect-ratio="4:3"]{padding-bottom:75%}.rhdocs .zoom-body iframe{height:100%;left:0;position:absolute;top:0;width:100%}.rhdocs .para>.title[class],.rhdocs p.title[class]{font-size:1rem;font-style:normal;font-weight:700;line-height:1.6667;margin:1.25rem 0 0;text-transform:none}.rhdocs .para>.title[class]+.content>:first-child,.rhdocs .para>.title[class]+p,.rhdocs p.title[class]+.content>:first-child,.rhdocs p.title[class]+p{margin-top:0}.rhdocs [class] pre .caution,.rhdocs [class] pre .important,.rhdocs [class] pre .note,.rhdocs [class] pre .tip,.rhdocs [class] pre .warning{background:transparent;border:0;color:inherit;font:inherit;margin:0;padding:0}.rhdocs [class] pre .caution:after,.rhdocs [class] pre .important:after,.rhdocs [class] pre .note:after,.rhdocs [class] pre .tip:after,.rhdocs [class] pre .warning:after{content:none}.rhdocs [class] code.email{background-color:transparent;font:inherit;padding:0}.rhdocs [class] .author{margin-bottom:1.5rem}.rhdocs [class] .author .author{margin-bottom:0}.rhdocs table{margin:2rem 0}.rhdocs [class] table{width:auto}.rhdocs table .table-contents table{max-width:100%;overflow:auto}.rhdocs rh-table table{margin:0;max-width:9999em;overflow:visible}.rhdocs td,.rhdocs th{border-left:0;padding:.5em 1rem;transition:background .25s ease-out}.rhdocs td.content--md[class][class],.rhdocs th.content--md[class][class]{min-width:13em}.rhdocs td.content--lg[class][class],.rhdocs th.content--lg[class][class]{min-width:20em}.rhdocs thead th{padding-top:1.5em}.rhdocs caption{color:currentColor;color:var(--pfe-table__caption--Color,currentColor);font-weight:700;margin-bottom:.5rem;margin-top:.5rem;text-align:center}.rhdocs .revhistory table td,.rhdocs .revhistory table th{border-color:transparent}.rhdocs .revhistory table td{padding:.625rem .875rem}.rhdocs .revhistory table.simplelist{margin:0}@media print{#masthead{display:none!important}}.rh-table--is-full-screen #to-top{display:none}.rhdocs{--rh-table--maxHeight:calc(100vh - 6.25rem);color:#151515;font-family:var(--rh-font-family-body-text,RedHatText,"Red Hat Text","Noto Sans Arabic","Noto Sans Hebrew","Noto Sans JP","Noto Sans KR","Noto Sans Malayalam","Noto Sans SC","Noto Sans TC","Noto Sans Thai",Helvetica,Arial,sans-serif);font-size:var(--rh-body-copy-lage,1.125rem);line-height:1.6667;-moz-tab-size:4;-o-tab-size:4;tab-size:4}.rhdocs rh-codeblock::slotted(#content){border-radius:.25rem;padding:var (--rh-space-lg,16px)}.rhdocs rh-codeblock .screen{display:grid;grid-template-columns:1fr 4.375rem}.rhdocs rh-codeblock[class][class][class][class][class]{max-width:99999em}.rhdocs .codeblock__copy span{display:block;height:0;position:absolute;visibility:hidden;width:0}.rhdocs .codeblock__copy:focus{outline:.0625rem dashed currentcolor}.rhdocs .codeblock__copy svg#icon--copy{height:1rem;width:1rem}.rhdocs pre{border:0;max-height:-moz-max-content;max-height:max-content}.rhdocs pre,pre[class]{margin:0;padding:1.25em 1em;position:relative}.rhdocs rh-code-block>div.codeblock__inner-wrapper>pre,.rhdocs rh-code-block>div.codeblock__inner-wrapper>pre[class]{margin:0;padding:0;position:relative}.rhdocs code[class*=language-],pre[class*=language-]{color:#151515;-moz-tab-size:4;-o-tab-size:4;tab-size:4}.rhdocs code.literal{background:#eee;border-radius:.25rem;color:#000;font-size:.875rem;line-height:1.6667;overflow-wrap:break-word;padding:.125em .5em;word-break:break-word}.rhdocs code.literal,.rhdocs kbd,.rhdocs span.keycap{font-family:RedHatMono,Red Hat Mono,Consolas,monospace}.rhdocs kbd,.rhdocs span.keycap{background-color:#eee;background-image:linear-gradient(180deg,#ddd,#eee,#fff);border-radius:.1875rem;box-shadow:0 -.0625rem 0 0 #fff,0 .0625rem 0 .1875rem #aaa;font-size:90%;font-weight:400;margin:0 .25rem;padding:.125rem .375rem}.rhdocs ol,.rhdocs ul{margin:1rem 0;padding:0 0 0 1.5rem}.rhdocs ._additional-resources[class][class],.rhdocs ._additional-resources[class][class][id]:last-child{background:#fff;border:.0625rem solid #d2d2d2;border-radius:.1875rem;margin:2em 0 4em;padding:2rem 2rem 1rem}.rhdocs ._additional-resources[class][class] ul{border:0;list-style:none;margin:0;padding:0;position:relative}.rhdocs ._additional-resources[class][class] li{border-bottom:.0625rem solid #d2d2d2;box-sizing:content-box;margin:0;padding:1rem 1.5rem 1rem 0;-moz-column-break-inside:avoid;break-inside:avoid}.rhdocs ._additional-resources[class][class] li:last-child{border:0}.rhdocs section.section#additional_resource .additional-resources__heading,.rhdocs section.section#additional_resource .heading,.rhdocs section.section#additional_resource h1,.rhdocs section.section#additional_resource h2,.rhdocs section.section#additional_resource h3,.rhdocs section.section#additional_resource h4,.rhdocs section.section#additional_resource h5,.rhdocs section.section#additional_resource h6,.rhdocs section.section#additional_resource p.title{display:block;font-family:RedHatDisplay,Red Hat Display,Helvetica Neue,Arial,sans-serif;font-size:1.125rem;font-weight:700;line-height:1.5rem;margin:0 0 .5rem;padding:0;text-transform:uppercase}.rhdocs section.section:first-of-type{margin-top:var(--rh-space-4xl,64px)}.rhdocs section.section p{margin-bottom:var(--rh-space-lg,16px);margin-top:0;word-wrap:break-word}.rhdocs .section.section h1,.rhdocs .section.section h2,.rhdocs .section.section h3,.rhdocs .section.section h4,.rhdocs .section.section h5,.rhdocs .section.section h6,.rhdocs h1,.rhdocs h2,.rhdocs h3,.rhdocs h4,.rhdocs h5,.rhdocs h6{font-family:RedHatDisplay,Red Hat Display,Helvetica Neue,Arial,sans-serif;font-weight:400;line-height:1.3333}.rhdocs h1:first-of-type,.rhdocs h2:first-of-type,.rhdocs h3:first-of-type,.rhdocs h4:first-of-type,.rhdocs h5:first-of-type,.rhdocs h6:first-of-type{margin-top:0}.rhdocs h1,.rhdocs h2,.rhdocs h3,.rhdocs h4,.rhdocs h5,.rhdocs h6{font-family:RedHatDisplay,Red Hat Display,Helvetica,Arial,sans-serif;font-weight:400;line-height:1.3333}.rhdocs h2,.rhdocs section.section h2{font-size:var(--rh-font-size-heading-md,1.75rem)}.rhdocs h3,.rhdocs section.section h3{font-size:1.5rem;font-weight:400}.rhdocs dl dt{font-weight:600;margin:.5rem 0}.rhdocs dl{display:block;margin-block-end:1em;margin-block-start:1em;margin-inline-end:0;margin-inline-start:0}.rhdocs .para{margin:1.49963rem 0}.rhdocs dl.calloutlist[class] dt{float:none;margin:0;padding:0}.rhdocs dl.calloutlist[class] dd>:last-child{margin-bottom:0}.rhdocs dl.calloutlist[class]{display:grid;gap:1.25em .75em;grid-template-columns:fit-content(40%) 1fr}.rhdocs .calloutlist dt{clear:left;display:flex;flex-wrap:wrap;float:left;margin:0;padding:0 .5rem 0 0}.rhdocs .calloutlist dt a:not(:first-child){padding-left:4px}.rhdocs dl.calloutlist[class] dd{margin:0;padding:0}.rhdocs .callout,.rhdocs .colist>ol>li:before,.rhdocs .conum{background:#06c;border-radius:50%;color:#fff;display:inline-block;font-family:RedHatText,Red Hat Text,Helvetica Neue,Arial,sans-serif;font-size:.75rem;font-style:normal;font-weight:600;height:1.25rem;line-height:1.35rem;padding:0;position:relative;text-align:center;top:-.125em;vertical-align:middle;width:1.25rem}.rhdocs img,.rhdocs object,.rhdocs svg{display:inline-block;max-width:100%;vertical-align:middle}.rhdocs .titlepage .svg-img[data*="title_logo.svg"]{margin:1.5rem 0;width:15rem}.rhdocs[class] .author{margin-bottom:1.5rem}.rhdocs[class] .author .author{margin-bottom:0}.rhdocs .para>.title[class],p.title[class]{font-size:1rem;font-style:normal;font-weight:700;line-height:1.6667;margin:1.25rem 0 0}.rhdocs .example{border-left:.3125rem solid #ccc;margin-bottom:2rem;padding:1rem 0 1rem 1rem}.rhdocs code{background:#eee;color:#000;font-family:RedHatMono,Red Hat Mono,Consolas,monospace;font-size:.875rem;line-height:1.6667;overflow-wrap:break-word;padding:.125em .5em;word-break:break-word}.rhdocs .para[class]{margin-bottom:1.49963rem}.rhdocs[class] code.email{background-color:transparent;font:inherit;padding:0}rh-alert.admonition #description,rh-alert.admonition p{font-size:var(--rh-font-size-body-text-md,1rem)}rh-alert{width:-moz-fit-content;width:fit-content}.rhdocs .producttitle{color:#000;font-size:1.25rem;text-transform:uppercase}.rhdocs dl{margin:1rem 0}.rhdocs dl dt{font-weight:600;margin:.5rem 0}.rhdocs ol ol{list-style:lower-roman}.rhdocs .codeblock--processed pf-clipboard-copy::part(input),.rhdocs .codeblock--processed pf-clipboard-copy::part(span){display:none}.token.tag{color:#c9190b}.calloutlist div.para{margin:0}rh-alert.admonition{margin-bottom:var(--rh-space-lg,1rem)}.guibutton,.guimenu,.guimenuitem{font-weight:700}.guibutton{font-size:90%;padding:.1875rem}.guibutton:before{content:"["}.guibutton:after{content:"]"}.docs-content-container,.rhdocs{--rh-table--maxHeight:calc(100vh - 6.25rem);color:#151515;font-family:RedHatText,Red Hat Text,Helvetica Neue,Arial,sans-serif;font-size:1.125rem;line-height:1.6667;-moz-tab-size:4;-o-tab-size:4;tab-size:4}pre[hidden]{display:none}.codeblock[class][class][class][class][class]{max-width:99999em}.codeblock__wrapper{background:var(--rh-color-surface-lighter,#f2f2f2);margin:1rem 0;overflow:visible;position:relative;transform:translate(0);z-index:0}.codeblock__inner-wrapper:after{content:"";display:block;min-height:.625rem;width:4.375rem}.codeblock__copy{--pfe-clipboard--icon--Color--hover:#06c;-webkit-appearance:none;-moz-appearance:none;appearance:none;background:#f0efef;height:1.75rem;left:calc(100% - 2.75rem - var(--scrollbar__width, 0px));padding:.3125rem .375rem;position:absolute;top:1rem;width:1.75rem;z-index:2}.codeblock__inner-wrapper pre{border:0;max-height:-moz-max-content;max-height:max-content}.pfe-clipboard:not([copied]) .pfe-clipboard__text--success,:host(:not([copied])) .pfe-clipboard__text--success{display:none!important}.codeblock[class]{margin:0;overflow:visible;padding-right:0}pre{display:block;font-size:.8125rem;line-height:1.42857;margin:0 0 .625rem;word-break:break-all;word-wrap:break-word;background-color:var(--rh-color-surface-lighter,#f2f2f2);border:.0625rem solid #ccc;border-radius:.25rem;color:#333}.docs-content-container pre,.rhdocs pre{background:var(--rh-color-surface-lighter,#f2f2f2);color:#151515;font-family:RedHatMono,Red Hat Mono,Consolas,monospace;font-size:.875rem;line-height:1.6667;overflow-wrap:normal;white-space:pre;word-break:normal}.rhdocs pre[class]{line-height:1.6667;overflow-x:auto}rh-codeblock pre[class][class]{overflow-x:auto}.pfe-clipboard__text--success{background-color:#ddd;border:1px solid #000;border-radius:2px}*,:after,:before{box-sizing:border-box}:root{--rh-space-xs:4px;--rh-space-sm:6px;--rh-space-md:8px;--rh-space-lg:16px;--rh-space-xl:24px;--rh-space-2xl:32px;--rh-space-3xl:48px;--rh-space-4xl:64px;--rh-space-5xl:80px;--rh-space-6xl:96px;--rh-space-7xl:128px;--rh-font-size-body-text-xs:.75rem;--rh-font-size-body-text-sm:.875rem;--rh-font-size-body-text-md:1rem;--rh-font-size-body-text-lg:1.125rem;--rh-font-size-body-text-xl:1.25rem;--rh-font-size-body-text-2xl:1.5rem;--rh-font-size-heading-xs:1.25rem;--rh-font-size-heading-sm:1.5rem;--rh-font-size-heading-md:1.75rem;--rh-font-size-heading-lg:2.25rem;--rh-font-size-heading-xl:2.5rem;--rh-font-size-heading-2xl:3rem;--pfe-navigation--logo--maxWidth:200px;--pfe-navigation__logo--height:40px;--pfe-navigation--fade-transition-delay:500ms;--pfe-navigation__nav-bar--highlight-color:var(--rh-color-brand-red-on-dark,#e00);--pf-global--icon--FontSize--sm:.75rem}body,html{font-family:Red Hat Text,sans-serif;font-size:var(--rh-font-size-body-text-md,1rem);line-height:var(--rh-line-height-body-text,1.5);margin:0}h1,h2,h3,h4,h5,h6{font-family:Red Hat Display,sans-serif;font-weight:400;line-height:var(--rh-line-height-heading,1.3)}h1{font-size:var(--rh-font-size-heading-2xl,3rem);line-height:62px}h2{font-size:var(--rh-font-size-heading-xl,2.5rem);line-height:48px}h3{font-size:var(--rh-font-size-heading-lg,2.25rem)}h4{font-size:var(--rh-font-size-heading-md,2.25rem)}h5{font-size:var(--rh-font-size-heading-sm,2.25rem)}h6{font-size:var(--rh-font-size-heading-xs,2.25rem)}main{line-height:30px}section{padding-bottom:3rem;padding-top:3rem}img{height:auto;max-width:100%}a{color:var(--rh-color-interactive-blue-darker,#06c);text-decoration:none}a:hover{color:var(--rh-color-interactive-blue-darkest,#004080)}rh-alert.html-container a{text-decoration:underline}.container{padding-left:12px;padding-right:12px}.container,.container-fluid{margin-left:auto;margin-right:auto;width:100%}.container-fluid{padding:12px}@media (min-width:576px){.container{max-width:540px}}@media (min-width:768px){.container{max-width:720px}}@media (min-width:992px){.container{max-width:960px}}@media (min-width:1200px){.container{min-width:1140px}}@media (min-width:1400px){.container{min-width:1320px}}.grid{display:grid;gap:var(--rh-space-xl,24px)}.grid-center{margin:auto}.grid.grid-col-2{grid-template-columns:repeat(2,1fr)}.grid.grid-col-3{grid-template-columns:repeat(3,1fr)}.grid.grid-col-4{grid-template-columns:repeat(4,1fr)}.grid.grid-col-5{grid-template-columns:repeat(5,1fr)}.grid.grid-col-6{grid-template-columns:repeat(6,1fr)}.grid.grid-col-7{grid-template-columns:repeat(7,1fr)}.grid.grid-col-8{grid-template-columns:repeat(8,1fr)}.grid.grid-col-9{grid-template-columns:repeat(9,1fr)}.grid.grid-col-10{grid-template-columns:repeat(10,1fr)}.grid.grid-col-11{grid-template-columns:repeat(11,1fr)}.grid.grid-col-12{grid-template-columns:repeat(12,1fr)}@media (min-width:768px){.grid.grid-col-md-2{grid-template-columns:repeat(2,1fr)}.grid.grid-col-md-3{grid-template-columns:repeat(3,1fr)}.grid.grid-col-md-4{grid-template-columns:repeat(4,1fr)}.grid.grid-col-md-5{grid-template-columns:repeat(5,1fr)}.grid.grid-col-md-6{grid-template-columns:repeat(6,1fr)}.grid.grid-col-md-7{grid-template-columns:repeat(7,1fr)}.grid.grid-col-md-8{grid-template-columns:repeat(8,1fr)}.grid.grid-col-md-9{grid-template-columns:repeat(9,1fr)}.grid.grid-col-md-10{grid-template-columns:repeat(10,1fr)}.grid.grid-col-md-11{grid-template-columns:repeat(11,1fr)}.grid.grid-col-md-12{grid-template-columns:repeat(12,1fr)}}@media (min-width:992px){.grid.grid-col-lg-2{grid-template-columns:repeat(2,1fr)}.grid.grid-col-lg-3{grid-template-columns:repeat(3,1fr)}.grid.grid-col-lg-4{grid-template-columns:repeat(4,1fr)}.grid.grid-col-lg-5{grid-template-columns:repeat(5,1fr)}.grid.grid-col-lg-6{grid-template-columns:repeat(6,1fr)}.grid.grid-col-lg-7{grid-template-columns:repeat(7,1fr)}.grid.grid-col-lg-8{grid-template-columns:repeat(8,1fr)}.grid.grid-col-lg-9{grid-template-columns:repeat(9,1fr)}.grid.grid-col-lg-10{grid-template-columns:repeat(10,1fr)}.grid.grid-col-lg-11{grid-template-columns:repeat(11,1fr)}.grid.grid-col-lg-12{grid-template-columns:repeat(12,1fr)}}.span-1{grid-column:span 1}.span-2{grid-column:span 2}.span-3{grid-column:span 3}.span-4{grid-column:span 4}.span-5{grid-column:span 5}.span-6{grid-column:span 6}.span-7{grid-column:span 7}.span-8{grid-column:span 8}.span-9{grid-column:span 9}.span-10{grid-column:span 10}.span-11{grid-column:span 11}.span-12{grid-column:span 12}@media (min-width:399px){.span-xs-1{grid-column:span 1}.span-xs-2{grid-column:span 2}.span-xs-3{grid-column:span 3}.span-xs-4{grid-column:span 4}.span-xs-5{grid-column:span 5}.span-xs-6{grid-column:span 6}.span-xs-7{grid-column:span 7}.span-xs-8{grid-column:span 8}.span-xs-9{grid-column:span 9}.span-xs-10{grid-column:span 10}.span-xs-11{grid-column:span 11}.span-xs-12{grid-column:span 12}}@media (min-width:768px){.span-md-1{grid-column:span 1}.span-md-2{grid-column:span 2}.span-md-3{grid-column:span 3}.span-md-4{grid-column:span 4}.span-md-5{grid-column:span 5}.span-md-6{grid-column:span 6}.span-md-7{grid-column:span 7}.span-md-8{grid-column:span 8}.span-md-9{grid-column:span 9}.span-md-10{grid-column:span 10}.span-md-11{grid-column:span 11}.span-md-12{grid-column:span 12}}@media (min-width:992px){.span-lg-1{grid-column:span 1}.span-lg-2{grid-column:span 2}.span-lg-3{grid-column:span 3}.span-lg-4{grid-column:span 4}.span-lg-5{grid-column:span 5}.span-lg-6{grid-column:span 6}.span-lg-7{grid-column:span 7}.span-lg-8{grid-column:span 8}.span-lg-9{grid-column:span 9}.span-lg-10{grid-column:span 10}.span-lg-11{grid-column:span 11}.span-lg-12{grid-column:span 12}}@media (min-width:1025px){.span-xl-1{grid-column:span 1}.span-xl-2{grid-column:span 2}.span-xl-3{grid-column:span 3}.span-xl-4{grid-column:span 4}.span-xl-5{grid-column:span 5}.span-xl-6{grid-column:span 6}.span-xl-7{grid-column:span 7}.span-xl-8{grid-column:span 8}.span-xl-9{grid-column:span 9}.span-xl-10{grid-column:span 10}.span-xl-11{grid-column:span 11}.span-xl-12{grid-column:span 12}}@media (min-width:1200px){.span-2xl-1{grid-column:span 1}.span-2xl-2{grid-column:span 2}.span-2xl-3{grid-column:span 3}.span-2xl-4{grid-column:span 4}.span-2xl-5{grid-column:span 5}.span-2xl-6{grid-column:span 6}.span-2xl-7{grid-column:span 7}.span-2xl-8{grid-column:span 8}.span-2xl-9{grid-column:span 9}.span-2xl-10{grid-column:span 10}.span-2xl-11{grid-column:span 11}.span-2xl-12{grid-column:span 12}}.flex{display:flex;flex-direction:column;gap:var(--rh-space-lg,16px)}.flex-row{flex-direction:row}.flex-column{flex-direction:column}@media (min-width:768px){.flex-md-row{flex-direction:row}.flex-md-column{flex-direction:column}}.typography-h1{font-size:var(--rh-font-size-heading-2xl,3rem)}.typography-h2{font-size:var(--rh-font-size-heading-xl,2.5rem)}.typography-h3{font-size:var(--rh-font-size-heading-lg,2.25rem)}.typography-h4{font-size:var(--rh-font-size-heading-md,1.75rem)}.typography-h5{font-size:var(--rh-font-size-heading-sm,1.5rem)}.typography-h6{font-size:var(--rh-font-size-heading-xs,1.25rem)}.content section{padding:0}.content h1,.content h2,.content h3,.content h4,.content h5,.content h6{margin:var(--rh-space-lg,16px) 0}.sr-only{height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px;clip:rect(0,0,0,0);border:0}.list-unstyled{list-style:none;padding-left:0}.tooltip-content{align-items:center;display:flex;font-family:Red Hat Text;justify-content:center;text-transform:none}.tooltip-content .check-icon{margin-left:var(--rh-space-md,8px)}.doc-image-link{display:inline-block;text-decoration:none}.modal-img{display:block;width:100%}.modal-helper-text{margin-top:.5rem;text-align:center}.modal-helper-text a{color:#000;cursor:pointer}.modal-helper-text a:hover{text-decoration:underline}.modal-helper-text a:after{content:"⿻";margin-left:.25rem}pf-modal.pf-img-modal{--pf-c-modal-box--MaxHeight:90vh;overflow-y:scroll}pf-modal.pf-img-modal::part(close-button){background-color:#fff;border-radius:50%;color:#000;margin-right:-2rem;margin-top:-2rem}pf-modal.pf-img-modal::part(close-button):hover{opacity:.7}h2.truste-title{line-height:normal;margin-top:0}rh-alert p[slot=header]{color:#002952}@media (width < 992px){html:has(nav.mobile-nav .mobile-nav-wrapper){scroll-behavior:smooth;scroll-padding-top:4rem}html:has(nav.mobile-nav .mobile-jump-links){scroll-padding-top:7rem}html:has(nav.mobile-nav.hide-mobile-nav){scroll-padding-top:2rem}}.highlight{background:#fff4cc;color:#000}</style>
<style>rh-alert[data-v-84359384]{width:100%}</style>
<style>.search-btn[data-v-edc0d12c]{align-items:center;background-color:var(--rh-color-canvas-black,#151515);border:3px solid var(--rh-color-canvas-black,#151515);cursor:pointer;display:flex;flex-direction:column;height:100%;justify-content:center;outline:none;padding:14px var(--rh-space-md,8px)}.search-btn[data-v-edc0d12c]:focus{border-top:3px solid var(--rh-color-accent-brand-on-light,#e00);outline:2px dotted var(--rh-color-white,#fff)}.search-btn .search-icon[data-v-edc0d12c]{height:26px;padding:2px 0 var(--rh-space-xs,4px);width:20px}.search-btn .search-icon[data-v-edc0d12c],.search-icon-helper-text[data-v-edc0d12c]{color:var(--rh-color-white,#fff)}.search-mobile[data-v-edc0d12c]{margin-bottom:var(--rh-space-2xl,32px)}.search-mobile form[data-v-edc0d12c]{display:flex;gap:var(--rh-space-md,8px);margin:auto}.search-box[data-v-edc0d12c]{width:35rem}nav[data-v-edc0d12c]{background-color:#151515;justify-content:space-between;width:100%}a[data-v-edc0d12c],a[data-v-edc0d12c]:visited{color:#fff;display:inline-block;font-size:var(--rh-font-size-body-text-md,1rem)}.skip-link[class][class][data-v-edc0d12c]{font-size:var(--pf-global--FontSize--sm,.875rem);line-height:18px}.skip-link[class][class][data-v-edc0d12c]:focus{border-radius:.21429em;height:auto;left:50%;padding:.42857em .57143em;position:fixed;top:8px;transform:translateX(-50%);width:auto;z-index:99999;clip:auto;background:#fff;background:var(--pfe-navigation__skip-link--BackgroundColor,var(--pfe-theme--color--surface--lightest,#fff));color:#06c;color:var(--pfe-navigation__skip-link--Color,var(--pfe-theme--color--link,#06c));text-decoration:none}.visually-hidden[data-v-edc0d12c]{border:1px solid #06c;height:1px;overflow:hidden;padding:0;position:absolute;width:1px;clip:rect(0,0,0,0);white-space:nowrap}h3[data-v-edc0d12c]{color:#464646;font-family:var(--rh-font-family-heading,"Red Hat Display",Helvetica,Arial,sans-serif);font-size:var(--rh-font-size-body-text-lg,1.125rem)}.language-picker[data-v-edc0d12c]{align-items:center;background-color:#fff;display:flex;flex-direction:column;padding:var(--rh-space-xl,24px);width:100%}.language-picker h3[data-v-edc0d12c]{margin:0;padding:0 1rem 1rem}.language-picker ul[data-v-edc0d12c]{margin:0;padding:0}.language-picker a[data-v-edc0d12c]{color:#06c}.language-picker li[data-v-edc0d12c]{list-style:none}.language-dropdown[data-v-edc0d12c]{background:#fff;box-shadow:0 3px 6px rgba(0,0,0,.098);display:block!important;position:absolute;right:0;width:100%;z-index:104}.pfe-navigation.pfe-navigation--processed>[slot=secondary-links][data-v-edc0d12c]{height:auto;overflow:visible;visibility:visible;width:auto}.upper-navigation[data-v-edc0d12c]{padding:0 var(--rh-space-2xl,32px)}.upper-nav-container[data-v-edc0d12c]{border-bottom:1px solid #404040;margin:0}.upper-nav-hidden[data-v-edc0d12c]:not(:focus):not(:active){clip:rect(0 0 0 0);clip-path:inset(50%);height:1px;overflow:hidden;position:absolute;white-space:nowrap;width:1px}.upper-nav-menu[data-v-edc0d12c]{align-items:center;display:flex;justify-content:flex-end;line-height:1.444;list-style:none;margin-bottom:0;margin-top:0;padding-left:0}.upper-nav-menu[data-v-edc0d12c],.upper-nav-menu>li[data-v-edc0d12c]{position:relative}.upper-nav-menu>li:not(:first-child)>a[data-v-edc0d12c]:before,.upper-nav-menu>li:not(:first-child)>button[data-v-edc0d12c]:before{background-color:#404040;content:"";height:40%;left:0;position:absolute;top:30%;width:1px}li[data-v-edc0d12c]{display:list-item;margin:0;padding:0;text-align:-webkit-match-parent}.upper-nav-menu button.upper-nav-links[data-v-edc0d12c]{border:0;border-top:3px solid transparent;cursor:pointer;line-height:1.444}.upper-nav-menu button.upper-nav-links[aria-expanded=true][data-v-edc0d12c]{outline-color:#151515}.upper-nav-menu button.upper-nav-links[aria-expanded=true] .upper-nav-arrow[data-v-edc0d12c]{filter:invert(0) sepia(2%) saturate(21%) hue-rotate(257deg) brightness(108%) contrast(100%);transform:rotate(270deg)}.upper-nav-menu button.upper-nav-links[aria-expanded=true][data-v-edc0d12c]:before{display:none}.upper-nav-menu .upper-nav-links[data-v-edc0d12c]{background-color:var(--pfe-navigation--BackgroundColor,var(--pfe-theme--color--surface--darkest,#151515));border-top:3px solid transparent;color:#fff;display:block;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:var(--rh-font-size-body-text-sm,.875rem);outline:none;padding:12px 12px 14px;text-decoration:none}.upper-nav-menu .upper-nav-links[data-v-edc0d12c]:hover{border-top-color:#b8bbbe}.upper-nav-menu .upper-nav-links[data-v-edc0d12c]:focus-within{outline:1px dashed var(--rh-color-white,#fff);outline-offset:-1px}.upper-nav-menu .upper-nav-links[data-v-edc0d12c]:focus-within:before{display:none}.upper-nav-dropdown-container[data-v-edc0d12c]{background:#fff;box-shadow:0 3px 6px rgba(0,0,0,.098);display:none;padding:5px 30px 24px;position:absolute;right:0;top:100%;width:500px;z-index:105}.upper-nav-dropdown-container>ul[data-v-edc0d12c]{-moz-column-count:2;column-count:2;list-style-type:none;padding:0;width:auto}.upper-nav-dropdown-container>ul li[data-v-edc0d12c]{color:#151515;font-family:var(--rh-font-family-heading,"Red Hat Display",Helvetica,Arial,sans-serif);font-size:var(--rh-font-size-body-text-sm,.875rem);list-style-type:none;margin-bottom:0}.upper-nav-dropdown-container>ul li span[data-v-edc0d12c]{font-weight:var(--rh-font-weight-body-text-medium,500)}.upper-nav-dropdown-container>ul ul[data-v-edc0d12c]{padding-left:0;padding-top:9px}.upper-nav-dropdown-container>ul>li[data-v-edc0d12c]{padding-top:19px;-moz-column-break-inside:avoid;break-inside:avoid}.upper-nav-dropdown-container>ul>li>ul>li[data-v-edc0d12c]{line-height:1.45;padding:4px 0}.upper-nav-menu .upper-nav-arrow[data-v-edc0d12c]{display:inline-block;filter:invert(100%) sepia(8%) saturate(7%) hue-rotate(1turn) brightness(100%) contrast(93%);height:18px;margin-left:5px;transform:rotate(90deg);vertical-align:middle;width:8px}#pfe-navigation__secondary-links .show[data-v-edc0d12c],.upper-navigation .show[data-v-edc0d12c]{display:block}.upper-nav-menu .upper-nav-links[aria-expanded=true][data-v-edc0d12c]:active,.upper-nav-menu .upper-nav-links[aria-expanded=true][data-v-edc0d12c]:focus,.upper-nav-menu .upper-nav-links[aria-expanded=true][data-v-edc0d12c]:hover{background-color:#fff;color:#151515}.upper-nav-menu .upper-nav-links[aria-expanded=true][data-v-edc0d12c]{background-color:#fff;border-top-color:#b8bbbe;color:#000;position:relative;z-index:1}.upper-nav-dropdown-container>ul a[data-v-edc0d12c]{color:var(--rh-color-accent-base-on-light,#06c);font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:14px;text-decoration:none}.pfe-navigation__search[data-v-edc0d12c]{background-color:var(--rh-color-white,#fff)}.pfe-navigation__search form[data-v-edc0d12c]{display:flex;gap:var(--rh-space-md,8px);margin:auto;max-width:992px}pfe-navigation [slot=secondary-links] .buttons[data-v-edc0d12c]{display:flex;flex-wrap:wrap;gap:var(--rh-space-md,8px);margin-top:4px}pfe-navigation [slot=secondary-links] .buttons a[data-v-edc0d12c]{border:1px solid #d2d2d2;border-radius:3px;color:#06c;cursor:pointer;flex-basis:calc(50% - 5px);font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-weight:var(--rh-font-weight-code-regular,400);padding:1em;text-align:center;text-decoration:none}pfe-navigation [slot=secondary-links] .mobile-lang-select[data-v-edc0d12c]{border:1px solid #d2d2d2;border-bottom-color:#3c3f42;cursor:pointer;display:flex;margin:3rem 0;position:relative}pfe-navigation [slot=secondary-links] .mobile-lang-select label[data-v-edc0d12c]{bottom:100%;color:#000;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:14px;font-weight:500;margin-bottom:5px;position:absolute}pfe-navigation [slot=secondary-links] .mobile-lang-select select[data-v-edc0d12c]{-webkit-appearance:none;-moz-appearance:none;appearance:none;background-color:#fff;border-style:none;color:#000;flex-basis:100%;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:16px;line-height:24px;padding:6px 24px 6px 8px}select[data-v-edc0d12c]{background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' width='10' height='6' fill='none' viewBox='0 0 10 6'%3E%3Cpath fill='%23151515' d='M.678 0h8.644c.596 0 .895.797.497 1.195l-4.372 4.58c-.298.3-.695.3-.993 0L.18 1.196C-.216.797.081 0 .678 0'/%3E%3C/svg%3E");background-position:98% 50%;background-repeat:no-repeat}#inputLabel[data-v-edc0d12c]{align-items:center;display:flex;position:relative}#inputLabel form[data-v-edc0d12c]{width:100%}.input-box[data-v-edc0d12c]{font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:var(--rh-font-size-body-text-md,1rem);height:36px;padding:0 8px;width:100%}.input-box[data-v-edc0d12c]::-moz-placeholder{color:#6a6e73;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:var(--rh-font-size-body-text-md,1rem);font-weight:var(--rh-font-weight-code-regular,400);line-height:24px}.input-box[data-v-edc0d12c]::placeholder{color:#6a6e73;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:var(--rh-font-size-body-text-md,1rem);font-weight:var(--rh-font-weight-code-regular,400);line-height:24px}@media(max-width:960px){.search-box[data-v-edc0d12c]{width:28rem}}@media (max-width:768px){.right-navigation[data-v-edc0d12c],.upper-navigation[data-v-edc0d12c]{display:none}}@media (min-width:767px){.pfe-navigation__search form[data-v-edc0d12c]{padding:var(--rh-space-2xl,32px) 0}}</style>
<style>.element-invisible,.sr-only,.visually-hidden{height:1px;overflow:hidden;padding:0;position:absolute;width:1px;clip:rect(0,0,0,0);border:0;white-space:nowrap}@keyframes reveal-nav{0%{max-height:72px;max-height:var(--pfe-navigation__nav-bar--Height,72px);opacity:0;visibility:hidden}99%{max-height:72px;max-height:var(--pfe-navigation__nav-bar--Height,72px)}to{max-height:9999em;opacity:1;visibility:visible}}@keyframes reveal-nav-parts{0%{max-height:72px;max-height:var(--pfe-navigation__nav-bar--Height,72px);opacity:0;visibility:hidden}1%{visibility:visible}99%{max-height:72px;max-height:var(--pfe-navigation__nav-bar--Height,72px)}to{max-height:9999em;opacity:1;visibility:visible}}@media print{.pfe-navigation__menu,pfe-navigation [slot]{display:none!important}}pfe-navigation{--pfe-broadcasted--text:var(--pfe-theme--color--text,#151515);--pfe-broadcasted--text--muted:var(--pfe-theme--color--text--muted,#6a6e73);--pfe-broadcasted--link:var(--pfe-theme--color--link,#06c);--pfe-broadcasted--link--hover:var(--pfe-theme--color--link--hover,#004080);--pfe-broadcasted--link--focus:var(--pfe-theme--color--link--focus,#004080);--pfe-broadcasted--link--visited:var(--pfe-theme--color--link--visited,#6753ac);--pfe-broadcasted--link-decoration:var(--pfe-theme--link-decoration,none);--pfe-broadcasted--link-decoration--hover:var(--pfe-theme--link-decoration--hover,underline);--pfe-broadcasted--link-decoration--focus:var(--pfe-theme--link-decoration--focus,underline);--pfe-broadcasted--link-decoration--visited:var(--pfe-theme--link-decoration--visited,none)}@supports (display:grid){pfe-navigation{animation:reveal-nav .1618s 4s 1 forwards;max-height:72px;max-height:var(--pfe-navigation__nav-bar--Height,72px)}pfe-navigation>*{animation:reveal-nav-parts .1618s 4s 1 forwards;opacity:0;transition:opacity .1618s ease-in-out;transition:opacity var(--pfe-reveal-duration,.1618s) ease-in-out;visibility:hidden}}pfe-navigation.pfe-navigation--processed,pfe-navigation.pfe-navigation--processed>*{animation:none;opacity:1;visibility:visible}pfe-navigation pfe-primary-detail{display:none}pfe-navigation[pfelement]{display:block}pfe-navigation-dropdown{color:#151515;color:var(--pfe-navigation__dropdown--Color,#151515)}#pfe-navigation[breakpoint=desktop] .hidden-at-desktop[class][class][class],#pfe-navigation[breakpoint=mobile] .hidden-at-mobile[class][class][class],#pfe-navigation[breakpoint=tablet] .hidden-at-tablet[class][class][class],pfe-navigation[breakpoint=desktop] .hidden-at-desktop[class][class][class],pfe-navigation[breakpoint=mobile] .hidden-at-mobile[class][class][class],pfe-navigation[breakpoint=tablet] .hidden-at-tablet[class][class][class]{display:none}#pfe-navigation,#pfe-navigation *,pfe-navigation,pfe-navigation *{box-sizing:border-box}#pfe-navigation [pfelement] .pfe-navigation__log-in-link,pfe-navigation [pfelement] .pfe-navigation__log-in-link{display:none}#pfe-navigation,pfe-navigation{align-items:stretch;background:#151515;background:var(--pfe-navigation__nav-bar--Background,#151515);color:#fff;color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));display:flex;font-family:Red Hat Text,RedHatText,Arial,Helvetica,sans-serif;font-family:var(--pfe-navigation--FontFamily,Red Hat Text,RedHatText,Arial,Helvetica,sans-serif);font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);height:72px;height:var(--pfe-navigation__nav-bar--Height,72px);height:auto;line-height:1.5;margin:0;max-width:9999em;min-height:72px;min-height:var(--pfe-navigation__nav-bar--Height,72px);padding:0 16px;position:relative;z-index:95;z-index:var(--pfe-navigation--ZIndex,var(--pfe-theme--zindex--navigation,95))}@media (min-width:768px){#pfe-navigation,pfe-navigation{flex-wrap:wrap;margin:0;max-width:9999em;padding:0 16px}}@media (min-width:1200px){#pfe-navigation,pfe-navigation{margin:0 auto;padding:0 32px}}#pfe-navigation .pfe-navigation__dropdown,#pfe-navigation pfe-navigation-dropdown,pfe-navigation .pfe-navigation__dropdown,pfe-navigation pfe-navigation-dropdown{display:none}#pfe-navigation>[slot=account],#pfe-navigation>[slot=search],#pfe-navigation>[slot=secondary-links],pfe-navigation>[slot=account],pfe-navigation>[slot=search],pfe-navigation>[slot=secondary-links]{height:0;overflow:hidden;visibility:hidden;width:0}@media (min-width:768px){#pfe-navigation nav.pfe-navigation,pfe-navigation nav.pfe-navigation{align-items:stretch;display:flex;flex-wrap:wrap}}@media (min-width:992px){#pfe-navigation nav.pfe-navigation,pfe-navigation nav.pfe-navigation{flex-wrap:nowrap}}#pfe-navigation .pfe-navigation__logo-wrapper,pfe-navigation .pfe-navigation__logo-wrapper{align-items:center;display:flex;justify-content:flex-start;margin:0;min-width:150px;padding:10px 16px 10px 0}@media (min-width:768px){.pfe-navigation--no-main-menu #pfe-navigation .pfe-navigation__logo-wrapper,.pfe-navigation--no-main-menu pfe-navigation .pfe-navigation__logo-wrapper{margin-right:auto}}.pfe-navigation--collapse-secondary-links .pfe-navigation--no-main-menu #pfe-navigation .pfe-navigation__logo-wrapper,.pfe-navigation--collapse-secondary-links .pfe-navigation--no-main-menu pfe-navigation .pfe-navigation__logo-wrapper{margin-right:0}#pfe-navigation .pfe-navigation__logo-link,pfe-navigation .pfe-navigation__logo-link{border-radius:3px;display:block;margin-left:-8px;outline:0;padding:6px 8px;position:relative}#pfe-navigation .pfe-navigation__logo-link:focus,pfe-navigation .pfe-navigation__logo-link:focus{outline:0}#pfe-navigation .pfe-navigation__logo-link:focus:after,pfe-navigation .pfe-navigation__logo-link:focus:after{border:1px dashed #fff;bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation .pfe-navigation__logo-image,pfe-navigation .pfe-navigation__logo-image{display:block;height:auto;width:100%}@media (min-width:576px){#pfe-navigation .pfe-navigation__logo-image,pfe-navigation .pfe-navigation__logo-image{height:40px;height:var(--pfe-navigation__logo--height,40px);width:auto}}@media print{#pfe-navigation .pfe-navigation__logo-image,pfe-navigation .pfe-navigation__logo-image{display:none}}#pfe-navigation .pfe-navigation__logo-image:only-child,pfe-navigation .pfe-navigation__logo-image:only-child{display:block}@media (min-width:576px){#pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--small,pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--small{height:32px;height:var(--pfe-navigation__logo--height,32px)}}@media print{#pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--screen,pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--screen{display:none!important}}@media screen{#pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--print,pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--print{display:none!important}}#pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--screen.pfe-navigation__logo-image--print,pfe-navigation .pfe-navigation__logo-image.pfe-navigation__logo-image--screen.pfe-navigation__logo-image--print{display:inline-block!important}#pfe-navigation .pfe-navigation__fallback-links a,#pfe-navigation .pfe-navigation__log-in-link,#pfe-navigation .pfe-navigation__menu-link,#pfe-navigation .pfe-navigation__secondary-link,pfe-navigation .pfe-navigation__fallback-links a,pfe-navigation .pfe-navigation__log-in-link,pfe-navigation .pfe-navigation__menu-link,pfe-navigation .pfe-navigation__secondary-link{--pfe-icon--color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));align-items:center;-webkit-appearance:none;-moz-appearance:none;appearance:none;background:0 0;border:0;color:#06c;color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));color:#fff;color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));cursor:pointer;display:flex;font-family:inherit;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);justify-content:flex-start;justify-content:center;margin:0;outline:0;padding:8px 24px;position:relative;text-align:center;text-decoration:none;white-space:nowrap;width:100%}@media print{#pfe-navigation .pfe-navigation__fallback-links a,#pfe-navigation .pfe-navigation__log-in-link,#pfe-navigation .pfe-navigation__menu-link,#pfe-navigation .pfe-navigation__secondary-link,pfe-navigation .pfe-navigation__fallback-links a,pfe-navigation .pfe-navigation__log-in-link,pfe-navigation .pfe-navigation__menu-link,pfe-navigation .pfe-navigation__secondary-link{display:none!important}}@media (min-width:768px){#pfe-navigation .pfe-navigation__fallback-links a,#pfe-navigation .pfe-navigation__log-in-link,#pfe-navigation .pfe-navigation__menu-link,#pfe-navigation .pfe-navigation__secondary-link,pfe-navigation .pfe-navigation__fallback-links a,pfe-navigation .pfe-navigation__log-in-link,pfe-navigation .pfe-navigation__menu-link,pfe-navigation .pfe-navigation__secondary-link{--pfe-icon--color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));color:#fff;color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));display:flex;flex-direction:column;font-size:12px;font-size:var(--pfe-navigation--FontSize--xs,12px);height:72px;height:var(--pfe-navigation__nav-bar--Height,72px);justify-content:flex-end;padding:14px 8px;width:auto}@supports (display:grid){#pfe-navigation .pfe-navigation__fallback-links a,#pfe-navigation .pfe-navigation__log-in-link,#pfe-navigation .pfe-navigation__menu-link,#pfe-navigation .pfe-navigation__secondary-link,pfe-navigation .pfe-navigation__fallback-links a,pfe-navigation .pfe-navigation__log-in-link,pfe-navigation .pfe-navigation__menu-link,pfe-navigation .pfe-navigation__secondary-link{align-items:center;display:grid;grid-template-rows:26px 18px;justify-items:center}}#pfe-navigation .pfe-navigation__fallback-links a[class]:focus,#pfe-navigation .pfe-navigation__fallback-links a[class]:hover,#pfe-navigation .pfe-navigation__log-in-link[class]:focus,#pfe-navigation .pfe-navigation__log-in-link[class]:hover,#pfe-navigation .pfe-navigation__menu-link[class]:focus,#pfe-navigation .pfe-navigation__menu-link[class]:hover,#pfe-navigation .pfe-navigation__secondary-link[class]:focus,#pfe-navigation .pfe-navigation__secondary-link[class]:hover,pfe-navigation .pfe-navigation__fallback-links a[class]:focus,pfe-navigation .pfe-navigation__fallback-links a[class]:hover,pfe-navigation .pfe-navigation__log-in-link[class]:focus,pfe-navigation .pfe-navigation__log-in-link[class]:hover,pfe-navigation .pfe-navigation__menu-link[class]:focus,pfe-navigation .pfe-navigation__menu-link[class]:hover,pfe-navigation .pfe-navigation__secondary-link[class]:focus,pfe-navigation .pfe-navigation__secondary-link[class]:hover{box-shadow:inset 0 4px 0 0 #06c;box-shadow:inset 0 4px 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}}#pfe-navigation .pfe-navigation__fallback-links a:focus,#pfe-navigation .pfe-navigation__fallback-links a:hover,#pfe-navigation .pfe-navigation__log-in-link:focus,#pfe-navigation .pfe-navigation__log-in-link:hover,#pfe-navigation .pfe-navigation__menu-link:focus,#pfe-navigation .pfe-navigation__menu-link:hover,#pfe-navigation .pfe-navigation__secondary-link:focus,#pfe-navigation .pfe-navigation__secondary-link:hover,pfe-navigation .pfe-navigation__fallback-links a:focus,pfe-navigation .pfe-navigation__fallback-links a:hover,pfe-navigation .pfe-navigation__log-in-link:focus,pfe-navigation .pfe-navigation__log-in-link:hover,pfe-navigation .pfe-navigation__menu-link:focus,pfe-navigation .pfe-navigation__menu-link:hover,pfe-navigation .pfe-navigation__secondary-link:focus,pfe-navigation .pfe-navigation__secondary-link:hover{box-shadow:inset 4px 0 0 0 #06c;box-shadow:inset 4px 0 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}@media (min-width:768px){#pfe-navigation .pfe-navigation__fallback-links a:focus,#pfe-navigation .pfe-navigation__fallback-links a:hover,#pfe-navigation .pfe-navigation__log-in-link:focus,#pfe-navigation .pfe-navigation__log-in-link:hover,#pfe-navigation .pfe-navigation__menu-link:focus,#pfe-navigation .pfe-navigation__menu-link:hover,#pfe-navigation .pfe-navigation__secondary-link:focus,#pfe-navigation .pfe-navigation__secondary-link:hover,pfe-navigation .pfe-navigation__fallback-links a:focus,pfe-navigation .pfe-navigation__fallback-links a:hover,pfe-navigation .pfe-navigation__log-in-link:focus,pfe-navigation .pfe-navigation__log-in-link:hover,pfe-navigation .pfe-navigation__menu-link:focus,pfe-navigation .pfe-navigation__menu-link:hover,pfe-navigation .pfe-navigation__secondary-link:focus,pfe-navigation .pfe-navigation__secondary-link:hover{box-shadow:inset 0 4px 0 0 #06c;box-shadow:inset 0 4px 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}}.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__fallback-links a:focus,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__fallback-links a:hover,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__log-in-link:focus,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__log-in-link:hover,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__menu-link:focus,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__menu-link:hover,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__secondary-link:focus,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__secondary-link:hover,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__fallback-links a:focus,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__fallback-links a:hover,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__log-in-link:focus,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__log-in-link:hover,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__menu-link:focus,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__menu-link:hover,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__secondary-link:focus,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__secondary-link:hover{box-shadow:inset 4px 0 0 0 #06c;box-shadow:inset 4px 0 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}#pfe-navigation .pfe-navigation__fallback-links a:focus,#pfe-navigation .pfe-navigation__log-in-link:focus,#pfe-navigation .pfe-navigation__menu-link:focus,#pfe-navigation .pfe-navigation__secondary-link:focus,pfe-navigation .pfe-navigation__fallback-links a:focus,pfe-navigation .pfe-navigation__log-in-link:focus,pfe-navigation .pfe-navigation__menu-link:focus,pfe-navigation .pfe-navigation__secondary-link:focus{outline:0}#pfe-navigation .pfe-navigation__fallback-links a:focus:after,#pfe-navigation .pfe-navigation__log-in-link:focus:after,#pfe-navigation .pfe-navigation__menu-link:focus:after,#pfe-navigation .pfe-navigation__secondary-link:focus:after,pfe-navigation .pfe-navigation__fallback-links a:focus:after,pfe-navigation .pfe-navigation__log-in-link:focus:after,pfe-navigation .pfe-navigation__menu-link:focus:after,pfe-navigation .pfe-navigation__secondary-link:focus:after{border:1px dashed;bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation .pfe-navigation__fallback-links a pfe-icon,#pfe-navigation .pfe-navigation__log-in-link pfe-icon,#pfe-navigation .pfe-navigation__menu-link pfe-icon,#pfe-navigation .pfe-navigation__secondary-link pfe-icon,pfe-navigation .pfe-navigation__fallback-links a pfe-icon,pfe-navigation .pfe-navigation__log-in-link pfe-icon,pfe-navigation .pfe-navigation__menu-link pfe-icon,pfe-navigation .pfe-navigation__secondary-link pfe-icon{pointer-events:none}#pfe-navigation .pfe-navigation__fallback-links a .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__fallback-links a>pfe-icon,#pfe-navigation .pfe-navigation__log-in-link .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__log-in-link>pfe-icon,#pfe-navigation .pfe-navigation__menu-link .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__menu-link>pfe-icon,#pfe-navigation .pfe-navigation__secondary-link .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__secondary-link>pfe-icon,pfe-navigation .pfe-navigation__fallback-links a .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__fallback-links a>pfe-icon,pfe-navigation .pfe-navigation__log-in-link .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__log-in-link>pfe-icon,pfe-navigation .pfe-navigation__menu-link .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__menu-link>pfe-icon,pfe-navigation .pfe-navigation__secondary-link .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__secondary-link>pfe-icon{--pfe-icon--size:18px;padding-right:5px}@media (min-width:768px){#pfe-navigation .pfe-navigation__fallback-links a .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__fallback-links a>pfe-icon,#pfe-navigation .pfe-navigation__log-in-link .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__log-in-link>pfe-icon,#pfe-navigation .pfe-navigation__menu-link .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__menu-link>pfe-icon,#pfe-navigation .pfe-navigation__secondary-link .secondary-link__icon-wrapper,#pfe-navigation .pfe-navigation__secondary-link>pfe-icon,pfe-navigation .pfe-navigation__fallback-links a .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__fallback-links a>pfe-icon,pfe-navigation .pfe-navigation__log-in-link .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__log-in-link>pfe-icon,pfe-navigation .pfe-navigation__menu-link .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__menu-link>pfe-icon,pfe-navigation .pfe-navigation__secondary-link .secondary-link__icon-wrapper,pfe-navigation .pfe-navigation__secondary-link>pfe-icon{padding-right:0;padding:2px 0 4px}}.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__fallback-links a .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__fallback-links a>pfe-icon,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__log-in-link .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__log-in-link>pfe-icon,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__menu-link .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__menu-link>pfe-icon,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__secondary-link .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__secondary-link>pfe-icon,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__fallback-links a .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__fallback-links a>pfe-icon,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__log-in-link .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__log-in-link>pfe-icon,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__menu-link .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__menu-link>pfe-icon,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__secondary-link .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__secondary-link>pfe-icon{padding:0 16px 0 0}#pfe-navigation .pfe-navigation__fallback-links a pfe-icon,#pfe-navigation .pfe-navigation__log-in-link pfe-icon,#pfe-navigation .pfe-navigation__menu-link pfe-icon,#pfe-navigation .pfe-navigation__secondary-link pfe-icon,pfe-navigation .pfe-navigation__fallback-links a pfe-icon,pfe-navigation .pfe-navigation__log-in-link pfe-icon,pfe-navigation .pfe-navigation__menu-link pfe-icon,pfe-navigation .pfe-navigation__secondary-link pfe-icon{display:block;height:18px}#pfe-navigation .pfe-navigation__fallback-links a[class],#pfe-navigation .pfe-navigation__fallback-links a[href],#pfe-navigation .pfe-navigation__log-in-link[class],#pfe-navigation .pfe-navigation__log-in-link[href],#pfe-navigation .pfe-navigation__menu-link[class],#pfe-navigation .pfe-navigation__menu-link[href],#pfe-navigation .pfe-navigation__secondary-link[class],#pfe-navigation .pfe-navigation__secondary-link[href],pfe-navigation .pfe-navigation__fallback-links a[class],pfe-navigation .pfe-navigation__fallback-links a[href],pfe-navigation .pfe-navigation__log-in-link[class],pfe-navigation .pfe-navigation__log-in-link[href],pfe-navigation .pfe-navigation__menu-link[class],pfe-navigation .pfe-navigation__menu-link[href],pfe-navigation .pfe-navigation__secondary-link[class],pfe-navigation .pfe-navigation__secondary-link[href]{align-items:center;justify-content:center}#pfe-navigation .pfe-navigation__account-toggle,#pfe-navigation [slot=account]>a[href],pfe-navigation .pfe-navigation__account-toggle,pfe-navigation [slot=account]>a[href]{--pfe-icon--color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));align-items:center;-webkit-appearance:none;-moz-appearance:none;appearance:none;background:0 0;border:0;color:#06c;color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));cursor:pointer;font-family:inherit;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);justify-content:flex-start;margin:0;outline:0;position:relative;text-align:center;text-decoration:none;white-space:nowrap;width:100%;--pfe-icon--color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));color:#fff;color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));display:flex;flex-direction:column;font-size:12px;font-size:var(--pfe-navigation--FontSize--xs,12px);height:72px;height:var(--pfe-navigation__nav-bar--Height,72px);justify-content:flex-end;padding:14px 8px;width:auto}@media print{#pfe-navigation .pfe-navigation__account-toggle,#pfe-navigation [slot=account]>a[href],pfe-navigation .pfe-navigation__account-toggle,pfe-navigation [slot=account]>a[href]{display:none!important}}#pfe-navigation .pfe-navigation__account-toggle:focus,#pfe-navigation .pfe-navigation__account-toggle:hover,#pfe-navigation [slot=account]>a[href]:focus,#pfe-navigation [slot=account]>a[href]:hover,pfe-navigation .pfe-navigation__account-toggle:focus,pfe-navigation .pfe-navigation__account-toggle:hover,pfe-navigation [slot=account]>a[href]:focus,pfe-navigation [slot=account]>a[href]:hover{box-shadow:inset 4px 0 0 0 #06c;box-shadow:inset 4px 0 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}#pfe-navigation .pfe-navigation__account-toggle:focus,#pfe-navigation [slot=account]>a[href]:focus,pfe-navigation .pfe-navigation__account-toggle:focus,pfe-navigation [slot=account]>a[href]:focus{outline:0}#pfe-navigation .pfe-navigation__account-toggle:focus:after,#pfe-navigation [slot=account]>a[href]:focus:after,pfe-navigation .pfe-navigation__account-toggle:focus:after,pfe-navigation [slot=account]>a[href]:focus:after{border:1px dashed;bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation .pfe-navigation__account-toggle pfe-icon,#pfe-navigation [slot=account]>a[href] pfe-icon,pfe-navigation .pfe-navigation__account-toggle pfe-icon,pfe-navigation [slot=account]>a[href] pfe-icon{pointer-events:none}@supports (display:grid){#pfe-navigation .pfe-navigation__account-toggle,#pfe-navigation [slot=account]>a[href],pfe-navigation .pfe-navigation__account-toggle,pfe-navigation [slot=account]>a[href]{align-items:center;display:grid;grid-template-rows:26px 18px;justify-items:center}}#pfe-navigation .pfe-navigation__account-toggle[class]:focus,#pfe-navigation .pfe-navigation__account-toggle[class]:hover,#pfe-navigation [slot=account]>a[href][class]:focus,#pfe-navigation [slot=account]>a[href][class]:hover,pfe-navigation .pfe-navigation__account-toggle[class]:focus,pfe-navigation .pfe-navigation__account-toggle[class]:hover,pfe-navigation [slot=account]>a[href][class]:focus,pfe-navigation [slot=account]>a[href][class]:hover{box-shadow:inset 0 4px 0 0 #06c;box-shadow:inset 0 4px 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}@media print{#pfe-navigation .pfe-navigation__account-toggle,#pfe-navigation [slot=account]>a[href],pfe-navigation .pfe-navigation__account-toggle,pfe-navigation [slot=account]>a[href]{display:none}}#pfe-navigation .pfe-navigation__account-toggle pfe-icon,#pfe-navigation [slot=account]>a[href] pfe-icon,pfe-navigation .pfe-navigation__account-toggle pfe-icon,pfe-navigation [slot=account]>a[href] pfe-icon{--pfe-icon--size:18px;padding:2px 0 4px}@media (min-width:768px){#pfe-navigation .pfe-navigation__account-toggle pfe-icon,#pfe-navigation [slot=account]>a[href] pfe-icon,pfe-navigation .pfe-navigation__account-toggle pfe-icon,pfe-navigation [slot=account]>a[href] pfe-icon{padding-right:0}}#pfe-navigation .pfe-navigation__account-toggle:focus,#pfe-navigation .pfe-navigation__account-toggle:hover,#pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true],#pfe-navigation [slot=account]>a[href][href]:focus,#pfe-navigation [slot=account]>a[href][href]:hover,pfe-navigation .pfe-navigation__account-toggle:focus,pfe-navigation .pfe-navigation__account-toggle:hover,pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true],pfe-navigation [slot=account]>a[href][href]:focus,pfe-navigation [slot=account]>a[href][href]:hover{box-shadow:inset 0 4px 0 0 #06c;box-shadow:inset 0 4px 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}#pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true],pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true]{--pfe-icon--color:var(--pfe-navigation__nav-bar--Color--active,var(--pfe-theme--color--text,#151515));background:#fff;background:var(--pfe-navigation__nav-bar--toggle--BackgroundColor--active,var(--pfe-theme--color--surface--lightest,#fff));color:#151515;color:var(--pfe-navigation__nav-bar--Color--active,var(--pfe-theme--color--text,#151515))}#pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true]:focus,pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true]:focus{outline:0}#pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true]:focus:after,pfe-navigation .pfe-navigation__account-toggle[aria-expanded=true]:focus:after{border:1px dashed #151515;border:1px dashed var(--pfe-navigation__nav-bar--Color--active,var(--pfe-theme--color--text,#151515));bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation .pfe-navigation__fallback-links,#pfe-navigation .pfe-navigation__menu,pfe-navigation .pfe-navigation__fallback-links,pfe-navigation .pfe-navigation__menu{font-size:inherit;list-style:none;margin:0;padding:0}@media (min-width:768px){#pfe-navigation .pfe-navigation__fallback-links,#pfe-navigation .pfe-navigation__menu,pfe-navigation .pfe-navigation__fallback-links,pfe-navigation .pfe-navigation__menu{align-items:stretch;display:flex}}#pfe-navigation .pfe-navigation__fallback-links li,#pfe-navigation .pfe-navigation__menu li,pfe-navigation .pfe-navigation__fallback-links li,pfe-navigation .pfe-navigation__menu li{font-size:inherit;margin:0;padding:0}#pfe-navigation .pfe-navigation__fallback-links li:before,#pfe-navigation .pfe-navigation__menu li:before,pfe-navigation .pfe-navigation__fallback-links li:before,pfe-navigation .pfe-navigation__menu li:before{content:none}#pfe-navigation .pfe-navigation__fallback-links,pfe-navigation .pfe-navigation__fallback-links{margin-left:auto}#pfe-navigation .pfe-navigation__menu-link,pfe-navigation .pfe-navigation__menu-link{display:flex;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);white-space:nowrap}#pfe-navigation.pfe-navigation--processed,pfe-navigation.pfe-navigation--processed{display:block;padding:0}#pfe-navigation.pfe-navigation--processed:before,pfe-navigation.pfe-navigation--processed:before{content:none}#pfe-navigation.pfe-navigation--processed>[slot=account],#pfe-navigation.pfe-navigation--processed>[slot=search],#pfe-navigation.pfe-navigation--processed>[slot=secondary-links],pfe-navigation.pfe-navigation--processed>[slot=account],pfe-navigation.pfe-navigation--processed>[slot=search],pfe-navigation.pfe-navigation--processed>[slot=secondary-links]{height:auto;overflow:visible;visibility:visible;width:auto}#pfe-navigation.pfe-navigation--processed pfe-navigation-dropdown,pfe-navigation.pfe-navigation--processed pfe-navigation-dropdown{display:block}#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown,#pfe-navigation.pfe-navigation--processed pfe-navigation-dropdown,#pfe-navigation.pfe-navigation--processed>[slot],pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown,pfe-navigation.pfe-navigation--processed pfe-navigation-dropdown,pfe-navigation.pfe-navigation--processed>[slot]{animation:none;opacity:1}#pfe-navigation.pfe-navigation--processed [slot=secondary-links],pfe-navigation.pfe-navigation--processed [slot=secondary-links]{display:block;height:auto;list-style:none;margin:0 0 8px;padding:0;width:auto}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links],pfe-navigation.pfe-navigation--processed [slot=secondary-links]{margin:0}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links],.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]{margin:0 0 8px}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button{--pfe-icon--color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));align-items:center;-webkit-appearance:none;-moz-appearance:none;appearance:none;background:0 0;border:0;color:#06c;color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));cursor:pointer;display:flex;font-family:inherit;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);justify-content:flex-start;margin:0;outline:0;padding:8px 24px;position:relative;text-align:center;text-decoration:none;white-space:nowrap;width:100%}@media print{#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button{display:none!important}}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button{--pfe-icon--color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));color:#fff;color:var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));display:flex;flex-direction:column;font-size:12px;font-size:var(--pfe-navigation--FontSize--xs,12px);height:72px;height:var(--pfe-navigation__nav-bar--Height,72px);justify-content:flex-end;padding:14px 8px;width:auto}@supports (display:grid){#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button{align-items:center;display:grid;grid-template-rows:26px 18px;justify-items:center}}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[class]:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[class]:hover,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[class]:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[class]:hover,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[class]:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[class]:hover,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[class]:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[class]:hover{box-shadow:inset 0 4px 0 0 #06c;box-shadow:inset 0 4px 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:hover,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:hover,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:hover,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:hover{box-shadow:inset 4px 0 0 0 #06c;box-shadow:inset 4px 0 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:hover,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:hover,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:hover,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:hover{box-shadow:inset 0 4px 0 0 #06c;box-shadow:inset 0 4px 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:hover,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:hover,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:hover,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:hover{box-shadow:inset 4px 0 0 0 #06c;box-shadow:inset 4px 0 0 0 var(--pfe-navigation__nav-bar--highlight-color,var(--pfe-theme--color--ui-accent,#06c))}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus{outline:0}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus:after,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus:after,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus:after,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus:after{border:1px dashed;bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a pfe-icon,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button pfe-icon{pointer-events:none}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a .secondary-link__icon-wrapper,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a>pfe-icon,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button .secondary-link__icon-wrapper,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button>pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a .secondary-link__icon-wrapper,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a>pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button .secondary-link__icon-wrapper,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button>pfe-icon{--pfe-icon--size:18px;padding-right:5px}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a .secondary-link__icon-wrapper,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a>pfe-icon,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button .secondary-link__icon-wrapper,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button>pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a .secondary-link__icon-wrapper,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a>pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button .secondary-link__icon-wrapper,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button>pfe-icon{padding-right:0;padding:2px 0 4px}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a>pfe-icon,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button>pfe-icon,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a>pfe-icon,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button .secondary-link__icon-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button>pfe-icon{padding:0 16px 0 0}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a pfe-icon,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a pfe-icon,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button pfe-icon{display:block;height:18px}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus{outline:0}#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus:after,#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus:after,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus:after,pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus:after{border:1px dashed #fff;border:1px dashed var(--pfe-navigation__nav-bar--Color--default,var(--pfe-theme--color--ui-base--text,#fff));bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a:focus,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button:focus{box-shadow:none}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[aria-expanded=true],#pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[aria-expanded=true],pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[aria-expanded=true],pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[aria-expanded=true]{--pfe-icon--color:var(--pfe-navigation__nav-bar--Color--active,var(--pfe-theme--color--text,#151515));background:#fff;background:var(--pfe-navigation__nav-bar--toggle--BackgroundColor--active,var(--pfe-theme--color--surface--lightest,#fff));color:#151515;color:var(--pfe-navigation__nav-bar--Color--active,var(--pfe-theme--color--text,#151515))}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[aria-expanded=true],.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[aria-expanded=true],.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>a[aria-expanded=true],.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links]>button[aria-expanded=true]{background:0 0;box-shadow:none}#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown__wrapper--single-column,pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown__wrapper--single-column{position:relative}#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown__wrapper,pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown__wrapper{display:block}#pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class],pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class]{height:0;transition:height .25s ease-in-out;transition:var(--pfe-navigation--accordion-transition,height .25s ease-in-out)}@media (prefers-reduced-motion){#pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class],pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class]{transition:none}}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class],pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class]{position:absolute;right:0;top:72px;top:var(--pfe-navigation__nav-bar--Height,72px)}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class],.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class]{position:static}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class][aria-hidden=false],pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class][aria-hidden=false]{height:auto}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class][aria-hidden=false],.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed [slot=secondary-links] .pfe-navigation__dropdown-wrapper[class][aria-hidden=false]{height:0}#pfe-navigation.pfe-navigation--processed[breakpoint=mobile] [slot=secondary-links][mobile-slider] .pfe-navigation__dropdown-wrapper,pfe-navigation.pfe-navigation--processed[breakpoint=mobile] [slot=secondary-links][mobile-slider] .pfe-navigation__dropdown-wrapper{left:100vw;left:calc(100vw - 32px);left:calc(100vw - var(--pfe-navigation__mobile-dropdown--PaddingHorizontal,32px));position:absolute;top:0;width:100vw}#pfe-navigation.pfe-navigation--processed[breakpoint=mobile] [slot=secondary-links][mobile-slider] .pfe-navigation__dropdown-wrapper[aria-hidden=false],pfe-navigation.pfe-navigation--processed[breakpoint=mobile] [slot=secondary-links][mobile-slider] .pfe-navigation__dropdown-wrapper[aria-hidden=false]{height:100vh;height:calc(100vh - 72px);height:calc(100vh - var(--pfe-navigation__nav-bar--Height,72px));overflow-y:scroll}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles,#pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown-wrapper,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles,pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown-wrapper{background:#fff;background:var(--pfe-navigation__dropdown--Background,var(--pfe-theme--color--surface--lightest,#fff));padding:0 24px;padding:0 var(--pfe-navigation__dropdown--full-width--spacing--mobile,24px)}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles,#pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown-wrapper,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles,pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown-wrapper{padding:0 64px24px;padding:0 var(--pfe-navigation__dropdown--full-width--spacing--desktop,64px) var(--pfe-navigation__dropdown--full-width--spacing--mobile,24px)}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown-wrapper{padding:0 24px;padding:0 var(--pfe-navigation__dropdown--full-width--spacing--mobile,24px)}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a{border:1px solid transparent;color:#06c;color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));display:inline-block}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a:hover{color:#036;color:var(--pfe-navigation__dropdown--link--Color--hover,#036);text-decoration:underline}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a:focus{border:1px dashed;outline:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level],#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level],#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level],pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level],pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6{margin:32px 0 .75em;margin:var(--pfe-navigation--gutter,32px) 0 .75em;padding:0;-moz-column-break-inside:avoid;break-inside:avoid;color:#464646;color:var(--pfe-navigation__dropdown--headings--Color,#464646);font-family:Red Hat Display,RedHatDisplay,Arial,Helvetica,sans-serif;font-family:var(--pfe-navigation--FontFamilyHeadline,Red Hat Display,RedHatDisplay,Arial,Helvetica,sans-serif);font-size:1.125rem;font-size:var(--pf-global--FontSize--lg,1.125rem);font-weight:500}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level]:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level]:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5:first-child,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level]:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level]:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5:first-child,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6:first-child{margin-top:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a{border:1px solid transparent;color:#464646;color:var(--pfe-navigation__dropdown--headings--Color,#464646);text-decoration:underline}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a:hover{color:#036;color:var(--pfe-navigation__dropdown--link--Color--hover,#036);text-decoration:none}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container .pfe-link-list--header a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container [role=heading][aria-heading-level] a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h2 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h3 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h4 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h5 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container h6 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles .pfe-link-list--header a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles [role=heading][aria-heading-level] a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h2 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h3 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h4 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h5 a:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles h6 a:focus{border:1px dashed;outline:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container li,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles li,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container li,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles li{margin:0 0 16px;-moz-column-break-inside:avoid;break-inside:avoid}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-card,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-card,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container a,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-card,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles a,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-card,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta{-moz-column-break-inside:avoid;break-inside:avoid}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[pfe-priority=primary],#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[priority=primary],#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[pfe-priority=primary],#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[priority=primary],pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[pfe-priority=primary],pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[priority=primary],pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[pfe-priority=primary],pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[priority=primary]{--pfe-cta--BackgroundColor:var(--pfe-navigation__dropdown--pfe-cta--BackgroundColor,#e00);--pfe-cta--BackgroundColor--hover:var(--pfe-navigation__dropdown--pfe-cta--hover--BackgroundColor,#c00);--pfe-theme--ui--border-width:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[pfe-priority=primary]:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[pfe-priority=primary]:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[priority=primary]:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[priority=primary]:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[pfe-priority=primary]:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[pfe-priority=primary]:hover,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[priority=primary]:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[priority=primary]:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[pfe-priority=primary]:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[pfe-priority=primary]:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[priority=primary]:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta[priority=primary]:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[pfe-priority=primary]:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[pfe-priority=primary]:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[priority=primary]:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta[priority=primary]:hover{--pfe-cta--BackgroundColor:var(--pfe-navigation__dropdown--pfe-cta--hover--BackgroundColor,#c00)}pfe-card #pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta,pfe-card #pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta,pfe-card pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container pfe-cta,pfe-card pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles pfe-cta{margin-top:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container li,#pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container ul,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles li,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles ul,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container li,pfe-navigation.pfe-navigation--processed .pfe-navigation-item__tray--container ul,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles li,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--default-styles ul{list-style:none;margin:0;padding:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles,pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles{color:#151515;color:var(--pfe-navigation__dropdown--Color,#151515);-moz-column-count:auto;column-count:auto;display:block;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);gap:0;margin-left:auto;margin-right:auto;max-width:1136px;max-width:var(--pfe-navigation--content-max-width,1136px);padding-bottom:12px;padding-top:12px;width:calc(100% + 32px)}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles,pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles{-moz-column-count:3;column-count:3;display:block;gap:32px;gap:var(--pfe-navigation--gutter,32px);padding-bottom:12px;padding-top:12px}}@media (min-width:1200px){#pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles,pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles{-moz-column-count:auto;column-count:auto;display:flex;flex-wrap:wrap;padding-bottom:32px;padding-top:32px}@supports (display:grid){#pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles,pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles{display:grid;gap:32px;gap:var(--pfe-navigation--gutter,32px);grid-auto-flow:row;grid-template-columns:repeat(4,minmax(0,1fr))}}}.pfe-navigation--collapse-main-menu #pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,.pfe-navigation--collapse-main-menu #pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles,.pfe-navigation--collapse-main-menu pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,.pfe-navigation--collapse-main-menu pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles{-moz-column-count:3;column-count:3;display:block;gap:32px;gap:var(--pfe-navigation--gutter,32px);padding-bottom:12px;padding-top:12px}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles{-moz-column-count:auto;column-count:auto;display:block;gap:0;margin-left:-16px;margin-right:-16px;max-width:1136px;max-width:var(--pfe-navigation--content-max-width,1136px);padding-bottom:12px;padding-top:12px;width:calc(100% + 32px)}.pfe-navigation__menu-item--open #pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,.pfe-navigation__menu-item--open #pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles,.pfe-navigation__menu-item--open pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,.pfe-navigation__menu-item--open pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles{transition-delay:0s;visibility:visible}#pfe-navigation.pfe-navigation--processed .pfe-navigation-grid>*,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles>*,pfe-navigation.pfe-navigation--processed .pfe-navigation-grid>*,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles>*{margin:0 0 18px;-moz-column-break-inside:avoid;break-inside:avoid}@media (min-width:1200px){#pfe-navigation.pfe-navigation--processed .pfe-navigation-grid>*,#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles>*,pfe-navigation.pfe-navigation--processed .pfe-navigation-grid>*,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles>*{margin:0}}.pfe-navigation--collapse-main-menu #pfe-navigation.pfe-navigation--processed .pfe-navigation-grid>*,.pfe-navigation--collapse-main-menu #pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles>*,.pfe-navigation--collapse-main-menu pfe-navigation.pfe-navigation--processed .pfe-navigation-grid>*,.pfe-navigation--collapse-main-menu pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--default-styles>*{margin:0 0 18px}#pfe-navigation.pfe-navigation--processed .pfe-navigation-grid,pfe-navigation.pfe-navigation--processed .pfe-navigation-grid{max-width:100%}#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--1-x,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown--1-x{display:block}#pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown,pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown{background:#fff}#pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher,pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher{margin-left:auto;margin-right:auto;max-width:1136px;max-width:var(--pfe-navigation--content-max-width,1136px);padding:12px 24px;padding:12px var(--pfe-navigation__dropdown--full-width--spacing--mobile,24px)}@media (min-width:1200px){#pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher,pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher{padding:32px 64px;padding:32px var(--pfe-navigation__dropdown--full-width--spacing--desktop,64px)}}.pfe-navigation--collapse-main-menu #pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher,.pfe-navigation--collapse-main-menu pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher{padding:12px 24px;padding:12px var(--pfe-navigation__dropdown--full-width--spacing--mobile,24px)}#pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher .container,pfe-navigation.pfe-navigation--processed .pfe-navigation__site-switcher .pfe-navigation__dropdown site-switcher .container{margin:0;padding:0;width:auto}#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--invisible[class],pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--invisible[class]{padding:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--invisible pfe-navigation-dropdown,pfe-navigation.pfe-navigation--processed .pfe-navigation__dropdown-wrapper--invisible pfe-navigation-dropdown{visibility:hidden}#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class],pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class]{padding:0}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class],pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class]{box-shadow:0 1px 2px rgba(0,0,0,.12);box-shadow:var(--pfe-navigation__dropdown--BoxShadow,0 1px 2px rgba(0,0,0,.12));max-width:100%;min-width:13em;padding:0 32px;position:absolute;top:100%}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class],.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class]{box-shadow:none;max-width:100%;position:static}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class],pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--single-column[class]{right:0}}#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class],pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class]{width:100%}@media (min-width:768px){#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class],pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class]{left:0;position:absolute;right:0}}.pfe-navigation--collapse-secondary-links #pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class],.pfe-navigation--collapse-secondary-links pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class]{position:static}#pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class] .pfe-navigation__dropdown,pfe-navigation.pfe-navigation--processed .pfe-navigation__custom-dropdown--full[class] .pfe-navigation__dropdown{width:100%}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles{padding-left:16px;padding-right:16px}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles form,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles form{align-items:center;display:flex}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button,#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input{padding:10px;transition:box-shadow .2s}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input{border:1px solid #f0f0f0;border-bottom-color:#8b8e91;color:#717579;flex-basis:0%;flex-grow:1;flex-shrink:1;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);margin-right:8px}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input::-moz-placeholder,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input::-moz-placeholder{color:#717579}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input::placeholder,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input::placeholder{color:#717579}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button{background-color:#e00;border:1px solid #e00;border-radius:2px;color:#fff;flex-basis:auto;flex-grow:0;flex-shrink:1;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem)}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:hover{outline:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:focus:after,#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:hover:after,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:focus:after,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles input:hover:after{border:1px dashed #000;bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:focus,#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:hover,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:focus,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:hover{outline:0}#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:focus:after,#pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:hover:after,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:focus:after,pfe-navigation.pfe-navigation--processed .pfe-navigation__search--default-styles button:hover:after{border:1px dashed #fff;bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation .pfe-navigation__site-switcher__back-wrapper,pfe-navigation .pfe-navigation__site-switcher__back-wrapper{border-bottom:1px solid #d2d2d2;border-bottom:var(--pfe-navigation__dropdown--separator--Border,1px solid var(--pfe-theme--color--ui--border--lighter,#d2d2d2));display:block}@media (min-width:768px){#pfe-navigation .pfe-navigation__site-switcher__back-wrapper,pfe-navigation .pfe-navigation__site-switcher__back-wrapper{display:none}}.pfe-navigation--collapse-secondary-links #pfe-navigation .pfe-navigation__site-switcher__back-wrapper,.pfe-navigation--collapse-secondary-links pfe-navigation .pfe-navigation__site-switcher__back-wrapper{display:block}#pfe-navigation .pfe-navigation__site-switcher__back-button,pfe-navigation .pfe-navigation__site-switcher__back-button{background-color:transparent;border:1px solid transparent;color:#06c;color:var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));cursor:pointer;font-size:1rem;font-size:var(--pf-global--FontSize--md,1rem);padding:21px 21px 21px 45px;position:relative;text-align:left;width:100%}#pfe-navigation .pfe-navigation__site-switcher__back-button:before,pfe-navigation .pfe-navigation__site-switcher__back-button:before{border:2px solid #06c;border:2px solid var(--pfe-navigation__dropdown--link--Color,var(--pfe-theme--color--link,#06c));border-right:0;border-top:0;content:"";display:block;height:8px;left:35px;position:absolute;right:auto;top:27px;transform:rotate(45deg);transform-origin:left top;width:8px}#pfe-navigation .pfe-navigation__site-switcher__back-button:focus,#pfe-navigation .pfe-navigation__site-switcher__back-button:hover,pfe-navigation .pfe-navigation__site-switcher__back-button:focus,pfe-navigation .pfe-navigation__site-switcher__back-button:hover{border:1px dashed #151515;border-top:1px dashed #151515;border:1px dashed var(--pfe-navigation__dropdown--Color,#151515);color:#036;color:var(--pfe-navigation__dropdown--link--Color--hover,#036);outline:0}#pfe-navigation.pfe-navigation--processed site-switcher,pfe-navigation.pfe-navigation--processed site-switcher{-moz-columns:auto;columns:auto;display:block}#pfe-navigation.pfe-navigation--stuck,pfe-navigation.pfe-navigation--stuck{left:0;position:fixed;top:0;width:100%;z-index:95;z-index:var(--pfe-theme--zindex--navigation,95)}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__outer-menu-wrapper__inner,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__outer-menu-wrapper__inner{opacity:1!important}#pfe-navigation.pfe-navigation--in-crusty-browser pfe-navigation-account,#pfe-navigation.pfe-navigation--in-crusty-browser rh-account-dropdown,pfe-navigation.pfe-navigation--in-crusty-browser pfe-navigation-account,pfe-navigation.pfe-navigation--in-crusty-browser rh-account-dropdown{display:none!important}#pfe-navigation.pfe-navigation--in-crusty-browser[open-toggle=pfe-navigation__account-toggle] pfe-navigation-account,#pfe-navigation.pfe-navigation--in-crusty-browser[open-toggle=pfe-navigation__account-toggle] rh-account-dropdown,pfe-navigation.pfe-navigation--in-crusty-browser[open-toggle=pfe-navigation__account-toggle] pfe-navigation-account,pfe-navigation.pfe-navigation--in-crusty-browser[open-toggle=pfe-navigation__account-toggle] rh-account-dropdown{display:block!important}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-item,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-item{display:block}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-link[aria-expanded=true],pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-link[aria-expanded=true]{--pfe-icon--color:var(--pfe-navigation__nav-bar--Color--active,var(--pfe-theme--color--text,#151515));background:#fff;background:var(--pfe-navigation__nav-bar--toggle--BackgroundColor--active,var(--pfe-theme--color--surface--lightest,#fff));color:#151515;color:var(--pfe-navigation__nav-bar--Color--active,var(--pfe-theme--color--text,#151515))}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-link[aria-expanded=true]:focus,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-link[aria-expanded=true]:focus{outline:0}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-link[aria-expanded=true]:focus:after,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__menu-link[aria-expanded=true]:focus:after{border:1px dashed;bottom:0;content:"";display:block;left:0;position:absolute;right:0;top:0}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown{display:flex;flex-wrap:wrap}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown>.style-scope,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown>.style-scope{flex-basis:25%}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown .pfe-navigation__footer.style-scope,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown .pfe-navigation__footer.style-scope{flex-basis:100%}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown .pfe-navigation__footer.style-scope>.style-scope,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown .pfe-navigation__footer.style-scope>.style-scope{margin-right:16px}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column,#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column ul,#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column>.style-scope,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column ul,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column>.style-scope{display:flex;flex-direction:column;flex-wrap:nowrap}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column>.style-scope,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__dropdown--single-column>.style-scope{flex-basis:auto}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle,#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link,#pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link,pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a{color:#fff!important;justify-content:center!important}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle[aria-expanded=true],#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link[aria-expanded=true],#pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a[aria-expanded=true],pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle[aria-expanded=true],pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link[aria-expanded=true],pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a[aria-expanded=true]{color:#151515!important}#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__account-wrapper--logged-in .pfe-navigation__log-in-link,#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle .secondary-link__icon-wrapper,#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle pfe-icon,#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link .secondary-link__icon-wrapper,#pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link pfe-icon,#pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a .secondary-link__icon-wrapper,#pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a pfe-icon,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__account-wrapper--logged-in .pfe-navigation__log-in-link,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle .secondary-link__icon-wrapper,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__search-toggle pfe-icon,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link .secondary-link__icon-wrapper,pfe-navigation.pfe-navigation--in-crusty-browser .pfe-navigation__secondary-link pfe-icon,pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a .secondary-link__icon-wrapper,pfe-navigation.pfe-navigation--in-crusty-browser [slot=secondary-links]>a pfe-icon{display:none!important}[id=pfe-navigation__account-dropdown][class][class]{display:block;height:auto;left:0;padding:0;position:absolute;top:72px;top:var(--pfe-navigation__nav-bar--Height,72px);width:100%}[id=pfe-navigation__account-dropdown].pfe-navigation__dropdown-wrapper--invisible[class]{display:none}.pfe-navigation__dropdown-wrapper{overflow:hidden}@media (min-width:768px){.pfe-navigation__custom-dropdown--single-column{min-width:25em}}.pfe-navigation--collapse-secondary-links .pfe-navigation__custom-dropdown--single-column{min-width:0}.secondary-link__icon-wrapper{align-items:center;display:flex;justify-content:center}.secondary-link__alert-count{background:#06c;background:var(--pfe-navigation__nav-bar--alert-color,var(--pfe-theme--color--link,#06c));border-radius:20px;color:#fff;color:var(--pfe-navigation__nav-bar--Color--on-highlight,var(--pfe-theme--color--text--on-saturated,#fff));display:block;font-size:12px;font-size:var(--pfe-navigation--FontSize--xs,12px);line-height:20px;margin:0 4px 0 2px;min-width:23px;overflow:hidden;padding:0 8px}.secondary-link__alert-count:empty{display:none}#pfe-navigation__1x-skip-links{left:0;position:absolute;top:0}#pfe-navigation__1x-skip-links,#pfe-navigation__1x-skip-links li{height:0;list-style:none;margin:0;padding:0;width:0}.skip-link[class][class]{font-size:.875rem;font-size:var(--pf-global--FontSize--sm,.875rem);line-height:18px}.skip-link[class][class]:focus{border-radius:.21429em;height:auto;left:50%;padding:.42857em .57143em;position:fixed;top:8px;transform:translateX(-50%);width:auto;z-index:99999;clip:auto;background:#fff;background:var(--pfe-navigation__skip-link--BackgroundColor,var(--pfe-theme--color--surface--lightest,#fff));color:#06c;color:var(--pfe-navigation__skip-link--Color,var(--pfe-theme--color--link,#06c));text-decoration:none}pfe-navigation pfe-navigation-account[slot=account]{background:#fff;background:var(--pfe-navigation__dropdown--Background,var(--pfe-theme--color--surface--lightest,#fff));width:100%}</style>
<style>:host([size=sm]) #container[data-v-8589d091]{--_size:var(--pf-global--icon--FontSize--sm,12px)}.content-wrapper[data-v-8589d091]{height:auto;margin:0 auto;min-height:46vh}.content[data-v-8589d091]{max-width:1000px}#left-content[data-v-8589d091]{max-width:330px;z-index:1}.line-below-chp[data-v-8589d091]{margin:var(--rh-space-xl,24px) 0 var(--rh-space-3xl,48px)}.toc-container[data-v-8589d091]{border-right:1px solid var(--rh-color-gray-30,#c7c7c7);min-height:100vh;position:sticky;top:0;transition:transform .3s ease-in-out}nav#toc[data-v-8589d091]{height:auto;overflow-y:auto;padding-bottom:var(--rh-space-2xl,32px)}.max-height-85[data-v-8589d091]{max-height:85vh}.max-height-75[data-v-8589d091]{max-height:75vh}.toc-filter[data-v-8589d091]{background-color:#fff;padding:var(--rh-space-lg,16px) var(--rh-space-2xl,32px);position:sticky;top:-1px;width:100%;z-index:1}#text[data-v-8589d091],.toc-filter[data-v-8589d091]{align-items:center;display:flex}#text[data-v-8589d091]{flex:1;flex-direction:row}#search-icon[data-v-8589d091]{color:#151515;left:2.5rem;position:absolute;top:55%;transform:translateY(-50%)}pf-icon[data-v-8589d091]{--pf-icon--size:16px}#text:focus-within #icon[data-v-8589d091],#text:hover #icon[data-v-8589d091]{color:#151515}#text[data-v-8589d091]:after,#text[data-v-8589d091]:before{content:"";inset:0;pointer-events:none;position:absolute}#text-input[data-v-8589d091]:focus,#text-input:focus+#utilities[data-v-8589d091]{border-bottom:2px solid #06c;outline:none}#text-input[data-v-8589d091]{background-color:transparent;border:1px solid #f0f0f0;border-bottom-color:#8a8d90;color:#151515;font-family:inherit;font-size:100%;grid-area:text-input;line-height:1.5;overflow:hidden;padding:.375rem .25rem .375rem 2rem;position:relative;text-overflow:ellipsis;white-space:nowrap;width:100%}#utilities[data-v-8589d091]{align-items:center;border:1px solid #f0f0f0;border-bottom:1px solid #8a8d90;border-left:0;display:flex}#utilities rh-badge[data-v-8589d091]{border-radius:80px;font-weight:var(--rh-font-weight-heading-medium,500);--_background-color:#e0e0e0;margin-right:8px}#clear-button[data-v-8589d091]{--pf-c-button--PaddingTop:0.625rem;--pf-c-button--PaddingRight:.25rem;--pf-c-button--PaddingBottom:0.625rem;--pf-c-button--PaddingLeft:.25rem;margin-right:8px}#text-input.no-right-border[data-v-8589d091]{border-right:0}.btn-container[data-v-8589d091]{bottom:0;display:flex;justify-content:flex-end;padding:1rem;pointer-events:none;position:fixed;right:0;z-index:2}.top-scroll-btn[data-v-8589d091]{--pf-c-button--BorderRadius:64px;pointer-events:all}.focusable[data-v-8589d091]:focus-visible{border:2px solid var(--rh-color-interactive-blue,#06c)}.mobile-nav-wrapper[data-v-8589d091]{align-items:center;border-bottom:1px solid #c7c7c7;display:flex;height:auto;justify-content:space-between;padding:var(--rh-space-sm,.5rem)}.mobile-nav[data-v-8589d091]{align-items:center;background-color:var(--rh-color-bg-page,#fff);min-height:51px;position:sticky;top:0;z-index:5}.active-mobile-menu[data-v-8589d091]{color:#151515;padding-left:.5rem}.hidden[data-v-8589d091]{display:none}.mobile-nav-btn[data-v-8589d091]{background-color:transparent;border:none;font-family:inherit;font-size:.875rem;font-weight:500;margin:0;min-height:40px;min-width:40px}.border-right[data-v-8589d091]{border-right:1px solid #c7c7c7}.toc-focus-container[data-v-8589d091]{position:sticky;top:0;z-index:2}.toc-focus-btn[data-v-8589d091]{align-items:center;background-color:var(--rh-color-white,#fff);border:1px solid var(--rh-color-blue-50,#06c);border-radius:50%;cursor:pointer;display:flex;height:40px;justify-content:center;position:absolute;right:-20px;top:15px;width:40px}.toc-focus-btn[data-v-8589d091]:focus-visible,.toc-focus-btn[data-v-8589d091]:hover{background-color:var(--rh-color-blue-10,#e0f0ff);box-shadow:var(--rh-box-shadow-sm,0 2px 4px 0 hsla(0,0%,8%,.2))}.toc-focus-btn[data-v-8589d091]:focus-visible{border:2px solid var(--rh-color-blue-50,#06c)}.toc-focus-btn-icon[data-v-8589d091]{color:var(--rh-color-blue-50,#06c)}.toc-wrapper[data-v-8589d091]{padding:0}.product-container[data-v-8589d091]{border-bottom:1px solid var(--rh-color-gray-30,#c7c7c7);padding:var(--rh-space-xl,24px) var(--rh-space-2xl,32px)}.product-container h1.product-title[data-v-8589d091]{font-size:var(--rh-font-size-code-xl,1.25rem);line-height:30px;margin-bottom:0;margin-top:0}.product-container .product-version[data-v-8589d091]{align-items:center;display:flex;flex-wrap:wrap;margin-top:var(--rh-space-lg,16px)}.product-version .version-label[data-v-8589d091]{color:var(--rh-color-canvas-black,#151515);font-family:Red Hat Text;font-size:var(--rh-font-size-body-text-sm,.875rem);font-weight:500}.product-version .version-select-dropdown[data-v-8589d091]{margin:var(--rh-space-md,8px) var(--rh-space-sm,6px);max-width:9.75rem;min-height:2rem;min-width:3rem;overflow:hidden;width:-moz-min-content;width:min-content;word-wrap:nowrap;-webkit-appearance:none;-moz-appearance:none;background:var(--rh-color-white,#fff);background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' width='10' height='6' fill='none' viewBox='0 0 10 6'%3E%3Cpath fill='%23151515' d='M.678 0h8.644c.596 0 .895.797.497 1.195l-4.372 4.58c-.298.3-.695.3-.993 0L.18 1.196C-.216.797.081 0 .678 0'/%3E%3C/svg%3E");background-position-x:85%;background-position-y:50%;background-repeat:no-repeat;border:1px solid var(--rh-color-gray-30,#c7c7c7);border-bottom:0;box-shadow:0 -1px 0 0 var(--rh-color-gray-60,#4d4d4d) inset;cursor:pointer;font-size:var(--rh-font-size-body-text-md,1rem);padding:var(--rh-space-md,8px);padding-right:24px;text-overflow:ellipsis}pf-popover[data-v-8589d091]{margin-top:var(--rh-space-md,8px);--pf-c-popover__arrow--BackgroundColor:var(--rh-color-canvas-black,#151515);--pf-c-popover__content--BackgroundColor:var(--rh-color-canvas-black,#151515);--pf-c-popover--BoxShadow:0px 4px 8px 0px #15151540;--pf-c-popover__title-text--Color:var(--rh-color-white,#fff);--pf-c-popover--MaxWidth:300px;--pf-c-popover--MinWidth:300px;--pf-c-popover--c-button--Top:20px;--pf-c-popover--c-button--Right:4px;--pf-c-button--m-plain--hover--Color:var(--rh-color-white,#fff)}pf-popover[data-v-8589d091]::part(content){padding:var(--rh-space-2xl,32px)}pf-popover[data-v-8589d091]::part(body){margin-top:var(--rh-space-lg,16px)}pf-popover[data-v-8589d091]::part(close-button){--pf-c-button--m-plain--focus--Color:var(--rh-color-gray-30,#c7c7c7);--pf-c-button--m-plain--Color:var(--rh-color-gray-30,#c7c7c7)}.popover-header-text[data-v-8589d091]{color:var(--rh-color-white,#fff);font-size:var(--rh-font-size-code-md,1rem);margin:0;max-width:80%;padding:0}.popover-body-link[data-v-8589d091]{color:var(--rh-color-blue-30,#92c5f9);text-decoration:none}.popover-trigger-btn[data-v-8589d091]{align-items:center;background:none;border:none;cursor:pointer;display:flex;justify-content:center}#first-button[data-v-8589d091]{width:80%}#second-button[data-v-8589d091]{text-align:right;width:20%}#toc-btn[data-v-8589d091]{text-align:left;width:100%}.toc-error[data-v-8589d091]{margin:0;max-width:100%}#layout label[data-v-8589d091]{font-weight:500}.page-layout-options[data-v-8589d091]{background-color:#fff;display:flex;flex-direction:column;padding-bottom:var(--rh-space-lg,16px)}.sticky-top[data-v-8589d091]{position:sticky;top:0}summary[data-v-8589d091]{cursor:pointer;list-style:none;position:relative}summary[data-v-8589d091]::-webkit-details-marker{display:none}details#jump-links-details .jump-links-heading[data-v-8589d091]{background-color:var(--rh-color-white,#fff);display:block;margin-top:var(--rh-space-lg,16px);padding:var(--rh-space-lg,16px) 0 0 var(--rh-space-xl,24px);position:sticky;top:0}details#jump-links-details .jump-links-heading[data-v-8589d091]:before{border-right:3px solid #151515;border-top:3px solid #151515;color:#151515;content:"";display:flex;height:9px;left:2px;position:absolute;top:26px;transform:rotate(-135deg);width:9px}details#jump-links-details[open] .jump-links-heading[data-v-8589d091]:before{transform:rotate(135deg)}.table-of-contents>ol[data-v-8589d091]{list-style:none;margin:0;padding:0}nav.table-of-contents[data-v-8589d091]{z-index:1}nav.table-of-contents ol[data-v-8589d091]{list-style:none;margin:0;padding:0}#mobile-browse-docs[data-v-8589d091]{font-weight:var(--rh-font-weight-body-text-medium,500);padding-left:var(--rh-space-2xl,32px)}.docs-content-container[data-v-8589d091]{font-size:var(--rh-font-size-body-text-lg,1.125rem);font-weight:var(--rh-font-weight-body-text-regular,400);line-height:1.6667;padding-left:6rem;padding-right:6rem;padding-top:var(--rh-space-3xl,4rem)}.chapter-title[data-v-8589d091]{font-family:Red Hat Display}h1.chapter-title[data-v-8589d091]{font-size:var(--rh-fontsize-heading-xl,2.25rem);line-height:46.8px;margin:0;padding:0}.chapter .section h4[data-v-8589d091]{font-size:24px;font-weight:400}.banner-wrapper[data-v-8589d091]{padding:3rem 6rem 0}.page-format-dropdown[data-v-8589d091]{-webkit-appearance:none;-moz-appearance:none;appearance:none;background:#fff;background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAABtSURBVHgBhc4xCoAwDAXQxE6li0K76w0cPZqjHsEb6AlcHb2BR9ADdG7GmIKTWv0QSOAFPjrndgAo4TtHxszTD4JoVAhh1VoXiNgkUO+971Q8iGgxxlSy1jc3CGof39baWTrzNSOkkksEbG/oBGEJIn6gD3jAAAAAAElFTkSuQmCC");background-position:8.5rem;background-repeat:no-repeat;background-size:auto;border:1px solid #c7c7c7;box-shadow:inset 0 -1px 0 0 #4d4d4d;font-family:Red Hat Text;font-size:var(--rh-font-size-body-text-md,1rem);margin-top:var(--rh-space-xs,4px);max-width:168px;min-height:36px;min-width:168px;overflow:hidden;padding:0 var(--rh-space-xl,24px) 0 var(--rh-space-md,8px);text-overflow:ellipsis;white-space:nowrap}.content-format-selectors[data-v-8589d091]{color:#151515;font-family:Red Hat Text;font-size:var(--rh-font-size-body-text-sm,.875rem);font-weight:var(--rh-font-weight-body-text-medium,500);margin-right:var(--rh-space-2xl,32px);max-width:250px;min-width:250px;padding-top:var(--rh-space-2xl,32px)}.chapter .section .simpara[data-v-8589d091],.chapter .section p[data-v-8589d091]{font-family:Red Hat Text;font-size:var(--rh-font-size-body-text-lg,1.125rem);font-weight:var(--rh-font-weight-body-text-regular,400);line-height:30px}#toggle-focus-mode[data-v-8589d091]{padding-right:var(--rh-space-lg,1rem)}@keyframes slideaway-left-8589d091{0%{display:block}to{opacity:0;transform:translateX(-40px)}}@keyframes slideaway-right-8589d091{0%{display:block}to{opacity:0;transform:translateX(40px)}}@keyframes enter-left-8589d091{0%{display:none;transform:translateX(-40px)}to{opacity:1}}@keyframes enter-right-8589d091{0%{display:none;transform:translateX(40px)}to{opacity:1}}.hide-left[data-v-8589d091]{animation:slideaway-left-8589d091 .2s;display:none}.enter-left[data-v-8589d091]{animation:enter-left-8589d091 .3s;border-right:none;display:block}.enter-right[data-v-8589d091]{animation:enter-right-8589d091 .3s;display:block}.hide-right[data-v-8589d091]{animation:slideaway-right-8589d091 .2s;display:none}.toc-container.enter-toc-container-left[data-v-8589d091]{transform:translateX(-85%)}.alert-section[data-v-8589d091]{padding-bottom:3rem}rh-alert[data-v-8589d091]{width:auto}@media (min-width:992px){#mobile-nav[data-v-8589d091],#toc-list-mobile[data-v-8589d091]{display:none}}@media (min-width:992px) and (max-width:1400px){.docs-ocp-content-container[data-v-8589d091]{padding:var(--rh-space-4xl,64px) var(--rh-space-lg,16px) 0}}@media (width < 992px){#breadcrumbs[data-v-8589d091],.content-format-selectors[data-v-8589d091],.toc-container[data-v-8589d091]{display:none}#mobile-nav-content-wrapper[data-v-8589d091]{border-bottom:1px solid #c7c7c7;border-top:1px solid #c7c7c7}#toc-wrapper-mobile[data-v-8589d091]{padding:var(--rh-space-md,1.5rem)}.product-container[data-v-8589d091]{padding:var(--rh-space-2xl,32px) var(--rh-space-lg,16px)}.product-container.shrink-product-padding[data-v-8589d091]{padding:var(--rh-space-lg,16px)}.product-version .version-select-dropdown[data-v-8589d091]{margin:0 var(--rh-space-lg,16px)}#page-content-options-mobile[data-v-8589d091]{padding:var(--rh-space-lg,2rem)}label[for=page-format][data-v-8589d091],label[for=toggle-focus-mode][data-v-8589d091]{display:block}.page-format-dropdown[data-v-8589d091]{background-position:97%;max-width:100%}nav#mobile-toc-menu[data-v-8589d091]{max-height:50vh;overflow-y:scroll}.mobile-jump-links #first-button[data-v-8589d091]{width:100%}#jump-links-btn[data-v-8589d091]{text-align:left;width:100%}#mobile-jump-links-content-wrapper[data-v-8589d091]{border-bottom:1px solid #c7c7c7;border-top:1px solid #c7c7c7;padding:0 var(--rh-space-lg,16px) var(--rh-space-2xl,32px)}.table-of-contents #browse-docs[data-v-8589d091]{margin-top:1rem;padding-top:var(--rh-space-md,1.5rem)}.mobile-nav[data-v-8589d091]{display:block}.hide-mobile-nav[data-v-8589d091],.mobile-nav[data-v-8589d091]{transition:transform .3s ease-in-out}.hide-mobile-nav[data-v-8589d091]{transform:translateY(-100%)}.docs-content-container[data-v-8589d091]{padding-left:1.25rem;padding-right:1.25rem;padding-top:var(--rh-space-xl,24px)}.banner-wrapper[data-v-8589d091]{padding:0 1rem}.toc-filter-mobile[data-v-8589d091]{align-items:center;display:flex;padding:var(--rh-space-lg,16px);width:100%}#text-input[data-v-8589d091]{padding-left:.5rem}.toc-filter[data-v-8589d091]{display:none}}@media (width <=576px){.content-format-selectors[data-v-8589d091]{display:none}}.informaltable[data-v-8589d091],.rhdocs .informaltable[data-v-8589d091],.rhdocs .table-contents[data-v-8589d091],.rhdocs .table-wrapper[data-v-8589d091],.table-contents[data-v-8589d091],.table-wrapper[data-v-8589d091]{max-height:var(--rh-table--maxHeight);overflow:auto}rh-table[data-v-8589d091]{display:block;margin:2rem 0;max-width:100%}.pvof-doc__wrapper[data-v-8589d091],.rhdocs[data-v-8589d091]{--rh-table--maxHeight:calc(100vh - 12.5rem)}</style>
<style>:is(rh-footer-block) a[data-v-97dd2752]{text-decoration:underline}</style>
<style>:is(rh-footer,:is(rh-footer-universal,rh-global-footer)) a{color:var(--rh-color-link-inline-on-dark,var(--rh-color-interactive-blue-lighter,#92c5f9));text-decoration:none}:is(rh-footer,:is(rh-footer-universal,rh-global-footer)) a:hover{color:var(--rh-color-link-inline-hover-on-dark,var(--rh-color-interactive-blue-lightest,#b9dafc));text-decoration:underline}:is(rh-footer,:is(rh-footer-universal,rh-global-footer)) a:is(:focus,:focus-within){color:var(--rh-color-link-inline-focus-on-dark,var(--rh-color-interactive-blue-lightest,#b9dafc));text-decoration:underline}:is(rh-footer,:is(rh-footer-universal,rh-global-footer)) a:visited{color:var(--rh-color-link-inline-visited-on-dark,var(--rh-color-interactive-blue-lightest,#b9dafc));text-decoration:none}:is(rh-footer,:is(rh-footer-universal,rh-global-footer)) a[slot^=logo]{display:block}:is(rh-footer) a[slot^=logo]>img{display:block;height:100%;height:var(--rh-size-icon-04,40px);width:auto}:is(rh-footer,rh-footer-universal,rh-global-footer) :is(h1,h2,h3,h4,h5,h6){font-family:var(--rh-font-family-heading,RedHatDisplay,"Red Hat Display","Noto Sans Arabic","Noto Sans Hebrew","Noto Sans JP","Noto Sans KR","Noto Sans Malayalam","Noto Sans SC","Noto Sans TC","Noto Sans Thai",Helvetica,Arial,sans-serif);line-height:var(--rh-line-height-heading,1.3)}rh-footer [slot=links]:is(h1,h2,h3,h4,h5):nth-of-type(n+5){--_link-header-margin:calc(var(--rh-space-2xl, 32px) - var(--rh-space-lg, 16px))}rh-footer [slot^=links] a{gap:var(--rh-footer-links-gap,var(--rh-space-md,8px))}:is(rh-footer,:is(rh-footer-universal,rh-global-footer)) [slot^=links] li{display:contents;margin:0;padding:0}:is(rh-footer,:is(rh-footer-universal,rh-global-footer)) [slot^=links] a{color:var(--rh-color-text-primary-on-dark,#fff)!important;display:block;font-size:var(--rh-footer-link-font-size,var(--rh-font-size-body-text-sm,.875rem));width:-moz-fit-content;width:fit-content}:is(rh-footer-universal,rh-global-footer) [slot^=links] a{font-size:inherit}:is(rh-footer,rh-footer-universal,rh-global-footer){--rh-footer-section-side-gap:var(--rh-space-lg,16px)}@media screen and (min-width:768px){:is(rh-footer,rh-footer-universal,rh-global-footer){--rh-footer-section-side-gap:var(--rh-space-2xl,32px)}}@media screen and (min-width:1440px){:is(rh-footer,rh-footer-universal,rh-global-footer){--rh-footer-section-side-gap:var(--rh-space-4xl,64px)}}rh-footer:not(:defined){background-color:var(--rh-color-surface-darker,#1f1f1f);display:grid;grid-template-areas:"footer" "global";grid-template-rows:1fr auto;min-height:var(--rh-footer-nojs-min-height,750px);width:100%}:is(rh-footer-universal,rh-global-footer):not(:defined):before{grid-area:global}rh-footer:not(:defined)>[slot=logo]{padding:var(--rh-space-2xl,32px) var(--_section-side-gap)}:is(rh-footer-universal,rh-global-footer):not(:defined)>*,rh-footer:not(:defined)>:not([slot=logo],:is(rh-footer-universal,rh-global-footer)){border:0;clip:rect(1px,1px,1px,1px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}:is(rh-footer-universal,rh-global-footer):not(:defined){background-color:var(--rh-color-surface-darkest,#151515);display:block;min-height:176px;width:100%}rh-footer-universal rh-footer-copyright{grid-column:-1/1}</style>
<style>.status-legal .status-page-widget[data-v-5f538988]{display:block;margin:.5rem 0;width:11.3125rem;width:-moz-max-content;width:max-content}.status-page-widget[data-v-5f538988]{align-items:center;display:flex;flex-direction:row}.status-legal .status-page-widget .status-description[data-v-5f538988]{color:#ccc;font-weight:600;letter-spacing:.0125rem;line-height:1.5;margin-right:.5rem}.status-good[data-v-5f538988]{background-color:#3e8536}.status-critical[data-v-5f538988]{background-color:#a30100}.status-partial[data-v-5f538988]{background-color:#f5c12d}.status-maintentance[data-v-5f538988]{background-color:#316dc1}.status-minor[data-v-5f538988]{background-color:#b85c00}.status-description[data-v-5f538988]{color:#ccc;font-weight:500;letter-spacing:.2px;line-height:1.5}.current-status-indicator[data-v-5f538988]{border-radius:6px;display:inline-block;height:12px;margin:0 0 0 5px;width:12px}.current-status-indicator.small[data-v-5f538988]{border-radius:4px;display:inline-block;height:8px;margin:0 0 0 5px;width:8px}</style>
<style>.breadcrumbs[data-v-798f280c]{align-items:center;background-color:#f6f6f6;display:flex;gap:var(--rh-space-xl,24px);padding:var(--rh-space-lg,16px) var(--rh-space-2xl,32px)}nav[data-v-798f280c]{flex:1}ol[data-v-798f280c]{background-color:#f6f6f6;font-size:.875rem;list-style:none;margin:0;padding:0}li[data-v-798f280c]{color:#151515;display:inline}a[data-v-798f280c]:after{border-bottom-color:transparent;border-left-color:transparent;box-shadow:inset .25rem .25rem 0 .0625rem #8a8d8d;content:"";display:inline-block;height:1.07143em;margin:0 .5em;position:relative;right:0;top:.75em;transform:translateY(-.5em) rotate(135deg) scale(.5);width:1.07143em}</style>
<style>ol[data-v-fa0dae77]{margin:0;padding:0}li[data-v-fa0dae77],ol[data-v-fa0dae77],ul[data-v-fa0dae77]{list-style:none;margin:0}.chapter-title[data-v-fa0dae77]{font-size:1em}.sub-chapter-title[data-v-fa0dae77],.sub-chapter-title a[data-v-fa0dae77]{font-size:.875rem}#toc .link[data-v-fa0dae77],#toc-mobile .link[data-v-fa0dae77],.heading[data-v-fa0dae77],.sub-nav .link[data-v-fa0dae77],.sub-nav .link .link[data-v-fa0dae77]{display:block;padding:var(--rh-space-md,8px) var(--rh-space-2xl,32px);padding-right:2.5em;text-decoration:none;transition:background-color .25s}.heading[data-v-fa0dae77]:hover,.link[data-v-fa0dae77]:hover,.sub-nav .link[data-v-fa0dae77]:hover{background:var(--rh-color-surface-lighter,#f2f2f2);box-shadow:inset 3px 0 0 0 #d2d2d2;color:#151515}details[open]:first-of-type>.heading[data-v-fa0dae77]:after{transform:rotate(135deg)}.item[data-v-fa0dae77]{line-height:22px}#toc .link[data-v-fa0dae77],#toc-mobile .link[data-v-fa0dae77]{color:var(--rh-color-text-primary-on-light,#151515)}.sub-nav[data-v-fa0dae77],.toc-wrapper[data-v-fa0dae77]{list-style:none;margin:0}.toc-wrapper[data-v-fa0dae77]{min-width:100%;padding:0}.sub-nav[data-v-fa0dae77]{font-size:1em;line-height:24px;padding-left:1rem;padding-left:16px}.sub-nav .link[data-v-fa0dae77]:hover{color:#151515}.active[data-v-fa0dae77]{background:var(--rh-color-surface-lighter,#f2f2f2);box-shadow:inset 3px 0 0 0 var(--rh-color-icon-primary-on-light,#e00)}.chapter-landing-page[data-v-fa0dae77]{font-weight:500}summary[data-v-fa0dae77]{cursor:pointer;list-style:none;position:relative}summary[data-v-fa0dae77]::-webkit-details-marker{display:none}@keyframes slideDown-fa0dae77{0%{height:0;opacity:0}to{height:var(--details-height-open,"100%");opacity:1}}html[data-v-fa0dae77]{--details-transition-time:400ms}details[data-v-fa0dae77]{max-height:var(--details-height-closed,auto);transition:all ease-out var(--details-transition-time,0)}details[open][data-v-fa0dae77]{max-height:var(--details-height-open,auto)}details .heading.sub-chapter-title[data-v-fa0dae77]:after,details .heading[data-v-fa0dae77]:after{border-right:3px solid #151515;border-top:3px solid #151515;color:#151515;content:"";display:flex;float:right;height:9px;margin-left:16px;position:absolute;right:var(--rh-space-xl,24px);top:14px;transform:rotate(45deg);width:9px}details .heading.sub-chapter-title[data-v-fa0dae77]:after{height:8px;width:8px}</style>
<style>.item[data-v-b883c74f]{line-height:22px}#toc .link[data-v-b883c74f],#toc-mobile .link[data-v-b883c74f]{color:var(--rh-color-text-primary-on-light,#151515)}#toc .link[data-v-b883c74f],#toc-mobile .link[data-v-b883c74f],.heading[data-v-b883c74f],.sub-nav .link[data-v-b883c74f],.sub-nav .link .link[data-v-b883c74f]{display:block;padding:var(--rh-space-md,8px) var(--rh-space-2xl,32px);padding-right:2.5em;text-decoration:none;transition:background-color .25s}.heading[data-v-b883c74f]:hover,.link[data-v-b883c74f]:hover,.sub-nav .link[data-v-b883c74f]:hover{background:var(--rh-color-surface-lighter,#f2f2f2);box-shadow:inset 3px 0 0 0 #d2d2d2;color:#151515}ol[data-v-b883c74f]{margin:0;padding:0}li[data-v-b883c74f],ol[data-v-b883c74f],ul[data-v-b883c74f]{list-style:none;margin:0}.chapter-title[data-v-b883c74f]{font-size:1em}.sub-nav[data-v-b883c74f]{font-size:1em;line-height:24px;list-style:none;margin:0;padding-left:1rem;padding-left:16px}.sub-nav .link[data-v-b883c74f]:hover{color:#151515}.active[data-v-b883c74f]{background:var(--rh-color-surface-lighter,#f2f2f2);box-shadow:inset 3px 0 0 0 var(--rh-color-icon-primary-on-light,#e00)}.sub-chapter-title[data-v-b883c74f],.sub-chapter-title a[data-v-b883c74f]{font-size:.875rem}summary[data-v-b883c74f]{cursor:pointer;list-style:none;position:relative}summary[data-v-b883c74f]::-webkit-details-marker{display:none}html[data-v-b883c74f]{--details-transition-time:400ms}.chapter-landing-page[data-v-b883c74f]{font-weight:500}details[open]:first-of-type>.heading[data-v-b883c74f]:after{transform:rotate(135deg)}details[data-v-b883c74f]{max-height:var(--details-height-closed,auto);transition:all ease-out var(--details-transition-time,0)}details[open][data-v-b883c74f]{max-height:var(--details-height-open,auto)}details .heading.sub-chapter-title[data-v-b883c74f]:after,details .heading[data-v-b883c74f]:after{border-right:3px solid #151515;border-top:3px solid #151515;color:#151515;content:"";display:flex;float:right;height:9px;margin-left:16px;position:absolute;right:var(--rh-space-xl,24px);top:14px;transform:rotate(45deg);width:9px}details .heading.sub-chapter-title[data-v-b883c74f]:after{height:8px;width:8px}</style>
<style>.html-container[data-v-9c2a9ddb]{padding:2rem 1.5rem 0}rh-alert[data-v-9c2a9ddb]{color:#151515}@media (max-width:772px){.html-container[data-v-9c2a9ddb]{padding:3rem 1rem 0}}</style>
<style>.search-container[data-v-69710f44]{height:100%}.form-box[data-v-69710f44],.search-container[data-v-69710f44]{justify-content:center}.form-box[data-v-69710f44],.search-box[data-v-69710f44],.search-container[data-v-69710f44]{align-items:center;display:flex;width:100%}.search-box[data-v-69710f44]{justify-content:space-between;position:relative}ul[data-v-69710f44]{list-style-type:none}#search-list[data-v-69710f44]{background:#fff;border:1px solid #f0f0f0;box-shadow:0 4px 4px 0 #00000040;color:#151515;display:block;left:0;margin:0;padding:0;position:absolute;top:37px;width:100%;z-index:200}#search-list li[data-v-69710f44]{align-items:center;display:flex;justify-content:space-between;padding:.75rem}#search-list .active[data-v-69710f44],#search-list li[data-v-69710f44]:hover{background:#f0f0f0}.group-title[data-v-69710f44]{border-top:1px solid #4d4d4d;color:#4d4d4d;cursor:default;font-size:12px;font-weight:400;pointer-events:none}.group-title[data-v-69710f44],.group-title[data-v-69710f44]:hover{background:#fff}.search-item-text-elipsis[data-v-69710f44]{display:inline-block;max-width:48%;overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.search-item-text[data-v-69710f44]{padding-left:1.75rem}.search-item-chip[data-v-69710f44]{float:right}.search-product-version[data-v-69710f44]{background:#fff;border:1px solid #f0f0f0;border-radius:3px;font-size:1rem;font-weight:400;line-height:24px}.search-icon-form[data-v-69710f44]{color:var(--rh-color-gray-50,#707070);left:12px;position:absolute}.input-search-box[data-v-69710f44]{-webkit-appearance:none;-moz-appearance:none;appearance:none;border:0;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:var(--rh-font-size-body-text-md,1rem);height:36px;padding:0 40px;width:100%}.input-clear-btn[data-v-69710f44]{align-items:center;background-color:transparent;border:none;display:flex;height:36px;justify-content:center;margin-right:-30px;outline:none;transform:translateX(-30px)}.input-clear-btn[data-v-69710f44]:focus{border:1px solid var(--rh-color-accent-base-on-light,#06c)}.input-clear-btn:focus .input-clear-icon[data-v-69710f44]{color:var(--rh-color-canvas-black,#151515)}.input-clear-icon[data-v-69710f44]{color:#6b6e72;cursor:pointer}.input-clear-icon[data-v-69710f44]:hover{color:var(--rh-color-canvas-black,#151515)}.form-submit-btn[data-v-69710f44]::part(button){align-items:center;background-color:var(--rh-color-gray-20,#e0e0e0);border-radius:0;display:flex;height:36px;justify-content:center;--_default-border-color:var(--rh-color-gray-20,#e0e0e0)}.input-close-btn[data-v-69710f44]{background:none;border:none;cursor:pointer;margin:0 var(--rh-space-lg,16px)}.input-close-icon[data-v-69710f44]{color:var(--rh-color-white,#fff)}@media (max-width:992px){.form-box[data-v-69710f44]{gap:var(--rh-space-md,8px);margin:auto;width:100%}.input-search-box[data-v-69710f44]{border:1px solid var(--rh-color-gray-30,#c7c7c7)}.input-search-box[data-v-69710f44]::-moz-placeholder{color:#6a6e73;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:var(--rh-font-size-body-text-md,1rem);font-weight:var(--rh-font-weight-code-regular,400);line-height:24px}.input-search-box[data-v-69710f44]::placeholder{color:#6a6e73;font-family:var(--rh-font-family-body-text,"Red Hat Text","RedHatText",Arial,sans-serif);font-size:var(--rh-font-size-body-text-md,1rem);font-weight:var(--rh-font-weight-code-regular,400);line-height:24px}.search-item-text-elipsis[data-v-69710f44]{max-width:60%}}@media (max-width:767px){.input-close-btn[data-v-69710f44]{display:none}.search-container[data-v-69710f44]{border:1px solid #f0f0f0}}</style>
<link rel="stylesheet" href="/_nuxt/entry.DNAluCmw.css" integrity="sha384-FnrZajt9k3u4tri3ClqikI96k+jP7kyrvgKKgdzBr5Y8CQEi3gusKE0+eMSLvGxm">
<link rel="stylesheet" href="/_nuxt/SearchAutocomplete.DkrJaF8R.css" integrity="sha384-Zw8gf6w7SrpWDmknvrab5QanIN+DSJRS/bPB9/lgDkhR7JhDmG/VF++MTMKU8mKB">
<link rel="stylesheet" href="/_nuxt/Breadcrumbs.BLkLxUMB.css" integrity="sha384-f6iEfCywVoZB0/hIKTRaxywtS21KzhBx2JOI/uVjjU9aqFcP7X9cqLH29w2HQvLe">
<link rel="stylesheet" href="/_nuxt/Alert.fTkXFs3h.css" integrity="sha384-yFMpT5E64LAQi3qJ10AJJ1oOH3DrI68OvLSjSivjNCQXZ25pmFxlPavPQ0TEyEpq">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/C3PvZR0C.js" integrity="sha384-H9Can2ny34kmIBk2SNFJRaBH6FotFjI6LVF8tQ4HqJnqY0a1tGDF/4RHxK2LFs7c">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/Ds9UYxCD.js" integrity="sha384-PFyBx6Pvk48yXM+prD4CDErW/3HFtPEqORVyAch5qDvJSyubA93+vR4jsWGRQKZs">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BUXK7nH-.js" integrity="sha384-Kqf3r1QrD7+NOn9EuK/ba9sTBZjn7vLAjnCoRtq7IKz+QV0JUUBFf1Snf1yVkkgr">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BxBriHwM.js" integrity="sha384-bejD6BN5N1iVRo5Ymn+LdVN0jvrnJTzgqgFD9xQ5+Yi46U6ff+s5W2AISzRWvJaM">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DB5Lt3x5.js" integrity="sha384-Ae/gV/4Dt4jnJApC0CWbckKJ7omq+W9Hb4jjdMyhxGJpwk6FlO7pGYMD3bPO/h1y">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/D8BFaW0S.js" integrity="sha384-/m09v/yYk14L9Hra6/9jYtMTn2IdkKQvzpFPRQ5P/baRKwdZQN98873hqvwuYXFS">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/_58Q44fW.js" integrity="sha384-EdSd8jpnetO/BWDAc8C4SpQExZ7pYjMygNPvbLmiA1TaYsNUdVOqIIyOIzaErEeU">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DOCUiUcq.js" integrity="sha384-9H868K5q7MLzC8TSRXT3pVqDzsOVxZLYvUQ8cLGpHPZCFEyEJjnwSHE+/7FWrJPV">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CiTVMRA7.js" integrity="sha384-xBIJ2xO0t62/oREKqSxPtW1tRxKY+5YYXbGcJ0QR9IRM28JhL+53NOiAvVFn3rmF">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/CR3F0y4K.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/Bn-QuJwp.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/T_zvNmZe.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/Ciddaed-.js">
<link rel="prefetch" as="image" type="image/svg+xml" href="/_nuxt/Footer_Cloud.DpSdW8MR.svg">
<script type="module">
      import "@rhds/elements/rh-cta/rh-cta.js";
      </script>
<script type="module">
      import '@rhds/elements/rh-alert/rh-alert.js';
      </script>
<script type="module">
        import "/scripts/v1/@cpelements/pfe-navigation/dist/pfe-navigation.min.js";
        import "/scripts/v1/@rhds/elements/elements/rh-button/rh-button.js";
      </script>
<script type="module">import "@rhds/elements/rh-footer/rh-footer.js"</script>
<link rel="canonical" href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/configuring_and_managing_high_availability_clusters/index">
<script type="module">
      import "@rhds/elements/rh-alert/rh-alert.js";
      import "@rhds/elements/rh-code-block/rh-code-block.js";
      import '@rhds/elements/rh-cta/rh-cta.js';
      import '@patternfly/elements/pf-switch/pf-switch.js';
      import '@cpelements/rh-table/dist/rh-table.js';
      import '@patternfly/pfe-clipboard/dist/pfe-clipboard.min.js';
      import '@patternfly/elements/pf-button/pf-button.js';
      import '@patternfly/elements/pf-modal/pf-modal.js';
      import '@patternfly/elements/pf-icon/pf-icon.js';
      import '@patternfly/elements/pf-popover/pf-popover.js';
      import '@patternfly/elements/pf-tooltip/pf-tooltip.js';
      import '@rhds/elements/rh-badge/rh-badge.js';
      </script>
<meta name="description" content="Configuring and managing high availability clusters | Red Hat Documentation">
<meta name="app-version" content="v0.0.1">
<script type="module">
        import "/scripts/v1/@rhds/elements/elements/rh-button/rh-button.js";
      </script>
<script type="module">
      import '@rhds/elements/rh-alert/rh-alert.js';
      </script>
<script type="module">
        import "/scripts/v1/@rhds/elements/elements/rh-button/rh-button.js";
        import "@patternfly/elements/pf-badge/pf-badge.js";
      </script>
<script type="module" src="/_nuxt/C3PvZR0C.js" crossorigin integrity="sha384-H9Can2ny34kmIBk2SNFJRaBH6FotFjI6LVF8tQ4HqJnqY0a1tGDF/4RHxK2LFs7c"></script></head><body><div id="__nuxt"><!--[--><!--[--><!----><header data-v-edc0d12c><a href="#pfe-navigation" id="global-skip-to-nav" class="skip-link visually-hidden" data-v-edc0d12c>Skip to navigation</a><a href="#main-content" class="skip-link visually-hidden" data-v-edc0d12c>Skip to content</a><nav id="upper-navigation" class="upper-navigation" aria-labelledby="upper-navigation-label" data-analytics-region="upper-navigation" data-v-edc0d12c><p id="upper-navigation-label" class="upper-nav-hidden" data-v-edc0d12c>Featured links</p><div class="upper-nav-container" data-v-edc0d12c><ul class="upper-nav-menu" data-v-edc0d12c><li data-v-edc0d12c><a href="https://access.redhat.com/" class="upper-nav-links" data-analytics-text="Support" data-analytics-category="Featured Links" data-v-edc0d12c>Support</a></li><li data-v-edc0d12c><a href="https://console.redhat.com/" class="upper-nav-links" data-analytics-text="Console" data-analytics-category="Featured Links" data-v-edc0d12c>Console</a></li><li data-v-edc0d12c><a href="https://developers.redhat.com/" class="upper-nav-links" data-analytics-text="Developers" data-analytics-category="Featured Links" data-v-edc0d12c>Developers</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/en/products/trials" class="upper-nav-links" data-analytics-text="Start a trial" data-analytics-category="Featured Links" data-v-edc0d12c>Start a trial</a></li><li data-v-edc0d12c><button id="all-red-hat" class="upper-nav-links" data-analytics-text="All Red Hat" data-analytics-category="Featured Links" aria-expanded="false" data-analytics-linktype="tab" data-v-edc0d12c>All Red Hat<svg class="upper-nav-arrow" xmlns="http://www.w3.org/2000/svg" width="1024" height="1024" viewBox="0 0 1024 1024" aria-hidden="true" data-v-edc0d12c=""><path d="M810.642 511.557c0 8.905-3.447 16.776-10.284 23.613L322.31 1013.216c-6.835 6.837-14.706 10.284-23.61 10.284s-16.776-3.447-23.613-10.284l-51.303-51.303c-6.837-6.837-10.284-14.707-10.284-23.612s3.447-16.775 10.284-23.61L626.972 511.5 223.784 108.31c-6.837-6.835-10.284-14.706-10.284-23.61s3.447-16.776 10.284-23.613l51.303-51.303C281.924 2.947 289.794-.5 298.7-.5s16.775 3.447 23.61 10.284L800.36 487.83c6.837 6.837 10.284 14.708 10.284 23.613v.114" data-v-edc0d12c=""/></svg></button><div class="upper-nav-dropdown-container" data-v-edc0d12c><ul data-v-edc0d12c><li data-v-edc0d12c><span data-v-edc0d12c>For customers</span><ul data-v-edc0d12c><li data-v-edc0d12c><a href="https://access.redhat.com/support" data-pzn-audience="customers" data-analytics-category="All Red Hat|For customers" data-analytics-text="Customer support" data-v-edc0d12c>Customer support</a></li><li data-v-edc0d12c><a href="/products" data-pzn-audience="customers" data-analytics-category="All Red Hat|For customers" data-analytics-text="Documentation" data-v-edc0d12c>Documentation</a></li><li data-v-edc0d12c><a href="https://access.redhat.com/support/cases" data-pzn-audience="customers" data-analytics-category="All Red Hat|For customers" data-analytics-text="Support cases" data-v-edc0d12c>Support Cases</a></li><li data-v-edc0d12c><a href="https://access.redhat.com/management" data-pzn-audience="customers" data-analytics-category="All Red Hat|For customers" data-analytics-text="Subscription management" data-v-edc0d12c>Subscription management</a></li><li data-v-edc0d12c><a href="https://catalog.redhat.com/" data-analytics-category="All Red Hat|For customers" data-analytics-text="Red Hat Ecosystem Catalog" data-v-edc0d12c>Red Hat Ecosystem Catalog</a></li><li data-v-edc0d12c><a href="https://catalog.redhat.com/partners" data-analytics-category="All Red Hat|For customers" data-analytics-text="Find a partner" data-v-edc0d12c>Find a partner</a></li></ul></li><li data-v-edc0d12c><span data-v-edc0d12c>For partners</span><ul data-v-edc0d12c><li data-v-edc0d12c><a href="https://connect.redhat.com/login" data-pzn-audience="partners" data-analytics-category="All Red Hat|For partners" data-analytics-text="Partner login" data-v-edc0d12c>Partner login</a></li><li data-v-edc0d12c><a href="https://connect.redhat.com/en/support" data-pzn-audience="partners" data-analytics-category="All Red Hat|For partners" data-analytics-text="Partner support" data-v-edc0d12c>Partner support</a></li><li data-v-edc0d12c><a href="https://connect.redhat.com/" data-pzn-audience="partners" data-analytics-category="All Red Hat|For partners" data-analytics-text="Become a partner " data-v-edc0d12c>Become a partner</a></li></ul></li><li data-v-edc0d12c><span data-v-edc0d12c>Try, buy, &amp; sell</span><ul data-v-edc0d12c><li data-v-edc0d12c><a href="https://marketplace.redhat.com/en-us" data-analytics-category="All Red Hat|Try, buy, &amp; sell" data-analytics-text="Red Hat Marketplace" data-v-edc0d12c>Red Hat Marketplace</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/en/store" data-analytics-category="All Red Hat|Try, buy, &amp; sell" data-analytics-text="Red Hat Store" data-v-edc0d12c>Red Hat Store</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/en/contact" data-analytics-category="All Red Hat|Try, buy, &amp; sell" data-analytics-text="Contact sales" data-v-edc0d12c>Contact Sales</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/en/products/trials" data-analytics-category="All Red Hat|Try, buy, &amp; sell" data-analytics-text="Start a trial" data-v-edc0d12c>Start a trial</a></li></ul></li><li data-v-edc0d12c><span data-v-edc0d12c>Learning resources</span><ul data-v-edc0d12c><li data-v-edc0d12c><a href="https://www.redhat.com/en/services/training-and-certification" data-analytics-category="All Red Hat|Learning resources" data-analytics-text="Training and certification " data-v-edc0d12c>Training and certification</a></li><li data-v-edc0d12c><a href="https://developers.redhat.com/" data-pzn-audience="developers|community" data-analytics-category="All Red Hat|Learning resources" data-analytics-text="For developers" data-v-edc0d12c>For developers</a></li><li data-v-edc0d12c><a href="https://cloud.redhat.com/learn" data-analytics-category="All Red Hat|Learning resources" data-analytics-text="Hybrid cloud learning hub" data-v-edc0d12c>Hybrid cloud learning hub</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/en/interactive-labs" data-analytics-category="All Red Hat|Learning resources" data-analytics-text="Interactive labs" data-v-edc0d12c>Interactive labs</a></li><li data-v-edc0d12c><a href="https://learn.redhat.com/" data-analytics-category="All Red Hat|Learning resources" data-analytics-text="Learning community" data-v-edc0d12c>Learning community</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/en/tv" data-analytics-category="All Red Hat|Learning resources" data-analytics-text="Red Hat TV" data-v-edc0d12c>Red Hat TV</a></li></ul></li><li data-v-edc0d12c><span data-v-edc0d12c>Open source communities</span><ul data-v-edc0d12c><li data-v-edc0d12c><a href="https://www.ansible.com/community" data-analytics-category="All Red Hat|Open source communities" data-analytics-text="Ansible" data-v-edc0d12c>Ansible</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/sysadmin/" id="community" data-analytics-category="All Red Hat|Open source communities" data-analytics-text="For system administrators" data-v-edc0d12c>For system administrators</a></li><li data-v-edc0d12c><a href="https://www.redhat.com/architect/" data-pzn-audience="community" data-analytics-category="All Red Hat|Open source communities" data-analytics-text="For architects" data-v-edc0d12c>For architects</a></li></ul></li></ul></div></li></ul></div></nav><pfe-navigation full-width id="pfe-navigation" pf-sticky="true" lang="en" data-v-edc0d12c><nav class="pfe-navigation" aria-label="Main Navigation" data-v-edc0d12c><div class="pfe-navigation__logo-wrapper" id="pfe-navigation__logo-wrapper" data-v-edc0d12c><a href="/en" class="pfe-navigation__logo-link" data-v-edc0d12c><img class="pfe-navigation__logo-image pfe-navigation__logo-image--screen pfe-navigation__logo-image--small" src="/Logo-Red_Hat-Documentation-A-Reverse-RGB.svg" width="240" height="40" alt="Red Hat Documentation" data-v-edc0d12c></a></div></nav><span data-v-edc0d12c></span><div slot="secondary-links" data-v-edc0d12c><div class="hidden-at-desktop hidden-at-tablet search-mobile" data-v-edc0d12c><div id="search-form" class="search-container" opensearchbox="true" data-v-edc0d12c data-v-69710f44><form role="search" class="form-box" autocomplete="off" data-v-69710f44><div class="search-box" data-v-69710f44><pf-icon icon="search" size="md" class="search-icon-form" data-v-69710f44></pf-icon><input type="text" id="input-search" class="input-search-box" placeholder="Search documentation" value aria-autocomplete="list" data-v-69710f44><!----><!----><!----><rh-button disabled variant="tertiary" class="form-submit-btn" data-v-69710f44><img src="data:image/svg+xml,%3csvg%20width=&#39;14&#39;%20height=&#39;14&#39;%20viewBox=&#39;0%200%2014%2014&#39;%20fill=&#39;none&#39;%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%3e%3cpath%20d=&#39;M7%200L5.6%201.4L10.3%206H0V8H10.3L5.6%2012.6L7%2014L14%207L7%200Z&#39;%20fill=&#39;%23707070&#39;/%3e%3c/svg%3e" alt="Submit button" data-v-69710f44></rh-button></div></form><!----></div></div><div class="hidden-at-desktop hidden-at-tablet buttons" data-v-edc0d12c><a href="https://access.redhat.com/" data-analytics-category="More Red Hat" data-analytics-text="Support" data-v-edc0d12c>Support</a><a href="https://console.redhat.com/" data-analytics-category="More Red Hat" data-analytics-text="Console" data-v-edc0d12c>Console</a><a href="https://developers.redhat.com/" data-analytics-category="More Red Hat" data-analytics-text="Developers" data-v-edc0d12c>Developers</a><a href="https://www.redhat.com/en/products/trials" data-analytics-category="More Red Hat" data-analytics-text="Start a trial" data-v-edc0d12c>Start a trial</a><a href="https://www.redhat.com/en/contact" data-analytics-category="More Red Hat" data-analytics-text="Contact" data-v-edc0d12c>Contact</a></div><div class="hidden-at-desktop hidden-at-tablet mobile-lang-select" data-v-edc0d12c><label for="lang_selection" data-v-edc0d12c>Select your language</label><select id="lang_selection" data-v-edc0d12c><!--[--><option value="en" xml:lang="en" hreflang="en" data-v-edc0d12c>English</option><option value="fr" xml:lang="fr" hreflang="fr" data-v-edc0d12c>Français</option><option value="ko" xml:lang="ko" hreflang="ko" data-v-edc0d12c>한국어</option><option value="ja" xml:lang="ja" hreflang="ja" data-v-edc0d12c>日本語</option><option value="zh-cn" xml:lang="zh-cn" hreflang="zh-cn" data-v-edc0d12c>中文 (中国)</option><option value="de" xml:lang="de" hreflang="de" data-v-edc0d12c>Deutsch</option><option value="it" xml:lang="it" hreflang="it" data-v-edc0d12c>Italiano</option><option value="pt-br" xml:lang="pt-br" hreflang="pt-br" data-v-edc0d12c>Português</option><option value="es" xml:lang="es" hreflang="es" data-v-edc0d12c>Español</option><!--]--></select></div></div></pfe-navigation></header><main id="main-content"><!--[--><!--[--><!--[--><!----><!----><!--]--><div class="breadcrumbs" id="breadcrumbs" data-v-8589d091 data-v-798f280c><nav aria-label="Breadcrumb" class="breadcrumb" data-v-798f280c><ol data-v-798f280c><li data-v-798f280c><a href="/" data-v-798f280c>Home</a></li><li data-v-798f280c><a href="/en/products" data-v-798f280c>Products</a></li><!--[--><li data-v-798f280c><a href="/en/documentation/red_hat_enterprise_linux/" data-v-798f280c>Red Hat Enterprise Linux</a></li><li data-v-798f280c><a href="/en/documentation/red_hat_enterprise_linux/9/" data-v-798f280c>9</a></li><li data-v-798f280c><!--[-->Configuring and managing high availability clusters<!--]--></li><!--]--></ol></nav><span data-v-798f280c></span></div><!----><nav id="mobile-nav" class="mobile-nav" aria-label="mobile menu" data-v-8589d091><div class="mobile-nav-wrapper" data-v-8589d091><div id="first-button" data-v-8589d091><button id="toc-btn" aria-expanded="false" aria-controls="mobile-nav-content-wrapper" class="mobile-nav-btn" data-v-8589d091><span class="sr-only" data-v-8589d091>Open </span>Table of contents</button></div><div id="second-button" data-v-8589d091><button id="settings-btn" aria-expanded="false" aria-controls="mobile-nav-content-wrapper" class="mobile-nav-btn" data-v-8589d091><pf-icon icon="cog" size="md" data-v-8589d091></pf-icon><span class="sr-only" data-v-8589d091>Open page settings</span></button></div></div><div id="mobile-nav-content-wrapper" class="hidden" role="navigation" tabindex="0" data-v-8589d091><div id="toc-mobile" class="hidden" aria-labelledby="toc-btn" data-v-8589d091><div class="shrink-product-padding product-container" id="product-container-mobile" data-v-8589d091><h1 class="product-title" data-v-8589d091>Red Hat Enterprise Linux</h1><!----></div><div class="toc-filter-mobile" data-v-8589d091><span id="text" part="text" data-v-8589d091><input id="text-input" aria-label="Search input" part="text-input" placeholder="Filter table of contents" type="text" class value data-v-8589d091><!----></span></div><div id="toc-wrapper-mobile" class="span-xs-12 span-sm-4 span-md-3" lang="en" data-v-8589d091><nav id="mobile-toc-menu" class="table-of-contents" aria-label="table of content - mobile" data-v-8589d091><!----></nav></div><!----></div><div id="page-content-options-mobile" class="hidden" aria-labelledby="settings-btn" data-v-8589d091><div class="page-layout-options" data-v-8589d091><label for="page-format-mobile" data-v-8589d091>Format</label><select id="page-format-mobile" class="page-format-dropdown" data-v-8589d091><option class="page-type" value="html" data-v-8589d091>Multi-page</option><option selected class="page-type" value="html-single" data-v-8589d091>Single-page</option><option class="page-type" value="pdf" data-v-8589d091>View full doc as PDF</option></select></div></div></div><!----><!----></nav><div class="grid grid-col-12 content-wrapper" data-v-8589d091><aside id="left-content" class="span-xs-12 span-sm-4 span-md-3" lang="en-us" aria-label="left navigation" xml:lang="en-us" data-v-8589d091><div class="toc-container" id="toc-container" visible="true" data-v-8589d091><div class="toc-focus-container" id="toc-focus-container" data-v-8589d091><button class="toc-focus-btn" aria-label="toggle left menu" aria-controls="toc-container" aria-expanded="true" data-v-8589d091><pf-icon size="md" icon="angle-left" class="toc-focus-btn-icon" data-v-8589d091></pf-icon></button></div><div class="product-container" id="product-container-desktop" data-v-8589d091><h1 class="product-title" data-v-8589d091>Red Hat Enterprise Linux</h1><!----></div><div class="toc-filter" id="toc-filter" data-v-8589d091><span id="text" part="text" data-v-8589d091><span id="search-icon" part="search-icon" data-v-8589d091><pf-icon icon="filter" size="md" data-v-8589d091></pf-icon></span><input id="text-input" aria-label="Search input" part="text-input" placeholder="Filter table of contents" type="text" class value data-v-8589d091><!----></span></div><div id="toc-wrapper" class="toc-wrapper" data-v-8589d091><nav id="toc" class="max-height-85 table-of-contents" aria-label="Table of contents" data-v-8589d091><ol id="toc-list" data-v-8589d091><!--[--><li class="item chapter" data-v-fa0dae77><a class="link active" href id="chapter-index" data-v-fa0dae77>Configuring and managing high availability clusters</a></li><li class="item chapter" data-v-fa0dae77><a class="link" href="#proc_providing-feedback-on-red-hat-documentation_configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_providing-feedback-on-red-hat-documentation_configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Providing feedback on Red Hat documentation</a></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_overview-of-high-availability-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_overview-of-high-availability-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>1. High Availability Add-On overview</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_overview-of-high-availability-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_overview-of-high-availability-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>High Availability Add-On overview</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_high-availability-add-on-components-overview-of-high-availability" id="sub-link-to-configuring_and_managing_high_availability_clusters-con_high-availability-add-on-components-overview-of-high-availability" data-v-b883c74f>1.1. High Availability Add-On components</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--con_high-availability-add-on-concepts-overview-of-high-availability" data-v-b883c74f><summary class="heading sub-chapter-title" id="con_high-availability-add-on-concepts-overview-of-high-availability--summary" data-v-b883c74f>1.2. High Availability Add-On concepts</summary><ol id="sub-nav--con_high-availability-add-on-concepts-overview-of-high-availability" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#con_high-availability-add-on-concepts-overview-of-high-availability" id="chapter-landing--configuring_and_managing_high_availability_clusters-con_high-availability-add-on-concepts-overview-of-high-availability" data-v-b883c74f>High Availability Add-On concepts</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#fencing" id="sub-link-to-configuring_and_managing_high_availability_clusters-fencing" data-v-b883c74f>1.2.1. Fencing</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#quorum" id="sub-link-to-configuring_and_managing_high_availability_clusters-quorum" data-v-b883c74f>1.2.2. Quorum</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#cluster_resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-cluster_resources" data-v-b883c74f>1.2.3. Cluster resources</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--con_pacemaker-overview-overview-of-high-availability" data-v-b883c74f><summary class="heading sub-chapter-title" id="con_pacemaker-overview-overview-of-high-availability--summary" data-v-b883c74f>1.3. Pacemaker overview</summary><ol id="sub-nav--con_pacemaker-overview-overview-of-high-availability" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#con_pacemaker-overview-overview-of-high-availability" id="chapter-landing--configuring_and_managing_high_availability_clusters-con_pacemaker-overview-overview-of-high-availability" data-v-b883c74f>Pacemaker overview</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#pacemaker_architecture_components" id="sub-link-to-configuring_and_managing_high_availability_clusters-pacemaker_architecture_components" data-v-b883c74f>1.3.1. Pacemaker architecture components</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#pacemaker_configuration_and_management_tools" id="sub-link-to-configuring_and_managing_high_availability_clusters-pacemaker_configuration_and_management_tools" data-v-b883c74f>1.3.2. Pacemaker configuration and management tools</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#the_cluster_and_pacemaker_configuration_files" id="sub-link-to-configuring_and_managing_high_availability_clusters-the_cluster_and_pacemaker_configuration_files" data-v-b883c74f>1.3.3. The cluster and Pacemaker configuration files</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--con_HA-lvm-shared-volumes-overview-of-high-availability" data-v-b883c74f><summary class="heading sub-chapter-title" id="con_HA-lvm-shared-volumes-overview-of-high-availability--summary" data-v-b883c74f>1.4. LVM logical volumes in a Red Hat high availability cluster</summary><ol id="sub-nav--con_HA-lvm-shared-volumes-overview-of-high-availability" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#con_HA-lvm-shared-volumes-overview-of-high-availability" id="chapter-landing--configuring_and_managing_high_availability_clusters-con_HA-lvm-shared-volumes-overview-of-high-availability" data-v-b883c74f>LVM logical volumes in a Red Hat high availability cluster</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#choosing_ha_lvm_or_shared_volumes" id="sub-link-to-configuring_and_managing_high_availability_clusters-choosing_ha_lvm_or_shared_volumes" data-v-b883c74f>1.4.1. Choosing HA-LVM or shared volumes</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#configuring_lvm_volumes_in_a_cluster" id="sub-link-to-configuring_and_managing_high_availability_clusters-configuring_lvm_volumes_in_a_cluster" data-v-b883c74f>1.4.2. Configuring LVM volumes in a cluster</a></li><!--]--><!--]--></ol></details></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_getting-started-with-pacemaker-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_getting-started-with-pacemaker-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>2. Getting started with Pacemaker</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_getting-started-with-pacemaker-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_getting-started-with-pacemaker-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Getting started with Pacemaker</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_learning-to-use-pacemaker-getting-started-with-pacemaker" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_learning-to-use-pacemaker-getting-started-with-pacemaker" data-v-b883c74f>2.1. Learning to use Pacemaker</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_learning-to-configure-failover-getting-started-with-pacemaker" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_learning-to-configure-failover-getting-started-with-pacemaker" data-v-b883c74f>2.2. Learning to configure failover</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_pcs-operation-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_pcs-operation-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>3. The pcs command-line interface</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_pcs-operation-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_pcs-operation-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>The pcs command-line interface</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_pcs-help-pcs-operation" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_pcs-help-pcs-operation" data-v-b883c74f>3.1. pcs help display</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_raw-config-pcs-operation" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_raw-config-pcs-operation" data-v-b883c74f>3.2. Viewing the raw cluster configuration</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configure-testfile-pcs-operation" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configure-testfile-pcs-operation" data-v-b883c74f>3.3. Saving a configuration change to a working file</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_cluster-status-pcs-operation" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_cluster-status-pcs-operation" data-v-b883c74f>3.4. Displaying cluster status</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_cluster-config-display-pcs-operation" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_cluster-config-display-pcs-operation" data-v-b883c74f>3.5. Displaying the full cluster configuration</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_resource-status-pcs-operation" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_resource-status-pcs-operation" data-v-b883c74f>3.6. Displaying resource status</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_pcs-corosync-manage-pcs-operation" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_pcs-corosync-manage-pcs-operation" data-v-b883c74f>3.7. Modifying the corosync.conf file with the pcs command</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_pcs-corosync-display-pcs-operation" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_pcs-corosync-display-pcs-operation" data-v-b883c74f>3.8. Displaying the corosync.conf file with the pcs command</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>4. Creating a Red Hat High-Availability cluster with Pacemaker</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Creating a Red Hat High-Availability cluster with Pacemaker</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_installing-cluster-software-creating-high-availability-cluster" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_installing-cluster-software-creating-high-availability-cluster" data-v-b883c74f>4.1. Installing cluster software</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_installing-pcp-zeroconf-creating-high-availability-cluster" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_installing-pcp-zeroconf-creating-high-availability-cluster" data-v-b883c74f>4.2. Installing the pcp-zeroconf package (recommended)</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_creating-high-availability-cluster-creating-high-availability-cluster" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_creating-high-availability-cluster-creating-high-availability-cluster" data-v-b883c74f>4.3. Creating a high availability cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configure-multiple-ip-cluster-creating-high-availability-cluster" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configure-multiple-ip-cluster-creating-high-availability-cluster" data-v-b883c74f>4.4. Creating a high availability cluster with multiple links</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-fencing-creating-high-availability-cluster" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-fencing-creating-high-availability-cluster" data-v-b883c74f>4.5. Configuring fencing</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_cluster-backup-creating-high-availability-cluster" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_cluster-backup-creating-high-availability-cluster" data-v-b883c74f>4.6. Backing up and restoring a cluster configuration</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_enabling-ports-for-high-availability-creating-high-availability-cluster" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_enabling-ports-for-high-availability-creating-high-availability-cluster" data-v-b883c74f>4.7. Enabling ports for the High Availability Add-On</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>5. Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http" data-v-b883c74f>5.1. Configuring an LVM volume with an XFS file system in a Pacemaker cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-apache-http-web-server-configuring-ha-http" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-apache-http-web-server-configuring-ha-http" data-v-b883c74f>5.2. Configuring an Apache HTTP Server</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-resources-for-http-server-in-a-cluster-configuring-ha-http" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-resources-for-http-server-in-a-cluster-configuring-ha-http" data-v-b883c74f>5.3. Creating the resources and resource groups</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_testing-resource-configuration-in-a-cluster-configuring-ha-http" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_testing-resource-configuration-in-a-cluster-configuring-ha-http" data-v-b883c74f>5.4. Testing the resource configuration</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>6. Configuring an active/passive NFS server in a Red Hat High Availability cluster</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Configuring an active/passive NFS server in a Red Hat High Availability cluster</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs" data-v-b883c74f>6.1. Configuring an LVM volume with an XFS file system in a Pacemaker cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-nfs-share-configuring-ha-nfs" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-nfs-share-configuring-ha-nfs" data-v-b883c74f>6.2. Configuring an NFS share</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring_resources_for_nfs_server_in_a_cluster-configuring-ha-nfs" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring_resources_for_nfs_server_in_a_cluster-configuring-ha-nfs" data-v-b883c74f>6.3. Configuring the resources and resource group for an NFS server in a cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_testing-nfs-resource-configuration-configuring-ha-nfs" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_testing-nfs-resource-configuration-configuring-ha-nfs--summary" data-v-b883c74f>6.4. Testing the NFS resource configuration</summary><ol id="sub-nav--proc_testing-nfs-resource-configuration-configuring-ha-nfs" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_testing-nfs-resource-configuration-configuring-ha-nfs" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_testing-nfs-resource-configuration-configuring-ha-nfs" data-v-b883c74f>Testing the NFS resource configuration</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#testing_the_nfs_export" id="sub-link-to-configuring_and_managing_high_availability_clusters-testing_the_nfs_export" data-v-b883c74f>6.4.1. Testing the NFS export</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#testing_for_failover" id="sub-link-to-configuring_and_managing_high_availability_clusters-testing_for_failover" data-v-b883c74f>6.4.2. Testing for failover</a></li><!--]--><!--]--></ol></details></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>7. GFS2 file systems in a cluster</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>GFS2 file systems in a cluster</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster" data-v-b883c74f>7.1. Configuring a GFS2 file system in a cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster--summary" data-v-b883c74f>7.2. Configuring an encrypted GFS2 file system in a cluster</summary><ol id="sub-nav--proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster" data-v-b883c74f>Configuring an encrypted GFS2 file system in a cluster</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#configure_a_shared_logical_volume_in_a_pacemaker_cluster" id="sub-link-to-configuring_and_managing_high_availability_clusters-configure_a_shared_logical_volume_in_a_pacemaker_cluster" data-v-b883c74f>7.2.1. Configure a shared logical volume in a Pacemaker cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#encrypt_the_logical_volume_and_create_a_crypt_resource" id="sub-link-to-configuring_and_managing_high_availability_clusters-encrypt_the_logical_volume_and_create_a_crypt_resource" data-v-b883c74f>7.2.2. Encrypt the logical volume and create a crypt resource</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#format_the_encrypted_logical_volume_with_a_gfs2_file_system_and_create_a_file_system_resource_for_the_cluster" id="sub-link-to-configuring_and_managing_high_availability_clusters-format_the_encrypted_logical_volume_with_a_gfs2_file_system_and_create_a_file_system_resource_for_the_cluster" data-v-b883c74f>7.2.3. Format the encrypted logical volume with a GFS2 file system and create a file system resource for the cluster</a></li><!--]--><!--]--></ol></details></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-active-active-samba-in-a-cluster-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-active-active-samba-in-a-cluster-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>8. Configuring an active/active Samba server in a Red Hat High Availability cluster</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-active-active-samba-in-a-cluster-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_configuring-active-active-samba-in-a-cluster-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Configuring an active/active Samba server in a Red Hat High Availability cluster</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-gfs2-for-clustered-samba_adoc-configuring-ha-samba" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-gfs2-for-clustered-samba_adoc-configuring-ha-samba" data-v-b883c74f>8.1. Configuring a GFS2 file system for a Samba service in a high availability cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-samba-for-ha-cluster_adoc-configuring-ha-samba" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-samba-for-ha-cluster_adoc-configuring-ha-samba" data-v-b883c74f>8.2. Configuring Samba in a high availability cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-samba-cluster-resources_adoc-configuring-ha-samba" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-samba-cluster-resources_adoc-configuring-ha-samba" data-v-b883c74f>8.3. Configuring Samba cluster resources</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_verifying-clustered-samba-configuration.adoc-configuring-ha-samba" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_verifying-clustered-samba-configuration.adoc-configuring-ha-samba" data-v-b883c74f>8.4. Verifying clustered Samba configuration</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>9. Getting started with the pcsd Web UI</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Getting started with the pcsd Web UI</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_setting-up-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_setting-up-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui" data-v-b883c74f>9.1. Setting up the pcsd Web UI</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-ha-pcsd-web-ui-getting-started-with-the-pcsd-web-ui" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-ha-pcsd-web-ui-getting-started-with-the-pcsd-web-ui" data-v-b883c74f>9.2. Configuring a high availability pcsd Web UI</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-fencing-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-fencing-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>10. Configuring fencing in a Red Hat High Availability cluster</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-fencing-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_configuring-fencing-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Configuring fencing in a Red Hat High Availability cluster</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_displaying-fence-agents-configuring-fencing" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_displaying-fence-agents-configuring-fencing" data-v-b883c74f>10.1. Displaying available fence agents and their options</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_creating-fence-devices-configuring-fencing" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_creating-fence-devices-configuring-fencing" data-v-b883c74f>10.2. Creating a fence device</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_general-fence-device-properties-configuring-fencing" id="sub-link-to-configuring_and_managing_high_availability_clusters-ref_general-fence-device-properties-configuring-fencing" data-v-b883c74f>10.3. General properties of fencing devices</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_fence-delays-configuring-fencing" id="sub-link-to-configuring_and_managing_high_availability_clusters-ref_fence-delays-configuring-fencing" data-v-b883c74f>10.4. Fencing delays</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_testing-fence-devices-configuring-fencing" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_testing-fence-devices-configuring-fencing" data-v-b883c74f>10.5. Testing a fence device</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-fencing-levels-configuring-fencing" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-fencing-levels-configuring-fencing" data-v-b883c74f>10.6. Configuring fencing levels</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-fencing-for-redundant-power-configuring-fencing" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-fencing-for-redundant-power-configuring-fencing" data-v-b883c74f>10.7. Configuring fencing for redundant power supplies</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_displaying-configuring-fence-devices-configuring-fencing" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_displaying-configuring-fence-devices-configuring-fencing" data-v-b883c74f>10.8. Displaying configured fence devices</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_exporting-fence-devices-configuring-fencing" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_exporting-fence-devices-configuring-fencing" data-v-b883c74f>10.9. Exporting fence devices as pcs commands</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_modifying-fence-devices-configuring-fencing" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_modifying-fence-devices-configuring-fencing" data-v-b883c74f>10.10. Modifying and deleting fence devices</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_manually-fencing-a-node-configuring-fencing" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_manually-fencing-a-node-configuring-fencing" data-v-b883c74f>10.11. Manually fencing a cluster node</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_disabling-a-fence-device-configuring-fencing" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_disabling-a-fence-device-configuring-fencing" data-v-b883c74f>10.12. Disabling a fence device</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_preventing-a-node-from-using-a-fence-device-configuring-fencing" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_preventing-a-node-from-using-a-fence-device-configuring-fencing" data-v-b883c74f>10.13. Preventing a node from using a fencing device</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_configuring-acpi-for-fence-devices-configuring-fencing" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_configuring-acpi-for-fence-devices-configuring-fencing--summary" data-v-b883c74f>10.14. Configuring ACPI for use with integrated fence devices</summary><ol id="sub-nav--proc_configuring-acpi-for-fence-devices-configuring-fencing" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_configuring-acpi-for-fence-devices-configuring-fencing" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_configuring-acpi-for-fence-devices-configuring-fencing" data-v-b883c74f>Configuring ACPI for use with integrated fence devices</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#s2-bios-setting-CA" id="sub-link-to-configuring_and_managing_high_availability_clusters-s2-bios-setting-CA" data-v-b883c74f>10.14.1. Disabling ACPI Soft-Off with the BIOS</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#s2-acpi-disable-logind-CA" id="sub-link-to-configuring_and_managing_high_availability_clusters-s2-acpi-disable-logind-CA" data-v-b883c74f>10.14.2. Disabling ACPI Soft-Off in the logind.conf file</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#s2-acpi-disable-boot-CA" id="sub-link-to-configuring_and_managing_high_availability_clusters-s2-acpi-disable-boot-CA" data-v-b883c74f>10.14.3. Disabling ACPI completely in the GRUB 2 file</a></li><!--]--><!--]--></ol></details></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>11. Configuring cluster resources</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Configuring cluster resources</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_resource-properties.adoc-configuring-cluster-resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-ref_resource-properties.adoc-configuring-cluster-resources" data-v-b883c74f>11.1. Resource agent identifiers</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_displaying-resource-specific-parameters-configuring-cluster-resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_displaying-resource-specific-parameters-configuring-cluster-resources" data-v-b883c74f>11.2. Displaying resource-specific parameters</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_configuring-resource-meta-options-configuring-cluster-resources" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_configuring-resource-meta-options-configuring-cluster-resources--summary" data-v-b883c74f>11.3. Configuring resource meta options</summary><ol id="sub-nav--proc_configuring-resource-meta-options-configuring-cluster-resources" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_configuring-resource-meta-options-configuring-cluster-resources" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_configuring-resource-meta-options-configuring-cluster-resources" data-v-b883c74f>Configuring resource meta options</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#changing_the_default_value_of_a_resource_option" id="sub-link-to-configuring_and_managing_high_availability_clusters-changing_the_default_value_of_a_resource_option" data-v-b883c74f>11.3.1. Changing the default value of a resource option</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#changing_the_default_value_of_a_resource_option_for_sets_of_resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-changing_the_default_value_of_a_resource_option_for_sets_of_resources" data-v-b883c74f>11.3.2. Changing the default value of a resource option for sets of resources</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#displaying_currently_configured_resource_defaults" id="sub-link-to-configuring_and_managing_high_availability_clusters-displaying_currently_configured_resource_defaults" data-v-b883c74f>11.3.3. Displaying currently configured resource defaults</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#setting_meta_options_on_resource_creation" id="sub-link-to-configuring_and_managing_high_availability_clusters-setting_meta_options_on_resource_creation" data-v-b883c74f>11.3.4. Setting meta options on resource creation</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_creating-resource-groups-configuring-cluster-resources" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_creating-resource-groups-configuring-cluster-resources--summary" data-v-b883c74f>11.4. Configuring resource groups</summary><ol id="sub-nav--proc_creating-resource-groups-configuring-cluster-resources" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_creating-resource-groups-configuring-cluster-resources" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_creating-resource-groups-configuring-cluster-resources" data-v-b883c74f>Configuring resource groups</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#creating_a_resource_group" id="sub-link-to-configuring_and_managing_high_availability_clusters-creating_a_resource_group" data-v-b883c74f>11.4.1. Creating a resource group</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#removing_a_resource_group" id="sub-link-to-configuring_and_managing_high_availability_clusters-removing_a_resource_group" data-v-b883c74f>11.4.2. Removing a resource group</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#displaying_resource_groups" id="sub-link-to-configuring_and_managing_high_availability_clusters-displaying_resource_groups" data-v-b883c74f>11.4.3. Displaying resource groups</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#s2-group_options-HAAR" id="sub-link-to-configuring_and_managing_high_availability_clusters-s2-group_options-HAAR" data-v-b883c74f>11.4.4. Group options</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#s2-group_stickiness-HAAR" id="sub-link-to-configuring_and_managing_high_availability_clusters-s2-group_stickiness-HAAR" data-v-b883c74f>11.4.5. Group stickiness</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_determining-resource-behavior-configuring-cluster-resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-con_determining-resource-behavior-configuring-cluster-resources" data-v-b883c74f>11.5. Determining resource behavior</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>12. Determining which nodes a resource can run on</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Determining which nodes a resource can run on</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-location-constraints-determining-which-node-a-resource-runs-on" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-location-constraints-determining-which-node-a-resource-runs-on" data-v-b883c74f>12.1. Configuring location constraints</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_limiting-resource-discovery-to-a-subset-of-nodes-determining-which-node-a-resource-runs-on" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_limiting-resource-discovery-to-a-subset-of-nodes-determining-which-node-a-resource-runs-on" data-v-b883c74f>12.2. Limiting resource discovery to a subset of nodes</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_configuring-location-constraint-strategy-determining-which-node-a-resource-runs-on" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_configuring-location-constraint-strategy-determining-which-node-a-resource-runs-on--summary" data-v-b883c74f>12.3. Configuring a location constraint strategy</summary><ol id="sub-nav--proc_configuring-location-constraint-strategy-determining-which-node-a-resource-runs-on" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_configuring-location-constraint-strategy-determining-which-node-a-resource-runs-on" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_configuring-location-constraint-strategy-determining-which-node-a-resource-runs-on" data-v-b883c74f>Configuring a location constraint strategy</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#s3-optin-clusters-HAAR" id="sub-link-to-configuring_and_managing_high_availability_clusters-s3-optin-clusters-HAAR" data-v-b883c74f>12.3.1. Configuring an &quot;Opt-In&quot; cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#s3-optout-clusters-HAAR" id="sub-link-to-configuring_and_managing_high_availability_clusters-s3-optout-clusters-HAAR" data-v-b883c74f>12.3.2. Configuring an &quot;Opt-Out&quot; cluster</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on" data-v-b883c74f>12.4. Configuring a resource to prefer its current node</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_exporting-constraints-determining-which-node-a-resource-runs-on" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_exporting-constraints-determining-which-node-a-resource-runs-on" data-v-b883c74f>12.5. Exporting resource constraints as pcs commands</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>13. Determining the order in which cluster resources are run</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Determining the order in which cluster resources are run</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_configuring-mandatory-ordering-determining-resource-order" id="sub-link-to-configuring_and_managing_high_availability_clusters-con_configuring-mandatory-ordering-determining-resource-order" data-v-b883c74f>13.1. Configuring mandatory ordering</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-advisory-ordering-determining-resource-order" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-advisory-ordering-determining-resource-order" data-v-b883c74f>13.2. Configuring advisory ordering</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-ordered-resource-sets.adocdetermining-resource-order" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-ordered-resource-sets.adocdetermining-resource-order" data-v-b883c74f>13.3. Configuring ordered resource sets</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order" data-v-b883c74f>13.4. Configuring startup order for resource dependencies not managed by Pacemaker</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>14. Colocating cluster resources</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Colocating cluster resources</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_specifying-mandatory-placement.adoc-colocating-cluster-resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_specifying-mandatory-placement.adoc-colocating-cluster-resources" data-v-b883c74f>14.1. Specifying mandatory placement of resources</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_specifying-advisory-placement-colocating-cluster-resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-con_specifying-advisory-placement-colocating-cluster-resources" data-v-b883c74f>14.2. Specifying advisory placement of resources</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_colocating-resource-sets.adoc-colocating-cluster-resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_colocating-resource-sets.adoc-colocating-cluster-resources" data-v-b883c74f>14.3. Colocating sets of resources</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><a class="link" href="#proc_displaying-resource-constraints.adoc-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_displaying-resource-constraints.adoc-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>15. Displaying resource constraints and resource dependencies</a></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_determining-resource-location-with-rules-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_determining-resource-location-with-rules-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>16. Determining resource location with rules</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_determining-resource-location-with-rules-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_determining-resource-location-with-rules-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Determining resource location with rules</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--ref_pacemaker-rules.adoc-determining-resource-location-with-rules" data-v-b883c74f><summary class="heading sub-chapter-title" id="ref_pacemaker-rules.adoc-determining-resource-location-with-rules--summary" data-v-b883c74f>16.1. Pacemaker rules</summary><ol id="sub-nav--ref_pacemaker-rules.adoc-determining-resource-location-with-rules" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#ref_pacemaker-rules.adoc-determining-resource-location-with-rules" id="chapter-landing--configuring_and_managing_high_availability_clusters-ref_pacemaker-rules.adoc-determining-resource-location-with-rules" data-v-b883c74f>Pacemaker rules</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#node_attribute_expressions" id="sub-link-to-configuring_and_managing_high_availability_clusters-node_attribute_expressions" data-v-b883c74f>16.1.1. Node attribute expressions</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#time_date_based_expressions" id="sub-link-to-configuring_and_managing_high_availability_clusters-time_date_based_expressions" data-v-b883c74f>16.1.2. Time/date based expressions</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#date_specifications" id="sub-link-to-configuring_and_managing_high_availability_clusters-date_specifications" data-v-b883c74f>16.1.3. Date specifications</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_configuring-constraint-using-rules.adoc-determining-resource-location-with-rules" id="sub-link-to-configuring_and_managing_high_availability_clusters-ref_configuring-constraint-using-rules.adoc-determining-resource-location-with-rules" data-v-b883c74f>16.2. Configuring a Pacemaker location constraint using rules</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_managing-cluster-resources-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_managing-cluster-resources-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>17. Managing cluster resources</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_managing-cluster-resources-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_managing-cluster-resources-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Managing cluster resources</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_display-configured-resources-managing-cluster-resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_display-configured-resources-managing-cluster-resources" data-v-b883c74f>17.1. Displaying configured resources</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_exporting-resources-managing-cluster-resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_exporting-resources-managing-cluster-resources" data-v-b883c74f>17.2. Exporting cluster resources as pcs commands</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_modify-resource-parameters-managing-cluster-resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_modify-resource-parameters-managing-cluster-resources" data-v-b883c74f>17.3. Modifying resource parameters</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_cleanup-cluster-resources-managing-cluster-resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_cleanup-cluster-resources-managing-cluster-resources" data-v-b883c74f>17.4. Clearing failure status of cluster resources</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_moving-cluster-resources-managing-cluster-resources" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_moving-cluster-resources-managing-cluster-resources--summary" data-v-b883c74f>17.5. Moving resources in a cluster</summary><ol id="sub-nav--proc_moving-cluster-resources-managing-cluster-resources" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_moving-cluster-resources-managing-cluster-resources" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_moving-cluster-resources-managing-cluster-resources" data-v-b883c74f>Moving resources in a cluster</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#moving_resources_due_to_failure" id="sub-link-to-configuring_and_managing_high_availability_clusters-moving_resources_due_to_failure" data-v-b883c74f>17.5.1. Moving resources due to failure</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#moving_resources_due_to_connectivity_changes" id="sub-link-to-configuring_and_managing_high_availability_clusters-moving_resources_due_to_connectivity_changes" data-v-b883c74f>17.5.2. Moving resources due to connectivity changes</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_disabling-monitor-operationmanaging-cluster-resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_disabling-monitor-operationmanaging-cluster-resources" data-v-b883c74f>17.6. Disabling a monitor operation</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_tagging-cluster-resources-managing-cluster-resources" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_tagging-cluster-resources-managing-cluster-resources--summary" data-v-b883c74f>17.7. Configuring and managing cluster resource tags</summary><ol id="sub-nav--proc_tagging-cluster-resources-managing-cluster-resources" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_tagging-cluster-resources-managing-cluster-resources" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_tagging-cluster-resources-managing-cluster-resources" data-v-b883c74f>Configuring and managing cluster resource tags</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#tagging_cluster_resources_for_administration_by_category" id="sub-link-to-configuring_and_managing_high_availability_clusters-tagging_cluster_resources_for_administration_by_category" data-v-b883c74f>17.7.1. Tagging cluster resources for administration by category</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#deleting_a_tagged_cluster_resource" id="sub-link-to-configuring_and_managing_high_availability_clusters-deleting_a_tagged_cluster_resource" data-v-b883c74f>17.7.2. Deleting a tagged cluster resource</a></li><!--]--><!--]--></ol></details></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_creating-multinode-resources-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_creating-multinode-resources-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>18. Creating cluster resources that are active on multiple nodes (cloned resources)</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_creating-multinode-resources-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_creating-multinode-resources-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Creating cluster resources that are active on multiple nodes (cloned resources)</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_creating-cloned-resource-creating-multinode-resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_creating-cloned-resource-creating-multinode-resources" data-v-b883c74f>18.1. Creating and removing a cloned resource</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-clone-constraints-creating-multinode-resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-clone-constraints-creating-multinode-resources" data-v-b883c74f>18.2. Configuring clone resource constraints</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_creating-promotable-clone-resources-creating-multinode-resources" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_creating-promotable-clone-resources-creating-multinode-resources--summary" data-v-b883c74f>18.3. Promotable clone resources</summary><ol id="sub-nav--proc_creating-promotable-clone-resources-creating-multinode-resources" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_creating-promotable-clone-resources-creating-multinode-resources" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_creating-promotable-clone-resources-creating-multinode-resources" data-v-b883c74f>Promotable clone resources</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#creating_a_promotable_clone_resource" id="sub-link-to-configuring_and_managing_high_availability_clusters-creating_a_promotable_clone_resource" data-v-b883c74f>18.3.1. Creating a promotable clone resource</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#configuring_promotable_resource_constraints" id="sub-link-to-configuring_and_managing_high_availability_clusters-configuring_promotable_resource_constraints" data-v-b883c74f>18.3.2. Configuring promotable resource constraints</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_recovering-promoted-node-creating-multinode-resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_recovering-promoted-node-creating-multinode-resources" data-v-b883c74f>18.4. Demoting a promoted resource on failure</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_clusternode-management-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_clusternode-management-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>19. Managing cluster nodes</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_clusternode-management-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_clusternode-management-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Managing cluster nodes</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_cluster-stop-clusternode-management" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_cluster-stop-clusternode-management" data-v-b883c74f>19.1. Stopping cluster services</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_cluster-enable-clusternode-management" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_cluster-enable-clusternode-management" data-v-b883c74f>19.2. Enabling and disabling cluster services</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_cluster-nodeadd-clusternode-management" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_cluster-nodeadd-clusternode-management" data-v-b883c74f>19.3. Adding cluster nodes</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_cluster-noderemove-clusternode-management" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_cluster-noderemove-clusternode-management" data-v-b883c74f>19.4. Removing cluster nodes</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_add-nodes-to-multiple-ip-cluster-clusternode-management" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_add-nodes-to-multiple-ip-cluster-clusternode-management" data-v-b883c74f>19.5. Adding a node to a cluster with multiple links</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_changing-links-in-multiple-ip-cluster-clusternode-management" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_changing-links-in-multiple-ip-cluster-clusternode-management--summary" data-v-b883c74f>19.6. Adding and modifying links in an existing cluster</summary><ol id="sub-nav--proc_changing-links-in-multiple-ip-cluster-clusternode-management" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_changing-links-in-multiple-ip-cluster-clusternode-management" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_changing-links-in-multiple-ip-cluster-clusternode-management" data-v-b883c74f>Adding and modifying links in an existing cluster</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#adding_and_removing_links_in_an_existing_cluster" id="sub-link-to-configuring_and_managing_high_availability_clusters-adding_and_removing_links_in_an_existing_cluster" data-v-b883c74f>19.6.1. Adding and removing links in an existing cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#modifying_a_link_in_a_cluster_with_multiple_links" id="sub-link-to-configuring_and_managing_high_availability_clusters-modifying_a_link_in_a_cluster_with_multiple_links" data-v-b883c74f>19.6.2. Modifying a link in a cluster with multiple links</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#modifying_the_link_addresses_in_a_cluster_with_a_single_link" id="sub-link-to-configuring_and_managing_high_availability_clusters-modifying_the_link_addresses_in_a_cluster_with_a_single_link" data-v-b883c74f>19.6.3. Modifying the link addresses in a cluster with a single link</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#modifying_the_link_options_for_a_link_in_a_cluster_with_a_single_link" id="sub-link-to-configuring_and_managing_high_availability_clusters-modifying_the_link_options_for_a_link_in_a_cluster_with_a_single_link" data-v-b883c74f>19.6.4. Modifying the link options for a link in a cluster with a single link</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#modifying_a_link_when_adding_a_new_link_is_not_possible" id="sub-link-to-configuring_and_managing_high_availability_clusters-modifying_a_link_when_adding_a_new_link_is_not_possible" data-v-b883c74f>19.6.5. Modifying a link when adding a new link is not possible</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_tracking-node-health-clusternode-management" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_tracking-node-health-clusternode-management" data-v-b883c74f>19.7. Configuring a node health strategy</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-large-clusters-clusternode-management" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-large-clusters-clusternode-management" data-v-b883c74f>19.8. Configuring a large cluster with many resources</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_cluster-permissions-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_cluster-permissions-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>20. Setting user permissions for a Pacemaker cluster</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_cluster-permissions-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_cluster-permissions-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Setting user permissions for a Pacemaker cluster</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_setting-cluster-access-over-network-cluster-permissions" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_setting-cluster-access-over-network-cluster-permissions" data-v-b883c74f>20.1. Setting permissions for node access over a network</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_setting-local-cluster-permissions-cluster-permissions" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_setting-local-cluster-permissions-cluster-permissions" data-v-b883c74f>20.2. Setting local permissions using ACLs</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_resource-monitoring-operations-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_resource-monitoring-operations-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>21. Resource monitoring operations</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_resource-monitoring-operations-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_resource-monitoring-operations-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Resource monitoring operations</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-resource-monitoring-operations-resource-monitoring-operations" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-resource-monitoring-operations-resource-monitoring-operations" data-v-b883c74f>21.1. Configuring resource monitoring operations</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_configuring-global-resource-operation-defaults-resource-monitoring-operations" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_configuring-global-resource-operation-defaults-resource-monitoring-operations--summary" data-v-b883c74f>21.2. Configuring global resource operation defaults</summary><ol id="sub-nav--proc_configuring-global-resource-operation-defaults-resource-monitoring-operations" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_configuring-global-resource-operation-defaults-resource-monitoring-operations" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_configuring-global-resource-operation-defaults-resource-monitoring-operations" data-v-b883c74f>Configuring global resource operation defaults</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#overriding_resource_specific_operation_values" id="sub-link-to-configuring_and_managing_high_availability_clusters-overriding_resource_specific_operation_values" data-v-b883c74f>21.2.1. Overriding resource-specific operation values</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#changing_the_default_value_of_a_resource_operation_for_sets_of_resources" id="sub-link-to-configuring_and_managing_high_availability_clusters-changing_the_default_value_of_a_resource_operation_for_sets_of_resources" data-v-b883c74f>21.2.2. Changing the default value of a resource operation for sets of resources</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#displaying_currently_configured_resource_operation_default_values" id="sub-link-to-configuring_and_managing_high_availability_clusters-displaying_currently_configured_resource_operation_default_values" data-v-b883c74f>21.2.3. Displaying currently configured resource operation default values</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-multiple-monitoring-operations-resource-monitoring-operations" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-multiple-monitoring-operations-resource-monitoring-operations" data-v-b883c74f>21.3. Configuring multiple monitoring operations</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_controlling-cluster-behavior-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_controlling-cluster-behavior-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>22. Pacemaker cluster properties</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_controlling-cluster-behavior-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_controlling-cluster-behavior-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Pacemaker cluster properties</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_cluster-properties-options-controlling-cluster-behavior" id="sub-link-to-configuring_and_managing_high_availability_clusters-ref_cluster-properties-options-controlling-cluster-behavior" data-v-b883c74f>22.1. Summary of cluster properties and options</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#setting-cluster-properties-controlling-cluster-behavior" id="sub-link-to-configuring_and_managing_high_availability_clusters-setting-cluster-properties-controlling-cluster-behavior" data-v-b883c74f>22.2. Setting and removing cluster properties</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_querying-cluster-property-settings-controlling-cluster-behavior" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_querying-cluster-property-settings-controlling-cluster-behavior" data-v-b883c74f>22.3. Querying cluster property settings</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_exporting-properties-controlling-cluster-behavior" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_exporting-properties-controlling-cluster-behavior" data-v-b883c74f>22.4. Exporting cluster properties as pcs commands</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-resources-to-remain-stopped-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-resources-to-remain-stopped-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>23. Configuring resources to remain stopped on clean node shutdown</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-resources-to-remain-stopped-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_configuring-resources-to-remain-stopped-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Configuring resources to remain stopped on clean node shutdown</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_cluster-properties-shutdown-lock-configuring-resources-to-remain-stopped" id="sub-link-to-configuring_and_managing_high_availability_clusters-ref_cluster-properties-shutdown-lock-configuring-resources-to-remain-stopped" data-v-b883c74f>23.1. Cluster properties to configure resources to remain stopped on clean node shutdown</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_setting-shutdown-lock-configuring-resources-to-remain-stopped" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_setting-shutdown-lock-configuring-resources-to-remain-stopped" data-v-b883c74f>23.2. Setting the shutdown-lock cluster property</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>24. Configuring a node placement strategy</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Configuring a node placement strategy</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--configuring-utilization-attributes-configuring-node-placement-strategy" data-v-b883c74f><summary class="heading sub-chapter-title" id="configuring-utilization-attributes-configuring-node-placement-strategy--summary" data-v-b883c74f>24.1. Utilization attributes and placement strategy</summary><ol id="sub-nav--configuring-utilization-attributes-configuring-node-placement-strategy" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#configuring-utilization-attributes-configuring-node-placement-strategy" id="chapter-landing--configuring_and_managing_high_availability_clusters-configuring-utilization-attributes-configuring-node-placement-strategy" data-v-b883c74f>Utilization attributes and placement strategy</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#configuring_node_and_resource_capacity" id="sub-link-to-configuring_and_managing_high_availability_clusters-configuring_node_and_resource_capacity" data-v-b883c74f>24.1.1. Configuring node and resource capacity</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#configuring_placement_strategy" id="sub-link-to-configuring_and_managing_high_availability_clusters-configuring_placement_strategy" data-v-b883c74f>24.1.2. Configuring placement strategy</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--pacemaker-resource-allocation-configuring-node-placement-strategy" data-v-b883c74f><summary class="heading sub-chapter-title" id="pacemaker-resource-allocation-configuring-node-placement-strategy--summary" data-v-b883c74f>24.2. Pacemaker resource allocation</summary><ol id="sub-nav--pacemaker-resource-allocation-configuring-node-placement-strategy" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#pacemaker-resource-allocation-configuring-node-placement-strategy" id="chapter-landing--configuring_and_managing_high_availability_clusters-pacemaker-resource-allocation-configuring-node-placement-strategy" data-v-b883c74f>Pacemaker resource allocation</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#node_preference" id="sub-link-to-configuring_and_managing_high_availability_clusters-node_preference" data-v-b883c74f>24.2.1. Node preference</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#node_capacity" id="sub-link-to-configuring_and_managing_high_availability_clusters-node_capacity" data-v-b883c74f>24.2.2. Node capacity</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#resource_allocation_preference" id="sub-link-to-configuring_and_managing_high_availability_clusters-resource_allocation_preference" data-v-b883c74f>24.2.3. Resource allocation preference</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#resource-placement-strategy-guidelines-configuring-node-placement-strategy" id="sub-link-to-configuring_and_managing_high_availability_clusters-resource-placement-strategy-guidelines-configuring-node-placement-strategy" data-v-b883c74f>24.3. Resource placement strategy guidelines</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#node-utilization-resource-agent-configuring-node-placement-strategy" id="sub-link-to-configuring_and_managing_high_availability_clusters-node-utilization-resource-agent-configuring-node-placement-strategy" data-v-b883c74f>24.4. The NodeUtilization resource agent</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>25. Configuring a virtual domain as a resource</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Configuring a virtual domain as a resource</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_virtual-domain-resource-options-configuring-virtual-domain-as-a-resource" id="sub-link-to-configuring_and_managing_high_availability_clusters-ref_virtual-domain-resource-options-configuring-virtual-domain-as-a-resource" data-v-b883c74f>25.1. Virtual domain resource options</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_creating-virtual-domain-resource-configuring-virtual-domain-as-a-resource" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_creating-virtual-domain-resource-configuring-virtual-domain-as-a-resource" data-v-b883c74f>25.2. Creating the virtual domain resource</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>26. Configuring cluster quorum</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Configuring cluster quorum</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_quorum-options-configuring-cluster-quorum" id="sub-link-to-configuring_and_managing_high_availability_clusters-ref_quorum-options-configuring-cluster-quorum" data-v-b883c74f>26.1. Configuring quorum options</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_modifying-quorum-options-configuring-cluster-quorum" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_modifying-quorum-options-configuring-cluster-quorum" data-v-b883c74f>26.2. Modifying quorum options</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_displaying-quorum-configuration-status-configuring-cluster-quorum" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_displaying-quorum-configuration-status-configuring-cluster-quorum" data-v-b883c74f>26.3. Displaying quorum configuration and status</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_running-inquorate-clusters-configuring-cluster-quorum" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_running-inquorate-clusters-configuring-cluster-quorum" data-v-b883c74f>26.4. Running inquorate clusters</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-quorum-devices-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-quorum-devices-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>27. Configuring quorum devices</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-quorum-devices-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_configuring-quorum-devices-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Configuring quorum devices</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_installing-quorum-device-packages-configuring-quorum-devices" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_installing-quorum-device-packages-configuring-quorum-devices" data-v-b883c74f>27.1. Installing quorum device packages</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_configuring-quorum-device-configuring-quorum-devices" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_configuring-quorum-device-configuring-quorum-devices" data-v-b883c74f>27.2. Configuring a quorum device</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_managing-quorum-device-service-configuring-quorum-devices" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_managing-quorum-device-service-configuring-quorum-devices" data-v-b883c74f>27.3. Managing the quorum device service</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_managing-quorum-device-settings_configuring-quorum-devices" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_managing-quorum-device-settings_configuring-quorum-devices--summary" data-v-b883c74f>27.4. Managing a quorum device in a cluster</summary><ol id="sub-nav--proc_managing-quorum-device-settings_configuring-quorum-devices" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_managing-quorum-device-settings_configuring-quorum-devices" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_managing-quorum-device-settings_configuring-quorum-devices" data-v-b883c74f>Managing a quorum device in a cluster</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#changing_quorum_device_settings" id="sub-link-to-configuring_and_managing_high_availability_clusters-changing_quorum_device_settings" data-v-b883c74f>27.4.1. Changing quorum device settings</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#removing_a_quorum_device" id="sub-link-to-configuring_and_managing_high_availability_clusters-removing_a_quorum_device" data-v-b883c74f>27.4.2. Removing a quorum device</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#destroying_a_quorum_device" id="sub-link-to-configuring_and_managing_high_availability_clusters-destroying_a_quorum_device" data-v-b883c74f>27.4.3. Destroying a quorum device</a></li><!--]--><!--]--></ol></details></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-pacemaker-alert-agents_configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-pacemaker-alert-agents_configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>28. Triggering scripts for cluster events</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-pacemaker-alert-agents_configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_configuring-pacemaker-alert-agents_configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Triggering scripts for cluster events</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#using-sample-alert-agents-configuring-pacemaker-alert-agents" id="sub-link-to-configuring_and_managing_high_availability_clusters-using-sample-alert-agents-configuring-pacemaker-alert-agents" data-v-b883c74f>28.1. Installing and configuring sample alert agents</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#creating-cluster-alert-configuring-pacemaker-alert-agents" id="sub-link-to-configuring_and_managing_high_availability_clusters-creating-cluster-alert-configuring-pacemaker-alert-agents" data-v-b883c74f>28.2. Creating a cluster alert</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#managing-cluster-alert-configuring-pacemaker-alert-agents" id="sub-link-to-configuring_and_managing_high_availability_clusters-managing-cluster-alert-configuring-pacemaker-alert-agents" data-v-b883c74f>28.3. Displaying, modifying, and removing cluster alerts</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#configuring-alert-recipients-configuring-pacemaker-alert-agents" id="sub-link-to-configuring_and_managing_high_availability_clusters-configuring-alert-recipients-configuring-pacemaker-alert-agents" data-v-b883c74f>28.4. Configuring cluster alert recipients</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#cluster-alert-meta-options-configuring-pacemaker-alert-agents" id="sub-link-to-configuring_and_managing_high_availability_clusters-cluster-alert-meta-options-configuring-pacemaker-alert-agents" data-v-b883c74f>28.5. Alert meta options</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#cluster-alert-configuration-configuring-pacemaker-alert-agents" id="sub-link-to-configuring_and_managing_high_availability_clusters-cluster-alert-configuration-configuring-pacemaker-alert-agents" data-v-b883c74f>28.6. Cluster alert configuration command examples</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#writing-cluster-alert-agent-configuring-pacemaker-alert-agents" id="sub-link-to-configuring_and_managing_high_availability_clusters-writing-cluster-alert-agent-configuring-pacemaker-alert-agents" data-v-b883c74f>28.7. Writing a cluster alert agent</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-multisite-cluster-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-multisite-cluster-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>29. Multi-site Pacemaker clusters</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-multisite-cluster-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_configuring-multisite-cluster-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Multi-site Pacemaker clusters</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#con_booth-cluster-ticket-manager-configuring-multisite-cluster" id="sub-link-to-configuring_and_managing_high_availability_clusters-con_booth-cluster-ticket-manager-configuring-multisite-cluster" data-v-b883c74f>29.1. Overview of Booth cluster ticket manager</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc-configuring-multisite-with-booth-configuring-multisite-cluster" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc-configuring-multisite-with-booth-configuring-multisite-cluster" data-v-b883c74f>29.2. Configuring multi-site clusters with Pacemaker</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_remote-node-management-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_remote-node-management-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>30. Integrating non-corosync nodes into a cluster: the pacemaker_remote service</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_remote-node-management-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_remote-node-management-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Integrating non-corosync nodes into a cluster: the pacemaker_remote service</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_host-and-guest-authentication-of-remote-nodes-remote-node-management" id="sub-link-to-configuring_and_managing_high_availability_clusters-ref_host-and-guest-authentication-of-remote-nodes-remote-node-management" data-v-b883c74f>30.1. Host and guest authentication of pacemaker_remote nodes</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_configuring-kvm-guest-nodes-remote-node-management" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_configuring-kvm-guest-nodes-remote-node-management--summary" data-v-b883c74f>30.2. Configuring KVM guest nodes</summary><ol id="sub-nav--proc_configuring-kvm-guest-nodes-remote-node-management" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_configuring-kvm-guest-nodes-remote-node-management" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_configuring-kvm-guest-nodes-remote-node-management" data-v-b883c74f>Configuring KVM guest nodes</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#guest_node_resource_options" id="sub-link-to-configuring_and_managing_high_availability_clusters-guest_node_resource_options" data-v-b883c74f>30.2.1. Guest node resource options</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#integrating_a_virtual_machine_as_a_guest_node" id="sub-link-to-configuring_and_managing_high_availability_clusters-integrating_a_virtual_machine_as_a_guest_node" data-v-b883c74f>30.2.2. Integrating a virtual machine as a guest node</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_configuring-remote-nodes-remote-node-management" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_configuring-remote-nodes-remote-node-management--summary" data-v-b883c74f>30.3. Configuring Pacemaker remote nodes</summary><ol id="sub-nav--proc_configuring-remote-nodes-remote-node-management" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_configuring-remote-nodes-remote-node-management" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_configuring-remote-nodes-remote-node-management" data-v-b883c74f>Configuring Pacemaker remote nodes</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#remote_node_resource_options" id="sub-link-to-configuring_and_managing_high_availability_clusters-remote_node_resource_options" data-v-b883c74f>30.3.1. Remote node resource options</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#remote_node_configuration_overview" id="sub-link-to-configuring_and_managing_high_availability_clusters-remote_node_configuration_overview" data-v-b883c74f>30.3.2. Remote node configuration overview</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_changing-default-port-location-remote-node-management" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_changing-default-port-location-remote-node-management" data-v-b883c74f>30.4. Changing the default port location</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_upgrading-systems-with-pacemaker-remote-remote-node-management" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_upgrading-systems-with-pacemaker-remote-remote-node-management" data-v-b883c74f>30.5. Upgrading systems with pacemaker_remote nodes</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>31. Performing cluster maintenance</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Performing cluster maintenance</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_stopping-individual-node-cluster-maintenance" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_stopping-individual-node-cluster-maintenance" data-v-b883c74f>31.1. Putting a node into standby mode</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><details id="nested-toc--proc_manually-move-resources-cluster-maintenance" data-v-b883c74f><summary class="heading sub-chapter-title" id="proc_manually-move-resources-cluster-maintenance--summary" data-v-b883c74f>31.2. Manually moving cluster resources</summary><ol id="sub-nav--proc_manually-move-resources-cluster-maintenance" class="sub-nav" data-v-b883c74f><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title chapter-landing-page" href="#proc_manually-move-resources-cluster-maintenance" id="chapter-landing--configuring_and_managing_high_availability_clusters-proc_manually-move-resources-cluster-maintenance" data-v-b883c74f>Manually moving cluster resources</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#moving_a_resource_from_its_current_node" id="sub-link-to-configuring_and_managing_high_availability_clusters-moving_a_resource_from_its_current_node" data-v-b883c74f>31.2.1. Moving a resource from its current node</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#moving_a_resource_to_its_preferred_node" id="sub-link-to-configuring_and_managing_high_availability_clusters-moving_a_resource_to_its_preferred_node" data-v-b883c74f>31.2.2. Moving a resource to its preferred node</a></li><!--]--><!--]--></ol></details></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_disabling-resources-cluster-maintenance" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_disabling-resources-cluster-maintenance" data-v-b883c74f>31.3. Disabling, enabling, and banning cluster resources</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_unmanaging-resources-cluster-maintenance" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_unmanaging-resources-cluster-maintenance" data-v-b883c74f>31.4. Setting a resource to unmanaged mode</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_setting-maintenance-mode-cluster-maintenance" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_setting-maintenance-mode-cluster-maintenance" data-v-b883c74f>31.5. Putting a cluster in maintenance mode</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_updating-cluster-packages-cluster-maintenance" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_updating-cluster-packages-cluster-maintenance" data-v-b883c74f>31.6. Updating a RHEL high availability cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_upgrading-remote-nodes-cluster-maintenance" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_upgrading-remote-nodes-cluster-maintenance" data-v-b883c74f>31.7. Upgrading remote nodes and guest nodes</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_migrating-cluster-vms-cluster-maintenance" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_migrating-cluster-vms-cluster-maintenance" data-v-b883c74f>31.8. Migrating VMs in a RHEL cluster</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#identifying-cluster-by-uuid-cluster-maintenance" id="sub-link-to-configuring_and_managing_high_availability_clusters-identifying-cluster-by-uuid-cluster-maintenance" data-v-b883c74f>31.9. Identifying clusters by UUID</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><details id="toc--assembly_configuring-disaster-recovery-configuring-and-managing-high-availability-clusters" data-v-fa0dae77><summary class="heading chapter-title" id="assembly_configuring-disaster-recovery-configuring-and-managing-high-availability-clusters--summary" data-v-fa0dae77>32. Configuring disaster recovery clusters</summary><ol class="sub-nav" data-v-fa0dae77><li class="item sub-chapter" data-v-fa0dae77><a class="link sub-chapter-title chapter-landing-page" href="#assembly_configuring-disaster-recovery-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-assembly_configuring-disaster-recovery-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>Configuring disaster recovery clusters</a></li><!--[--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#ref_recovery-considerations-configuring-disaster-recovery" id="sub-link-to-configuring_and_managing_high_availability_clusters-ref_recovery-considerations-configuring-disaster-recovery" data-v-b883c74f>32.1. Considerations for disaster recovery clusters</a></li><!--]--><!--[--><li class="item sub-chapter" data-v-b883c74f><a class="link sub-chapter-title" href="#proc_disaster-recovery-display-configuring-disaster-recovery" id="sub-link-to-configuring_and_managing_high_availability_clusters-proc_disaster-recovery-display-configuring-disaster-recovery" data-v-b883c74f>32.2. Displaying status of recovery clusters</a></li><!--]--><!--]--></ol></details></li><li class="item chapter" data-v-fa0dae77><a class="link" href="#ref_interpreting-resource-exit-codes-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-ref_interpreting-resource-exit-codes-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>33. Interpreting resource agent OCF return codes</a></li><li class="item chapter" data-v-fa0dae77><a class="link" href="#ref_ibmz-configuring-and-managing-high-availability-clusters" id="chapter-landing--configuring_and_managing_high_availability_clusters-ref_ibmz-configuring-and-managing-high-availability-clusters" data-v-fa0dae77>34. Configuring a Red Hat High Availability cluster with IBM z/VM instances as cluster members</a></li><li class="item chapter" data-v-fa0dae77><a class="link" href="#idm140686156711152" id="chapter-landing--configuring_and_managing_high_availability_clusters-idm140686156711152" data-v-fa0dae77>Legal Notice</a></li><!--]--></ol></nav><!----></div></div></aside><article class="content span-xs-12 span-sm-6 span-md-12 span-lg-7" aria-live="polite" data-v-8589d091><!----><div lang="en-us" xml:lang="en-us" class="docs-content-container" data-v-8589d091><!----><!----><h1 data-id="content_chapter" class="chapter-title" data-v-8589d091>Configuring and managing high availability clusters</h1><hr class="line-below-chp" data-v-8589d091><section class="rhdocs" data-v-8589d091><body><div xml:lang="en-US" class="book" id="idm140686152248144"><div class="titlepage"><div><div class="producttitle"><span class="productname">Red Hat Enterprise Linux</span> <span class="productnumber">9</span></div><div><h3 class="subtitle">Using the Red Hat High Availability Add-On to create and maintain Pacemaker clusters</h3></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat</span> <span class="orgdiv">Customer Content Services</span></div></div><div><a href="#idm140686156711152">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				The Red Hat High Availability Add-On configures high availability clusters that use the Pacemaker cluster resource manager. This title provides procedures to familiarize you with Pacemaker cluster configuration as well as example procedures for configuring active/active and active/passive clusters.
			</div></div></div></div><hr></div><section class="preface" id="proc_providing-feedback-on-red-hat-documentation_configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Providing feedback on Red Hat documentation</h2></div></div></div><p class="_abstract _abstract">
			We appreciate your feedback on our documentation. Let us know how we can improve it.
		</p><div class="orderedlist"><p class="title"><strong>Submitting feedback through Jira (account required)</strong></p><ol class="orderedlist" type="1"><li class="listitem">
					Log in to the <a class="link" href="https://issues.redhat.com/projects/RHELDOCS/issues">Jira</a> website.
				</li><li class="listitem">
					Click <span class="strong strong"><strong>Create</strong></span> in the top navigation bar
				</li><li class="listitem">
					Enter a descriptive title in the <span class="strong strong"><strong>Summary</strong></span> field.
				</li><li class="listitem">
					Enter your suggestion for improvement in the <span class="strong strong"><strong>Description</strong></span> field. Include links to the relevant parts of the documentation.
				</li><li class="listitem">
					Click <span class="strong strong"><strong>Create</strong></span> at the bottom of the dialogue.
				</li></ol></div></section><section class="chapter" id="assembly_overview-of-high-availability-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 1. High Availability Add-On overview</h2></div></div></div><p class="_abstract _abstract">
			The High Availability Add-On is a clustered system that provides reliability, scalability, and availability to critical production services.
		</p><p>
			A cluster is two or more computers (called <span class="emphasis"><em>nodes</em></span> or <span class="emphasis"><em>members</em></span>) that work together to perform a task. Clusters can be used to provide highly available services or resources. The redundancy of multiple machines is used to guard against failures of many types.
		</p><p>
			High availability clusters provide highly available services by eliminating single points of failure and by failing over services from one cluster node to another in case a node becomes inoperative. Typically, services in a high availability cluster read and write data (by means of read-write mounted file systems). Therefore, a high availability cluster must maintain data integrity as one cluster node takes over control of a service from another cluster node. Node failures in a high availability cluster are not visible from clients outside the cluster. (High availability clusters are sometimes referred to as failover clusters.) The High Availability Add-On provides high availability clustering through its high availability service management component, <code class="literal command">Pacemaker</code>.
		</p><p>
			Red Hat provides a variety of documentation for planning, configuring, and maintaining a Red Hat high availability cluster. For a listing of articles that provide guided indexes to the various areas of Red Hat cluster documentation, see the <a class="link" href="https://access.redhat.com/articles/6565761"> Red Hat High Availability Add-On Documentation Guide</a>.
		</p><section class="section" id="con_high-availability-add-on-components-overview-of-high-availability"><div class="titlepage"><div><div><h3 class="title">1.1. High Availability Add-On components</h3></div></div></div><p class="_abstract _abstract">
				The Red Hat High Availability Add-On consists of several components that provide the high availability service.
			</p><p>
				The major components of the High Availability Add-On are as follows:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Cluster infrastructure — Provides fundamental functions for nodes to work together as a cluster: configuration file management, membership management, lock management, and fencing.
					</li><li class="listitem">
						High availability service management — Provides failover of services from one cluster node to another in case a node becomes inoperative.
					</li><li class="listitem">
						Cluster administration tools — Configuration and management tools for setting up, configuring, and managing the High Availability Add-On. The tools are for use with the cluster infrastructure components, the high availability and service management components, and storage.
					</li></ul></div><p>
				You can supplement the High Availability Add-On with the following components:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Red Hat GFS2 (Global File System 2) — Part of the Resilient Storage Add-On, this provides a cluster file system for use with the High Availability Add-On. GFS2 allows multiple nodes to share storage at a block level as if the storage were connected locally to each cluster node. GFS2 cluster file system requires a cluster infrastructure.
					</li><li class="listitem">
						LVM Locking Daemon (<code class="literal">lvmlockd</code>) — Part of the Resilient Storage Add-On, this provides volume management of cluster storage. <code class="literal">lvmlockd</code> support also requires cluster infrastructure.
					</li><li class="listitem">
						HAProxy — Routing software that provides high availability load balancing and failover in layer 4 (TCP) and layer 7 (HTTP, HTTPS) services.
					</li></ul></div></section><section class="section" id="con_high-availability-add-on-concepts-overview-of-high-availability"><div class="titlepage"><div><div><h3 class="title">1.2. High Availability Add-On concepts</h3></div></div></div><p class="_abstract _abstract">
				Some of the key concepts of a Red Hat High Availability Add-On cluster are as follows.
			</p><section class="section" id="fencing"><div class="titlepage"><div><div><h4 class="title">1.2.1. Fencing</h4></div></div></div><p>
					If communication with a single node in the cluster fails, then other nodes in the cluster must be able to restrict or release access to resources that the failed cluster node may have access to. his cannot be accomplished by contacting the cluster node itself as the cluster node may not be responsive. Instead, you must provide an external method, which is called fencing with a fence agent. A fence device is an external device that can be used by the cluster to restrict access to shared resources by an errant node, or to issue a hard reboot on the cluster node.
				</p><p>
					Without a fence device configured you do not have a way to know that the resources previously used by the disconnected cluster node have been released, and this could prevent the services from running on any of the other cluster nodes. Conversely, the system may assume erroneously that the cluster node has released its resources and this can lead to data corruption and data loss. Without a fence device configured data integrity cannot be guaranteed and the cluster configuration will be unsupported.
				</p><p>
					When the fencing is in progress no other cluster operation is allowed to run. Normal operation of the cluster cannot resume until fencing has completed or the cluster node rejoins the cluster after the cluster node has been rebooted.
				</p><p>
					For more information about fencing, see the Red Hat Knowledgebase solution <a class="link" href="https://access.redhat.com/solutions/15575">Fencing in a Red Hat High Availability Cluster</a>.
				</p></section><section class="section" id="quorum"><div class="titlepage"><div><div><h4 class="title">1.2.2. Quorum</h4></div></div></div><p>
					In order to maintain cluster integrity and availability, cluster systems use a concept known as <span class="emphasis"><em>quorum</em></span> to prevent data corruption and loss. A cluster has quorum when more than half of the cluster nodes are online. To mitigate the chance of data corruption due to failure, Pacemaker by default stops all resources if the cluster does not have quorum.
				</p><p>
					Quorum is established using a voting system. When a cluster node does not function as it should or loses communication with the rest of the cluster, the majority working nodes can vote to isolate and, if needed, fence the node for servicing.
				</p><p>
					For example, in a 6-node cluster, quorum is established when at least 4 cluster nodes are functioning. If the majority of nodes go offline or become unavailable, the cluster no longer has quorum and Pacemaker stops clustered services.
				</p><p>
					The quorum features in Pacemaker prevent what is also known as <span class="emphasis"><em>split-brain</em></span>, a phenomenon where the cluster is separated from communication but each part continues working as separate clusters, potentially writing to the same data and possibly causing corruption or loss. For more information about what it means to be in a split-brain state, and on quorum concepts in general, see the Red Hat Knowledgebase article <a class="link" href="https://access.redhat.com/articles/2824071">Exploring Concepts of RHEL High Availability Clusters - Quorum</a>.
				</p><p>
					A Red Hat Enterprise Linux High Availability Add-On cluster uses the <code class="literal">votequorum</code> service, in conjunction with fencing, to avoid split brain situations. A number of votes is assigned to each system in the cluster, and cluster operations are allowed to proceed only when a majority of votes is present.
				</p></section><section class="section" id="cluster_resources"><div class="titlepage"><div><div><h4 class="title">1.2.3. Cluster resources</h4></div></div></div><p>
					A <span class="emphasis"><em>cluster resource</em></span> is an instance of program, data, or application to be managed by the cluster service. These resources are abstracted by <span class="emphasis"><em>agents</em></span> that provide a standard interface for managing the resource in a cluster environment.
				</p><p>
					To ensure that resources remain healthy, you can add a monitoring operation to a resource’s definition. If you do not specify a monitoring operation for a resource, one is added by default.
				</p><p>
					You can determine the behavior of a resource in a cluster by configuring <span class="emphasis"><em>constraints</em></span>. You can configure the following categories of constraints:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							location constraints — A location constraint determines which nodes a resource can run on.
						</li><li class="listitem">
							ordering constraints — An ordering constraint determines the order in which the resources run.
						</li><li class="listitem">
							colocation constraints — A colocation constraint determines where resources will be placed relative to other resources.
						</li></ul></div><p>
					One of the most common elements of a cluster is a set of resources that need to be located together, start sequentially, and stop in the reverse order. To simplify this configuration, Pacemaker supports the concept of <span class="emphasis"><em>groups</em></span>.
				</p></section></section><section class="section" id="con_pacemaker-overview-overview-of-high-availability"><div class="titlepage"><div><div><h3 class="title">1.3. Pacemaker overview</h3></div></div></div><p class="_abstract _abstract">
				Pacemaker is a cluster resource manager. It achieves maximum availability for your cluster services and resources by making use of the cluster infrastructure’s messaging and membership capabilities to deter and recover from node and resource-level failure.
			</p><section class="section" id="pacemaker_architecture_components"><div class="titlepage"><div><div><h4 class="title">1.3.1. Pacemaker architecture components</h4></div></div></div><p>
					A cluster configured with Pacemaker comprises separate component daemons that monitor cluster membership, scripts that manage the services, and resource management subsystems that monitor the disparate resources.
				</p><p>
					The following components form the Pacemaker architecture:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Cluster Information Base (CIB)</span></dt><dd>
								The Pacemaker information daemon, which uses XML internally to distribute and synchronize current configuration and status information from the Designated Coordinator (DC) — a node assigned by Pacemaker to store and distribute cluster state and actions by means of the CIB — to all other cluster nodes.
							</dd><dt><span class="term">Cluster Resource Management Daemon (CRMd)</span></dt><dd><p class="simpara">
								Pacemaker cluster resource actions are routed through this daemon. Resources managed by CRMd can be queried by client systems, moved, instantiated, and changed when needed.
							</p><p class="simpara">
								Each cluster node also includes a local resource manager daemon (LRMd) that acts as an interface between CRMd and resources. LRMd passes commands from CRMd to agents, such as starting and stopping and relaying status information.
							</p></dd><dt><span class="term">Shoot the Other Node in the Head (STONITH)</span></dt><dd>
								STONITH is the Pacemaker fencing implementation. It acts as a cluster resource in Pacemaker that processes fence requests, forcefully shutting down nodes and removing them from the cluster to ensure data integrity. STONITH is configured in the CIB and can be monitored as a normal cluster resource.
							</dd><dt><span class="term">corosync</span></dt><dd><p class="simpara">
								<code class="literal">corosync</code> is the component - and a daemon of the same name - that serves the core membership and member-communication needs for high availability clusters. It is required for the High Availability Add-On to function.
							</p><p class="simpara">
								In addition to those membership and messaging functions, <code class="literal">corosync</code> also:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Manages quorum rules and determination.
									</li><li class="listitem">
										Provides messaging capabilities for applications that coordinate or operate across multiple members of the cluster and thus must communicate stateful or other information between instances.
									</li><li class="listitem">
										Uses the <code class="literal">kronosnet</code> library as its network transport to provide multiple redundant links and automatic failover.
									</li></ul></div></dd></dl></div></section><section class="section" id="pacemaker_configuration_and_management_tools"><div class="titlepage"><div><div><h4 class="title">1.3.2. Pacemaker configuration and management tools</h4></div></div></div><p>
					The High Availability Add-On features two configuration tools for cluster deployment, monitoring, and management.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal command">pcs</code></span></dt><dd><p class="simpara">
								The <code class="literal command">pcs</code> command-line interface controls and configures Pacemaker and the <code class="literal">corosync</code> heartbeat daemon. A command-line based program, <code class="literal command">pcs</code> can perform the following cluster management tasks:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Create and configure a Pacemaker/Corosync cluster
									</li><li class="listitem">
										Modify configuration of the cluster while it is running
									</li><li class="listitem">
										Remotely configure both Pacemaker and Corosync as well as start, stop, and display status information of the cluster
									</li></ul></div></dd><dt><span class="term"><code class="literal command">pcsd</code> Web UI</span></dt><dd>
								A graphical user interface to create and configure Pacemaker/Corosync clusters.
							</dd></dl></div></section><section class="section" id="the_cluster_and_pacemaker_configuration_files"><div class="titlepage"><div><div><h4 class="title">1.3.3. The cluster and Pacemaker configuration files</h4></div></div></div><p>
					The configuration files for the Red Hat High Availability Add-On are <code class="literal">corosync.conf</code> and <code class="literal">cib.xml</code>.
				</p><p>
					The <code class="literal">corosync.conf</code> file provides the cluster parameters used by <code class="literal">corosync</code>, the cluster manager that Pacemaker is built on. In general, you should not edit the <code class="literal">corosync.conf</code> directly but, instead, use the <code class="literal command">pcs</code> or <code class="literal command">pcsd</code> interface.
				</p><p>
					The <code class="literal">cib.xml</code> file is an XML file that represents both the cluster’s configuration and the current state of all resources in the cluster. This file is used by Pacemaker’s Cluster Information Base (CIB). The contents of the CIB are automatically kept in sync across the entire cluster. Do not edit the <code class="literal">cib.xml</code> file directly; use the <code class="literal command">pcs</code> or <code class="literal command">pcsd</code> interface instead.
				</p></section></section><section class="section" id="con_HA-lvm-shared-volumes-overview-of-high-availability"><div class="titlepage"><div><div><h3 class="title">1.4. LVM logical volumes in a Red Hat high availability cluster</h3></div></div></div><p class="_abstract _abstract">
				The Red Hat High Availability Add-On provides support for LVM volumes in two distinct cluster configurations.
			</p><p>
				The cluster configurations you can choose are as follows:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						High availability LVM volumes (HA-LVM) in active/passive failover configurations in which only a single node of the cluster accesses the storage at any one time.
					</li><li class="listitem">
						LVM volumes that use the <code class="literal">lvmlockd</code> daemon to manage storage devices in active/active configurations in which more than one node of the cluster requires access to the storage at the same time. The <code class="literal">lvmlockd</code> daemon is part of the Resilient Storage Add-On.
					</li></ul></div><section class="section" id="choosing_ha_lvm_or_shared_volumes"><div class="titlepage"><div><div><h4 class="title">1.4.1. Choosing HA-LVM or shared volumes</h4></div></div></div><p>
					When to use HA-LVM or shared logical volumes managed by the <code class="literal">lvmlockd</code> daemon should be based on the needs of the applications or services being deployed.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If multiple nodes of the cluster require simultaneous read/write access to LVM volumes in an active/active system, then you must use the <code class="literal">lvmlockd</code> daemon and configure your volumes as shared volumes. The <code class="literal">lvmlockd</code> daemon provides a system for coordinating activation of and changes to LVM volumes across nodes of a cluster concurrently. The <code class="literal">lvmlockd</code> daemon’s locking service provides protection to LVM metadata as various nodes of the cluster interact with volumes and make changes to their layout. This protection is contingent upon configuring any volume group that will be activated simultaneously across multiple cluster nodes as a shared volume.
						</li><li class="listitem">
							If the high availability cluster is configured to manage shared resources in an active/passive manner with only one single member needing access to a given LVM volume at a time, then you can use HA-LVM without the <code class="literal">lvmlockd</code> locking service.
						</li></ul></div><p>
					Most applications will run better in an active/passive configuration, as they are not designed or optimized to run concurrently with other instances. Choosing to run an application that is not cluster-aware on shared logical volumes can result in degraded performance. This is because there is cluster communication overhead for the logical volumes themselves in these instances. A cluster-aware application must be able to achieve performance gains above the performance losses introduced by cluster file systems and cluster-aware logical volumes. This is achievable for some applications and workloads more easily than others. Determining what the requirements of the cluster are and whether the extra effort toward optimizing for an active/active cluster will pay dividends is the way to choose between the two LVM variants. Most users will achieve the best HA results from using HA-LVM.
				</p><p>
					HA-LVM and shared logical volumes using <code class="literal">lvmlockd</code> are similar in the fact that they prevent corruption of LVM metadata and its logical volumes, which could otherwise occur if multiple machines are allowed to make overlapping changes. HA-LVM imposes the restriction that a logical volume can only be activated exclusively; that is, active on only one machine at a time. This means that only local (non-clustered) implementations of the storage drivers are used. Avoiding the cluster coordination overhead in this way increases performance. A shared volume using <code class="literal">lvmlockd</code> does not impose these restrictions and a user is free to activate a logical volume on all machines in a cluster; this forces the use of cluster-aware storage drivers, which allow for cluster-aware file systems and applications to be put on top.
				</p></section><section class="section" id="configuring_lvm_volumes_in_a_cluster"><div class="titlepage"><div><div><h4 class="title">1.4.2. Configuring LVM volumes in a cluster</h4></div></div></div><p>
					Clusters are managed through Pacemaker. Both HA-LVM and shared logical volumes are supported only in conjunction with Pacemaker clusters, and must be configured as cluster resources.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						If an LVM volume group used by a Pacemaker cluster contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you configure a <code class="literal">systemd resource-agents-deps</code> target and a <code class="literal">systemd</code> drop-in unit for the target to ensure that the service starts before Pacemaker starts. For information on configuring a <code class="literal">systemd resource-agents-deps</code> target, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order">Configuring startup order for resource dependencies not managed by Pacemaker</a>.
					</p></div></rh-alert><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							For examples of procedures for configuring an HA-LVM volume as part of a Pacemaker cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters">Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters">Configuring an active/passive NFS server in a Red Hat High Availability cluster</a>.
						</p><p class="simpara">
							Note that these procedures include the following steps:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Ensuring that only the cluster is capable of activating the volume group
								</li><li class="listitem">
									Configuring an LVM logical volume
								</li><li class="listitem">
									Configuring the LVM volume as a cluster resource
								</li></ul></div></li><li class="listitem">
							For procedures for configuring shared LVM volumes that use the <code class="literal">lvmlockd</code> daemon to manage storage devices in active/active configurations, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters">GFS2 file systems in a cluster</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-active-samba-in-a-cluster-configuring-and-managing-high-availability-clusters">Configuring an active/active Samba server in a Red Hat High Availability cluster</a>.
						</li></ul></div></section></section></section><section class="chapter" id="assembly_getting-started-with-pacemaker-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 2. Getting started with Pacemaker</h2></div></div></div><p class="_abstract _abstract">
			To familiarize yourself with the tools and processes you use to create a Pacemaker cluster, you can run the following procedures. They are intended for users who are interested in seeing what the cluster software looks like and how it is administered, without needing to configure a working cluster.
		</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
				These procedures do not create a supported Red Hat cluster, which requires at least two nodes and the configuration of a fencing device. For full information about Red Hat’s support policies, requirements, and limitations for RHEL High Availability clusters, see <a class="link" href="https://access.redhat.com/articles/2912891/">Support Policies for RHEL High Availability Clusters</a>.
			</p></div></rh-alert><section class="section" id="proc_learning-to-use-pacemaker-getting-started-with-pacemaker"><div class="titlepage"><div><div><h3 class="title">2.1. Learning to use Pacemaker</h3></div></div></div><p class="_abstract _abstract">
				By working through this procedure, you will learn how to use Pacemaker to set up a cluster, how to display cluster status, and how to configure a cluster service. This example creates an Apache HTTP server as a cluster resource and shows how the cluster responds when the resource fails.
			</p><p>
				In this example:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The node is <code class="literal">z1.example.com</code>.
					</li><li class="listitem">
						The floating IP address is 192.168.122.120.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						A single node running RHEL 9
					</li><li class="listitem">
						A floating IP address that resides on the same network as one of the node’s statically assigned IP addresses
					</li><li class="listitem">
						The name of the node on which you are running is in your <code class="literal">/etc/hosts</code> file
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Install the Red Hat High Availability Add-On software packages from the High Availability channel, and start and enable the <code class="literal">pcsd</code> service.
					</p><pre class="literallayout"># <span class="strong strong"><strong>dnf install pcs pacemaker fence-agents-all</strong></span>
...
# <span class="strong strong"><strong>systemctl start pcsd.service</strong></span>
# <span class="strong strong"><strong>systemctl enable pcsd.service</strong></span></pre><p class="simpara">
						If you are running the <code class="literal">firewalld</code> daemon, enable the ports that are required by the Red Hat High Availability Add-On.
					</p><pre class="literallayout"># <span class="strong strong"><strong>firewall-cmd --permanent --add-service=high-availability</strong></span>
# <span class="strong strong"><strong>firewall-cmd --reload</strong></span></pre></li><li class="listitem"><p class="simpara">
						Set a password for user <code class="literal">hacluster</code> on each node in the cluster and authenticate user <code class="literal">hacluster</code> for each node in the cluster on the node from which you will be running the <code class="literal">pcs</code> commands. This example is using only a single node, the node from which you are running the commands, but this step is included here since it is a necessary step in configuring a supported Red Hat High Availability multi-node cluster.
					</p><pre class="literallayout"># <span class="strong strong"><strong>passwd hacluster</strong></span>
...
# <span class="strong strong"><strong>pcs host auth z1.example.com</strong></span></pre></li><li class="listitem"><p class="simpara">
						Create a cluster named <code class="literal">my_cluster</code> with one member and check the status of the cluster. This command creates and starts the cluster in one step.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster setup my_cluster --start z1.example.com</strong></span>
...
# <span class="strong strong"><strong>pcs cluster status</strong></span>
Cluster Status:
 Stack: corosync
 Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
 Last updated: Thu Oct 11 16:11:18 2018
 Last change: Thu Oct 11 16:11:00 2018 by hacluster via crmd on z1.example.com
 1 node configured
 0 resources configured

PCSD Status:
  z1.example.com: Online</pre></li><li class="listitem"><p class="simpara">
						A Red Hat High Availability cluster requires that you configure fencing for the cluster. The reasons for this requirement are described in the Red Hat Knowledgebase solution <a class="link" href="https://access.redhat.com/solutions/15575">Fencing in a Red Hat High Availability Cluster</a>. For this introduction, however, which is intended to show only how to use the basic Pacemaker commands, disable fencing by setting the <code class="literal">stonith-enabled</code> cluster option to <code class="literal">false</code>.
					</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
							The use of <code class="literal">stonith-enabled=false</code> is completely inappropriate for a production cluster. It tells the cluster to simply pretend that failed nodes are safely fenced.
						</p></div></rh-alert><pre class="literallayout"># <span class="strong strong"><strong>pcs property set stonith-enabled=false</strong></span></pre></li><li class="listitem"><p class="simpara">
						Configure a web browser on your system and create a web page to display a simple text message. If you are running the <code class="literal">firewalld</code> daemon, enable the ports that are required by <code class="literal">httpd</code>.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							Do not use <code class="literal command">systemctl enable</code> to enable any services that will be managed by the cluster to start at system boot.
						</p></div></rh-alert><pre class="literallayout"># <span class="strong strong"><strong>dnf install -y httpd wget</strong></span>
...
# <span class="strong strong"><strong>firewall-cmd --permanent --add-service=http</strong></span>
# <span class="strong strong"><strong>firewall-cmd --reload</strong></span>

# <span class="strong strong"><strong>cat &lt;&lt;-END &gt;/var/www/html/index.html</strong></span>
<span class="strong strong"><strong>&lt;html&gt;</strong></span>
<span class="strong strong"><strong>&lt;body&gt;My Test Site - $(hostname)&lt;/body&gt;</strong></span>
<span class="strong strong"><strong>&lt;/html&gt;</strong></span>
<span class="strong strong"><strong>END</strong></span></pre><p class="simpara">
						In order for the Apache resource agent to get the status of Apache, create the following addition to the existing configuration to enable the status server URL.
					</p><pre class="literallayout"># <span class="strong strong"><strong>cat &lt;&lt;-END &gt; /etc/httpd/conf.d/status.conf</strong></span>
<span class="strong strong"><strong>&lt;Location /server-status&gt;</strong></span>
<span class="strong strong"><strong>SetHandler server-status</strong></span>
<span class="strong strong"><strong>Order deny,allow</strong></span>
<span class="strong strong"><strong>Deny from all</strong></span>
<span class="strong strong"><strong>Allow from 127.0.0.1</strong></span>
<span class="strong strong"><strong>Allow from ::1</strong></span>
<span class="strong strong"><strong>&lt;/Location&gt;</strong></span>
<span class="strong strong"><strong>END</strong></span></pre></li><li class="listitem"><p class="simpara">
						Create <code class="literal">IPaddr2</code> and <code class="literal">apache</code> resources for the cluster to manage. The 'IPaddr2' resource is a floating IP address that must not be one already associated with a physical node. If the 'IPaddr2' resource’s NIC device is not specified, the floating IP must reside on the same network as the statically assigned IP address used by the node.
					</p><p class="simpara">
						You can display a list of all available resource types with the <code class="literal command">pcs resource list</code> command. You can use the <code class="literal command">pcs resource describe <span class="emphasis"><em>resourcetype</em></span></code> command to display the parameters you can set for the specified resource type. For example, the following command displays the parameters you can set for a resource of type <code class="literal">apache</code>:
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource describe apache</strong></span>
...</pre><p class="simpara">
						In this example, the IP address resource and the apache resource are both configured as part of a group named <code class="literal">apachegroup</code>, which ensures that the resources are kept together to run on the same node when you are configuring a working multi-node cluster.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create ClusterIP ocf:heartbeat:IPaddr2 ip=192.168.122.120 --group apachegroup</strong></span>

# <span class="strong strong"><strong>pcs resource create WebSite ocf:heartbeat:apache configfile=/etc/httpd/conf/httpd.conf statusurl="http://localhost/server-status" --group apachegroup</strong></span>

# <span class="strong strong"><strong>pcs status</strong></span>
Cluster name: my_cluster
Stack: corosync
Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
Last updated: Fri Oct 12 09:54:33 2018
Last change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com

1 node configured
2 resources configured

Online: [ z1.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com
    WebSite    (ocf::heartbeat:apache):        Started z1.example.com

PCSD Status:
  z1.example.com: Online
...</pre><p class="simpara">
						After you have configured a cluster resource, you can use the <code class="literal command">pcs resource config</code> command to display the options that are configured for that resource.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource config WebSite</strong></span>
Resource: WebSite (class=ocf provider=heartbeat type=apache)
 Attributes: configfile=/etc/httpd/conf/httpd.conf statusurl=http://localhost/server-status
 Operations: start interval=0s timeout=40s (WebSite-start-interval-0s)
             stop interval=0s timeout=60s (WebSite-stop-interval-0s)
             monitor interval=1min (WebSite-monitor-interval-1min)</pre></li><li class="listitem">
						Point your browser to the website you created using the floating IP address you configured. This should display the text message you defined.
					</li><li class="listitem"><p class="simpara">
						Stop the apache web service and check the cluster status. Using <code class="literal command">killall -9</code> simulates an application-level crash.
					</p><pre class="literallayout"># <span class="strong strong"><strong>killall -9 httpd</strong></span></pre><p class="simpara">
						Check the cluster status. You should see that stopping the web service caused a failed action, but that the cluster software restarted the service and you should still be able to access the website.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs status</strong></span>
Cluster name: my_cluster
...
Current DC: z1.example.com (version 1.1.13-10.el7-44eb2dd) - partition with quorum
1 node and 2 resources configured

Online: [ z1.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com
    WebSite    (ocf::heartbeat:apache):        Started z1.example.com

Failed Resource Actions:
* WebSite_monitor_60000 on z1.example.com 'not running' (7): call=13, status=complete, exitreason='none',
    last-rc-change='Thu Oct 11 23:45:50 2016', queued=0ms, exec=0ms

PCSD Status:
    z1.example.com: Online</pre><p class="simpara">
						You can clear the failure status on the resource that failed once the service is up and running again and the failed action notice will no longer appear when you view the cluster status.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource cleanup WebSite</strong></span></pre></li><li class="listitem"><p class="simpara">
						When you are finished looking at the cluster and the cluster status, stop the cluster services on the node. Even though you have only started services on one node for this introduction, the <code class="literal">--all</code> parameter is included since it would stop cluster services on all nodes on an actual multi-node cluster.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster stop --all</strong></span></pre></li></ol></div></section><section class="section" id="proc_learning-to-configure-failover-getting-started-with-pacemaker"><div class="titlepage"><div><div><h3 class="title">2.2. Learning to configure failover</h3></div></div></div><p class="_abstract _abstract">
				The following procedure provides an introduction to creating a Pacemaker cluster running a service that will fail over from one node to another when the node on which the service is running becomes unavailable. By working through this procedure, you can learn how to create a service in a two-node cluster and you can then observe what happens to that service when it fails on the node on which it running.
			</p><p>
				This example procedure configures a two-node Pacemaker cluster running an Apache HTTP server. You can then stop the Apache service on one node to see how the service remains available.
			</p><p>
				In this example:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The nodes are <code class="literal">z1.example.com</code> and <code class="literal">z2.example.com</code>.
					</li><li class="listitem">
						The floating IP address is 192.168.122.120.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Two nodes running RHEL 9 that can communicate with each other
					</li><li class="listitem">
						A floating IP address that resides on the same network as one of the node’s statically assigned IP addresses
					</li><li class="listitem">
						The name of the node on which you are running is in your <code class="literal">/etc/hosts</code> file
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On both nodes, install the Red Hat High Availability Add-On software packages from the High Availability channel, and start and enable the <code class="literal">pcsd</code> service.
					</p><pre class="literallayout"># <span class="strong strong"><strong>dnf install pcs pacemaker fence-agents-all</strong></span>
...
# <span class="strong strong"><strong>systemctl start pcsd.service</strong></span>
# <span class="strong strong"><strong>systemctl enable pcsd.service</strong></span></pre><p class="simpara">
						If you are running the <code class="literal">firewalld</code> daemon, on both nodes enable the ports that are required by the Red Hat High Availability Add-On.
					</p><pre class="literallayout"># <span class="strong strong"><strong>firewall-cmd --permanent --add-service=high-availability</strong></span>
# <span class="strong strong"><strong>firewall-cmd --reload</strong></span></pre></li><li class="listitem"><p class="simpara">
						On both nodes in the cluster, set a password for user <code class="literal">hacluster</code> .
					</p><pre class="literallayout"># <span class="strong strong"><strong>passwd hacluster</strong></span></pre></li><li class="listitem"><p class="simpara">
						Authenticate user <code class="literal">hacluster</code> for each node in the cluster on the node from which you will be running the <code class="literal">pcs</code> commands.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs host auth z1.example.com z2.example.com</strong></span></pre></li><li class="listitem"><p class="simpara">
						Create a cluster named <code class="literal">my_cluster</code> with both nodes as cluster members. This command creates and starts the cluster in one step. You only need to run this from one node in the cluster because <code class="literal">pcs</code> configuration commands take effect for the entire cluster.
					</p><p class="simpara">
						On one node in cluster, run the following command.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster setup my_cluster --start z1.example.com z2.example.com</strong></span></pre></li></ol></div><p>
				A Red Hat High Availability cluster requires that you configure fencing for the cluster. The reasons for this requirement are described in the Red Hat Knowledgebase solution <a class="link" href="https://access.redhat.com/solutions/15575">Fencing in a Red Hat High Availability Cluster</a>. For this introduction, however, to show only how failover works in this configuration, disable fencing by setting the <code class="literal">stonith-enabled</code> cluster option to <code class="literal">false</code>.
			</p><p>
				+
			</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
					The use of <code class="literal">stonith-enabled=false</code> is completely inappropriate for a production cluster. It tells the cluster to simply pretend that failed nodes are safely fenced.
				</p></div></rh-alert><p>
				+
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs property set stonith-enabled=false</strong></span></pre><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						After creating a cluster and disabling fencing, check the status of the cluster.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							When you run the <code class="literal command">pcs cluster status</code> command, it may show output that temporarily differs slightly from the examples as the system components start up.
						</p></div></rh-alert><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster status</strong></span>
Cluster Status:
 Stack: corosync
 Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
 Last updated: Thu Oct 11 16:11:18 2018
 Last change: Thu Oct 11 16:11:00 2018 by hacluster via crmd on z1.example.com
 2 nodes configured
 0 resources configured

PCSD Status:
  z1.example.com: Online
  z2.example.com: Online</pre></li><li class="listitem"><p class="simpara">
						On both nodes, configure a web browser and create a web page to display a simple text message. If you are running the <code class="literal">firewalld</code> daemon, enable the ports that are required by <code class="literal">httpd</code>.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							Do not use <code class="literal command">systemctl enable</code> to enable any services that will be managed by the cluster to start at system boot.
						</p></div></rh-alert><pre class="literallayout"># <span class="strong strong"><strong>dnf install -y httpd wget</strong></span>
...
# <span class="strong strong"><strong>firewall-cmd --permanent --add-service=http</strong></span>
# <span class="strong strong"><strong>firewall-cmd --reload</strong></span>

# <span class="strong strong"><strong>cat &lt;&lt;-END &gt;/var/www/html/index.html</strong></span>
<span class="strong strong"><strong>&lt;html&gt;</strong></span>
<span class="strong strong"><strong>&lt;body&gt;My Test Site - $(hostname)&lt;/body&gt;</strong></span>
<span class="strong strong"><strong>&lt;/html&gt;</strong></span>
<span class="strong strong"><strong>END</strong></span></pre><p class="simpara">
						In order for the Apache resource agent to get the status of Apache, on each node in the cluster create the following addition to the existing configuration to enable the status server URL.
					</p><pre class="literallayout"># <span class="strong strong"><strong>cat &lt;&lt;-END &gt; /etc/httpd/conf.d/status.conf</strong></span>
<span class="strong strong"><strong>&lt;Location /server-status&gt;</strong></span>
<span class="strong strong"><strong>SetHandler server-status</strong></span>
<span class="strong strong"><strong>Order deny,allow</strong></span>
<span class="strong strong"><strong>Deny from all</strong></span>
<span class="strong strong"><strong>Allow from 127.0.0.1</strong></span>
<span class="strong strong"><strong>Allow from ::1</strong></span>
<span class="strong strong"><strong>&lt;/Location&gt;</strong></span>
<span class="strong strong"><strong>END</strong></span></pre></li><li class="listitem"><p class="simpara">
						Create <code class="literal">IPaddr2</code> and <code class="literal">apache</code> resources for the cluster to manage. The 'IPaddr2' resource is a floating IP address that must not be one already associated with a physical node. If the 'IPaddr2' resource’s NIC device is not specified, the floating IP must reside on the same network as the statically assigned IP address used by the node.
					</p><p class="simpara">
						You can display a list of all available resource types with the <code class="literal command">pcs resource list</code> command. You can use the <code class="literal command">pcs resource describe <span class="emphasis"><em>resourcetype</em></span></code> command to display the parameters you can set for the specified resource type. For example, the following command displays the parameters you can set for a resource of type <code class="literal">apache</code>:
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource describe apache</strong></span>
...</pre><p class="simpara">
						In this example, the IP address resource and the apache resource are both configured as part of a group named <code class="literal">apachegroup</code>, which ensures that the resources are kept together to run on the same node.
					</p><p class="simpara">
						Run the following commands from one node in the cluster:
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create ClusterIP ocf:heartbeat:IPaddr2 ip=192.168.122.120 --group apachegroup</strong></span>

# <span class="strong strong"><strong>pcs resource create WebSite ocf:heartbeat:apache configfile=/etc/httpd/conf/httpd.conf statusurl="http://localhost/server-status" --group apachegroup</strong></span>

# <span class="strong strong"><strong>pcs status</strong></span>
Cluster name: my_cluster
Stack: corosync
Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
Last updated: Fri Oct 12 09:54:33 2018
Last change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com

2 nodes configured
2 resources configured

Online: [ z1.example.com z2.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com
    WebSite    (ocf::heartbeat:apache):        Started z1.example.com

PCSD Status:
  z1.example.com: Online
  z2.example.com: Online
...</pre><p class="simpara">
						Note that in this instance, the <code class="literal">apachegroup</code> service is running on node z1.example.com.
					</p></li><li class="listitem"><p class="simpara">
						Access the website you created, stop the service on the node on which it is running, and note how the service fails over to the second node.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Point a browser to the website you created using the floating IP address you configured. This should display the text message you defined, displaying the name of the node on which the website is running.
							</li><li class="listitem"><p class="simpara">
								Stop the apache web service. Using <code class="literal command">killall -9</code> simulates an application-level crash.
							</p><pre class="literallayout"># <span class="strong strong"><strong>killall -9 httpd</strong></span></pre><p class="simpara">
								Check the cluster status. You should see that stopping the web service caused a failed action, but that the cluster software restarted the service on the node on which it had been running and you should still be able to access the web browser.
							</p><pre class="literallayout"># <span class="strong strong"><strong>pcs status</strong></span>
Cluster name: my_cluster
Stack: corosync
Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
Last updated: Fri Oct 12 09:54:33 2018
Last change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com

2 nodes configured
2 resources configured

Online: [ z1.example.com z2.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com
    WebSite    (ocf::heartbeat:apache):        Started z1.example.com

Failed Resource Actions:
* WebSite_monitor_60000 on z1.example.com 'not running' (7): call=31, status=complete, exitreason='none',
    last-rc-change='Fri Feb  5 21:01:41 2016', queued=0ms, exec=0ms</pre><p class="simpara">
								Clear the failure status once the service is up and running again.
							</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource cleanup WebSite</strong></span></pre></li><li class="listitem"><p class="simpara">
								Put the node on which the service is running into standby mode. Note that since we have disabled fencing we can not effectively simulate a node-level failure (such as pulling a power cable) because fencing is required for the cluster to recover from such situations.
							</p><pre class="literallayout"># <span class="strong strong"><strong>pcs node standby z1.example.com</strong></span></pre></li><li class="listitem"><p class="simpara">
								Check the status of the cluster and note where the service is now running.
							</p><pre class="literallayout"># <span class="strong strong"><strong>pcs status</strong></span>
Cluster name: my_cluster
Stack: corosync
Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
Last updated: Fri Oct 12 09:54:33 2018
Last change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com

2 nodes configured
2 resources configured

Node z1.example.com: standby
Online: [ z2.example.com ]

Full list of resources:

Resource Group: apachegroup
    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z2.example.com
    WebSite    (ocf::heartbeat:apache):        Started z2.example.com</pre></li><li class="listitem">
								Access the website. There should be no loss of service, although the display message should indicate the node on which the service is now running.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						To restore cluster services to the first node, take the node out of standby mode. This will not necessarily move the service back to that node.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs node unstandby z1.example.com</strong></span></pre></li><li class="listitem"><p class="simpara">
						For final cleanup, stop the cluster services on both nodes.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster stop --all</strong></span></pre></li></ol></div></section></section><section class="chapter" id="assembly_pcs-operation-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 3. The pcs command-line interface</h2></div></div></div><p class="_abstract _abstract">
			The <code class="literal command">pcs</code> command-line interface controls and configures cluster services such as <code class="literal">corosync</code>, <code class="literal">pacemaker</code>,<code class="literal">booth</code>, and <code class="literal">sbd</code> by providing an easier interface to their configuration files.
		</p><p>
			Note that you should not edit the <code class="literal">cib.xml</code> configuration file directly. In most cases, Pacemaker will reject a directly modified <code class="literal">cib.xml</code> file.
		</p><section class="section" id="proc_pcs-help-pcs-operation"><div class="titlepage"><div><div><h3 class="title">3.1. pcs help display</h3></div></div></div><p class="_abstract _abstract">
				You use the <code class="literal">-h</code> option of <code class="literal command">pcs</code> to display the parameters of a <code class="literal command">pcs</code> command and a description of those parameters.
			</p><p>
				The following command displays the parameters of the <code class="literal command">pcs resource</code> command.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource -h</strong></span></pre></section><section class="section" id="proc_raw-config-pcs-operation"><div class="titlepage"><div><div><h3 class="title">3.2. Viewing the raw cluster configuration</h3></div></div></div><p class="_abstract _abstract">
				Although you should not edit the cluster configuration file directly, you can view the raw cluster configuration with the <code class="literal command">pcs cluster cib</code> command.
			</p><p>
				You can save the raw cluster configuration to a specified file with the <code class="literal command">pcs cluster cib <span class="emphasis"><em>filename</em></span></code> command. If you have previously configured a cluster and there is already an active CIB, you use the following command to save the raw xml file.
			</p><pre class="literallayout">pcs cluster cib <span class="emphasis"><em>filename</em></span></pre><p>
				For example, the following command saves the raw xml from the CIB into a file named <code class="literal">testfile</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster cib testfile</strong></span></pre></section><section class="section" id="proc_configure-testfile-pcs-operation"><div class="titlepage"><div><div><h3 class="title">3.3. Saving a configuration change to a working file</h3></div></div></div><p class="_abstract _abstract">
				When configuring a cluster, you can save configuration changes to a specified file without affecting the active CIB. This allows you to specify configuration updates without immediately updating the currently running cluster configuration with each individual update.
			</p><p>
				For information about saving the CIB to a file, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_pcs-operation-configuring-and-managing-high-availability-clusters#proc_raw-config-pcs-operation">Viewing the raw cluster configuration</a>. Once you have created that file, you can save configuration changes to that file rather than to the active CIB by using the <code class="literal">-f</code> option of the <code class="literal">pcs</code> command. When you have completed the changes and are ready to update the active CIB file, you can push those file updates with the <code class="literal command">pcs cluster cib-push</code> command.
			</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					The following is the recommended procedure for pushing changes to the CIB file. This procedure creates a copy of the original saved CIB file and makes changes to that copy. When pushing those changes to the active CIB, this procedure specifies the <code class="literal">diff-against</code> option of the <code class="literal command">pcs cluster cib-push</code> command so that only the changes between the original file and the updated file are pushed to the CIB. This allows users to make changes in parallel that do not overwrite each other, and it reduces the load on Pacemaker which does not need to parse the entire configuration file.
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Save the active CIB to a file. This example saves the CIB to a file named <code class="literal">original.xml</code>.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster cib original.xml</strong></span></pre></li><li class="listitem"><p class="simpara">
						Copy the saved file to the working file you will be using for the configuration updates.
					</p><pre class="literallayout"># <span class="strong strong"><strong>cp original.xml updated.xml</strong></span></pre></li><li class="listitem"><p class="simpara">
						Update your configuration as needed. The following command creates a resource in the file <code class="literal">updated.xml</code> but does not add that resource to the currently running cluster configuration.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs -f updated.xml resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.120 op monitor interval=30s</strong></span></pre></li><li class="listitem"><p class="simpara">
						Push the updated file to the active CIB, specifying that you are pushing only the changes you have made to the original file.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster cib-push updated.xml diff-against=original.xml</strong></span></pre></li></ol></div><p>
				Alternately, you can push the entire current content of a CIB file with the following command.
			</p><pre class="literallayout">pcs cluster cib-push <span class="emphasis"><em>filename</em></span></pre><p>
				When pushing the entire CIB file, Pacemaker checks the version and does not allow you to push a CIB file which is older than the one already in a cluster. If you need to update the entire CIB file with a version that is older than the one currently in the cluster, you can use the <code class="literal">--config</code> option of the <code class="literal command">pcs cluster cib-push</code> command.
			</p><pre class="literallayout">pcs cluster cib-push --config <span class="emphasis"><em>filename</em></span></pre></section><section class="section" id="proc_cluster-status-pcs-operation"><div class="titlepage"><div><div><h3 class="title">3.4. Displaying cluster status</h3></div></div></div><p class="_abstract _abstract">
				There are a variety of commands you can use to display the status of a cluster and its components.
			</p><p>
				You can display the status of the cluster and the cluster resources with the following command.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs status</strong></span></pre><p>
				You can display the status of a particular cluster component with the <span class="emphasis"><em>commands</em></span> parameter of the <code class="literal command">pcs status</code> command, specifying <code class="literal">resources</code>, <code class="literal">cluster</code>, <code class="literal">nodes</code>, or <code class="literal">pcsd</code>.
			</p><pre class="literallayout">pcs status <span class="emphasis"><em>commands</em></span></pre><p>
				For example, the following command displays the status of the cluster resources.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs status resources</strong></span></pre><p>
				The following command displays the status of the cluster, but not the cluster resources.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster status</strong></span></pre><p>
				If you run the <code class="literal">pcs status</code> command before Pacemaker has completed any actions required by changes to the CIB, the cluster state at that time might not match the desired status. As of RHEL Hat Enterprixe Linux 9.5, you can ensure that Pacemaker does not need to take any further actions by running the <code class="literal">pcs status wait</code> command.
			</p><p>
				The <code class="literal">pcs status wait</code> command waits until the cluster has completed all current actions before returning a value. If any actions unrelated to your recent changes are in progress, the command waits until those are completed. The <code class="literal">pcs status wait</code> command returns a value of 0 as soon as Pacemaker completes pending actions.
			</p><p>
				You can specify a period of time to wait. If the current actions have not completed after that time period, the command prints an error and returns a value of 1.
			</p><p>
				The following command waits until Pacemaker has applied configuration changes.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs status wait</strong></span>
Waiting for the cluster to apply configuration changes...</pre><p>
				The following command waits up to one minute until Pacemaker has applied configuration changes.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs status wait 1min</strong></span>
Waiting for the cluster to apply configuration changes (timeout: 60 seconds)...</pre></section><section class="section" id="proc_cluster-config-display-pcs-operation"><div class="titlepage"><div><div><h3 class="title">3.5. Displaying the full cluster configuration</h3></div></div></div><p class="_abstract _abstract">
				Use the following command to display the full current cluster configuration.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs config</strong></span></pre></section><section class="section" id="proc_resource-status-pcs-operation"><div class="titlepage"><div><div><h3 class="title">3.6. Displaying resource status</h3></div></div></div><p>
				In a complex cluster setup, you might need to determine the status of an individual resource in a cluster before performing a cluster or resource action. As of Red Hat Enterprise Linux 9.5, you can query various attributes of a single resource in a cluster with the <code class="literal">pcs status query resource</code> commands. You can use these commands for pcs-based scripting because there is no need to parse plain text outputs.
			</p><p>
				The <code class="literal">pcs status query resource</code> commands query the following attributes:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						the existence of the resource
					</li><li class="listitem">
						the type of the resource
					</li><li class="listitem">
						the state of the resource
					</li><li class="listitem">
						various information about the members of a collective resource
					</li><li class="listitem">
						on which nodes the resource is running
					</li></ul></div><p>
				The following command queries whether a resource has started.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs status query resource</strong></span> <span class="emphasis"><em>RESOURCE_ID</em></span> <span class="strong strong"><strong>is-state started</strong></span></pre><p>
				For example, the following command queries whether the resource <code class="literal">resource1</code> has started.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs status query resource resource1 is-state started</strong></span>
True</pre><p>
				For a full list of the <code class="literal">pcs status query resource</code> commands, see the <code class="literal">pcs</code>(8) man page on your system.
			</p></section><section class="section" id="proc_pcs-corosync-manage-pcs-operation"><div class="titlepage"><div><div><h3 class="title">3.7. Modifying the corosync.conf file with the pcs command</h3></div></div></div><p class="_abstract _abstract">
				You can use the <code class="literal">pcs</code> command to modify the parameters in the <code class="literal">corosync.conf</code> file.
			</p><p>
				The following command modifies the parameters in the <code class="literal">corosync.conf</code> file.
			</p><pre class="literallayout">pcs cluster config update [transport pass:quotes[<span class="emphasis"><em>transport options</em></span>]] [compression pass:quotes[<span class="emphasis"><em>compression options</em></span>]] [crypto pass:quotes[<span class="emphasis"><em>crypto options</em></span>]] [totem pass:quotes[<span class="emphasis"><em>totem options</em></span>]] [--corosync_conf pass:quotes[<span class="emphasis"><em>path</em></span>]]</pre><p>
				The following example command udates the <code class="literal">knet_pmtud_interval</code> transport value and the <code class="literal">token</code> and <code class="literal">join</code> totem values.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster config update transport knet_pmtud_interval=35 totem token=10000 join=100</strong></span></pre><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						For information about adding and removing nodes from an existing cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_clusternode-management-configuring-and-managing-high-availability-clusters">Managing cluster nodes</a>.
					</li><li class="listitem">
						For information about adding and modifying links in an existing cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_clusternode-management-configuring-and-managing-high-availability-clusters#proc_changing-links-in-multiple-ip-cluster-clusternode-management">Adding and modifying links in an existing cluster</a>.
					</li><li class="listitem">
						For information about modifyng quorum options and managing the quorum device settings in a cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters">Configuring cluster quorum</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-quorum-devices-configuring-and-managing-high-availability-clusters">Configuring quorum devices</a>.
					</li></ul></div></section><section class="section" id="proc_pcs-corosync-display-pcs-operation"><div class="titlepage"><div><div><h3 class="title">3.8. Displaying the corosync.conf file with the pcs command</h3></div></div></div><p class="_abstract _abstract">
				The following command displays the contents of the <code class="literal">corosync.conf</code> cluster configuration file.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster corosync</strong></span></pre><p>
				You can print the contents of the <code class="literal">corosync.conf</code> file in a human-readable format with the <code class="literal">pcs cluster config</code> command, as in the following example.
			</p><p>
				The output for this command includes the UUID for the cluster if the cluster was created in RHEL 9.1 or later, or if the UUID was added manually as described in <a class="link" href="#identifying-cluster-by-uuid-cluster-maintenance" title="31.9. Identifying clusters by UUID">Identifying clusters by UUID</a>.
			</p><pre class="literallayout">[root@r8-node-01 ~]# <span class="strong strong"><strong>pcs cluster config</strong></span>
Cluster Name: HACluster
Cluster UUID: ad4ae07dcafe4066b01f1cc9391f54f5
Transport: knet
Nodes:
  r8-node-01:
    Link 0 address: r8-node-01
    Link 1 address: 192.168.122.121
    nodeid: 1
  r8-node-02:
    Link 0 address: r8-node-02
    Link 1 address: 192.168.122.122
    nodeid: 2
Links:
  Link 1:
    linknumber: 1
    ping_interval: 1000
    ping_timeout: 2000
    pong_count: 5
Compression Options:
  level: 9
  model: zlib
  threshold: 150
Crypto Options:
  cipher: aes256
  hash: sha256
Totem Options:
  downcheck: 2000
  join: 50
  token: 10000
Quorum Device: net
  Options:
    sync_timeout: 2000
    timeout: 3000
  Model Options:
    algorithm: lms
    host: r8-node-03
  Heuristics:
    exec_ping: ping -c 1 127.0.0.1</pre><p>
				You can run the <code class="literal">pcs cluster config show</code> command with the <code class="literal">--output-format=cmd</code> option to display the <code class="literal">pcs</code> configuration commands that can be used to recreate the existing <code class="literal">corosync.conf</code> file, as in the following example.
			</p><pre class="literallayout">[root@r8-node-01 ~]# <span class="strong strong"><strong>pcs cluster config show --output-format=cmd</strong></span>
pcs cluster setup HACluster \
  r8-node-01 addr=r8-node-01 addr=192.168.122.121 \
  r8-node-02 addr=r8-node-02 addr=192.168.122.122 \
  transport \
  knet \
    link \
      linknumber=1 \
      ping_interval=1000 \
      ping_timeout=2000 \
      pong_count=5 \
    compression \
      level=9 \
      model=zlib \
      threshold=150 \
    crypto \
      cipher=aes256 \
      hash=sha256 \
  totem \
    downcheck=2000 \
    join=50 \
    token=10000</pre></section></section><section class="chapter" id="assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 4. Creating a Red Hat High-Availability cluster with Pacemaker</h2></div></div></div><p class="_abstract _abstract">
			Create a Red Hat High Availability two-node cluster using the <code class="literal">pcs</code> command-line interface with the following procedure.
		</p><p>
			Configuring the cluster in this example requires that your system include the following components:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					2 nodes, which will be used to create the cluster. In this example, the nodes used are <code class="literal">z1.example.com</code> and <code class="literal">z2.example.com</code>.
				</li><li class="listitem">
					Network switches for the private network. We recommend but do not require a private network for communication among the cluster nodes and other cluster hardware such as network power switches and Fibre Channel switches.
				</li><li class="listitem">
					A fencing device for each node of the cluster. This example uses two ports of the APC power switch with a host name of <code class="literal">zapc.example.com</code>.
				</li></ul></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
				You must ensure that your configuration conforms to Red Hat’s support policies. For full information about Red Hat’s support policies, requirements, and limitations for RHEL High Availability clusters, see <a class="link" href="https://access.redhat.com/articles/2912891/">Support Policies for RHEL High Availability Clusters</a>.
			</p></div></rh-alert><section class="section" id="proc_installing-cluster-software-creating-high-availability-cluster"><div class="titlepage"><div><div><h3 class="title">4.1. Installing cluster software</h3></div></div></div><p class="_abstract _abstract">
				Install the cluster software and configure your system for cluster creation with the following procedure.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On each node in the cluster, enable the repository for high availability that corresponds to your system architecture. For example, to enable the high availability repository for an x86_64 system, you can enter the following <code class="literal">subscription-manager</code> command:
					</p><pre class="literallayout"># <span class="strong strong"><strong>subscription-manager repos --enable=rhel-9-for-x86_64-highavailability-rpms</strong></span></pre></li><li class="listitem"><p class="simpara">
						On each node in the cluster, install the Red Hat High Availability Add-On software packages along with all available fence agents from the High Availability channel.
					</p><pre class="literallayout"># <span class="strong strong"><strong>dnf install pcs pacemaker fence-agents-all</strong></span></pre><p class="simpara">
						Alternatively, you can install the Red Hat High Availability Add-On software packages along with only the fence agent that you require with the following command.
					</p><pre class="literallayout"># <span class="strong strong"><strong>dnf install pcs pacemaker fence-agents-<span class="emphasis"><em>model</em></span></strong></span></pre><p class="simpara">
						The following command displays a list of the available fence agents.
					</p><pre class="literallayout"># <span class="strong strong"><strong>rpm -q -a | grep fence</strong></span>
fence-agents-rhevm-4.0.2-3.el7.x86_64
fence-agents-ilo-mp-4.0.2-3.el7.x86_64
fence-agents-ipmilan-4.0.2-3.el7.x86_64
...</pre><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
							After you install the Red Hat High Availability Add-On packages, you should ensure that your software update preferences are set so that nothing is installed automatically. Installation on a running cluster can cause unexpected behaviors. For more information, see <a class="link" href="https://access.redhat.com/articles/2059253/">Recommended Practices for Applying Software Updates to a RHEL High Availability or Resilient Storage Cluster</a>.
						</p></div></rh-alert></li><li class="listitem"><p class="simpara">
						If you are running the <code class="literal command">firewalld</code> daemon, execute the following commands to enable the ports that are required by the Red Hat High Availability Add-On.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							You can determine whether the <code class="literal command">firewalld</code> daemon is installed on your system with the <code class="literal command">rpm -q firewalld</code> command. If it is installed, you can determine whether it is running with the <code class="literal command">firewall-cmd --state</code> command.
						</p></div></rh-alert><pre class="literallayout"># <span class="strong strong"><strong>firewall-cmd --permanent --add-service=high-availability</strong></span>
# <span class="strong strong"><strong>firewall-cmd --add-service=high-availability</strong></span></pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							The ideal firewall configuration for cluster components depends on the local environment, where you may need to take into account such considerations as whether the nodes have multiple network interfaces or whether off-host firewalling is present. The example here, which opens the ports that are generally required by a Pacemaker cluster, should be modified to suit local conditions. <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters#proc_enabling-ports-for-high-availability-creating-high-availability-cluster">Enabling ports for the High Availability Add-On</a> shows the ports to enable for the Red Hat High Availability Add-On and provides an explanation for what each port is used for.
						</p></div></rh-alert></li><li class="listitem"><p class="simpara">
						In order to use <code class="literal">pcs</code> to configure the cluster and communicate among the nodes, you must set a password on each node for the user ID <code class="literal">hacluster</code>, which is the <code class="literal">pcs</code> administration account. It is recommended that the password for user <code class="literal">hacluster</code> be the same on each node.
					</p><pre class="literallayout"># <span class="strong strong"><strong>passwd hacluster</strong></span>
Changing password for user hacluster.
New password:
Retype new password:
passwd: all authentication tokens updated successfully.</pre></li><li class="listitem"><p class="simpara">
						Before the cluster can be configured, the <code class="literal command">pcsd</code> daemon must be started and enabled to start up on boot on each node. This daemon works with the <code class="literal command">pcs</code> command to manage configuration across the nodes in the cluster.
					</p><p class="simpara">
						On each node in the cluster, execute the following commands to start the <code class="literal">pcsd</code> service and to enable <code class="literal">pcsd</code> at system start.
					</p><pre class="literallayout"># <span class="strong strong"><strong>systemctl start pcsd.service</strong></span>
# <span class="strong strong"><strong>systemctl enable pcsd.service</strong></span></pre></li></ol></div></section><section class="section" id="proc_installing-pcp-zeroconf-creating-high-availability-cluster"><div class="titlepage"><div><div><h3 class="title">4.2. Installing the pcp-zeroconf package (recommended)</h3></div></div></div><p class="_abstract _abstract">
				When you set up your cluster, it is recommended that you install the <code class="literal">pcp-zeroconf</code> package for the Performance Co-Pilot (PCP) tool. PCP is Red Hat’s recommended resource-monitoring tool for RHEL systems. Installing the <code class="literal">pcp-zeroconf</code> package allows you to have PCP running and collecting performance-monitoring data for the benefit of investigations into fencing, resource failures, and other events that disrupt the cluster.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					Cluster deployments where PCP is enabled will need sufficient space available for PCP’s captured data on the file system that contains <code class="literal">/var/log/pcp/</code>. Typical space usage by PCP varies across deployments, but 10Gb is usually sufficient when using the <code class="literal">pcp-zeroconf</code> default settings, and some environments may require less. Monitoring usage in this directory over a 14-day period of typical activity can provide a more accurate usage expectation.
				</p></div></rh-alert><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					To install the <code class="literal">pcp-zeroconf</code> package, run the following command.
				</p></div><pre class="literallayout"># <span class="strong strong"><strong>dnf install pcp-zeroconf</strong></span></pre><p>
				This package enables <code class="literal">pmcd</code> and sets up data capture at a 10-second interval.
			</p><p>
				For information about reviewing PCP data, see the Red Hat Knowledgebase solution <a class="link" href="https://access.redhat.com/solutions/4545111">Why did a RHEL High Availability cluster node reboot - and how can I prevent it from happening again?</a>.
			</p></section><section class="section" id="proc_creating-high-availability-cluster-creating-high-availability-cluster"><div class="titlepage"><div><div><h3 class="title">4.3. Creating a high availability cluster</h3></div></div></div><p class="_abstract _abstract">
				Create a Red Hat High Availability Add-On cluster with the following procedure. This example procedure creates a cluster that consists of the nodes <code class="literal">z1.example.com</code> and <code class="literal">z2.example.com</code>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Authenticate the <code class="literal command">pcs</code> user <code class="literal">hacluster</code> for each node in the cluster on the node from which you will be running <code class="literal command">pcs</code>.
					</p><p class="simpara">
						The following command authenticates user <code class="literal">hacluster</code> on <code class="literal">z1.example.com</code> for both of the nodes in a two-node cluster that will consist of <code class="literal">z1.example.com</code> and <code class="literal">z2.example.com</code>.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs host auth z1.example.com z2.example.com</strong></span>
Username: <span class="strong strong"><strong>hacluster</strong></span>
Password:
z1.example.com: Authorized
z2.example.com: Authorized</pre></li><li class="listitem"><p class="simpara">
						Execute the following command from <code class="literal">z1.example.com</code> to create the two-node cluster <code class="literal">my_cluster</code> that consists of nodes <code class="literal">z1.example.com</code> and <code class="literal">z2.example.com</code>. This will propagate the cluster configuration files to both nodes in the cluster. This command includes the <code class="literal">--start</code> option, which will start the cluster services on both nodes in the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs cluster setup my_cluster --start z1.example.com z2.example.com</strong></span></pre></li><li class="listitem"><p class="simpara">
						Enable the cluster services to run on each node in the cluster when the node is booted.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							For your particular environment, you may choose to leave the cluster services disabled by skipping this step. This allows you to ensure that if a node goes down, any issues with your cluster or your resources are resolved before the node rejoins the cluster. If you leave the cluster services disabled, you will need to manually start the services when you reboot a node by executing the <code class="literal command">pcs cluster start</code> command on that node.
						</p></div></rh-alert><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs cluster enable --all</strong></span></pre></li></ol></div><p>
				You can display the current status of the cluster with the <code class="literal command">pcs cluster status</code> command. Because there may be a slight delay before the cluster is up and running when you start the cluster services with the <code class="literal option">--start</code> option of the <code class="literal command">pcs cluster setup</code> command, you should ensure that the cluster is up and running before performing any subsequent actions on the cluster and its configuration.
			</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs cluster status</strong></span>
Cluster Status:
 Stack: corosync
 Current DC: z2.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum
 Last updated: Thu Oct 11 16:11:18 2018
 Last change: Thu Oct 11 16:11:00 2018 by hacluster via crmd on z2.example.com
 2 Nodes configured
 0 Resources configured

...</pre></section><section class="section" id="proc_configure-multiple-ip-cluster-creating-high-availability-cluster"><div class="titlepage"><div><div><h3 class="title">4.4. Creating a high availability cluster with multiple links</h3></div></div></div><p class="_abstract _abstract">
				You can use the <code class="literal command">pcs cluster setup</code> command to create a Red Hat High Availability cluster with multiple links by specifying all of the links for each node.
			</p><p>
				The format for the basic command to create a two-node cluster with two links is as follows.
			</p><pre class="literallayout">pcs cluster setup pass:quotes[<span class="emphasis"><em>cluster_name</em></span>] pass:quotes[<span class="emphasis"><em>node1_name</em></span>] addr=pass:quotes[<span class="emphasis"><em>node1_link0_address</em></span>] addr=pass:quotes[<span class="emphasis"><em>node1_link1_address</em></span>] pass:quotes[<span class="emphasis"><em>node2_name</em></span>] addr=pass:quotes[<span class="emphasis"><em>node2_link0_address</em></span>] addr=pass:quotes[<span class="emphasis"><em>node2_link1_address</em></span>]</pre><p>
				For the full syntax of this command, see the <code class="literal">pcs</code>(8) man page.
			</p><p>
				When creating a cluster with multiple links, you should take the following into account.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The order of the <code class="literal">addr=<span class="emphasis"><em>address</em></span></code> parameters is important. The first address specified after a node name is for <code class="literal">link0</code>, the second one for <code class="literal">link1</code>, and so forth.
					</li><li class="listitem">
						By default, if <code class="literal">link_priority</code> is not specified for a link, the link’s priority is equal to the link number. The link priorities are then 0, 1, 2, 3, and so forth, according to the order specified, with 0 being the highest link priority.
					</li><li class="listitem">
						The default link mode is <code class="literal">passive</code>, meaning the active link with the lowest-numbered link priority is used.
					</li><li class="listitem">
						With the default values of <code class="literal">link_mode</code> and <code class="literal">link_priority</code>, the first link specified will be used as the highest priority link, and if that link fails the next link specified will be used.
					</li><li class="listitem">
						It is possible to specify up to eight links using the <code class="literal">knet</code> transport protocol, which is the default transport protocol.
					</li><li class="listitem">
						All nodes must have the same number of <code class="literal">addr=</code> parameters.
					</li><li class="listitem">
						It is possible to add, remove, and change links in an existing cluster using the <code class="literal">pcs cluster link add</code>, the <code class="literal">pcs cluster link remove</code>, the <code class="literal">pcs cluster link delete</code>, and the <code class="literal">pcs cluster link update</code> commands.
					</li><li class="listitem">
						As with single-link clusters, do not mix IPv4 and IPv6 addresses in one link, although you can have one link running IPv4 and the other running IPv6.
					</li><li class="listitem">
						As with single-link clusters, you can specify addresses as IP addresses or as names as long as the names resolve to IPv4 or IPv6 addresses for which IPv4 and IPv6 addresses are not mixed in one link.
					</li></ul></div><p>
				The following example creates a two-node cluster named <code class="literal">my_twolink_cluster</code> with two nodes, <code class="literal">rh80-node1</code> and <code class="literal">rh80-node2</code>. <code class="literal">rh80-node1</code> has two interfaces, IP address 192.168.122.201 as <code class="literal">link0</code> and 192.168.123.201 as <code class="literal">link1</code>. <code class="literal">rh80-node2</code> has two interfaces, IP address 192.168.122.202 as <code class="literal">link0</code> and 192.168.123.202 as <code class="literal">link1</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster setup my_twolink_cluster rh80-node1 addr=192.168.122.201 addr=192.168.123.201 rh80-node2 addr=192.168.122.202 addr=192.168.123.202</strong></span></pre><p>
				To set a link priority to a different value than the default value, which is the link number, you can set the link priority with the <code class="literal">link_priority</code> option of the <code class="literal">pcs cluster setup</code> command. Each of the following two example commands creates a two-node cluster with two interfaces where the first link, link 0, has a link priority of 1 and the second link, link 1, has a link priority of 0. Link 1 will be used first and link 0 will serve as the failover link. Since link mode is not specified, it defaults to passive.
			</p><p>
				These two commands are equivalent. If you do not specify a link number following the <code class="literal">link</code> keyword, the <code class="literal">pcs</code> interface automatically adds a link number, starting with the lowest unused link number.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster setup my_twolink_cluster rh80-node1 addr=192.168.122.201 addr=192.168.123.201 rh80-node2 addr=192.168.122.202 addr=192.168.123.202 transport knet link link_priority=1 link link_priority=0</strong></span>

# <span class="strong strong"><strong>pcs cluster setup my_twolink_cluster rh80-node1 addr=192.168.122.201 addr=192.168.123.201 rh80-node2 addr=192.168.122.202 addr=192.168.123.202 transport knet link linknumber=1 link_priority=0 link link_priority=1</strong></span></pre><p>
				You can set the link mode to a different value than the default value of <code class="literal">passive</code> with the <code class="literal">link_mode</code> option of the <code class="literal">pcs cluster setup</code> command, as in the following example.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster setup my_twolink_cluster rh80-node1 addr=192.168.122.201 addr=192.168.123.201 rh80-node2 addr=192.168.122.202 addr=192.168.123.202 transport knet link_mode=active</strong></span></pre><p>
				The following example sets both the link mode and the link priority.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster setup my_twolink_cluster rh80-node1 addr=192.168.122.201 addr=192.168.123.201 rh80-node2 addr=192.168.122.202 addr=192.168.123.202 transport knet link_mode=active link link_priority=1 link link_priority=0</strong></span></pre><p>
				For information about adding nodes to an existing cluster with multiple links, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_clusternode-management-configuring-and-managing-high-availability-clusters#proc_add-nodes-to-multiple-ip-cluster-clusternode-management">Adding a node to a cluster with multiple links</a>.
			</p><p>
				For information about changing the links in an existing cluster with multiple links, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_clusternode-management-configuring-and-managing-high-availability-clusters#proc_changing-links-in-multiple-ip-cluster-clusternode-management">Adding and modifying links in an existing cluster</a>.
			</p></section><section class="section" id="proc_configuring-fencing-creating-high-availability-cluster"><div class="titlepage"><div><div><h3 class="title">4.5. Configuring fencing</h3></div></div></div><p class="_abstract _abstract">
				You must configure a fencing device for each node in the cluster. For information about the fence configuration commands and options, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters">Configuring fencing in a Red Hat High Availability cluster</a>.
			</p><p>
				For general information about fencing and its importance in a Red Hat High Availability cluster, see the Red Hat Knowledgebase solution <a class="link" href="https://access.redhat.com/solutions/15575">Fencing in a Red Hat High Availability Cluster</a>.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					When configuring a fencing device, attention should be given to whether that device shares power with any nodes or devices in the cluster. If a node and its fence device do share power, then the cluster may be at risk of being unable to fence that node if the power to it and its fence device should be lost. Such a cluster should either have redundant power supplies for fence devices and nodes, or redundant fence devices that do not share power. Alternative methods of fencing such as SBD or storage fencing may also bring redundancy in the event of isolated power losses.
				</p></div></rh-alert><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					This example uses the APC power switch with a host name of <code class="literal">zapc.example.com</code> to fence the nodes, and it uses the <code class="literal">fence_apc_snmp</code> fencing agent. Because both nodes will be fenced by the same fencing agent, you can configure both fencing devices as a single resource, using the <code class="literal">pcmk_host_map</code> option.
				</p></div><p>
				You create a fencing device by configuring the device as a <code class="literal">stonith</code> resource with the <code class="literal command">pcs stonith create</code> command. The following command configures a <code class="literal">stonith</code> resource named <code class="literal">myapc</code> that uses the <code class="literal">fence_apc_snmp</code> fencing agent for nodes <code class="literal">z1.example.com</code> and <code class="literal">z2.example.com</code>. The <code class="literal">pcmk_host_map</code> option maps <code class="literal">z1.example.com</code> to port 1, and <code class="literal">z2.example.com</code> to port 2. The login value and password for the APC device are both <code class="literal">apc</code>. By default, this device will use a monitor interval of sixty seconds for each node.
			</p><p>
				Note that you can use an IP address when specifying the host name for the nodes.
			</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs stonith create myapc fence_apc_snmp ipaddr="zapc.example.com" pcmk_host_map="z1.example.com:1;z2.example.com:2" login="apc" passwd="apc"</strong></span></pre><p>
				The following command displays the parameters of an existing fencing device.
			</p><pre class="literallayout">[root@rh7-1 ~]# <span class="strong strong"><strong>pcs stonith config myapc</strong></span>
 Resource: myapc (class=stonith type=fence_apc_snmp)
  Attributes: ipaddr=zapc.example.com pcmk_host_map=z1.example.com:1;z2.example.com:2 login=apc passwd=apc
  Operations: monitor interval=60s (myapc-monitor-interval-60s)</pre><p>
				After configuring your fence device, you should test the device. For information about testing a fence device, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters#proc_testing-fence-devices-configuring-fencing">Testing a fence device</a>.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					Do not test your fence device by disabling the network interface, as this will not properly test fencing.
				</p></div></rh-alert><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					Once fencing is configured and a cluster has been started, a network restart will trigger fencing for the node which restarts the network even when the timeout is not exceeded. For this reason, do not restart the network service while the cluster service is running because it will trigger unintentional fencing on the node.
				</p></div></rh-alert></section><section class="section" id="proc_cluster-backup-creating-high-availability-cluster"><div class="titlepage"><div><div><h3 class="title">4.6. Backing up and restoring a cluster configuration</h3></div></div></div><p class="_abstract _abstract">
				The following commands back up a cluster configuration in a tar archive and restore the cluster configuration files on all nodes from the backup.
			</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					Use the following command to back up the cluster configuration in a tar archive. If you do not specify a file name, the standard output will be used.
				</p></div><pre class="literallayout">pcs config backup <span class="emphasis"><em>filename</em></span></pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					The <code class="literal command">pcs config backup</code> command backs up only the cluster configuration itself as configured in the CIB; the configuration of resource daemons is out of the scope of this command. For example if you have configured an Apache resource in the cluster, the resource settings (which are in the CIB) will be backed up, while the Apache daemon settings (as set in`/etc/httpd`) and the files it serves will not be backed up. Similarly, if there is a database resource configured in the cluster, the database itself will not be backed up, while the database resource configuration (CIB) will be.
				</p></div></rh-alert><p>
				Use the following command to restore the cluster configuration files on all cluster nodes from the backup. Specifying the <code class="literal">--local</code> option restores the cluster configuration files only on the node from which you run this command. If you do not specify a file name, the standard input will be used.
			</p><pre class="literallayout">pcs config restore [--local] [<span class="emphasis"><em>filename</em></span>]</pre></section><section class="section" id="proc_enabling-ports-for-high-availability-creating-high-availability-cluster"><div class="titlepage"><div><div><h3 class="title">4.7. Enabling ports for the High Availability Add-On</h3></div></div></div><p class="_abstract _abstract">
				The ideal firewall configuration for cluster components depends on the local environment, where you may need to take into account such considerations as whether the nodes have multiple network interfaces or whether off-host firewalling is present.
			</p><p>
				If you are running the <code class="literal command">firewalld</code> daemon, execute the following commands to enable the ports that are required by the Red Hat High Availability Add-On.
			</p><pre class="literallayout"># <span class="strong strong"><strong>firewall-cmd --permanent --add-service=high-availability</strong></span>
# <span class="strong strong"><strong>firewall-cmd --add-service=high-availability</strong></span></pre><p>
				You may need to modify which ports are open to suit local conditions.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					You can determine whether the <code class="literal command">firewalld</code> daemon is installed on your system with the <code class="literal command">rpm -q firewalld</code> command. If the <code class="literal command">firewalld</code> daemon is installed, you can determine whether it is running with the <code class="literal command">firewall-cmd --state</code> command.
				</p></div></rh-alert><p>
				The following table shows the ports to enable for the Red Hat High Availability Add-On and provides an explanation for what the port is used for.
			</p><rh-table id="tb-portenable-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 4.1. Ports to Enable for High Availability Add-On</caption><colgroup><col style="width: 50%; " class="col_1"><!--Empty--><col style="width: 50%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686152097600" scope="col">Port</th><th align="left" valign="top" id="idm140686152096512" scope="col">When Required</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686152097600"> <p>
								TCP 2224
							</p>
							 </td><td align="left" valign="top" headers="idm140686152096512"> <p>
								Default <code class="literal">pcsd</code> port required on all nodes (needed by the pcsd Web UI and required for node-to-node communication). You can configure the <code class="literal">pcsd</code> port by means of the <code class="literal">PCSD_PORT</code> parameter in the <code class="literal">/etc/sysconfig/pcsd</code> file.
							</p>
							 <p>
								It is crucial to open port 2224 in such a way that <code class="literal command">pcs</code> from any node can talk to all nodes in the cluster, including itself. When using the Booth cluster ticket manager or a quorum device you must open port 2224 on all related hosts, such as Booth arbitrators or the quorum device host.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686152097600"> <p>
								TCP 3121
							</p>
							 </td><td align="left" valign="top" headers="idm140686152096512"> <p>
								Required on all nodes if the cluster has any Pacemaker Remote nodes
							</p>
							 <p>
								Pacemaker’s <code class="literal">pacemaker-based</code> daemon on the full cluster nodes will contact the <code class="literal">pacemaker_remoted</code> daemon on Pacemaker Remote nodes at port 3121. If a separate interface is used for cluster communication, the port only needs to be open on that interface. At a minimum, the port should open on Pacemaker Remote nodes to full cluster nodes. Because users may convert a host between a full node and a remote node, or run a remote node inside a container using the host’s network, it can be useful to open the port to all nodes. It is not necessary to open the port to any hosts other than nodes.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686152097600"> <p>
								TCP 5403
							</p>
							 </td><td align="left" valign="top" headers="idm140686152096512"> <p>
								Required on the quorum device host when using a quorum device with <code class="literal">corosync-qnetd</code>. The default value can be changed with the <code class="literal option">-p</code> option of the <code class="literal command">corosync-qnetd</code> command.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686152097600"> <p>
								UDP 5404-5412
							</p>
							 </td><td align="left" valign="top" headers="idm140686152096512"> <p>
								Required on corosync nodes to facilitate communication between nodes. It is crucial to open ports 5404-5412 in such a way that <code class="literal">corosync</code> from any node can talk to all nodes in the cluster, including itself.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686152097600"> <p>
								TCP 21064
							</p>
							 </td><td align="left" valign="top" headers="idm140686152096512"> <p>
								Required on all nodes if the cluster contains any resources requiring DLM (such as <code class="literal">GFS2</code>).
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686152097600"> <p>
								TCP 9929, UDP 9929
							</p>
							 </td><td align="left" valign="top" headers="idm140686152096512"> <p>
								Required to be open on all cluster nodes and Booth arbitrator nodes to connections from any of those same nodes when the Booth ticket manager is used to establish a multi-site cluster.
							</p>
							 </td></tr></tbody></table></rh-table></section></section><section class="chapter" id="assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 5. Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster</h2></div></div></div><p class="_abstract _abstract">
			Configure an active/passive Apache HTTP server in a two-node Red Hat Enterprise Linux High Availability Add-On cluster with the following procedure. In this use case, clients access the Apache HTTP server through a floating IP address. The web server runs on one of two nodes in the cluster. If the node on which the web server is running becomes inoperative, the web server starts up again on the second node of the cluster with minimal service interruption.
		</p><p>
			The following illustration shows a high-level overview of the cluster in which the cluster is a two-node Red Hat High Availability cluster which is configured with a network power switch and with shared storage. The cluster nodes are connected to a public network, for client access to the Apache HTTP server through a virtual IP. The Apache server runs on either Node 1 or Node 2, each of which has access to the storage on which the Apache data is kept. In this illustration, the web server is running on Node 1 while Node 2 is available to run the server if Node 1 becomes inoperative.
		</p><div class="figure" id="idm140686155947552"><p class="title"><strong>Figure 5.1. Apache in a Red Hat High Availability Two-Node Cluster</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/Red_Hat_Enterprise_Linux-9-Configuring_and_managing_high_availability_clusters-en-US/images/1329e69b8957bc6e39af37981d99dbb3/291627-haserver_cluster4.png" alt="Apache in a Red Hat High Availability Two-Node Cluster"></div></div></div><p>
			This use case requires that your system include the following components:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					A two-node Red Hat High Availability cluster with power fencing configured for each node. We recommend but do not require a private network. This procedure uses the cluster example provided in <a class="link" href="#assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters" title="Chapter 4. Creating a Red Hat High-Availability cluster with Pacemaker">Creating a Red Hat High-Availability cluster with Pacemaker</a>.
				</li><li class="listitem">
					A public virtual IP address, required for Apache.
				</li><li class="listitem">
					Shared storage for the nodes in the cluster, using iSCSI, Fibre Channel, or other shared network block device.
				</li></ul></div><p>
			The cluster is configured with an Apache resource group, which contains the cluster components that the web server requires: an LVM resource, a file system resource, an IP address resource, and a web server resource. This resource group can fail over from one node of the cluster to the other, allowing either node to run the web server. Before creating the resource group for this cluster, you will be performing the following procedures:
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					Configure an XFS file system on the logical volume <code class="literal">my_lv</code>.
				</li><li class="listitem">
					Configure a web server.
				</li></ol></div><p>
			After performing these steps, you create the resource group and the resources it contains.
		</p><section class="section" id="proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http"><div class="titlepage"><div><div><h3 class="title">5.1. Configuring an LVM volume with an XFS file system in a Pacemaker cluster</h3></div></div></div><p class="_abstract _abstract">
				Create an LVM logical volume on storage that is shared between the nodes of the cluster with the following procedure.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					LVM volumes and the corresponding partitions and devices used by cluster nodes must be connected to the cluster nodes only.
				</p></div></rh-alert><p>
				The following procedure creates an LVM logical volume and then creates an XFS file system on that volume for use in a Pacemaker cluster. In this example, the shared partition <code class="literal">/dev/sdb1</code> is used to store the LVM physical volume from which the LVM logical volume will be created.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On both nodes of the cluster, perform the following steps to set the value for the LVM system ID to the value of the <code class="literal">uname</code> identifier for the system. The LVM system ID will be used to ensure that only the cluster is capable of activating the volume group.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Set the <code class="literal">system_id_source</code> configuration option in the <code class="literal">/etc/lvm/lvm.conf</code> configuration file to <code class="literal">uname</code>.
							</p><pre class="literallayout"># Configuration option global/system_id_source.
system_id_source = "uname"</pre></li><li class="listitem"><p class="simpara">
								Verify that the LVM system ID on the node matches the <code class="literal">uname</code> for the node.
							</p><pre class="literallayout"># <span class="strong strong"><strong>lvm systemid</strong></span>
  system ID: z1.example.com
# <span class="strong strong"><strong>uname -n</strong></span>
  z1.example.com</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Create the LVM volume and create an XFS file system on that volume. Since the <code class="literal">/dev/sdb1</code> partition is storage that is shared, you perform this part of the procedure on one node only.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							If your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order">Configuring startup order for resource dependencies not managed by Pacemaker</a>.
						</p></div></rh-alert><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create an LVM physical volume on partition <code class="literal">/dev/sdb1</code>.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pvcreate /dev/sdb1</strong></span>
  Physical volume "/dev/sdb1" successfully created</pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									If your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order">Configuring startup order for resource dependencies not managed by Pacemaker</a>.
								</p></div></rh-alert></li><li class="listitem"><p class="simpara">
								Create the volume group <code class="literal">my_vg</code> that consists of the physical volume <code class="literal">/dev/sdb1</code>.
							</p><p class="simpara">
								Specify the <code class="literal">--setautoactivation n</code> flag to ensure that volume groups managed by Pacemaker in a cluster will not be automatically activated on startup. If you are using an existing volume group for the LVM volume you are creating, you can reset this flag with the <code class="literal">vgchange --setautoactivation n</code> command for the volume group.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>vgcreate --setautoactivation n my_vg /dev/sdb1</strong></span>
  Volume group "my_vg" successfully created</pre></li><li class="listitem"><p class="simpara">
								Verify that the new volume group has the system ID of the node on which you are running and from which you created the volume group.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>vgs -o+systemid</strong></span>
  VG    #PV #LV #SN Attr   VSize  VFree  System ID
  my_vg   1   0   0 wz--n- &lt;1.82t &lt;1.82t z1.example.com</pre></li><li class="listitem"><p class="simpara">
								Create a logical volume using the volume group <code class="literal">my_vg</code>.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvcreate -L450 -n my_lv my_vg</strong></span>
  Rounding up size to full physical extent 452.00 MiB
  Logical volume "my_lv" created</pre><p class="simpara">
								You can use the <code class="literal command">lvs</code> command to display the logical volume.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvs</strong></span>
  LV      VG      Attr      LSize   Pool Origin Data%  Move Log Copy%  Convert
  my_lv   my_vg   -wi-a---- 452.00m
  ...</pre></li><li class="listitem"><p class="simpara">
								Create an XFS file system on the logical volume <code class="literal">my_lv</code>.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>mkfs.xfs /dev/my_vg/my_lv</strong></span>
meta-data=/dev/my_vg/my_lv       isize=512    agcount=4, agsize=28928 blks
         =                       sectsz=512   attr=2, projid32bit=1
...</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						If the use of a devices file is enabled with the <code class="literal">use_devicesfile = 1</code> parameter in the <code class="literal">lvm.conf</code> file, add the shared device to the devices file on the second node in the cluster. This feature is enabled by default.
					</p><pre class="literallayout">[root@z2 ~]# <span class="strong strong"><strong>lvmdevices --adddev /dev/sdb1</strong></span></pre></li></ol></div></section><section class="section" id="proc_configuring-apache-http-web-server-configuring-ha-http"><div class="titlepage"><div><div><h3 class="title">5.2. Configuring an Apache HTTP Server</h3></div></div></div><p class="_abstract _abstract">
				Configure an Apache HTTP Server with the following procedure.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Ensure that the Apache HTTP Server is installed on each node in the cluster. You also need the <code class="literal">wget</code> tool installed on the cluster to be able to check the status of the Apache HTTP Server.
					</p><p class="simpara">
						On each node, execute the following command.
					</p><pre class="literallayout"># <span class="strong strong"><strong>dnf install -y httpd wget</strong></span></pre><p class="simpara">
						If you are running the <code class="literal">firewalld</code> daemon, on each node in the cluster enable the ports that are required by the Red Hat High Availability Add-On and enable the ports you will require for running <code class="literal">httpd</code>. This example enables the <code class="literal">httpd</code> ports for public access, but the specific ports to enable for <code class="literal">httpd</code> may vary for production use.
					</p><pre class="literallayout"># <span class="strong strong"><strong>firewall-cmd --permanent --add-service=http</strong></span>
# <span class="strong strong"><strong>firewall-cmd --permanent --zone=public --add-service=http</strong></span>
# <span class="strong strong"><strong>firewall-cmd --reload</strong></span></pre></li><li class="listitem"><p class="simpara">
						In order for the Apache resource agent to get the status of Apache, on each node in the cluster create the following addition to the existing configuration to enable the status server URL.
					</p><pre class="literallayout"># <span class="strong strong"><strong>cat &lt;&lt;-END &gt; /etc/httpd/conf.d/status.conf</strong></span>
<span class="strong strong"><strong>&lt;Location /server-status&gt;</strong></span>
    <span class="strong strong"><strong>SetHandler server-status</strong></span>
    <span class="strong strong"><strong>Require local</strong></span>
<span class="strong strong"><strong>&lt;/Location&gt;</strong></span>
<span class="strong strong"><strong>END</strong></span></pre></li><li class="listitem"><p class="simpara">
						Create a web page for Apache to serve up.
					</p><p class="simpara">
						On one node in the cluster, ensure that the logical volume you created in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http">Configuring an LVM volume with an XFS file system</a> is activated, mount the file system that you created on that logical volume, create the file <code class="literal">index.html</code> on that file system, and then unmount the file system.
					</p><pre class="literallayout"># <span class="strong strong"><strong>lvchange -ay my_vg/my_lv</strong></span>
# <span class="strong strong"><strong>mount /dev/my_vg/my_lv /var/www/</strong></span>
# <span class="strong strong"><strong>mkdir /var/www/html</strong></span>
# <span class="strong strong"><strong>mkdir /var/www/cgi-bin</strong></span>
# <span class="strong strong"><strong>mkdir /var/www/error</strong></span>
# <span class="strong strong"><strong>restorecon -R /var/www</strong></span>
# <span class="strong strong"><strong>cat &lt;&lt;-END &gt;/var/www/html/index.html</strong></span>
<span class="strong strong"><strong>&lt;html&gt;</strong></span>
<span class="strong strong"><strong>&lt;body&gt;Hello&lt;/body&gt;</strong></span>
<span class="strong strong"><strong>&lt;/html&gt;</strong></span>
<span class="strong strong"><strong>END</strong></span>
# <span class="strong strong"><strong>umount /var/www</strong></span></pre></li></ol></div></section><section class="section" id="proc_configuring-resources-for-http-server-in-a-cluster-configuring-ha-http"><div class="titlepage"><div><div><h3 class="title">5.3. Creating the resources and resource groups</h3></div></div></div><p class="_abstract _abstract">
				Create the resources for your cluster with the following procedure. To ensure these resources all run on the same node, they are configured as part of the resource group <code class="literal">apachegroup</code>. The resources to create are as follows, listed in the order in which they will start.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						An <code class="literal">LVM-activate</code> resource named <code class="literal">my_lvm</code> that uses the LVM volume group you created in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs">Configuring an LVM volume with an XFS file system</a>.
					</li><li class="listitem">
						A <code class="literal">Filesystem</code> resource named <code class="literal">my_fs</code>, that uses the file system device <code class="literal">/dev/my_vg/my_lv</code> you created in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs">Configuring an LVM volume with an XFS file system</a>.
					</li><li class="listitem">
						An <code class="literal">IPaddr2</code> resource, which is a floating IP address for the <code class="literal">apachegroup</code> resource group. The IP address must not be one already associated with a physical node. If the <code class="literal">IPaddr2</code> resource’s NIC device is not specified, the floating IP must reside on the same network as one of the node’s statically assigned IP addresses, otherwise the NIC device to assign the floating IP address cannot be properly detected.
					</li><li class="listitem">
						An <code class="literal">apache</code> resource named <code class="literal">Website</code> that uses the <code class="literal">index.html</code> file and the Apache configuration you defined in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-apache-http-web-server-configuring-ha-http">Configuring an Apache HTTP server</a>.
					</li></ol></div><p>
				The following procedure creates the resource group <code class="literal">apachegroup</code> and the resources that the group contains. The resources will start in the order in which you add them to the group, and they will stop in the reverse order in which they are added to the group. Run this procedure from one node of the cluster only.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						The following command creates the <code class="literal">LVM-activate</code> resource <code class="literal">my_lvm</code>. Because the resource group <code class="literal">apachegroup</code> does not yet exist, this command creates the resource group.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							Do not configure more than one <code class="literal">LVM-activate</code> resource that uses the same LVM volume group in an active/passive HA configuration, as this could cause data corruption. Additionally, do not configure an <code class="literal">LVM-activate</code> resource as a clone resource in an active/passive HA configuration.
						</p></div></rh-alert><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create my_lvm ocf:heartbeat:LVM-activate vgname=my_vg vg_access_mode=system_id --group apachegroup</strong></span></pre><p class="simpara">
						When you create a resource, the resource is started automatically. You can use the following command to confirm that the resource was created and has started.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource status</strong></span>
 Resource Group: apachegroup
     my_lvm	(ocf::heartbeat:LVM-activate):	Started</pre><p class="simpara">
						You can manually stop and start an individual resource with the <code class="literal command">pcs resource disable</code> and <code class="literal command">pcs resource enable</code> commands.
					</p></li><li class="listitem"><p class="simpara">
						The following commands create the remaining resources for the configuration, adding them to the existing resource group <code class="literal">apachegroup</code>.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create my_fs Filesystem device="/dev/my_vg/my_lv" directory="/var/www" fstype="xfs" --group apachegroup</strong></span>

[root@z1 ~]# <span class="strong strong"><strong>pcs resource create VirtualIP IPaddr2 ip=198.51.100.3 cidr_netmask=24 --group apachegroup</strong></span>

[root@z1 ~]# <span class="strong strong"><strong>pcs resource create Website apache configfile="/etc/httpd/conf/httpd.conf" statusurl="http://127.0.0.1/server-status" --group apachegroup</strong></span></pre></li><li class="listitem"><p class="simpara">
						After creating the resources and the resource group that contains them, you can check the status of the cluster. Note that all four resources are running on the same node.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status</strong></span>
Cluster name: my_cluster
Last updated: Wed Jul 31 16:38:51 2013
Last change: Wed Jul 31 16:42:14 2013 via crm_attribute on z1.example.com
Stack: corosync
Current DC: z2.example.com (2) - partition with quorum
Version: 1.1.10-5.el7-9abe687
2 Nodes configured
6 Resources configured

Online: [ z1.example.com z2.example.com ]

Full list of resources:
 myapc	(stonith:fence_apc_snmp):	Started z1.example.com
 Resource Group: apachegroup
     my_lvm	(ocf::heartbeat:LVM-activate):	Started z1.example.com
     my_fs	(ocf::heartbeat:Filesystem):	Started z1.example.com
     VirtualIP	(ocf::heartbeat:IPaddr2):	Started z1.example.com
     Website	(ocf::heartbeat:apache):	Started z1.example.com</pre><p class="simpara">
						Note that if you have not configured a fencing device for your cluster, by default the resources do not start.
					</p></li><li class="listitem"><p class="simpara">
						Once the cluster is up and running, you can point a browser to the IP address you defined as the <code class="literal">IPaddr2</code> resource to view the sample display, consisting of the simple word "Hello".
					</p><pre class="literallayout">Hello</pre><p class="simpara">
						If you find that the resources you configured are not running, you can run the <code class="literal command">pcs resource debug-start <span class="emphasis"><em>resource</em></span></code> command to test the resource configuration.
					</p></li><li class="listitem"><p class="simpara">
						When you use the <code class="literal">apache</code> resource agent to manage Apache, it does not use <code class="literal">systemd</code>. Because of this, you must edit the <code class="literal">logrotate</code> script supplied with Apache so that it does not use <code class="literal">systemctl</code> to reload Apache.
					</p><p class="simpara">
						Remove the following line in the <code class="literal">/etc/logrotate.d/httpd</code> file on each node in the cluster.
					</p><pre class="literallayout">/bin/systemctl reload httpd.service &gt; /dev/null 2&gt;/dev/null || true</pre><p class="simpara">
						Replace the line you removed with the following three lines, specifying <code class="literal">/var/run/httpd-<span class="emphasis"><em>website</em></span>.pid</code> as the PID file path where <span class="emphasis"><em>website</em></span> is the name of the Apache resource. In this example, the Apache resource name is <code class="literal">Website</code>.
					</p><pre class="literallayout">/usr/bin/test -f /var/run/httpd-Website.pid &gt;/dev/null 2&gt;/dev/null &amp;&amp;
/usr/bin/ps -q $(/usr/bin/cat /var/run/httpd-Website.pid) &gt;/dev/null 2&gt;/dev/null &amp;&amp;
/usr/sbin/httpd -f /etc/httpd/conf/httpd.conf -c "PidFile /var/run/httpd-Website.pid" -k graceful &gt; /dev/null 2&gt;/dev/null || true</pre></li></ol></div></section><section class="section" id="proc_testing-resource-configuration-in-a-cluster-configuring-ha-http"><div class="titlepage"><div><div><h3 class="title">5.4. Testing the resource configuration</h3></div></div></div><p class="_abstract _abstract">
				Test the resource configuration in a cluster with the following procedure.
			</p><p>
				In the cluster status display shown in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-resources-for-http-server-in-a-cluster-configuring-ha-http">Creating the resources and resource groups</a>, all of the resources are running on node <code class="literal">z1.example.com</code>. You can test whether the resource group fails over to node <code class="literal">z2.example.com</code> by using the following procedure to put the first node in <code class="literal">standby</code> mode, after which the node will no longer be able to host resources.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						The following command puts node <code class="literal">z1.example.com</code> in <code class="literal">standby</code> mode.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs node standby z1.example.com</strong></span></pre></li><li class="listitem"><p class="simpara">
						After putting node <code class="literal">z1</code> in <code class="literal">standby</code> mode, check the cluster status. Note that the resources should now all be running on <code class="literal">z2</code>.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status</strong></span>
Cluster name: my_cluster
Last updated: Wed Jul 31 17:16:17 2013
Last change: Wed Jul 31 17:18:34 2013 via crm_attribute on z1.example.com
Stack: corosync
Current DC: z2.example.com (2) - partition with quorum
Version: 1.1.10-5.el7-9abe687
2 Nodes configured
6 Resources configured

Node z1.example.com (1): standby
Online: [ z2.example.com ]

Full list of resources:

 myapc	(stonith:fence_apc_snmp):	Started z1.example.com
 Resource Group: apachegroup
     my_lvm	(ocf::heartbeat:LVM-activate):	Started z2.example.com
     my_fs	(ocf::heartbeat:Filesystem):	Started z2.example.com
     VirtualIP	(ocf::heartbeat:IPaddr2):	Started z2.example.com
     Website	(ocf::heartbeat:apache):	Started z2.example.com</pre><p class="simpara">
						The web site at the defined IP address should still display, without interruption.
					</p></li><li class="listitem"><p class="simpara">
						To remove <code class="literal">z1</code> from <code class="literal">standby</code> mode, enter the following command.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs node unstandby z1.example.com</strong></span></pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							Removing a node from <code class="literal">standby</code> mode does not in itself cause the resources to fail back over to that node. This will depend on the <code class="literal">resource-stickiness</code> value for the resources. For information about the <code class="literal">resource-stickiness</code> meta attribute, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters#proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on">Configuring a resource to prefer its current node</a>.
						</p></div></rh-alert></li></ol></div></section></section><section class="chapter" id="assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 6. Configuring an active/passive NFS server in a Red Hat High Availability cluster</h2></div></div></div><p class="_abstract _abstract">
			The Red Hat High Availability Add-On provides support for running a highly available active/passive NFS server on a Red Hat Enterprise Linux High Availability Add-On cluster using shared storage. In the following example, you are configuring a two-node cluster in which clients access the NFS file system through a floating IP address. The NFS server runs on one of the two nodes in the cluster. If the node on which the NFS server is running becomes inoperative, the NFS server starts up again on the second node of the cluster with minimal service interruption.
		</p><p>
			This use case requires that your system include the following components:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					A two-node Red Hat High Availability cluster with power fencing configured for each node. We recommend but do not require a private network. This procedure uses the cluster example provided in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters">Creating a Red Hat High-Availability cluster with Pacemaker</a>.
				</li><li class="listitem">
					A public virtual IP address, required for the NFS server.
				</li><li class="listitem">
					Shared storage for the nodes in the cluster, using iSCSI, Fibre Channel, or other shared network block device.
				</li></ul></div><p>
			Configuring a highly available active/passive NFS server on an existing two-node Red Hat Enterprise Linux High Availability cluster requires that you perform the following steps:
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					Configure a file system on an LVM logical volume on the shared storage for the nodes in the cluster.
				</li><li class="listitem">
					Configure an NFS share on the shared storage on the LVM logical volume.
				</li><li class="listitem">
					Create the cluster resources.
				</li><li class="listitem">
					Test the NFS server you have configured.
				</li></ol></div><section class="section" id="proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs"><div class="titlepage"><div><div><h3 class="title">6.1. Configuring an LVM volume with an XFS file system in a Pacemaker cluster</h3></div></div></div><p class="_abstract _abstract">
				Create an LVM logical volume on storage that is shared between the nodes of the cluster with the following procedure.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					LVM volumes and the corresponding partitions and devices used by cluster nodes must be connected to the cluster nodes only.
				</p></div></rh-alert><p>
				The following procedure creates an LVM logical volume and then creates an XFS file system on that volume for use in a Pacemaker cluster. In this example, the shared partition <code class="literal">/dev/sdb1</code> is used to store the LVM physical volume from which the LVM logical volume will be created.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On both nodes of the cluster, perform the following steps to set the value for the LVM system ID to the value of the <code class="literal">uname</code> identifier for the system. The LVM system ID will be used to ensure that only the cluster is capable of activating the volume group.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Set the <code class="literal">system_id_source</code> configuration option in the <code class="literal">/etc/lvm/lvm.conf</code> configuration file to <code class="literal">uname</code>.
							</p><pre class="literallayout"># Configuration option global/system_id_source.
system_id_source = "uname"</pre></li><li class="listitem"><p class="simpara">
								Verify that the LVM system ID on the node matches the <code class="literal">uname</code> for the node.
							</p><pre class="literallayout"># <span class="strong strong"><strong>lvm systemid</strong></span>
  system ID: z1.example.com
# <span class="strong strong"><strong>uname -n</strong></span>
  z1.example.com</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Create the LVM volume and create an XFS file system on that volume. Since the <code class="literal">/dev/sdb1</code> partition is storage that is shared, you perform this part of the procedure on one node only.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							If your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order">Configuring startup order for resource dependencies not managed by Pacemaker</a>.
						</p></div></rh-alert><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create an LVM physical volume on partition <code class="literal">/dev/sdb1</code>.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pvcreate /dev/sdb1</strong></span>
  Physical volume "/dev/sdb1" successfully created</pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									If your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order">Configuring startup order for resource dependencies not managed by Pacemaker</a>.
								</p></div></rh-alert></li><li class="listitem"><p class="simpara">
								Create the volume group <code class="literal">my_vg</code> that consists of the physical volume <code class="literal">/dev/sdb1</code>.
							</p><p class="simpara">
								Specify the <code class="literal">--setautoactivation n</code> flag to ensure that volume groups managed by Pacemaker in a cluster will not be automatically activated on startup. If you are using an existing volume group for the LVM volume you are creating, you can reset this flag with the <code class="literal">vgchange --setautoactivation n</code> command for the volume group.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>vgcreate --setautoactivation n my_vg /dev/sdb1</strong></span>
  Volume group "my_vg" successfully created</pre></li><li class="listitem"><p class="simpara">
								Verify that the new volume group has the system ID of the node on which you are running and from which you created the volume group.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>vgs -o+systemid</strong></span>
  VG    #PV #LV #SN Attr   VSize  VFree  System ID
  my_vg   1   0   0 wz--n- &lt;1.82t &lt;1.82t z1.example.com</pre></li><li class="listitem"><p class="simpara">
								Create a logical volume using the volume group <code class="literal">my_vg</code>.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvcreate -L450 -n my_lv my_vg</strong></span>
  Rounding up size to full physical extent 452.00 MiB
  Logical volume "my_lv" created</pre><p class="simpara">
								You can use the <code class="literal command">lvs</code> command to display the logical volume.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvs</strong></span>
  LV      VG      Attr      LSize   Pool Origin Data%  Move Log Copy%  Convert
  my_lv   my_vg   -wi-a---- 452.00m
  ...</pre></li><li class="listitem"><p class="simpara">
								Create an XFS file system on the logical volume <code class="literal">my_lv</code>.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>mkfs.xfs /dev/my_vg/my_lv</strong></span>
meta-data=/dev/my_vg/my_lv       isize=512    agcount=4, agsize=28928 blks
         =                       sectsz=512   attr=2, projid32bit=1
...</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						If the use of a devices file is enabled with the <code class="literal">use_devicesfile = 1</code> parameter in the <code class="literal">lvm.conf</code> file, add the shared device to the devices file on the second node in the cluster. This feature is enabled by default.
					</p><pre class="literallayout">[root@z2 ~]# <span class="strong strong"><strong>lvmdevices --adddev /dev/sdb1</strong></span></pre></li></ol></div></section><section class="section" id="proc_configuring-nfs-share-configuring-ha-nfs"><div class="titlepage"><div><div><h3 class="title">6.2. Configuring an NFS share</h3></div></div></div><p class="_abstract _abstract">
				Configure an NFS share for an NFS service failover with the following procedure.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On both nodes in the cluster, create the <code class="literal">/nfsshare</code> directory.
					</p><pre class="literallayout"># <span class="strong strong"><strong>mkdir /nfsshare</strong></span></pre></li><li class="listitem"><p class="simpara">
						On one node in the cluster, perform the following procedure.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Ensure that the logical volume you you created in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs">Configuring an LVM volume with an XFS file system</a> is activated, then mount the file system you created on the logical volume on the <code class="literal">/nfsshare</code> directory.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvchange -ay my_vg/my_lv</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>mount /dev/my_vg/my_lv /nfsshare</strong></span></pre></li><li class="listitem"><p class="simpara">
								Create an <code class="literal">exports</code> directory tree on the <code class="literal">/nfsshare</code> directory.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>mkdir -p /nfsshare/exports</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>mkdir -p /nfsshare/exports/export1</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>mkdir -p /nfsshare/exports/export2</strong></span></pre></li><li class="listitem"><p class="simpara">
								Place files in the <code class="literal">exports</code> directory for the NFS clients to access. For this example, we are creating test files named <code class="literal">clientdatafile1</code> and <code class="literal">clientdatafile2</code>.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>touch /nfsshare/exports/export1/clientdatafile1</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>touch /nfsshare/exports/export2/clientdatafile2</strong></span></pre></li><li class="listitem"><p class="simpara">
								Unmount the file system and deactivate the LVM volume group.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>umount /dev/my_vg/my_lv</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>vgchange -an my_vg</strong></span></pre></li></ol></div></li></ol></div></section><section class="section" id="proc_configuring_resources_for_nfs_server_in_a_cluster-configuring-ha-nfs"><div class="titlepage"><div><div><h3 class="title">6.3. Configuring the resources and resource group for an NFS server in a cluster</h3></div></div></div><p class="_abstract _abstract">
				Configure the cluster resources for an NFS server in a cluster with the following procedure.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					If you have not configured a fencing device for your cluster, by default the resources do not start.
				</p><p>
					If you find that the resources you configured are not running, you can run the <code class="literal command">pcs resource debug-start <span class="emphasis"><em>resource</em></span></code> command to test the resource configuration. This starts the service outside of the cluster’s control and knowledge. At the point the configured resources are running again, run <code class="literal command">pcs resource cleanup <span class="emphasis"><em>resource</em></span></code> to make the cluster aware of the updates.
				</p></div></rh-alert><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					The following procedure configures the system resources. To ensure these resources all run on the same node, they are configured as part of the resource group <code class="literal">nfsgroup</code>. The resources will start in the order in which you add them to the group, and they will stop in the reverse order in which they are added to the group. Run this procedure from one node of the cluster only.
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create the LVM-activate resource named <code class="literal">my_lvm</code>. Because the resource group <code class="literal">nfsgroup</code> does not yet exist, this command creates the resource group.
					</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
							Do not configure more than one <code class="literal">LVM-activate</code> resource that uses the same LVM volume group in an active/passive HA configuration, as this risks data corruption. Additionally, do not configure an <code class="literal">LVM-activate</code> resource as a clone resource in an active/passive HA configuration.
						</p></div></rh-alert><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create my_lvm ocf:heartbeat:LVM-activate vgname=my_vg vg_access_mode=system_id --group nfsgroup</strong></span></pre></li><li class="listitem"><p class="simpara">
						Check the status of the cluster to verify that the resource is running.
					</p><pre class="literallayout">root@z1 ~]#  <span class="strong strong"><strong>pcs status</strong></span>
Cluster name: my_cluster
Last updated: Thu Jan  8 11:13:17 2015
Last change: Thu Jan  8 11:13:08 2015
Stack: corosync
Current DC: z2.example.com (2) - partition with quorum
Version: 1.1.12-a14efad
2 Nodes configured
3 Resources configured

Online: [ z1.example.com z2.example.com ]

Full list of resources:
 myapc  (stonith:fence_apc_snmp):       Started z1.example.com
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM-activate):   Started z1.example.com

PCSD Status:
  z1.example.com: Online
  z2.example.com: Online

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled</pre></li><li class="listitem"><p class="simpara">
						Configure a <code class="literal">Filesystem</code> resource for the cluster.
					</p><p class="simpara">
						The following command configures an XFS <code class="literal">Filesystem</code> resource named <code class="literal">nfsshare</code> as part of the <code class="literal">nfsgroup</code> resource group. This file system uses the LVM volume group and XFS file system you created in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs">Configuring an LVM volume with an XFS file system</a> and will be mounted on the <code class="literal">/nfsshare</code> directory you created in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-nfs-share-configuring-ha-nfs">Configuring an NFS share</a>.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create nfsshare Filesystem device=/dev/my_vg/my_lv directory=/nfsshare fstype=xfs --group nfsgroup</strong></span></pre><p class="simpara">
						You can specify mount options as part of the resource configuration for a <code class="literal">Filesystem</code> resource with the <code class="literal">options=<span class="emphasis"><em>options</em></span></code> parameter. Run the <code class="literal command">pcs resource describe Filesystem</code> command for full configuration options.
					</p></li><li class="listitem"><p class="simpara">
						Verify that the <code class="literal">my_lvm</code> and <code class="literal">nfsshare</code> resources are running.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status</strong></span>
...
Full list of resources:
 myapc  (stonith:fence_apc_snmp):       Started z1.example.com
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM-activate):   Started z1.example.com
     nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com
...</pre></li><li class="listitem"><p class="simpara">
						Create the <code class="literal">nfsserver</code> resource named <code class="literal">nfs-daemon</code> as part of the resource group <code class="literal">nfsgroup</code>.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							The <code class="literal">nfsserver</code> resource allows you to specify an <code class="literal">nfs_shared_infodir</code> parameter, which is a directory that NFS servers use to store NFS-related stateful information.
						</p><p>
							It is recommended that this attribute be set to a subdirectory of one of the <code class="literal">Filesystem</code> resources you created in this collection of exports. This ensures that the NFS servers are storing their stateful information on a device that will become available to another node if this resource group needs to relocate. In this example;
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">/nfsshare</code> is the shared-storage directory managed by the <code class="literal">Filesystem</code> resource
								</li><li class="listitem">
									<code class="literal">/nfsshare/exports/export1</code> and <code class="literal">/nfsshare/exports/export2</code> are the export directories
								</li><li class="listitem">
									<code class="literal">/nfsshare/nfsinfo</code> is the shared-information directory for the <code class="literal">nfsserver</code> resource
								</li></ul></div></div></rh-alert><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create nfs-daemon nfsserver nfs_shared_infodir=/nfsshare/nfsinfo nfs_no_notify=true --group nfsgroup</strong></span>

[root@z1 ~]# <span class="strong strong"><strong>pcs status</strong></span>
...</pre></li><li class="listitem"><p class="simpara">
						Add the <code class="literal">exportfs</code> resources to export the <code class="literal">/nfsshare/exports</code> directory. These resources are part of the resource group <code class="literal">nfsgroup</code>. This builds a virtual directory for NFSv4 clients. NFSv3 clients can access these exports as well.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							The <code class="literal">fsid=0</code> option is required only if you want to create a virtual directory for NFSv4 clients. For more information, see the Red Hat Knowledgebase solution <a class="link" href="https://access.redhat.com/solutions/548083">How do I configure the fsid option in an NFS server’s /etc/exports file?</a>.
						</p></div></rh-alert><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create nfs-root exportfs clientspec=192.168.122.0/255.255.255.0 options=rw,sync,no_root_squash directory=/nfsshare/exports fsid=0 --group nfsgroup</strong></span>

[root@z1 ~]# <span class="strong strong"><strong>pcs resource create nfs-export1 exportfs clientspec=192.168.122.0/255.255.255.0 options=rw,sync,no_root_squash directory=/nfsshare/exports/export1 fsid=1 --group nfsgroup</strong></span>

[root@z1 ~]# <span class="strong strong"><strong>pcs resource create nfs-export2 exportfs clientspec=192.168.122.0/255.255.255.0 options=rw,sync,no_root_squash directory=/nfsshare/exports/export2 fsid=2 --group nfsgroup</strong></span></pre></li><li class="listitem"><p class="simpara">
						Add the floating IP address resource that NFS clients will use to access the NFS share. This resource is part of the resource group <code class="literal">nfsgroup</code>. For this example deployment, we are using 192.168.122.200 as the floating IP address.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create nfs_ip IPaddr2 ip=192.168.122.200 cidr_netmask=24 --group nfsgroup</strong></span></pre></li><li class="listitem"><p class="simpara">
						Add an <code class="literal">nfsnotify</code> resource for sending NFSv3 reboot notifications once the entire NFS deployment has initialized. This resource is part of the resource group <code class="literal">nfsgroup</code>.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							For the NFS notification to be processed correctly, the floating IP address must have a host name associated with it that is consistent on both the NFS servers and the NFS client.
						</p></div></rh-alert><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create nfs-notify nfsnotify source_host=192.168.122.200 --group nfsgroup</strong></span></pre></li><li class="listitem"><p class="simpara">
						After creating the resources and the resource constraints, you can check the status of the cluster. Note that all resources are running on the same node.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status</strong></span>
...
Full list of resources:
 myapc  (stonith:fence_apc_snmp):       Started z1.example.com
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM-activate):   Started z1.example.com
     nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com
     nfs-daemon (ocf::heartbeat:nfsserver):     Started z1.example.com
     nfs-root   (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs-export1        (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs-export2        (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z1.example.com
     nfs-notify (ocf::heartbeat:nfsnotify):     Started z1.example.com
...</pre></li></ol></div></section><section class="section" id="proc_testing-nfs-resource-configuration-configuring-ha-nfs"><div class="titlepage"><div><div><h3 class="title">6.4. Testing the NFS resource configuration</h3></div></div></div><p class="_abstract _abstract">
				You can validate your NFS resource configuration in a high availability cluster with the following procedures. You should be able to mount the exported file system with either NFSv3 or NFSv4.
			</p><section class="section" id="testing_the_nfs_export"><div class="titlepage"><div><div><h4 class="title">6.4.1. Testing the NFS export</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							If you are running the <code class="literal">firewalld</code> daemon on your cluster nodes, ensure that the ports that your system requires for NFS access are enabled on all nodes.
						</li><li class="listitem"><p class="simpara">
							On a node outside of the cluster, residing in the same network as the deployment, verify that the NFS share can be seen by mounting the NFS share. For this example, we are using the 192.168.122.0/24 network.
						</p><pre class="literallayout"># <span class="strong strong"><strong>showmount -e 192.168.122.200</strong></span>
Export list for 192.168.122.200:
/nfsshare/exports/export1 192.168.122.0/255.255.255.0
/nfsshare/exports         192.168.122.0/255.255.255.0
/nfsshare/exports/export2 192.168.122.0/255.255.255.0</pre></li><li class="listitem"><p class="simpara">
							To verify that you can mount the NFS share with NFSv4, mount the NFS share to a directory on the client node. After mounting, verify that the contents of the export directories are visible. Unmount the share after testing.
						</p><pre class="literallayout"># <span class="strong strong"><strong>mkdir nfsshare</strong></span>
# <span class="strong strong"><strong>mount -o "vers=4" 192.168.122.200:export1 nfsshare</strong></span>
# <span class="strong strong"><strong>ls nfsshare</strong></span>
clientdatafile1
# <span class="strong strong"><strong>umount nfsshare</strong></span></pre></li><li class="listitem"><p class="simpara">
							Verify that you can mount the NFS share with NFSv3. After mounting, verify that the test file <code class="literal">clientdatafile1</code> is visible. Unlike NFSv4, since NFSv3 does not use the virtual file system, you must mount a specific export. Unmount the share after testing.
						</p><pre class="literallayout"># <span class="strong strong"><strong>mkdir nfsshare</strong></span>
# <span class="strong strong"><strong>mount -o "vers=3" 192.168.122.200:/nfsshare/exports/export2 nfsshare</strong></span>
# <span class="strong strong"><strong>ls nfsshare</strong></span>
clientdatafile2
# <span class="strong strong"><strong>umount nfsshare</strong></span></pre></li></ol></div></section><section class="section" id="testing_for_failover"><div class="titlepage"><div><div><h4 class="title">6.4.2. Testing for failover</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On a node outside of the cluster, mount the NFS share and verify access to the <code class="literal">clientdatafile1</code> file you created in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-nfs-share-configuring-ha-nfs">Configuring an NFS share</a>.
						</p><pre class="literallayout"># <span class="strong strong"><strong>mkdir nfsshare</strong></span>
# <span class="strong strong"><strong>mount -o "vers=4" 192.168.122.200:export1 nfsshare</strong></span>
# <span class="strong strong"><strong>ls nfsshare</strong></span>
clientdatafile1</pre></li><li class="listitem"><p class="simpara">
							From a node within the cluster, determine which node in the cluster is running <code class="literal">nfsgroup</code>. In this example, <code class="literal">nfsgroup</code> is running on <code class="literal">z1.example.com</code>.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status</strong></span>
...
Full list of resources:
 myapc  (stonith:fence_apc_snmp):       Started z1.example.com
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM-activate):   Started z1.example.com
     nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com
     nfs-daemon (ocf::heartbeat:nfsserver):     Started z1.example.com
     nfs-root   (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs-export1        (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs-export2        (ocf::heartbeat:exportfs):      Started z1.example.com
     nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z1.example.com
     nfs-notify (ocf::heartbeat:nfsnotify):     Started z1.example.com
...</pre></li><li class="listitem"><p class="simpara">
							From a node within the cluster, put the node that is running <code class="literal">nfsgroup</code> in standby mode.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs node standby z1.example.com</strong></span></pre></li><li class="listitem"><p class="simpara">
							Verify that <code class="literal">nfsgroup</code> successfully starts on the other cluster node.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status</strong></span>
...
Full list of resources:
 Resource Group: nfsgroup
     my_lvm     (ocf::heartbeat:LVM-activate):   Started z2.example.com
     nfsshare   (ocf::heartbeat:Filesystem):    Started z2.example.com
     nfs-daemon (ocf::heartbeat:nfsserver):     Started z2.example.com
     nfs-root   (ocf::heartbeat:exportfs):      Started z2.example.com
     nfs-export1        (ocf::heartbeat:exportfs):      Started z2.example.com
     nfs-export2        (ocf::heartbeat:exportfs):      Started z2.example.com
     nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z2.example.com
     nfs-notify (ocf::heartbeat:nfsnotify):     Started z2.example.com
...</pre></li><li class="listitem"><p class="simpara">
							From the node outside the cluster on which you have mounted the NFS share, verify that this outside node still continues to have access to the test file within the NFS mount.
						</p><pre class="literallayout"># <span class="strong strong"><strong>ls nfsshare</strong></span>
clientdatafile1</pre><p class="simpara">
							Service will be lost briefly for the client during the failover but the client should recover it with no user intervention. By default, clients using NFSv4 may take up to 90 seconds to recover the mount; this 90 seconds represents the NFSv4 file lease grace period observed by the server on startup. NFSv3 clients should recover access to the mount in a matter of a few seconds.
						</p></li><li class="listitem"><p class="simpara">
							From a node within the cluster, remove the node that was initially running <code class="literal">nfsgroup</code> from standby mode.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								Removing a node from <code class="literal">standby</code> mode does not in itself cause the resources to fail back over to that node. This will depend on the <code class="literal">resource-stickiness</code> value for the resources. For information about the <code class="literal">resource-stickiness</code> meta attribute, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters#proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on">Configuring a resource to prefer its current node</a>.
							</p></div></rh-alert><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs node unstandby z1.example.com</strong></span></pre></li></ol></div></section></section></section><section class="chapter" id="assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 7. GFS2 file systems in a cluster</h2></div></div></div><p class="_abstract _abstract">
			Use the following administrative procedures to configure GFS2 file systems in a Red Hat high availability cluster.
		</p><section class="section" id="proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster"><div class="titlepage"><div><div><h3 class="title">7.1. Configuring a GFS2 file system in a cluster</h3></div></div></div><p class="_abstract _abstract">
				You can set up a Pacemaker cluster that includes GFS2 file systems with the following procedure. In this example, you create three GFS2 file systems on three logical volumes in a two-node cluster.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install and start the cluster software on both cluster nodes and create a basic two-node cluster.
					</li><li class="listitem">
						Configure fencing for the cluster.
					</li></ul></div><p>
				For information about creating a Pacemaker cluster and configuring fencing for the cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters">Creating a Red Hat High-Availability cluster with Pacemaker</a>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On both nodes in the cluster, enable the repository for Resilient Storage that corresponds to your system architecture. For example, to enable the Resilient Storage repository for an x86_64 system, you can enter the following <code class="literal">subscription-manager</code> command:
					</p><pre class="literallayout"># <span class="strong strong"><strong>subscription-manager repos --enable=rhel-9-for-x86_64-resilientstorage-rpms</strong></span></pre><p class="simpara">
						Note that the Resilient Storage repository is a superset of the High Availability repository. If you enable the Resilient Storage repository you do not also need to enable the High Availability repository.
					</p></li><li class="listitem"><p class="simpara">
						On both nodes of the cluster, install the <code class="literal">lvm2-lockd</code>, <code class="literal">gfs2-utils</code>, and <code class="literal">dlm</code> packages. To support these packages, you must be subscribed to the AppStream channel and the Resilient Storage channel.
					</p><pre class="literallayout"># <span class="strong strong"><strong>dnf install lvm2-lockd gfs2-utils dlm</strong></span></pre></li><li class="listitem"><p class="simpara">
						On both nodes of the cluster, set the <code class="literal">use_lvmlockd</code> configuration option in the <code class="literal">/etc/lvm/lvm.conf</code> file to <code class="literal">use_lvmlockd=1</code>.
					</p><pre class="literallayout">...
use_lvmlockd = 1
...</pre></li><li class="listitem"><p class="simpara">
						Set the global Pacemaker parameter <code class="literal">no-quorum-policy</code> to <code class="literal">freeze</code>.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							By default, the value of <code class="literal">no-quorum-policy</code> is set to <code class="literal">stop</code>, indicating that once quorum is lost, all the resources on the remaining partition will immediately be stopped. Typically this default is the safest and most optimal option, but unlike most resources, GFS2 requires quorum to function. When quorum is lost both the applications using the GFS2 mounts and the GFS2 mount itself cannot be correctly stopped. Any attempts to stop these resources without quorum will fail which will ultimately result in the entire cluster being fenced every time quorum is lost.
						</p><p>
							To address this situation, set <code class="literal">no-quorum-policy</code> to <code class="literal">freeze</code> when GFS2 is in use. This means that when quorum is lost, the remaining partition will do nothing until quorum is regained.
						</p></div></rh-alert><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs property set no-quorum-policy=freeze</strong></span></pre></li><li class="listitem"><p class="simpara">
						Set up a <code class="literal">dlm</code> resource. This is a required dependency for configuring a GFS2 file system in a cluster. This example creates the <code class="literal">dlm</code> resource as part of a resource group named <code class="literal">locking</code>.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence</strong></span></pre></li><li class="listitem"><p class="simpara">
						Clone the <code class="literal">locking</code> resource group so that the resource group can be active on both nodes of the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource clone locking interleave=true</strong></span></pre></li><li class="listitem"><p class="simpara">
						Set up an <code class="literal">lvmlockd</code> resource as part of the <code class="literal">locking</code> resource group.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence</strong></span></pre></li><li class="listitem"><p class="simpara">
						Check the status of the cluster to ensure that the <code class="literal">locking</code> resource group has started on both nodes of the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status --full</strong></span>
Cluster name: my_cluster
[...]

Online: [ z1.example.com (1) z2.example.com (2) ]

Full list of resources:

 smoke-apc      (stonith:fence_apc):    Started z1.example.com
 Clone Set: locking-clone [locking]
     Resource Group: locking:0
         dlm    (ocf::pacemaker:controld):      Started z1.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
     Resource Group: locking:1
         dlm    (ocf::pacemaker:controld):      Started z2.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
     Started: [ z1.example.com z2.example.com ]</pre></li><li class="listitem"><p class="simpara">
						On one node of the cluster, create two shared volume groups. One volume group will contain two GFS2 file systems, and the other volume group will contain one GFS2 file system.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							If your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order">Configuring startup order for resource dependencies not managed by Pacemaker</a>.
						</p></div></rh-alert><p class="simpara">
						The following command creates the shared volume group <code class="literal">shared_vg1</code> on <code class="literal">/dev/vdb</code>.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>vgcreate --shared shared_vg1 /dev/vdb</strong></span>
  Physical volume "/dev/vdb" successfully created.
  Volume group "shared_vg1" successfully created
  VG shared_vg1 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre><p class="simpara">
						The following command creates the shared volume group <code class="literal">shared_vg2</code> on <code class="literal">/dev/vdc</code>.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>vgcreate --shared shared_vg2 /dev/vdc</strong></span>
  Physical volume "/dev/vdc" successfully created.
  Volume group "shared_vg2" successfully created
  VG shared_vg2 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre></li><li class="listitem"><p class="simpara">
						On the second node in the cluster:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								If the use of a devices file is enabled with the <code class="literal">use_devicesfile = 1</code> parameter in the <code class="literal">lvm.conf</code> file, add the shared devices to the devices file This feature is enabled by default.
							</p><pre class="literallayout">[root@z2 ~]# <span class="strong strong"><strong>lvmdevices --adddev /dev/vdb</strong></span>
[root@z2 ~]# <span class="strong strong"><strong>lvmdevices --adddev /dev/vdc</strong></span></pre></li><li class="listitem"><p class="simpara">
								Start the lock manager for each of the shared volume groups.
							</p><pre class="literallayout">[root@z2 ~]# <span class="strong strong"><strong>vgchange --lockstart shared_vg1</strong></span>
  VG shared_vg1 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...
[root@z2 ~]# <span class="strong strong"><strong>vgchange --lockstart shared_vg2</strong></span>
  VG shared_vg2 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						On one node in the cluster, create the shared logical volumes and format the volumes with a GFS2 file system. One journal is required for each node that mounts the file system. Ensure that you create enough journals for each of the nodes in your cluster. The format of the lock table name is <span class="emphasis"><em>ClusterName:FSName</em></span> where <span class="emphasis"><em>ClusterName</em></span> is the name of the cluster for which the GFS2 file system is being created and <span class="emphasis"><em>FSName</em></span> is the file system name, which must be unique for all <code class="literal">lock_dlm</code> file systems over the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvcreate --activate sy -L5G -n shared_lv1 shared_vg1</strong></span>
  Logical volume "shared_lv1" created.
[root@z1 ~]# <span class="strong strong"><strong>lvcreate --activate sy -L5G -n shared_lv2 shared_vg1</strong></span>
  Logical volume "shared_lv2" created.
[root@z1 ~]# <span class="strong strong"><strong>lvcreate --activate sy -L5G -n shared_lv1 shared_vg2</strong></span>
  Logical volume "shared_lv1" created.

[root@z1 ~]# <span class="strong strong"><strong>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo1 /dev/shared_vg1/shared_lv1</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo2 /dev/shared_vg1/shared_lv2</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo3 /dev/shared_vg2/shared_lv1</strong></span></pre></li><li class="listitem"><p class="simpara">
						Create an <code class="literal">LVM-activate</code> resource for each logical volume to automatically activate that logical volume on all nodes.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create an <code class="literal">LVM-activate</code> resource named <code class="literal">sharedlv1</code> for the logical volume <code class="literal">shared_lv1</code> in volume group <code class="literal">shared_vg1</code>. This command also creates the resource group <code class="literal">shared_vg1</code> that includes the resource. In this example, the resource group has the same name as the shared volume group that includes the logical volume.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedlv1 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd</strong></span></pre></li><li class="listitem"><p class="simpara">
								Create an <code class="literal">LVM-activate</code> resource named <code class="literal">sharedlv2</code> for the logical volume <code class="literal">shared_lv2</code> in volume group <code class="literal">shared_vg1</code>. This resource will also be part of the resource group <code class="literal">shared_vg1</code>.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedlv2 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv2 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd</strong></span></pre></li><li class="listitem"><p class="simpara">
								Create an <code class="literal">LVM-activate</code> resource named <code class="literal">sharedlv3</code> for the logical volume <code class="literal">shared_lv1</code> in volume group <code class="literal">shared_vg2</code>. This command also creates the resource group <code class="literal">shared_vg2</code> that includes the resource.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedlv3 --group shared_vg2 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg2 activation_mode=shared vg_access_mode=lvmlockd</strong></span></pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Clone the two new resource groups.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource clone shared_vg1 interleave=true</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>pcs resource clone shared_vg2 interleave=true</strong></span></pre></li><li class="listitem"><p class="simpara">
						Configure ordering constraints to ensure that the <code class="literal">locking</code> resource group that includes the <code class="literal">dlm</code> and <code class="literal">lvmlockd</code> resources starts first.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs constraint order start locking-clone then shared_vg1-clone</strong></span>
Adding locking-clone shared_vg1-clone (kind: Mandatory) (Options: first-action=start then-action=start)
[root@z1 ~]# <span class="strong strong"><strong>pcs constraint order start locking-clone then shared_vg2-clone</strong></span>
Adding locking-clone shared_vg2-clone (kind: Mandatory) (Options: first-action=start then-action=start)</pre></li><li class="listitem"><p class="simpara">
						Configure colocation constraints to ensure that the <code class="literal">vg1</code> and <code class="literal">vg2</code> resource groups start on the same node as the <code class="literal">locking</code> resource group.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs constraint colocation add shared_vg1-clone with locking-clone</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>pcs constraint colocation add shared_vg2-clone with locking-clone</strong></span></pre></li><li class="listitem"><p class="simpara">
						On both nodes in the cluster, verify that the logical volumes are active. There may be a delay of a few seconds.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvs</strong></span>
  LV         VG          Attr       LSize
  shared_lv1 shared_vg1  -wi-a----- 5.00g
  shared_lv2 shared_vg1  -wi-a----- 5.00g
  shared_lv1 shared_vg2  -wi-a----- 5.00g

[root@z2 ~]# <span class="strong strong"><strong>lvs</strong></span>
  LV         VG          Attr       LSize
  shared_lv1 shared_vg1  -wi-a----- 5.00g
  shared_lv2 shared_vg1  -wi-a----- 5.00g
  shared_lv1 shared_vg2  -wi-a----- 5.00g</pre></li><li class="listitem"><p class="simpara">
						Create a file system resource to automatically mount each GFS2 file system on all nodes.
					</p><p class="simpara">
						You should not add the file system to the <code class="literal">/etc/fstab</code> file because it will be managed as a Pacemaker cluster resource. Mount options can be specified as part of the resource configuration with <code class="literal">options=<span class="emphasis"><em>options</em></span></code>. Run the <code class="literal command">pcs resource describe Filesystem</code> command to display the full configuration options.
					</p><p class="simpara">
						The following commands create the file system resources. These commands add each resource to the resource group that includes the logical volume resource for that file system.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/shared_vg1/shared_lv1" directory="/mnt/gfs1" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedfs2 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/shared_vg1/shared_lv2" directory="/mnt/gfs2" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedfs3 --group shared_vg2 ocf:heartbeat:Filesystem device="/dev/shared_vg2/shared_lv1" directory="/mnt/gfs3" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</strong></span></pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Verify that the GFS2 file systems are mounted on both nodes of the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>mount | grep gfs2</strong></span>
/dev/mapper/shared_vg1-shared_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg1-shared_lv2 on /mnt/gfs2 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg2-shared_lv1 on /mnt/gfs3 type gfs2 (rw,noatime,seclabel)

[root@z2 ~]# <span class="strong strong"><strong>mount | grep gfs2</strong></span>
/dev/mapper/shared_vg1-shared_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg1-shared_lv2 on /mnt/gfs2 type gfs2 (rw,noatime,seclabel)
/dev/mapper/shared_vg2-shared_lv1 on /mnt/gfs3 type gfs2 (rw,noatime,seclabel)</pre></li><li class="listitem"><p class="simpara">
						Check the status of the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status --full</strong></span>
Cluster name: my_cluster
[...]

Full list of resources:

 smoke-apc      (stonith:fence_apc):    Started z1.example.com
 Clone Set: locking-clone [locking]
     Resource Group: locking:0
         dlm    (ocf::pacemaker:controld):      Started z2.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
     Resource Group: locking:1
         dlm    (ocf::pacemaker:controld):      Started z1.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
     Started: [ z1.example.com z2.example.com ]
 Clone Set: shared_vg1-clone [shared_vg1]
     Resource Group: shared_vg1:0
         sharedlv1      (ocf::heartbeat:LVM-activate):  Started z2.example.com
         sharedlv2      (ocf::heartbeat:LVM-activate):  Started z2.example.com
         sharedfs1      (ocf::heartbeat:Filesystem):    Started z2.example.com
         sharedfs2      (ocf::heartbeat:Filesystem):    Started z2.example.com
     Resource Group: shared_vg1:1
         sharedlv1      (ocf::heartbeat:LVM-activate):  Started z1.example.com
         sharedlv2      (ocf::heartbeat:LVM-activate):  Started z1.example.com
         sharedfs1      (ocf::heartbeat:Filesystem):    Started z1.example.com
         sharedfs2      (ocf::heartbeat:Filesystem):    Started z1.example.com
     Started: [ z1.example.com z2.example.com ]
 Clone Set: shared_vg2-clone [shared_vg2]
     Resource Group: shared_vg2:0
         sharedlv3      (ocf::heartbeat:LVM-activate):  Started z2.example.com
         sharedfs3      (ocf::heartbeat:Filesystem):    Started z2.example.com
     Resource Group: shared_vg2:1
         sharedlv3      (ocf::heartbeat:LVM-activate):  Started z1.example.com
         sharedfs3      (ocf::heartbeat:Filesystem):    Started z1.example.com
     Started: [ z1.example.com z2.example.com ]

...</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/index">Configuring GFS2 file systems</a>
					</li><li class="listitem">
						<a class="link" href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/deploying_rhel_9_on_microsoft_azure/index#configuring-rhel-high-availability-on-azure_cloud-content-azure">Configuring a Red Hat High Availability cluster on Microsoft Azure</a>
					</li><li class="listitem">
						<a class="link" href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/deploying_rhel_9_on_amazon_web_services/index#configuring-a-red-hat-high-availability-cluster-on-aws_deploying-a-virtual-machine-on-aws">Configuring a Red Hat High Availability cluster on AWS</a>
					</li><li class="listitem">
						<a class="link" href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/deploying_rhel_9_on_google_cloud_platform/index#configuring-rhel-ha-on-gcp_cloud-content-gcp">Configuring a Red Hat High Availability Cluster on Google Cloud Platform</a>
					</li></ul></div></section><section class="section" id="proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster"><div class="titlepage"><div><div><h3 class="title">7.2. Configuring an encrypted GFS2 file system in a cluster</h3></div></div></div><p class="_abstract _abstract">
				You can create a Pacemaker cluster that includes a LUKS encrypted GFS2 file system with the following procedure. In this example, you create one GFS2 file systems on a logical volume and encrypt the file system. Encrypted GFS2 file systems are supported using the <code class="literal">crypt</code> resource agent, which provides support for LUKS encryption.
			</p><p>
				There are three parts to this procedure:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Configuring a shared logical volume in a Pacemaker cluster
					</li><li class="listitem">
						Encrypting the logical volume and creating a <code class="literal">crypt</code> resource
					</li><li class="listitem">
						Formatting the encrypted logical volume with a GFS2 file system and creating a file system resource for the cluster
					</li></ul></div><section class="section" id="configure_a_shared_logical_volume_in_a_pacemaker_cluster"><div class="titlepage"><div><div><h4 class="title">7.2.1. Configure a shared logical volume in a Pacemaker cluster</h4></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install and start the cluster software on two cluster nodes and create a basic two-node cluster.
						</li><li class="listitem">
							Configure fencing for the cluster.
						</li></ul></div><p>
					For information about creating a Pacemaker cluster and configuring fencing for the cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters">Creating a Red Hat High-Availability cluster with Pacemaker</a>.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On both nodes in the cluster, enable the repository for Resilient Storage that corresponds to your system architecture. For example, to enable the Resilient Storage repository for an x86_64 system, you can enter the following <code class="literal">subscription-manager</code> command:
						</p><pre class="literallayout"># <span class="strong strong"><strong>subscription-manager repos --enable=rhel-9-for-x86_64-resilientstorage-rpms</strong></span></pre><p class="simpara">
							Note that the Resilient Storage repository is a superset of the High Availability repository. If you enable the Resilient Storage repository you do not also need to enable the High Availability repository.
						</p></li><li class="listitem"><p class="simpara">
							On both nodes of the cluster, install the <code class="literal">lvm2-lockd</code>, <code class="literal">gfs2-utils</code>, and <code class="literal">dlm</code> packages. To support these packages, you must be subscribed to the AppStream channel and the Resilient Storage channel.
						</p><pre class="literallayout"># <span class="strong strong"><strong>dnf install lvm2-lockd gfs2-utils dlm</strong></span></pre></li><li class="listitem"><p class="simpara">
							On both nodes of the cluster, set the <code class="literal">use_lvmlockd</code> configuration option in the <code class="literal">/etc/lvm/lvm.conf</code> file to <code class="literal">use_lvmlockd=1</code>.
						</p><pre class="literallayout">...
use_lvmlockd = 1
...</pre></li><li class="listitem"><p class="simpara">
							Set the global Pacemaker parameter <code class="literal">no-quorum-policy</code> to <code class="literal">freeze</code>.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								By default, the value of <code class="literal">no-quorum-policy</code> is set to <code class="literal">stop</code>, indicating that when quorum is lost, all the resources on the remaining partition will immediately be stopped. Typically this default is the safest and most optimal option, but unlike most resources, GFS2 requires quorum to function. When quorum is lost both the applications using the GFS2 mounts and the GFS2 mount itself cannot be correctly stopped. Any attempts to stop these resources without quorum will fail which will ultimately result in the entire cluster being fenced every time quorum is lost.
							</p><p>
								To address this situation, set <code class="literal">no-quorum-policy</code> to <code class="literal">freeze</code> when GFS2 is in use. This means that when quorum is lost, the remaining partition will do nothing until quorum is regained.
							</p></div></rh-alert><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs property set no-quorum-policy=freeze</strong></span></pre></li><li class="listitem"><p class="simpara">
							Set up a <code class="literal">dlm</code> resource. This is a required dependency for configuring a GFS2 file system in a cluster. This example creates the <code class="literal">dlm</code> resource as part of a resource group named <code class="literal">locking</code>.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence</strong></span></pre></li><li class="listitem"><p class="simpara">
							Clone the <code class="literal">locking</code> resource group so that the resource group can be active on both nodes of the cluster.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource clone locking interleave=true</strong></span></pre></li><li class="listitem"><p class="simpara">
							Set up an <code class="literal">lvmlockd</code> resource as part of the group <code class="literal">locking</code>.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence</strong></span></pre></li><li class="listitem"><p class="simpara">
							Check the status of the cluster to ensure that the <code class="literal">locking</code> resource group has started on both nodes of the cluster.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status --full</strong></span>
Cluster name: my_cluster
[...]

Online: [ z1.example.com (1) z2.example.com (2) ]

Full list of resources:

 smoke-apc      (stonith:fence_apc):    Started z1.example.com
 Clone Set: locking-clone [locking]
     Resource Group: locking:0
         dlm    (ocf::pacemaker:controld):      Started z1.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
     Resource Group: locking:1
         dlm    (ocf::pacemaker:controld):      Started z2.example.com
         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
     Started: [ z1.example.com z2.example.com ]</pre></li><li class="listitem"><p class="simpara">
							On one node of the cluster, create a shared volume group.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								If your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order">Configuring startup order for resource dependencies not managed by Pacemaker</a>.
							</p></div></rh-alert><p class="simpara">
							The following command creates the shared volume group <code class="literal">shared_vg1</code> on <code class="literal">/dev/sda1</code>.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>vgcreate --shared shared_vg1 /dev/sda1</strong></span>
  Physical volume "/dev/sda1" successfully created.
  Volume group "shared_vg1" successfully created
  VG shared_vg1 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre></li><li class="listitem"><p class="simpara">
							On the second node in the cluster:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									If the use of a devices file is enabled with the <code class="literal">use_devicesfile = 1</code> parameter in the <code class="literal">lvm.conf</code> file, add the shared device to the devices file on the second node in the cluster. This feature is enabled by default.
								</p><pre class="literallayout">[root@z2 ~]# <span class="strong strong"><strong>lvmdevices --adddev /dev/sda1</strong></span></pre></li><li class="listitem"><p class="simpara">
									Start the lock manager for the shared volume group.
								</p><pre class="literallayout">[root@z2 ~]# <span class="strong strong"><strong>vgchange --lockstart shared_vg1</strong></span>
  VG shared_vg1 starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							On one node in the cluster, create the shared logical volume.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvcreate --activate sy -L5G -n shared_lv1 shared_vg1</strong></span>
  Logical volume "shared_lv1" created.</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">LVM-activate</code> resource for the logical volume to automatically activate the logical volume on all nodes.
						</p><p class="simpara">
							The following command creates an <code class="literal">LVM-activate</code> resource named <code class="literal">sharedlv1</code> for the logical volume <code class="literal">shared_lv1</code> in volume group <code class="literal">shared_vg1</code>. This command also creates the resource group <code class="literal">shared_vg1</code> that includes the resource. In this example, the resource group has the same name as the shared volume group that includes the logical volume.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedlv1 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd</strong></span></pre></li><li class="listitem"><p class="simpara">
							Clone the new resource group.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource clone shared_vg1 interleave=true</strong></span></pre></li><li class="listitem"><p class="simpara">
							Configure an ordering constraints to ensure that the <code class="literal">locking</code> resource group that includes the <code class="literal">dlm</code> and <code class="literal">lvmlockd</code> resources starts first.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs constraint order start locking-clone then shared_vg1-clone</strong></span>
Adding locking-clone shared_vg1-clone (kind: Mandatory) (Options: first-action=start then-action=start)</pre></li><li class="listitem"><p class="simpara">
							Configure a colocation constraints to ensure that the <code class="literal">vg1</code> and <code class="literal">vg2</code> resource groups start on the same node as the <code class="literal">locking</code> resource group.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs constraint colocation add shared_vg1-clone with locking-clone</strong></span></pre></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						On both nodes in the cluster, verify that the logical volume is active. There may be a delay of a few seconds.
					</p></div><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvs</strong></span>
  LV         VG          Attr       LSize
  shared_lv1 shared_vg1  -wi-a----- 5.00g

[root@z2 ~]# <span class="strong strong"><strong>lvs</strong></span>
  LV         VG          Attr       LSize
  shared_lv1 shared_vg1  -wi-a----- 5.00g</pre></section><section class="section" id="encrypt_the_logical_volume_and_create_a_crypt_resource"><div class="titlepage"><div><div><h4 class="title">7.2.2. Encrypt the logical volume and create a crypt resource</h4></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have configured a shared logical volume in a Pacemaker cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On one node in the cluster, create a new file that will contain the crypt key and set the permissions on the file so that it is readable only by root.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>touch /etc/crypt_keyfile</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>chmod 600 /etc/crypt_keyfile</strong></span></pre></li><li class="listitem"><p class="simpara">
							Create the crypt key.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>dd if=/dev/urandom bs=4K count=1 of=/etc/crypt_keyfile</strong></span>
1+0 records in
1+0 records out
4096 bytes (4.1 kB, 4.0 KiB) copied, 0.000306202 s, 13.4 MB/s
[root@z1 ~]# <span class="strong strong"><strong>scp /etc/crypt_keyfile root@z2.example.com:/etc/</strong></span></pre></li><li class="listitem"><p class="simpara">
							Distribute the crypt keyfile to the other nodes in the cluster, using the <code class="literal">-p</code> parameter to preserve the permissions you set.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>scp -p /etc/crypt_keyfile root@z2.example.com:/etc/</strong></span></pre></li><li class="listitem"><p class="simpara">
							Create the encrypted device on the LVM volume where you will configure the encrypted GFS2 file system.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>cryptsetup luksFormat /dev/shared_vg1/shared_lv1 --type luks2 --key-file=/etc/crypt_keyfile</strong></span>
WARNING!
========
This will overwrite data on /dev/shared_vg1/shared_lv1 irrevocably.

Are you sure? (Type 'yes' in capital letters): YES</pre></li><li class="listitem"><p class="simpara">
							Create the crypt resource as part of the <code class="literal">shared_vg1</code> volume group.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create crypt --group shared_vg1 ocf:heartbeat:crypt crypt_dev="luks_lv1" crypt_type=luks2 key_file=/etc/crypt_keyfile encrypted_dev="/dev/shared_vg1/shared_lv1"</strong></span></pre></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						Ensure that the crypt resource has created the crypt device, which in this example is <code class="literal">/dev/mapper/luks_lv1</code>.
					</p></div><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>ls -l /dev/mapper/</strong></span>
...
lrwxrwxrwx 1 root root 7 Mar 4 09:52 luks_lv1 -&gt; ../dm-3
...</pre></section><section class="section" id="format_the_encrypted_logical_volume_with_a_gfs2_file_system_and_create_a_file_system_resource_for_the_cluster"><div class="titlepage"><div><div><h4 class="title">7.2.3. Format the encrypted logical volume with a GFS2 file system and create a file system resource for the cluster</h4></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have encrypted the logical volume and created a crypt resource.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On one node in the cluster, format the volume with a GFS2 file system. One journal is required for each node that mounts the file system. Ensure that you create enough journals for each of the nodes in your cluster. The format of the lock table name is <span class="emphasis"><em>ClusterName:FSName</em></span> where <span class="emphasis"><em>ClusterName</em></span> is the name of the cluster for which the GFS2 file system is being created and <span class="emphasis"><em>FSName</em></span> is the file system name, which must be unique for all <code class="literal">lock_dlm</code> file systems over the cluster.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>mkfs.gfs2 -j3 -p lock_dlm -t my_cluster:gfs2-demo1 /dev/mapper/luks_lv1</strong></span>
/dev/mapper/luks_lv1 is a symbolic link to /dev/dm-3
This will destroy any data on /dev/dm-3
Are you sure you want to proceed? [y/n] y
Discarding device contents (may take a while on large devices): Done
Adding journals: Done
Building resource groups: Done
Creating quota file: Done
Writing superblock and syncing: Done
Device:                    /dev/mapper/luks_lv1
Block size:                4096
Device size:               4.98 GB (1306624 blocks)
Filesystem size:           4.98 GB (1306622 blocks)
Journals:                  3
Journal size:              16MB
Resource groups:           23
Locking protocol:          "lock_dlm"
Lock table:                "my_cluster:gfs2-demo1"
UUID:                      de263f7b-0f12-4d02-bbb2-56642fade293</pre></li><li class="listitem"><p class="simpara">
							Create a file system resource to automatically mount the GFS2 file system on all nodes.
						</p><p class="simpara">
							Do not add the file system to the <code class="literal">/etc/fstab</code> file because it will be managed as a Pacemaker cluster resource. Mount options can be specified as part of the resource configuration with <code class="literal">options=<span class="emphasis"><em>options</em></span></code>. Run the <code class="literal">pcs resource describe Filesystem</code> command for full configuration options.
						</p><p class="simpara">
							The following command creates the file system resource. This command adds the resource to the resource group that includes the logical volume resource for that file system.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/mapper/luks_lv1" directory="/mnt/gfs1" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence</strong></span></pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that the GFS2 file system is mounted on both nodes of the cluster.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>mount | grep gfs2</strong></span>
/dev/mapper/luks_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)

[root@z2 ~]# <span class="strong strong"><strong>mount | grep gfs2</strong></span>
/dev/mapper/luks_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)</pre></li><li class="listitem"><p class="simpara">
							Check the status of the cluster.
						</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status --full</strong></span>
Cluster name: my_cluster
[...]

Full list of resources:

  smoke-apc      (stonith:fence_apc):    Started z1.example.com
  Clone Set: locking-clone [locking]
      Resource Group: locking:0
          dlm    (ocf::pacemaker:controld):      Started z2.example.com
          lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
      Resource Group: locking:1
          dlm    (ocf::pacemaker:controld):      Started z1.example.com
          lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
     Started: [ z1.example.com z2.example.com ]
  Clone Set: shared_vg1-clone [shared_vg1]
     Resource Group: shared_vg1:0
             sharedlv1      (ocf::heartbeat:LVM-activate):  Started z2.example.com
             crypt       (ocf::heartbeat:crypt) Started z2.example.com
             sharedfs1      (ocf::heartbeat:Filesystem):    Started z2.example.com
    Resource Group: shared_vg1:1
             sharedlv1      (ocf::heartbeat:LVM-activate):  Started z1.example.com
             crypt      (ocf::heartbeat:crypt)  Started z1.example.com
             sharedfs1      (ocf::heartbeat:Filesystem):    Started z1.example.com
          Started:  [z1.example.com z2.example.com ]
...</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/index">Configuring GFS2 file systems</a>
						</li></ul></div></section></section></section><section class="chapter" id="assembly_configuring-active-active-samba-in-a-cluster-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 8. Configuring an active/active Samba server in a Red Hat High Availability cluster</h2></div></div></div><p>
			The Red Hat High Availability Add-On provides support for configuring Samba in an active/active cluster configuration. In the following example, you are configuring an active/active Samba server on a two-node RHEL cluster.
		</p><p>
			For information about support policies for Samba, see <a class="link" href="https://access.redhat.com/articles/3278591">Support Policies for RHEL High Availability - ctdb General Policies</a> and <a class="link" href="https://access.redhat.com/articles/3252211">Support Policies for RHEL Resilient Storage - Exporting gfs2 contents via other protocols</a> on the Red Hat Customer Portal.
		</p><p>
			To configure Samba in an active/active cluster:
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					Configure a GFS2 file system and its associated cluster resources.
				</li><li class="listitem">
					Configure Samba on the cluster nodes.
				</li><li class="listitem">
					Configure the Samba cluster resources.
				</li><li class="listitem">
					Test the Samba server you have configured.
				</li></ol></div><section class="section" id="proc_configuring-gfs2-for-clustered-samba_adoc-configuring-ha-samba"><div class="titlepage"><div><div><h3 class="title">8.1. Configuring a GFS2 file system for a Samba service in a high availability cluster</h3></div></div></div><p>
				Before configuring an active/active Samba service in a Pacemaker cluster, configure a GFS2 file system for the cluster.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						A two-node Red Hat High Availability cluster with fencing configured for each node
					</li><li class="listitem">
						Shared storage available for each cluster node
					</li><li class="listitem">
						A subscription to the AppStream channel and the Resilient Storage channel for each cluster node
					</li></ul></div><p>
				For information about creating a Pacemaker cluster and configuring fencing for the cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters">Creating a Red Hat High-Availability cluster with Pacemaker</a>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On both nodes in the cluster, perform the following initial setup steps.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Enable the repository for Resilient Storage that corresponds to your system architecture. For example, to enable the Resilient Storage repository for an x86_64 system, enter the following <code class="literal">subscription-manager</code> command:
							</p><pre class="literallayout"># <span class="strong strong"><strong>subscription-manager repos --enable=rhel-9-for-x86_64-resilientstorage-rpms</strong></span></pre><p class="simpara">
								The Resilient Storage repository is a superset of the High Availability repository. If you enable the Resilient Storage repository, you do not need to also enable the High Availability repository.
							</p></li><li class="listitem"><p class="simpara">
								Install the <code class="literal">lvm2-lockd</code>, <code class="literal">gfs2-utils</code>, and <code class="literal">dlm</code> packages.
							</p><pre class="literallayout"># <span class="strong strong"><strong>yum install lvm2-lockd gfs2-utils dlm</strong></span></pre></li><li class="listitem"><p class="simpara">
								Set the <code class="literal">use_lvmlockd</code> configuration option in the <code class="literal">/etc/lvm/lvm.conf</code> file to <code class="literal">use_lvmlockd=1</code>.
							</p><pre class="literallayout">...

use_lvmlockd = 1

...</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						On one node in the cluster, set the global Pacemaker parameter <code class="literal">no-quorum-policy</code> to <code class="literal">freeze</code>.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							By default, the value of <code class="literal">no-quorum-policy</code> is set to <code class="literal">stop</code>, indicating that once quorum is lost, all the resources on the remaining partition will immediately be stopped. Typically this default is the safest and most optimal option, but unlike most resources, GFS2 requires quorum to function. When quorum is lost both the applications using the GFS2 mounts and the GFS2 mount itself cannot be correctly stopped. Any attempts to stop these resources without quorum will fail which will ultimately result in the entire cluster being fenced every time quorum is lost.
						</p><p>
							To address this situation, set <code class="literal">no-quorum-policy</code> to <code class="literal">freeze</code> when GFS2 is in use. This means that when quorum is lost, the remaining partition will do nothing until quorum is regained.
						</p></div></rh-alert><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs property set no-quorum-policy=freeze</strong></span></pre></li><li class="listitem"><p class="simpara">
						Set up a <code class="literal">dlm</code> resource. This is a required dependency for configuring a GFS2 file system in a cluster. This example creates the <code class="literal">dlm</code> resource as part of a resource group named <code class="literal">locking</code>. If you have not previously configured fencing for the cluster, this step fails and the <code class="literal">pcs status</code> command displays a resource failure message.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence</strong></span></pre></li><li class="listitem"><p class="simpara">
						Clone the <code class="literal">locking</code> resource group so that the resource group can be active on both nodes of the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource clone locking interleave=true</strong></span></pre></li><li class="listitem"><p class="simpara">
						Set up an <code class="literal">lvmlockd</code> resource as part of the <code class="literal">locking</code> resource group.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence</strong></span></pre></li><li class="listitem"><p class="simpara">
						Create a physical volume and a shared volume group on the shared device <code class="literal">/dev/vdb</code>. This example creates the shared volume group <code class="literal">csmb_vg</code>.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pvcreate /dev/vdb</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>vgcreate -Ay --shared csmb_vg /dev/vdb</strong></span>
Volume group "csmb_vg" successfully created
VG csmb_vg starting dlm lockspace
Starting locking.  Waiting until locks are ready</pre></li><li class="listitem">
						On the second node in the cluster:
					</li><li class="listitem"><p class="simpara">
						If the use of a devices file is enabled with the <code class="literal">use_devicesfile = 1</code> parameter in the <code class="literal">lvm.conf</code> file, add the shared device to the devices file on the second node in the cluster. This feature is enabled by default.
					</p><pre class="literallayout">[root@z2 ~]# <span class="strong strong"><strong>lvmdevices --adddev /dev/vdb</strong></span></pre><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Start the lock manager for the shared volume group.
							</p><pre class="literallayout">[root@z2 ~]# <span class="strong strong"><strong>vgchange --lockstart csmb_vg</strong></span>
  VG csmb_vg starting dlm lockspace
  Starting locking.  Waiting until locks are ready...</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						On one node in the cluster, create a logical volume and format the volume with a GFS2 file system that will be used exclusively by CTDB for internal locking. Only one such file system is required in a cluster even if your deployment exports multiple shares.
					</p><p class="simpara">
						When specifying the lock table name with the <code class="literal">-t</code> option of the <code class="literal">mkfs.gfs2</code> command, ensure that the first component of the <span class="emphasis"><em>clustername:filesystemname</em></span> you specify matches the name of your cluster. In this example, the cluster name is <code class="literal">my_cluster</code>.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvcreate -L1G -n ctdb_lv csmb_vg</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>mkfs.gfs2 -j3 -p lock_dlm -t my_cluster:ctdb /dev/csmb_vg/ctdb_lv</strong></span></pre></li><li class="listitem"><p class="simpara">
						Create a logical volume for each GFS2 file system that will be shared over Samba and format the volume with the GFS2 file system. This example creates a single GFS2 file system and Samba share, but you can create multiple file systems and shares.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>lvcreate -L50G -n csmb_lv1 csmb_vg</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>mkfs.gfs2 -j3 -p lock_dlm -t my_cluster:csmb1 /dev/csmb_vg/csmb_lv1</strong></span></pre></li><li class="listitem"><p class="simpara">
						Set up <code class="literal">LVM_Activate</code> resources to ensure that the required shared volumes are activated. This example creates the <code class="literal">LVM_Activate</code> resources as part of a resource group <code class="literal">shared_vg</code>, and then clones that resource group so that it runs on all nodes in the cluster.
					</p><p class="simpara">
						Create the resources as disabled so they do not start automatically before you have configured the necessary order constraints.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create --disabled --group shared_vg ctdb_lv ocf:heartbeat:LVM-activate lvname=ctdb_lv vgname=csmb_vg activation_mode=shared vg_access_mode=lvmlockd</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>pcs resource create --disabled --group shared_vg csmb_lv1 ocf:heartbeat:LVM-activate lvname=csmb_lv1 vgname=csmb_vg activation_mode=shared vg_access_mode=lvmlockd</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>pcs resource clone shared_vg interleave=true</strong></span></pre></li><li class="listitem"><p class="simpara">
						Configure an ordering constraint to start all members of the <code class="literal">locking</code> resource group before the members of the <code class="literal">shared_vg</code> resource group.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs constraint order start locking-clone then shared_vg-clone</strong></span>
Adding locking-clone shared_vg-clone (kind: Mandatory) (Options: first-action=start then-action=start)</pre></li><li class="listitem"><p class="simpara">
						Enable the <code class="literal">LVM-activate</code> resources.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource enable ctdb_lv csmb_lv1</strong></span></pre></li><li class="listitem"><p class="simpara">
						On one node in the cluster, perform the following steps to create the <code class="literal">Filesystem</code> resources you require.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create <code class="literal">Filesystem</code> resources as cloned resources, using the GFS2 file systems you previously configured on your LVM volumes. This configures Pacemaker to mount and manage file systems.
							</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									You should not add the file system to the <code class="literal">/etc/fstab</code> file because it will be managed as a Pacemaker cluster resource. You can specify mount options as part of the resource configuration with <code class="literal">options=<span class="emphasis"><em>options</em></span></code>. Run the <code class="literal command">pcs resource describe Filesystem</code> command to display the full configuration options.
								</p></div></rh-alert><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create ctdb_fs Filesystem device="/dev/csmb_vg/ctdb_lv" directory="/mnt/ctdb" fstype="gfs2" op monitor interval=10s on-fail=fence clone interleave=true</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>pcs resource create csmb_fs1 Filesystem device="/dev/csmb_vg/csmb_lv1" directory="/srv/samba/share1" fstype="gfs2" op monitor interval=10s on-fail=fence clone interleave=true</strong></span></pre></li><li class="listitem"><p class="simpara">
								Configure ordering constraints to ensure that Pacemaker mounts the file systems after the shared volume group <code class="literal">shared_vg</code> has started.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs constraint order start shared_vg-clone then ctdb_fs-clone</strong></span>
Adding shared_vg-clone ctdb_fs-clone (kind: Mandatory) (Options: first-action=start then-action=start)
[root@z1 ~]# <span class="strong strong"><strong>pcs constraint order start shared_vg-clone then csmb_fs1-clone</strong></span>
Adding shared_vg-clone csmb_fs1-clone (kind: Mandatory) (Options: first-action=start then-action=start)</pre></li></ol></div></li></ol></div></section><section class="section" id="proc_configuring-samba-for-ha-cluster_adoc-configuring-ha-samba"><div class="titlepage"><div><div><h3 class="title">8.2. Configuring Samba in a high availability cluster</h3></div></div></div><p>
				To configure a Samba service in a Pacemaker cluster, configure the service on all nodes in the cluster.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						A two-node Red Hat High Availability cluster configured with a GFS2 file system, as described in <a class="link" href="#proc_configuring-gfs2-for-clustered-samba_adoc-configuring-ha-samba" title="8.1. Configuring a GFS2 file system for a Samba service in a high availability cluster">Configuring a GFS2 file system for a Samba service in a high availability cluster</a>.
					</li><li class="listitem">
						A public directory created on your GFS2 file system to use for the Samba share. In this example, the directory is <code class="literal">/srv/samba/share1</code>.
					</li><li class="listitem">
						Public virtual IP addresses that can be used to access the Samba share exported by this cluster.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On both nodes in the cluster, configure the Samba service and set up a share definition:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Install the Samba and CTDB packages.
							</p><pre class="literallayout"># <span class="strong strong"><strong>dnf -y install samba ctdb cifs-utils samba-winbind</strong></span></pre></li><li class="listitem"><p class="simpara">
								Ensure that the <code class="literal">ctdb</code>, <code class="literal">smb</code>, <code class="literal">nmb</code>, and <code class="literal">winbind</code> services are not running and do not start at boot.
							</p><pre class="literallayout"># <span class="strong strong"><strong>systemctl disable --now ctdb smb nmb winbind</strong></span></pre></li><li class="listitem"><p class="simpara">
								In the <code class="literal">/etc/samba/smb.conf</code> file, configure the Samba service and set up the share definition, as in the following example for a standalone server with one share.
							</p><pre class="literallayout">[global]
    netbios name = linuxserver
    workgroup = WORKGROUP
    security = user
    clustering = yes
[share1]
    path = /srv/samba/share1
    read only = no</pre></li><li class="listitem"><p class="simpara">
								Verify the <code class="literal">/etc/samba/smb.conf</code> file.
							</p><pre class="literallayout"># <span class="strong strong"><strong>testparm</strong></span></pre></li></ol></div></li><li class="listitem"><p class="simpara">
						On both nodes in the cluster, configure CTDB:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create the <code class="literal">/etc/ctdb/nodes</code> file and add the IP addresses of the cluster nodes, as in this example nodes file.
							</p><pre class="literallayout">192.0.2.11
192.0.2.12</pre></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">/etc/ctdb/public_addresses</code> file and add the IP addresses and network device names of the cluster’s public interfaces to the file. When assigning IP addresses in the <code class="literal">public_addresses</code> file, ensure that these addresses are not in use and that those addresses are routable from the intended client. The second field in each entry of the <code class="literal">/etc/ctdb/public_addresses</code> file is the interface to use on the cluster machines for the corresponding public address. In this example <code class="literal">public_addresses</code> file, the interface <code class="literal">enp1s0</code> is used for all the public addresses.
							</p><pre class="literallayout">192.0.2.201/24 enp1s0
192.0.2.202/24 enp1s0</pre><p class="simpara">
								The public interfaces of the cluster are the ones that clients use to access Samba from their network. For load balancing purposes, add an A record for each public IP address of the cluster to your DNS zone. Each of these records must resolve to the same hostname. Clients use the hostname to access Samba and DNS distributes the clients to the different nodes of the cluster.
							</p></li><li class="listitem"><p class="simpara">
								If you are running the <code class="literal">firewalld</code> service, enable the ports that are required by the <code class="literal">ctdb</code> and <code class="literal">samba</code> services.
							</p><pre class="literallayout"># <span class="strong strong"><strong>firewall-cmd --add-service=ctdb --add-service=samba --permanent</strong></span>
# <span class="strong strong"><strong>firewall-cmd --reload</strong></span></pre></li></ol></div></li><li class="listitem"><p class="simpara">
						On one node in the cluster, update the SELinux contexts:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Update the SELinux contexts on the GFS2 share.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>semanage fcontext -at ctdbd_var_run_t -s system_u "/mnt/ctdb(/.</strong></span>)?"
[root@z1 ~]# <span class="strong strong"><strong>restorecon -Rv /mnt/ctdb</strong></span></pre></li><li class="listitem"><p class="simpara">
								Update the SELinux context on the directory shared in Samba.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>semanage fcontext -at samba_share_t -s system_u "/srv/samba/share1(/.</strong></span>)?"
[root@z1 ~]# <span class="strong strong"><strong>restorecon -Rv /srv/samba/share1</strong></span></pre></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						For further information about configuring Samba as a standalone server, as in this example, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/deploying_different_types_of_servers/assembly_using-samba-as-a-server_deploying-different-types-of-servers">Using Samba as a server</a> chapter of <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/configuring_and_using_network_file_services/index">Configuring and using network file services</a>.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_networking_infrastructure_services/assembly_setting-up-and-configuring-a-bind-dns-server_networking-infrastructure-services#proc_setting-up-a-forward-zone-on-a-bind-primary-server_assembly_configuring-zones-on-a-bind-dns-server">Setting up a forward zone on a BIND primary server</a>.
					</li></ul></div></section><section class="section" id="proc_configuring-samba-cluster-resources_adoc-configuring-ha-samba"><div class="titlepage"><div><div><h3 class="title">8.3. Configuring Samba cluster resources</h3></div></div></div><p>
				After configuring a Samba service on both nodes of a two-node high availability cluster, configure the Samba cluster resources for the cluster.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						A two-node Red Hat High Availability cluster configured with a GFS2 file system, as described in <a class="link" href="#proc_configuring-gfs2-for-clustered-samba_adoc-configuring-ha-samba" title="8.1. Configuring a GFS2 file system for a Samba service in a high availability cluster">Configuring a GFS2 file system for a Samba service in a high availability cluster</a>.
					</li><li class="listitem">
						Samba service configured on both cluster nodes, as described in <a class="link" href="#proc_configuring-samba-for-ha-cluster_adoc-configuring-ha-samba" title="8.2. Configuring Samba in a high availability cluster">Configuring Samba in a high availability cluster</a>.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On one node in the cluster, configure the Samba cluster resources:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create the CTDB resource, in group <code class="literal">samba-group</code>. The CTDB resource agent uses the <code class="literal">ctdb_*</code> options specified with the <code class="literal">pcs</code> command to create the CTDB configuration file. Create the resource as disabled so it does not start automatically before you have configured the necessary order constraints.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create --disabled ctdb --group samba-group ocf:heartbeat:CTDB ctdb_recovery_lock=/mnt/ctdb/ctdb.lock ctdb_dbdir=/var/lib/ctdb ctdb_logfile=/var/log/ctdb.log op monitor interval=10 timeout=30 op start timeout=90 op stop timeout=100</strong></span></pre></li><li class="listitem"><p class="simpara">
								Clone the <code class="literal">samba-group</code> resource group.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource clone samba-group</strong></span></pre></li><li class="listitem"><p class="simpara">
								Create ordering constraints to ensure that all <code class="literal">Filesystem</code> resources are running before the resources in <code class="literal">samba-group</code>.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs constraint order start ctdb_fs-clone then samba-group-clone</strong></span>
[root@z1 ~]# <span class="strong strong"><strong>pcs constraint order start csmb_fs1-clone then samba-group-clone</strong></span></pre></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">samba</code> resource in the resource group <code class="literal">samba-group</code>. This creates an implicit ordering constraint between CTDB and Samba, based on the order they are added.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource create samba --group samba-group systemd:smb</strong></span></pre></li><li class="listitem"><p class="simpara">
								Enable the <code class="literal">ctdb</code> and <code class="literal">samba</code> resources.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs resource enable ctdb samba</strong></span></pre></li><li class="listitem"><p class="simpara">
								Check that all the services have started successfully.
							</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									It can take a couple of minutes for CTDB to start Samba, export the shares, and stabilize. If you check the cluster status before this process has completed, you may see that the <code class="literal">samba</code> services are not yet running.
								</p></div></rh-alert><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs status</strong></span>

...

Full List of Resources:
  * fence-z1   (stonith:fence_xvm): Started z1.example.com
  * fence-z2   (stonith:fence_xvm): Started z2.example.com
  * Clone Set: locking-clone [locking]:
	* Started: [ z1.example.com z2.example.com ]
  * Clone Set: shared_vg-clone [shared_vg]:
	* Started: [ z1.example.com z2.example.com ]
  * Clone Set: ctdb_fs-clone [ctdb_fs]:
	* Started: [ z1.example.com z2.example.com ]
  * Clone Set: csmb_fs1-clone [csmb_fs1]:
	* Started: [ z1.example.com z2.example.com ]
   * Clone Set: samba-group-clone [samba-group]:
	* Started: [ z1.example.com z2.example.com ]</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						On both nodes in the cluster, add a local user for the test share directory.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Add the user.
							</p><pre class="literallayout"># <span class="strong strong"><strong>useradd -M -s /sbin/nologin example_user</strong></span></pre></li><li class="listitem"><p class="simpara">
								Set a password for the user.
							</p><pre class="literallayout"># <span class="strong strong"><strong>passwd example_user</strong></span></pre></li><li class="listitem"><p class="simpara">
								Set an SMB password for the user.
							</p><pre class="literallayout"># <span class="strong strong"><strong>smbpasswd -a example_user</strong></span>
New SMB password:
Retype new SMB password:
Added user example_user</pre></li><li class="listitem"><p class="simpara">
								Activate the user in the Samba database.
							</p><pre class="literallayout"># <span class="strong strong"><strong>smbpasswd -e example_user</strong></span></pre></li><li class="listitem"><p class="simpara">
								Update the file ownership and permissions on the GFS2 share for the Samba user.
							</p><pre class="literallayout"># <span class="strong strong"><strong>chown example_user:users /srv/samba/share1/</strong></span>
# <span class="strong strong"><strong>chmod 755 /srv/samba/share1/</strong></span></pre></li></ol></div></li></ol></div></section><section class="section" id="proc_verifying-clustered-samba-configuration.adoc-configuring-ha-samba"><div class="titlepage"><div><div><h3 class="title">8.4. Verifying clustered Samba configuration</h3></div></div></div><p>
				If your clustered Samba configuration was successful, you are able to mount the Samba share. After mounting the share, you can test for Samba recovery if the cluster node that is exporting the Samba share becomes unavailable.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On a system that has access to one or more of the public IP addresses configured in the <code class="literal">/etc/ctdb/public_addresses</code> file on the cluster nodes, mount the Samba share using one of these public IP addresses.
					</p><pre class="literallayout">[root@testmount ~]# <span class="strong strong"><strong>mkdir /mnt/sambashare</strong></span>
[root@testmount ~]# <span class="strong strong"><strong>mount -t cifs -o user=example_user //192.0.2.201/share1 /mnt/sambashare</strong></span>
Password for example_user@//192.0.2.201/public: XXXXXXX</pre></li><li class="listitem"><p class="simpara">
						Verify that the file system is mounted.
					</p><pre class="literallayout">[root@testmount ~]# <span class="strong strong"><strong>mount | grep /mnt/sambashare</strong></span>
//192.0.2.201/public on /mnt/sambashare type cifs (rw,relatime,vers=1.0,cache=strict,username=example_user,domain=LINUXSERVER,uid=0,noforceuid,gid=0,noforcegid,addr=192.0.2.201,unix,posixpaths,serverino,mapposix,acl,rsize=1048576,wsize=65536,echo_interval=60,actimeo=1,user=example_user)</pre></li><li class="listitem"><p class="simpara">
						Verify that you can create a file on the mounted file system.
					</p><pre class="literallayout">[root@testmount ~]# <span class="strong strong"><strong>touch /mnt/sambashare/testfile1</strong></span>
[root@testmount ~]# <span class="strong strong"><strong>ls /mnt/sambashare</strong></span>
testfile1</pre></li><li class="listitem"><p class="simpara">
						Determine which cluster node is exporting the Samba share:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								On each cluster node, display the IP addresses assigned to the interface specified in the <code class="literal">public_addresses</code> file. The following commands display the IPv4 addresses assigned to the <code class="literal">enp1s0</code> interface on each node.
							</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>ip -4 addr show enp1s0 | grep inet</strong></span>
     inet 192.0.2.11/24 brd 192.0.2.255 scope global dynamic noprefixroute enp1s0
     inet 192.0.2.201/24 brd 192.0.2.255 scope global secondary enp1s0

[root@z2 ~]# <span class="strong strong"><strong>ip -4 addr show enp1s0 | grep inet</strong></span>
     inet 192.0.2.12/24 brd 192.0.2.255 scope global dynamic noprefixroute enp1s0
     inet 192.0.2.202/24 brd 192.0.2.255 scope global secondary enp1s0</pre></li><li class="listitem"><p class="simpara">
								In the output of the <code class="literal">ip</code> command, find the node with the IP address you specified with the <code class="literal">mount</code> command when you mounted the share.
							</p><p class="simpara">
								In this example, the IP address specified in the mount command is 192.0.2.201. The output of the <code class="literal">ip</code> command shows that the IP address 192.0.2.201 is assigned to <code class="literal">z1.example.com</code>.
							</p></li></ol></div></li><li class="listitem"><p class="simpara">
						Put the node exporting the Samba share in <code class="literal">standby</code> mode, which will cause the node to be unable to host any cluster resources.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs node standby z1.example.com</strong></span></pre></li><li class="listitem"><p class="simpara">
						From the system on which you mounted the file system, verify that you can still create a file on the file system.
					</p><pre class="literallayout">[root@testmount ~]# <span class="strong strong"><strong>touch /mnt/sambashare/testfile2</strong></span>
[root@testmount ~]# <span class="strong strong"><strong>ls /mnt/sambashare</strong></span>
testfile1  testfile2</pre></li><li class="listitem"><p class="simpara">
						Delete the files you have created to verify that the file system has successfully mounted. If you no longer require the file system to be mounted, unmount it at this point.
					</p><pre class="literallayout">[root@testmount ~]# <span class="strong strong"><strong>rm /mnt/sambashare/testfile1 /mnt/sambashare/testfile2</strong></span>
rm: remove regular empty file '/mnt/sambashare/testfile1'? <span class="strong strong"><strong>y</strong></span>
rm: remove regular empty file '/mnt/sambashare/testfile1'? <span class="strong strong"><strong>y</strong></span>
[root@testmount ~]# <span class="strong strong"><strong>umount /mnt/sambashare</strong></span></pre></li><li class="listitem"><p class="simpara">
						From one of the cluster nodes, restore cluster services to the node that you previously put into standby mode. This will not necessarily move the service back to that node.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs node unstandby z1.example.com</strong></span></pre></li></ol></div></section></section><section class="chapter" id="assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 9. Getting started with the pcsd Web UI</h2></div></div></div><p class="_abstract _abstract">
			The <code class="literal">pcsd</code> Web UI is a graphical user interface to create and configure Pacemaker/Corosync clusters.
		</p><section class="section" id="proc_setting-up-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui"><div class="titlepage"><div><div><h3 class="title">9.1. Setting up the pcsd Web UI</h3></div></div></div><p class="_abstract _abstract">
				Set up your system to use the <code class="literal">pcsd</code> Web UI to configure a cluster with the following procedure.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						The Pacemaker configuration tools are installed.
					</li><li class="listitem">
						Your system is set up for cluster configuration.
					</li></ul></div><p>
				For instructions on installing cluster software and setting up your system for cluster configuration, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters#proc_installing-cluster-software-creating-high-availability-cluster">Installing cluster software</a>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On any system, open a browser to the following URL, specifying one of the nodes of the cluster (note that this uses the <code class="literal">https</code> protocol). This brings up the <code class="literal command">pcsd</code> Web UI login screen.
					</p><pre class="literallayout">https://<span class="emphasis"><em>nodename</em></span>:2224</pre></li><li class="listitem">
						Log in as user <code class="literal">hacluster</code>. This brings up the <code class="literal">Clusters</code> page.
					</li></ol></div></section><section class="section" id="proc_configuring-ha-pcsd-web-ui-getting-started-with-the-pcsd-web-ui"><div class="titlepage"><div><div><h3 class="title">9.2. Configuring a high availability pcsd Web UI</h3></div></div></div><p class="_abstract _abstract">
				When you use the <code class="literal">pcsd</code> Web UI, you connect to one of the nodes of the cluster to display the cluster management pages. If the node to which you are connecting goes down or becomes unavailable, you can reconnect to the cluster by opening your browser to a URL that specifies a different node of the cluster. It is possible, however, to configure the <code class="literal">pcsd</code> Web UI itself for high availability, in which case you can continue to manage the cluster without entering a new URL.
			</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					To configure the <code class="literal">pcsd</code> Web UI for high availability, perform the following steps.
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Ensure that the <code class="literal">pcsd</code> certificates are synced across the nodes of the cluster by setting <code class="literal">PCSD_SSL_CERT_SYNC_ENABLED</code> to <code class="literal">true</code> in the <code class="literal">/etc/sysconfig/pcsd</code> configuration file. Enabling certificate syncing causes <code class="literal">pcsd</code> to sync the certificates for the cluster setup and node add commands. <code class="literal">PCSD_SSL_CERT_SYNC_ENABLED</code> is set to <code class="literal">false</code> by default.
					</li><li class="listitem">
						Create an <code class="literal">IPaddr2</code> cluster resource, which is a floating IP address that you will use to connect to the <code class="literal">pcsd</code> Web UI. The IP address must not be one already associated with a physical node. If the <code class="literal">IPaddr2</code> resource’s NIC device is not specified, the floating IP must reside on the same network as one of the node’s statically assigned IP addresses, otherwise the NIC device to assign the floating IP address cannot be properly detected.
					</li><li class="listitem"><p class="simpara">
						Create custom SSL certificates for use with <code class="literal">pcsd</code> and ensure that they are valid for the addresses of the nodes used to connect to the <code class="literal">pcsd</code> Web UI.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								To create custom SSL certificates, you can use either wildcard certificates or you can use the Subject Alternative Name certificate extension. For information about the Red Hat Certificate System, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_certificate_system/10/html/administration_guide/index">Red Hat Certificate System Administration Guide</a>.
							</li><li class="listitem">
								Install the custom certificates for <code class="literal">pcsd</code> with the <code class="literal">pcs pcsd certkey</code> command.
							</li><li class="listitem">
								Sync the <code class="literal">pcsd</code> certificates to all nodes in the cluster with the <code class="literal">pcs pcsd sync-certificates</code> command.
							</li></ol></div></li><li class="listitem">
						Connect to the <code class="literal">pcsd</code> Web UI using the floating IP address you configured as a cluster resource.
					</li></ol></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					Even when you configure the <code class="literal">pcsd</code> Web UI for high availability, you will be asked to log in again when the node to which you are connecting goes down.
				</p></div></rh-alert></section></section><section class="chapter" id="assembly_configuring-fencing-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 10. Configuring fencing in a Red Hat High Availability cluster</h2></div></div></div><p>
			A node that is unresponsive may still be accessing data. The only way to be certain that your data is safe is to fence the node using STONITH. STONITH is an acronym for "Shoot The Other Node In The Head" and it protects your data from being corrupted by rogue nodes or concurrent access. Using STONITH, you can be certain that a node is truly offline before allowing the data to be accessed from another node.
		</p><p>
			STONITH also has a role to play in the event that a clustered service cannot be stopped. In this case, the cluster uses STONITH to force the whole node offline, thereby making it safe to start the service elsewhere.
		</p><p>
			For more complete general information about fencing and its importance in a Red Hat High Availability cluster, see the Red Hat Knowledgebase solution <a class="link" href="https://access.redhat.com/solutions/15575">Fencing in a Red Hat High Availability Cluster</a>.
		</p><p>
			You implement STONITH in a Pacemaker cluster by configuring fence devices for the nodes of the cluster.
		</p><section class="section" id="proc_displaying-fence-agents-configuring-fencing"><div class="titlepage"><div><div><h3 class="title">10.1. Displaying available fence agents and their options</h3></div></div></div><p>
				The following commands can be used to view available fencing agents and the available options for specific fencing agents.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					Your system’s hardware determines the type of fencing device to use for your cluster. For information about supported platforms and architectures and the different fencing devices, see the <a class="link" href="https://access.redhat.com/articles/2912891#platforms">Cluster Platforms and Architectures</a> section of the article <a class="link" href="https://access.redhat.com/articles/2912891">Support Policies for RHEL High Availability Clusters</a>.
				</p></div></rh-alert><p>
				Run the following command to list all available fencing agents. When you specify a filter, this command displays only the fencing agents that match the filter.
			</p><pre class="literallayout">pcs stonith list [<span class="emphasis"><em>filter</em></span>]</pre><p>
				Run the following command to display the options for the specified fencing agent.
			</p><pre class="literallayout">pcs stonith describe [<span class="emphasis"><em>stonith_agent</em></span>]</pre><p>
				For example, the following command displays the options for the fence agent for APC over telnet/SSH.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs stonith describe fence_apc</strong></span>
Stonith options for: fence_apc
  ipaddr (required): IP Address or Hostname
  login (required): Login Name
  passwd: Login password or passphrase
  passwd_script: Script to retrieve password
  cmd_prompt: Force command prompt
  secure: SSH connection
  port (required): Physical plug number or name of virtual machine
  identity_file: Identity file for ssh
  switch: Physical switch number on device
  inet4_only: Forces agent to use IPv4 addresses only
  inet6_only: Forces agent to use IPv6 addresses only
  ipport: TCP port to use for connection with device
  action (required): Fencing Action
  verbose: Verbose mode
  debug: Write debug information to given file
  version: Display version information and exit
  help: Display help and exit
  separator: Separator for CSV created by operation list
  power_timeout: Test X seconds for status change after ON/OFF
  shell_timeout: Wait X seconds for cmd prompt after issuing command
  login_timeout: Wait X seconds for cmd prompt after login
  power_wait: Wait X seconds after issuing ON/OFF
  delay: Wait X seconds before fencing is started
  retry_on: Count of attempts to retry power on</pre><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
					For fence agents that provide a <code class="literal">method</code> option, with the exception of the <code class="literal">fence_sbd</code> agent a value of <code class="literal">cycle</code> is unsupported and should not be specified, as it may cause data corruption. Even for <code class="literal">fence_sbd</code>, however. you should not specify a method and instead use the default value.
				</p></div></rh-alert></section><section class="section" id="proc_creating-fence-devices-configuring-fencing"><div class="titlepage"><div><div><h3 class="title">10.2. Creating a fence device</h3></div></div></div><p class="_abstract _abstract">
				The format for the command to create a fence device is as follows. For a listing of the available fence device creation options, see the <code class="literal">pcs stonith -h</code> display.
			</p><pre class="literallayout">pcs stonith create <span class="emphasis"><em>stonith_id</em></span> <span class="emphasis"><em>stonith_device_type</em></span> [<span class="emphasis"><em>stonith_device_options</em></span>] [op  <span class="emphasis"><em>operation_action</em></span> <span class="emphasis"><em>operation_options</em></span>]</pre><p>
				The following command creates a single fencing device for a single node.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs stonith create MyStonith fence_virt pcmk_host_list=f1 op monitor interval=30s</strong></span></pre><p>
				Some fence devices can fence only a single node, while other devices can fence multiple nodes. The parameters you specify when you create a fencing device depend on what your fencing device supports and requires.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Some fence devices can automatically determine what nodes they can fence.
					</li><li class="listitem">
						You can use the <code class="literal">pcmk_host_list</code> parameter when creating a fencing device to specify all of the machines that are controlled by that fencing device.
					</li><li class="listitem">
						Some fence devices require a mapping of host names to the specifications that the fence device understands. You can map host names with the <code class="literal">pcmk_host_map</code> parameter when creating a fencing device.
					</li></ul></div><p>
				For information about the <code class="literal">pcmk_host_list</code> and <code class="literal">pcmk_host_map</code> parameters, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters#ref_general-fence-device-properties-configuring-fencing">General properties of fencing devices</a>.
			</p><p>
				After configuring a fence device, it is imperative that you test the device to ensure that it is working correctly. For information about testing a fence device, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters#proc_testing-fence-devices-configuring-fencing">Testing a fence device</a>.
			</p></section><section class="section" id="ref_general-fence-device-properties-configuring-fencing"><div class="titlepage"><div><div><h3 class="title">10.3. General properties of fencing devices</h3></div></div></div><p class="_abstract _abstract">
				There are many general properties you can set for fencing devices, as well as various cluster properties that determine fencing behavior.
			</p><p>
				Any cluster node can fence any other cluster node with any fence device, regardless of whether the fence resource is started or stopped. Whether the resource is started controls only the recurring monitor for the device, not whether it can be used, with the following exceptions:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You can disable a fencing device by running the <code class="literal command">pcs stonith disable <span class="emphasis"><em>stonith_id</em></span></code> command. This will prevent any node from using that device.
					</li><li class="listitem">
						To prevent a specific node from using a fencing device, you can configure location constraints for the fencing resource with the <code class="literal command">pcs constraint location …​ avoids</code> command.
					</li><li class="listitem">
						Configuring <code class="literal">stonith-enabled=false</code> will disable fencing altogether. Note, however, that Red Hat does not support clusters when fencing is disabled, as it is not suitable for a production environment.
					</li></ul></div><p>
				The following table describes the general properties you can set for fencing devices.
			</p><rh-table id="tb-fencedevice-props-HAAR"><table class="gt-4-cols lt-7-rows"><caption>Table 10.1. General Properties of Fencing Devices</caption><colgroup><col style="width: 25%; " class="col_1"><!--Empty--><col style="width: 13%; " class="col_2"><!--Empty--><col style="width: 25%; " class="col_3"><!--Empty--><col style="width: 38%; " class="col_4"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686147652064" scope="col">Field</th><th align="left" valign="top" id="idm140686147650976" scope="col">Type</th><th align="left" valign="top" id="idm140686147649888" scope="col">Default</th><th align="left" valign="top" id="idm140686147648800" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686147652064"> <p>
								<code class="literal">pcmk_host_map</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686147650976"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140686147649888"> </td><td align="left" valign="top" headers="idm140686147648800"> <p>
								A mapping of host names to port numbers for devices that do not support host names. For example: <code class="literal">node1:1;node2:2,3</code> tells the cluster to use port 1 for node1 and ports 2 and 3 for node2. the <code class="literal">pcmk_host_map</code> property supports special characters inside <code class="literal">pcmk_host_map</code> values using a backslash in front of the value. For example, you can specify <code class="literal">pcmk_host_map="node3:plug\ 1"</code> to include a space in the host alias.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686147652064"> <p>
								<code class="literal">pcmk_host_list</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686147650976"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140686147649888"> </td><td align="left" valign="top" headers="idm140686147648800"> <p>
								A list of machines controlled by this device (Optional unless <code class="literal">pcmk_host_check=static-list</code>).
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686147652064"> <p>
								<code class="literal">pcmk_host_check</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686147650976"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140686147649888"> <p>
								* <code class="literal">static-list</code> if either <code class="literal">pcmk_host_list</code> or <code class="literal">pcmk_host_map</code> is set
							</p>
							 <p>
								* Otherwise, <code class="literal">dynamic-list</code> if the fence device supports the <code class="literal">list</code> action
							</p>
							 <p>
								* Otherwise, <code class="literal">status</code> if the fence device supports the <code class="literal">status</code> action
							</p>
							 <p>
								*Otherwise, <code class="literal">none</code>.
							</p>
							 </td><td align="left" valign="top" headers="idm140686147648800"> <p>
								How to determine which machines are controlled by the device. Allowed values: <code class="literal">dynamic-list</code> (query the device), <code class="literal">static-list</code> (check the <code class="literal">pcmk_host_list</code> attribute), none (assume every device can fence every machine)
							</p>
							 </td></tr></tbody></table></rh-table><p>
				The following table summarizes additional properties you can set for fencing devices. Note that these properties are for advanced use only.
			</p><rh-table id="tb-fencepropsadvanced-HAAR"><table class="gt-4-cols lt-7-rows"><caption>Table 10.2. Advanced Properties of Fencing Devices</caption><colgroup><col style="width: 29%; " class="col_1"><!--Empty--><col style="width: 14%; " class="col_2"><!--Empty--><col style="width: 14%; " class="col_3"><!--Empty--><col style="width: 43%; " class="col_4"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686150721760" scope="col">Field</th><th align="left" valign="top" id="idm140686150720672" scope="col">Type</th><th align="left" valign="top" id="idm140686150719584" scope="col">Default</th><th align="left" valign="top" id="idm140686150718496" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_host_argument</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								port
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								An alternate parameter to supply instead of port. Some devices do not support the standard port parameter or may provide additional ones. Use this to specify an alternate, device-specific parameter that should indicate the machine to be fenced. A value of <code class="literal">none</code> can be used to tell the cluster not to supply any additional parameters.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_reboot_action</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								reboot
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								An alternate command to run instead of <code class="literal">reboot</code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the reboot action.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_reboot_timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								Specify an alternate timeout to use for reboot actions instead of <code class="literal">stonith-timeout</code>. Some devices need much more/less time to complete than normal. Use this to specify an alternate, device-specific, timeout for reboot actions.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_reboot_retries</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								integer
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								The maximum number of times to retry the <code class="literal">reboot</code> command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries reboot actions before giving up.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_off_action</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								off
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								An alternate command to run instead of <code class="literal">off</code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the off action.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_off_timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								Specify an alternate timeout to use for off actions instead of <code class="literal">stonith-timeout</code>. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for off actions.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_off_retries</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								integer
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								The maximum number of times to retry the off command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries off actions before giving up.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_list_action</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								list
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								An alternate command to run instead of <code class="literal">list</code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the list action.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_list_timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								Specify an alternate timeout to use for list actions. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for list actions.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_list_retries</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								integer
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								The maximum number of times to retry the <code class="literal">list</code> command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries list actions before giving up.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_monitor_action</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								monitor
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								An alternate command to run instead of <code class="literal">monitor</code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the monitor action.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_monitor_timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								Specify an alternate timeout to use for monitor actions instead of <code class="literal">stonith-timeout</code>. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for monitor actions.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_monitor_retries</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								integer
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								The maximum number of times to retry the <code class="literal">monitor</code> command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries monitor actions before giving up.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_status_action</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								status
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								An alternate command to run instead of <code class="literal">status</code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the status action.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_status_timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								Specify an alternate timeout to use for status actions instead of <code class="literal">stonith-timeout</code>. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for status actions.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_status_retries</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								integer
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								The maximum number of times to retry the status command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries status actions before giving up.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_delay_base</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								0s
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								Enables a base delay for fencing actions and specifies a base delay value. You can specify different values for different nodes with the <code class="literal">pcmk_delay_base</code> parameter. For general information about fencing delay parameters and their interactions, see <a class="link" href="#ref_fence-delays-configuring-fencing" title="10.4. Fencing delays">Fencing delays</a>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_delay_max</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								0s
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								Enables a random delay for fencing actions and specifies the maximum delay, which is the maximum value of the combined base delay and random delay. For example, if the base delay is 3 and <code class="literal">pcmk_delay_max</code> is 10, the random delay will be between 3 and 10. For general information about fencing delay parameters and their interactions, see <a class="link" href="#ref_fence-delays-configuring-fencing" title="10.4. Fencing delays">Fencing delays</a>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_action_limit</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								integer
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								1
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								The maximum number of actions that can be performed in parallel on this device. The cluster property <code class="literal">concurrent-fencing=true</code> needs to be configured first (this is the default value). A value of -1 is unlimited.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_on_action</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								string
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								on
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								For advanced use only: An alternate command to run instead of <code class="literal">on</code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the <code class="literal">on</code> action.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_on_timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								time
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								For advanced use only: Specify an alternate timeout to use for <code class="literal">on</code> actions instead of <code class="literal">stonith-timeout</code>. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for <code class="literal">on</code> actions.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686150721760"> <p>
								<code class="literal">pcmk_on_retries</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686150720672"> <p>
								integer
							</p>
							 </td><td align="left" valign="top" headers="idm140686150719584"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm140686150718496"> <p>
								For advanced use only: The maximum number of times to retry the <code class="literal">on</code> command within the timeout period. Some devices do not support multiple connections. Operations may <code class="literal">fail</code> if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries <code class="literal">on</code> actions before giving up.
							</p>
							 </td></tr></tbody></table></rh-table><p>
				In addition to the properties you can set for individual fence devices, there are also cluster properties you can set that determine fencing behavior, as described in the following table.
			</p><rh-table id="tb-clusterfenceprops-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 10.3. Cluster Properties that Determine Fencing Behavior</caption><colgroup><col style="width: 29%; " class="col_1"><!--Empty--><col style="width: 29%; " class="col_2"><!--Empty--><col style="width: 43%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686145198016" scope="col">Option</th><th align="left" valign="top" id="idm140686145196928" scope="col">Default</th><th align="left" valign="top" id="idm140686145195840" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686145198016"> <p>
								<code class="literal">stonith-enabled</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686145196928"> <p>
								true
							</p>
							 </td><td align="left" valign="top" headers="idm140686145195840"> <p>
								Indicates that failed nodes and nodes with resources that cannot be stopped should be fenced. Protecting your data requires that you set this <code class="literal">true</code>.
							</p>
							 <p>
								If <code class="literal">true</code>, or unset, the cluster will refuse to start resources unless one or more STONITH resources have been configured also.
							</p>
							 <p>
								Red Hat only supports clusters with this value set to <code class="literal">true</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686145198016"> <p>
								<code class="literal">stonith-action</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686145196928"> <p>
								reboot
							</p>
							 </td><td align="left" valign="top" headers="idm140686145195840"> <p>
								Action to send to fencing device. Allowed values: <code class="literal">reboot</code>, <code class="literal">off</code>. The value <code class="literal">poweroff</code> is also allowed, but is only used for legacy devices.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686145198016"> <p>
								<code class="literal">stonith-timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686145196928"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140686145195840"> <p>
								How long to wait for a STONITH action to complete.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686145198016"> <p>
								<code class="literal">stonith-max-attempts</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686145196928"> <p>
								10
							</p>
							 </td><td align="left" valign="top" headers="idm140686145195840"> <p>
								How many times fencing can fail for a target before the cluster will no longer immediately re-attempt it.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686145198016"> <p>
								<code class="literal">stonith-watchdog-timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686145196928"> </td><td align="left" valign="top" headers="idm140686145195840"> <p>
								The maximum time to wait until a node can be assumed to have been killed by the hardware watchdog. It is recommended that this value be set to twice the value of the hardware watchdog timeout. This option is needed only if watchdog-only SBD configuration is used for fencing.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686145198016"> <p>
								<code class="literal">concurrent-fencing</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686145196928"> <p>
								true
							</p>
							 </td><td align="left" valign="top" headers="idm140686145195840"> <p>
								Allow fencing operations to be performed in parallel.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686145198016"> <p>
								<code class="literal">fence-reaction</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686145196928"> <p>
								stop
							</p>
							 </td><td align="left" valign="top" headers="idm140686145195840"> <p>
								Determines how a cluster node should react if notified of its own fencing. A cluster node may receive notification of its own fencing if fencing is misconfigured, or if fabric fencing is in use that does not cut cluster communication. Allowed values are <code class="literal">stop</code> to attempt to immediately stop Pacemaker and stay stopped, or <code class="literal">panic</code> to attempt to immediately reboot the local node, falling back to stop on failure.
							</p>
							 <p>
								Although the default value for this property is <code class="literal">stop</code>, the safest choice for this value is <code class="literal">panic</code>, which attempts to immediately reboot the local node. If you prefer the stop behavior, as is most likely to be the case in conjunction with fabric fencing, it is recommended that you set this explicitly.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686145198016"> <p>
								<code class="literal">priority-fencing-delay</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686145196928"> <p>
								0 (disabled)
							</p>
							 </td><td align="left" valign="top" headers="idm140686145195840"> <p>
								Sets a fencing delay that allows you to configure a two-node cluster so that in a split-brain situation the node with the fewest or least important resources running is the node that gets fenced. For general information about fencing delay parameters and their interactions, see <a class="link" href="#ref_fence-delays-configuring-fencing" title="10.4. Fencing delays">Fencing delays</a>.
							</p>
							 </td></tr></tbody></table></rh-table><p>
				For information about setting cluster properties, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_controlling-cluster-behavior-configuring-and-managing-high-availability-clusters#setting-cluster-properties-controlling-cluster-behavior">Setting and removing cluster properties</a>.
			</p></section><section class="section" id="ref_fence-delays-configuring-fencing"><div class="titlepage"><div><div><h3 class="title">10.4. Fencing delays</h3></div></div></div><p>
				When cluster communication is lost in a two-node cluster, one node may detect this first and fence the other node. If both nodes detect this at the same time, however, each node may be able to initiate fencing of the other, leaving both nodes powered down or reset. By setting a fencing delay, you can decrease the likelihood of both cluster nodes fencing each other. You can set delays in a cluster with more than two nodes, but this is generally not of any benefit because only a partition with quorum will initiate fencing.
			</p><p>
				You can set different types of fencing delays, depending on your system requirements.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						<span class="strong strong"><strong>static fencing delays</strong></span>
					</p><p class="simpara">
						A static fencing delay is a fixed, predetermined delay. Setting a static delay on one node makes that node more likely to be fenced because it increases the chances that the other node will initiate fencing first after detecting lost communication. In an active/passive cluster, setting a delay on a passive node makes it more likely that the passive node will be fenced when communication breaks down. You configure a static delay by using the <code class="literal">pcs_delay_base</code> cluster property. You can set this property when a separate fence device is used for each node or when a single fence device is used for all nodes.
					</p></li><li class="listitem"><p class="simpara">
						<span class="strong strong"><strong>dynamic fencing delays</strong></span>
					</p><p class="simpara">
						A dynamic fencing delay is random. It can vary and is determined at the time fencing is needed. You configure a random delay and specify a maximum value for the combined base delay and random delay with the <code class="literal">pcs_delay_max</code> cluster property. When the fencing delay for each node is random, which node is fenced is also random. You may find this feature useful if your cluster is configured with a single fence device for all nodes in an active/active design.
					</p></li><li class="listitem"><p class="simpara">
						<span class="strong strong"><strong>priority fencing delays</strong></span>
					</p><p class="simpara">
						A priority fencing delay is based on active resource priorities. If all resources have the same priority, the node with the fewest resources running is the node that gets fenced. In most cases, you use only one delay-related parameter, but it is possible to combine them. Combining delay-related parameters adds the priority values for the resources together to create a total delay. You configure a priority fencing delay with the <code class="literal">priority-fencing-delay</code> cluster property. You may find this feature useful in an active/active cluster design because it can make the node running the fewest resources more likely to be fenced when communication between the nodes is lost.
					</p></li></ul></div><div class="formalpara"><p class="title"><strong>The <code class="literal">pcmk_delay_base</code> cluster property</strong></p><p>
					Setting the <code class="literal">pcmk_delay_base</code> cluster property enables a base delay for fencing and specifies a base delay value.
				</p></div><p>
				When you set the <code class="literal">pcmk_delay_max</code> cluster property in addition to the <code class="literal">pcmk_delay_base</code> property, the overall delay is derived from a random delay value added to this static delay so that the sum is kept below the maximum delay. When you set <code class="literal">pcmk_delay_base</code> but do not set <code class="literal">pcmk_delay_max</code>, there is no random component to the delay and it will be the value of <code class="literal">pcmk_delay_base</code>.
			</p><p>
				You can specify different values for different nodes with the <code class="literal">pcmk_delay_base</code> parameter. This allows a single fence device to be used in a two-node cluster, with a different delay for each node. You do not need to configure two separate devices to use separate delays. To specify different values for different nodes, you map the host names to the delay value for that node using a similar syntax to <code class="literal">pcmk_host_map</code>. For example, <code class="literal">node1:0;node2:10s</code> would use no delay when fencing <code class="literal">node1</code> and a 10-second delay when fencing <code class="literal">node2</code>.
			</p><div class="formalpara"><p class="title"><strong>The <code class="literal">pcmk_delay_max</code> cluster property</strong></p><p>
					Setting the <code class="literal">pcmk_delay_max</code> cluster property enables a random delay for fencing actions and specifies the maximum delay, which is the maximum value of the combined base delay and random delay. For example, if the base delay is 3 and <code class="literal">pcmk_delay_max</code> is 10, the random delay will be between 3 and 10.
				</p></div><p>
				When you set the <code class="literal">pcmk_delay_base</code> cluster property in addition to the <code class="literal">pcmk_delay_max</code> property, the overall delay is derived from a random delay value added to this static delay so that the sum is kept below the maximum delay. When you set <code class="literal">pcmk_delay_max</code> but do not set <code class="literal">pcmk_delay_base</code> there is no static component to the delay.
			</p><div class="formalpara"><p class="title"><strong>The <code class="literal">priority-fencing-delay</code> cluster property</strong></p><p>
					Setting the <code class="literal">priority-fencing-delay</code> cluster property allows you to configure a two-node cluster so that in a split-brain situation the node with the fewest or least important resources running is the node that gets fenced.
				</p></div><p>
				The <code class="literal">priority-fencing-delay</code> property can be set to a time duration. The default value for this property is 0 (disabled). If this property is set to a non-zero value, and the priority meta-attribute is configured for at least one resource, then in a split-brain situation the node with the highest combined priority of all resources running on it will be more likely to remain operational. For example, if you set <code class="literal">pcs resource defaults update priority=1</code> and <code class="literal">pcs property set priority-fencing-delay=15s</code> and no other priorities are set, then the node running the most resources will be more likely to remain operational because the other node will wait 15 seconds before initiating fencing. If a particular resource is more important than the rest, you can give it a higher priority.
			</p><p>
				The node running the promoted role of a promotable clone gets an extra 1 point if a priority has been configured for that clone.
			</p><div class="formalpara"><p class="title"><strong>Interaction of fencing delays</strong></p><p>
					Setting more than one type of fencing delay yields the following results:
				</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Any delay set with the <code class="literal">priority-fencing-delay</code> property is added to any delay from the <code class="literal">pcmk_delay_base</code> and <code class="literal">pcmk_delay_max</code> fence device properties. This behavior allows some delay when both nodes have equal priority, or both nodes need to be fenced for some reason other than node loss, as when <code class="literal">on-fail=fencing</code> is set for a resource monitor operation. When setting these delays in combination, set the <code class="literal">priority-fencing-delay</code> property to a value that is significantly greater than the maximum delay from <code class="literal">pcmk_delay_base</code> and <code class="literal">pcmk_delay_max</code> to be sure the prioritized node is preferred. Setting this property to twice this value is always safe.
					</li><li class="listitem">
						Only fencing scheduled by Pacemaker itself observes fencing delays. Fencing scheduled by external code such as <code class="literal">dlm_controld</code> and fencing implemented by the <code class="literal">pcs stonith fence</code> command do not provide the necessary information to the fence device.
					</li><li class="listitem">
						Some individual fence agents implement a delay parameter, with a name determined by the agent and which is independent of delays configured with a <code class="literal">pcmk_delay_</code>* property. If both of these delays are configured, they are added together and would generally not be used in conjunction.
					</li></ul></div></section><section class="section" id="proc_testing-fence-devices-configuring-fencing"><div class="titlepage"><div><div><h3 class="title">10.5. Testing a fence device</h3></div></div></div><p class="_abstract _abstract">
				Fencing is a fundamental part of the Red Hat Cluster infrastructure and it is important to validate or test that fencing is working properly.
			</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					Use the following procedure to test a fence device.
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Use ssh, telnet, HTTP, or whatever remote protocol is used to connect to the device to manually log in and test the fence device or see what output is given. For example, if you will be configuring fencing for an IPMI-enabled device,then try to log in remotely with <code class="literal command">ipmitool</code>. Take note of the options used when logging in manually because those options might be needed when using the fencing agent.
					</p><p class="simpara">
						If you are unable to log in to the fence device, verify that the device is pingable, there is nothing such as a firewall configuration that is preventing access to the fence device, remote access is enabled on the fencing device, and the credentials are correct.
					</p></li><li class="listitem"><p class="simpara">
						Run the fence agent manually, using the fence agent script. This does not require that the cluster services are running, so you can perform this step before the device is configured in the cluster. This can ensure that the fence device is responding properly before proceeding.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							These examples use the <code class="literal command">fence_ipmilan</code> fence agent script for an iLO device. The actual fence agent you will use and the command that calls that agent will depend on your server hardware. You should consult the man page for the fence agent you are using to determine which options to specify. You will usually need to know the login and password for the fence device and other information related to the fence device.
						</p></div></rh-alert><p class="simpara">
						The following example shows the format you would use to run the <code class="literal command">fence_ipmilan</code> fence agent script with <code class="literal">-o status</code> parameter to check the status of the fence device interface on another node without actually fencing it. This allows you to test the device and get it working before attempting to reboot the node. When running this command, you specify the name and password of an iLO user that has power on and off permissions for the iLO device.
					</p><pre class="literallayout"># <span class="strong strong"><strong>fence_ipmilan -a ipaddress -l username -p password -o status</strong></span></pre><p class="simpara">
						The following example shows the format you would use to run the <code class="literal command">fence_ipmilan</code> fence agent script with the <code class="literal">-o reboot</code> parameter. Running this command on one node reboots the node managed by this iLO device.
					</p><pre class="literallayout"># <span class="strong strong"><strong>fence_ipmilan -a ipaddress -l username -p password -o reboot</strong></span></pre><p class="simpara">
						If the fence agent failed to properly do a status, off, on, or reboot action, you should check the hardware, the configuration of the fence device, and the syntax of your commands. In addition, you can run the fence agent script with the debug output enabled. The debug output is useful for some fencing agents to see where in the sequence of events the fencing agent script is failing when logging into the fence device.
					</p><pre class="literallayout"># <span class="strong strong"><strong>fence_ipmilan -a ipaddress -l username -p password -o status -D /tmp/$(hostname)-fence_agent.debug</strong></span></pre><p class="simpara">
						When diagnosing a failure that has occurred, you should ensure that the options you specified when manually logging in to the fence device are identical to what you passed on to the fence agent with the fence agent script.
					</p><p class="simpara">
						For fence agents that support an encrypted connection, you may see an error due to certificate validation failing, requiring that you trust the host or that you use the fence agent’s <code class="literal">ssl-insecure</code> parameter. Similarly, if SSL/TLS is disabled on the target device, you may need to account for this when setting the SSL parameters for the fence agent.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							If the fence agent that is being tested is a <code class="literal command">fence_drac</code>, <code class="literal command">fence_ilo</code>, or some other fencing agent for a systems management device that continues to fail, then fall back to trying <code class="literal command">fence_ipmilan</code>. Most systems management cards support IPMI remote login and the only supported fencing agent is <code class="literal command">fence_ipmilan</code>.
						</p></div></rh-alert></li><li class="listitem"><p class="simpara">
						Once the fence device has been configured in the cluster with the same options that worked manually and the cluster has been started, test fencing with the <code class="literal command">pcs stonith fence</code> command from any node (or even multiple times from different nodes), as in the following example. The <code class="literal command">pcs stonith fence</code> command reads the cluster configuration from the CIB and calls the fence agent as configured to execute the fence action. This verifies that the cluster configuration is correct.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs stonith fence node_name</strong></span></pre><p class="simpara">
						If the <code class="literal command">pcs stonith fence</code> command works properly, that means the fencing configuration for the cluster should work when a fence event occurs. If the command fails, it means that cluster management cannot invoke the fence device through the configuration it has retrieved. Check for the following issues and update your cluster configuration as needed.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Check your fence configuration. For example, if you have used a host map you should ensure that the system can find the node using the host name you have provided.
							</li><li class="listitem">
								Check whether the password and user name for the device include any special characters that could be misinterpreted by the bash shell. Making sure that you enter passwords and user names surrounded by quotation marks could address this issue.
							</li><li class="listitem">
								Check whether you can connect to the device using the exact IP address or host name you specified in the <code class="literal command">pcs stonith</code> command. For example, if you give the host name in the stonith command but test by using the IP address, that is not a valid test.
							</li><li class="listitem"><p class="simpara">
								If the protocol that your fence device uses is accessible to you, use that protocol to try to connect to the device. For example many agents use ssh or telnet. You should try to connect to the device with the credentials you provided when configuring the device, to see if you get a valid prompt and can log in to the device.
							</p><p class="simpara">
								If you determine that all your parameters are appropriate but you still have trouble connecting to your fence device, you can check the logging on the fence device itself, if the device provides that, which will show if the user has connected and what command the user issued. You can also search through the <code class="literal">/var/log/messages</code> file for instances of stonith and error, which could give some idea of what is transpiring, but some agents can provide additional information.
							</p></li></ul></div></li><li class="listitem"><p class="simpara">
						Once the fence device tests are working and the cluster is up and running, test an actual failure. To do this, take an action in the cluster that should initiate a token loss.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Take down a network. How you take a network depends on your specific configuration. In many cases, you can physically pull the network or power cables out of the host. For information about simulating a network failure, see the Red Hat Knowledgebase solution <a class="link" href="https://access.redhat.com/solutions/79523/">What is the proper way to simulate a network failure on a RHEL Cluster?</a>.
							</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									Disabling the network interface on the local host rather than physically disconnecting the network or power cables is not recommended as a test of fencing because it does not accurately simulate a typical real-world failure.
								</p></div></rh-alert></li><li class="listitem"><p class="simpara">
								Block corosync traffic both inbound and outbound using the local firewall.
							</p><p class="simpara">
								The following example blocks corosync, assuming the default corosync port is used, <code class="literal">firewalld</code> is used as the local firewall, and the network interface used by corosync is in the default firewall zone:
							</p><pre class="literallayout"># <span class="strong strong"><strong>firewall-cmd --direct --add-rule ipv4 filter OUTPUT 2 -p udp --dport=5405 -j DROP</strong></span>
# <span class="strong strong"><strong>firewall-cmd --add-rich-rule='rule family="ipv4" port port="5405" protocol="udp" drop</strong></span></pre></li><li class="listitem"><p class="simpara">
								Simulate a crash and panic your machine with <code class="literal">sysrq-trigger</code>. Note, however, that triggering a kernel panic can cause data loss; it is recommended that you disable your cluster resources first.
							</p><pre class="literallayout"># <span class="strong strong"><strong>echo c &gt; /proc/sysrq-trigger</strong></span></pre></li></ul></div></li></ol></div></section><section class="section" id="proc_configuring-fencing-levels-configuring-fencing"><div class="titlepage"><div><div><h3 class="title">10.6. Configuring fencing levels</h3></div></div></div><p class="_abstract _abstract">
				Pacemaker supports fencing nodes with multiple devices through a feature called fencing topologies. To implement topologies, create the individual devices as you normally would and then define one or more fencing levels in the fencing topology section in the configuration.
			</p><p>
				Pacemaker processes fencing levels as follows:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Each level is attempted in ascending numeric order, starting at 1.
					</li><li class="listitem">
						If a device fails, processing terminates for the current level. No further devices in that level are exercised and the next level is attempted instead.
					</li><li class="listitem">
						If all devices are successfully fenced, then that level has succeeded and no other levels are tried.
					</li><li class="listitem">
						The operation is finished when a level has passed (success), or all levels have been attempted (failed).
					</li></ul></div><p>
				Use the following command to add a fencing level to a node. The devices are given as a comma-separated list of <code class="literal">stonith</code> ids, which are attempted for the node at that level.
			</p><pre class="literallayout">pcs stonith level add <span class="emphasis"><em>level</em></span> <span class="emphasis"><em>node</em></span> <span class="emphasis"><em>devices</em></span></pre><p>
				The following command lists all of the fencing levels that are currently configured.
			</p><pre class="literallayout">pcs stonith level</pre><p>
				In the following example, there are two fence devices configured for node <code class="literal">rh7-2</code>: an ilo fence device called <code class="literal">my_ilo</code> and an apc fence device called <code class="literal">my_apc</code>. These commands set up fence levels so that if the device <code class="literal">my_ilo</code> fails and is unable to fence the node, then Pacemaker will attempt to use the device <code class="literal">my_apc</code>. This example also shows the output of the <code class="literal">pcs stonith level</code> command after the levels are configured.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs stonith level add 1 rh7-2 my_ilo</strong></span>
# <span class="strong strong"><strong>pcs stonith level add 2 rh7-2 my_apc</strong></span>
# <span class="strong strong"><strong>pcs stonith level</strong></span>
 Node: rh7-2
  Level 1 - my_ilo
  Level 2 - my_apc</pre><p>
				The following command removes the fence level for the specified node and devices. If no nodes or devices are specified then the fence level you specify is removed from all nodes.
			</p><pre class="literallayout">pcs stonith level remove <span class="emphasis"><em>level</em></span>  [<span class="emphasis"><em>node_id</em></span>] [<span class="emphasis"><em>stonith_id</em></span>] ... [<span class="emphasis"><em>stonith_id</em></span>]</pre><p>
				The following command clears the fence levels on the specified node or stonith id. If you do not specify a node or stonith id, all fence levels are cleared.
			</p><pre class="literallayout">pcs stonith level clear [<span class="emphasis"><em>node</em></span>]|<span class="emphasis"><em>stonith_id</em></span>(s)]</pre><p>
				If you specify more than one stonith id, they must be separated by a comma and no spaces, as in the following example.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs stonith level clear dev_a,dev_b</strong></span></pre><p>
				The following command verifies that all fence devices and nodes specified in fence levels exist.
			</p><pre class="literallayout">pcs stonith level verify</pre><p>
				You can specify nodes in fencing topology by a regular expression applied on a node name and by a node attribute and its value. For example, the following commands configure nodes <code class="literal">node1</code>, <code class="literal">node2</code>, and <code class="literal">node3</code> to use fence devices <code class="literal">apc1</code> and <code class="literal">apc2</code>, and nodes <code class="literal">node4</code>, <code class="literal">node5</code>, and <code class="literal">node6</code> to use fence devices <code class="literal">apc3</code> and <code class="literal">apc4</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs stonith level add 1 "regexp%node[1-3]" apc1,apc2</strong></span>
# <span class="strong strong"><strong>pcs stonith level add 1 "regexp%node[4-6]" apc3,apc4</strong></span></pre><p>
				The following commands yield the same results by using node attribute matching.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs node attribute node1 rack=1</strong></span>
# <span class="strong strong"><strong>pcs node attribute node2 rack=1</strong></span>
# <span class="strong strong"><strong>pcs node attribute node3 rack=1</strong></span>
# <span class="strong strong"><strong>pcs node attribute node4 rack=2</strong></span>
# <span class="strong strong"><strong>pcs node attribute node5 rack=2</strong></span>
# <span class="strong strong"><strong>pcs node attribute node6 rack=2</strong></span>
# <span class="strong strong"><strong>pcs stonith level add 1 attrib%rack=1 apc1,apc2</strong></span>
# <span class="strong strong"><strong>pcs stonith level add 1 attrib%rack=2 apc3,apc4</strong></span></pre></section><section class="section" id="proc_configuring-fencing-for-redundant-power-configuring-fencing"><div class="titlepage"><div><div><h3 class="title">10.7. Configuring fencing for redundant power supplies</h3></div></div></div><p class="_abstract _abstract">
				When configuring fencing for redundant power supplies, the cluster must ensure that when attempting to reboot a host, both power supplies are turned off before either power supply is turned back on.
			</p><p>
				If the node never completely loses power, the node may not release its resources. This opens up the possibility of nodes accessing these resources simultaneously and corrupting them.
			</p><p>
				You need to define each device only once and to specify that both are required to fence the node, as in the following example.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs stonith create apc1 fence_apc_snmp ipaddr=apc1.example.com login=user passwd='7a4D#1j!pz864' pcmk_host_map="node1.example.com:1;node2.example.com:2"</strong></span>

# <span class="strong strong"><strong>pcs stonith create apc2 fence_apc_snmp ipaddr=apc2.example.com login=user passwd='7a4D#1j!pz864' pcmk_host_map="node1.example.com:1;node2.example.com:2"</strong></span>

# <span class="strong strong"><strong>pcs stonith level add 1 node1.example.com apc1,apc2</strong></span>
# <span class="strong strong"><strong>pcs stonith level add 1 node2.example.com apc1,apc2</strong></span></pre></section><section class="section" id="proc_displaying-configuring-fence-devices-configuring-fencing"><div class="titlepage"><div><div><h3 class="title">10.8. Displaying configured fence devices</h3></div></div></div><p class="_abstract _abstract">
				The following command shows all currently configured fence devices. If a <span class="emphasis"><em>stonith_id</em></span> is specified, the command shows the options for that configured fencing device only. If the <code class="literal">--full</code> option is specified, all configured fencing options are displayed.
			</p><pre class="literallayout">pcs stonith config [<span class="emphasis"><em>stonith_id</em></span>] [--full]</pre></section><section class="section" id="proc_exporting-fence-devices-configuring-fencing"><div class="titlepage"><div><div><h3 class="title">10.9. Exporting fence devices as <code class="literal">pcs</code> commands</h3></div></div></div><p class="_abstract _abstract">
				As of Red Hat Enterprise Linux 9.1, you can display the <code class="literal">pcs</code> commands that can be used to re-create configured fence devices on a different system using the <code class="literal">--output-format=cmd</code> option of the <code class="literal">pcs stonith config</code> command.
			</p><p>
				The following commands create a <code class="literal">fence_apc_snmp</code> fence device and display the <code class="literal">pcs</code> command you can use to re-create the device.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs stonith create myapc fence_apc_snmp ip="zapc.example.com" pcmk_host_map="z1.example.com:1;z2.example.com:2" username="apc" password="apc"</strong></span>
# <span class="strong strong"><strong>pcs stonith config --output-format=cmd</strong></span>
Warning: Only 'text' output format is supported for stonith levels
pcs stonith create --no-default-ops --force -- myapc fence_apc_snmp \
  ip=zapc.example.com password=apc 'pcmk_host_map=z1.example.com:1;z2.example.com:2' username=apc \
  op \
    monitor interval=60s id=myapc-monitor-interval-60s</pre></section><section class="section" id="proc_modifying-fence-devices-configuring-fencing"><div class="titlepage"><div><div><h3 class="title">10.10. Modifying and deleting fence devices</h3></div></div></div><p class="_abstract _abstract">
				Modify or add options to a currently configured fencing device with the following command.
			</p><pre class="literallayout">pcs stonith update <span class="emphasis"><em>stonith_id</em></span> [<span class="emphasis"><em>stonith_device_options</em></span>]</pre><p>
				Updating a SCSI fencing device with the <code class="literal">pcs stonith update</code> command causes a restart of all resources running on the same node where the fencing resource was running. You can use either version of the following command to update SCSI devices without causing a restart of other cluster resources. As of RHEL 9.1, SCSI fencing devices can be configured as multipath devices.
			</p><pre class="literallayout">pcs stonith update-scsi-devices <span class="emphasis"><em>stonith_id</em></span> set <span class="emphasis"><em>device-path1</em></span> <span class="emphasis"><em>device-path2</em></span>
pcs stonith update-scsi-devices <span class="emphasis"><em>stonith_id</em></span> add <span class="emphasis"><em>device-path1</em></span> remove <span class="emphasis"><em>device-path2</em></span></pre><p>
				Use the following command to remove a fencing device from the current configuration.
			</p><pre class="literallayout">pcs stonith delete <span class="emphasis"><em>stonith_id</em></span></pre></section><section class="section" id="proc_manually-fencing-a-node-configuring-fencing"><div class="titlepage"><div><div><h3 class="title">10.11. Manually fencing a cluster node</h3></div></div></div><p class="_abstract _abstract">
				You can fence a node manually with the following command. If you specify <code class="literal option">--off</code> this will use the <code class="literal">off</code> API call to stonith which will turn the node off instead of rebooting it.
			</p><pre class="literallayout">pcs stonith fence <span class="emphasis"><em>node</em></span> [--off]</pre><p>
				In a situation where no fence device is able to fence a node even if it is no longer active, the cluster may not be able to recover the resources on the node. If this occurs, after manually ensuring that the node is powered down you can enter the following command to confirm to the cluster that the node is powered down and free its resources for recovery.
			</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
					If the node you specify is not actually off, but running the cluster software or services normally controlled by the cluster, data corruption/cluster failure will occur.
				</p></div></rh-alert><pre class="literallayout">pcs stonith confirm <span class="emphasis"><em>node</em></span></pre></section><section class="section" id="proc_disabling-a-fence-device-configuring-fencing"><div class="titlepage"><div><div><h3 class="title">10.12. Disabling a fence device</h3></div></div></div><p class="_abstract _abstract">
				To disable a fencing device/resource, run the <code class="literal">pcs stonith disable</code> command.
			</p><p>
				The following command disables the fence device <code class="literal">myapc</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs stonith disable myapc</strong></span></pre></section><section class="section" id="proc_preventing-a-node-from-using-a-fence-device-configuring-fencing"><div class="titlepage"><div><div><h3 class="title">10.13. Preventing a node from using a fencing device</h3></div></div></div><p class="_abstract _abstract">
				To prevent a specific node from using a fencing device, you can configure location constraints for the fencing resource.
			</p><p>
				The following example prevents fence device <code class="literal">node1-ipmi</code> from running on <code class="literal">node1</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint location node1-ipmi avoids node1</strong></span></pre></section><section class="section" id="proc_configuring-acpi-for-fence-devices-configuring-fencing"><div class="titlepage"><div><div><h3 class="title">10.14. Configuring ACPI for use with integrated fence devices</h3></div></div></div><p class="_abstract _abstract">
				If your cluster uses integrated fence devices, you must configure ACPI (Advanced Configuration and Power Interface) to ensure immediate and complete fencing.
			</p><p>
				If a cluster node is configured to be fenced by an integrated fence device, disable ACPI Soft-Off for that node. Disabling ACPI Soft-Off allows an integrated fence device to turn off a node immediately and completely rather than attempting a clean shutdown (for example, <code class="literal command">shutdown -h now</code>). Otherwise, if ACPI Soft-Off is enabled, an integrated fence device can take four or more seconds to turn off a node (see the note that follows). In addition, if ACPI Soft-Off is enabled and a node panics or freezes during shutdown, an integrated fence device may not be able to turn off the node. Under those circumstances, fencing is delayed or unsuccessful. Consequently, when a node is fenced with an integrated fence device and ACPI Soft-Off is enabled, a cluster recovers slowly or requires administrative intervention to recover.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					The amount of time required to fence a node depends on the integrated fence device used. Some integrated fence devices perform the equivalent of pressing and holding the power button; therefore, the fence device turns off the node in four to five seconds. Other integrated fence devices perform the equivalent of pressing the power button momentarily, relying on the operating system to turn off the node; therefore, the fence device turns off the node in a time span much longer than four to five seconds.
				</p></div></rh-alert><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The preferred way to disable ACPI Soft-Off is to change the BIOS setting to "instant-off" or an equivalent setting that turns off the node without delay, as described in "Disabling ACPI Soft-Off with the Bios" below.
					</li></ul></div><p>
				Disabling ACPI Soft-Off with the BIOS may not be possible with some systems. If disabling ACPI Soft-Off with the BIOS is not satisfactory for your cluster, you can disable ACPI Soft-Off with one of the following alternate methods:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Setting <code class="literal">HandlePowerKey=ignore</code> in the <code class="literal">/etc/systemd/logind.conf</code> file and verifying that the node node turns off immediately when fenced, as described in "Disabling ACPI Soft-Off in the logind.conf file", below. This is the first alternate method of disabling ACPI Soft-Off.
					</li><li class="listitem"><p class="simpara">
						Appending <code class="literal">acpi=off</code> to the kernel boot command line, as described in "Disabling ACPI completely in the GRUB 2 file", below. This is the second alternate method of disabling ACPI Soft-Off, if the preferred or the first alternate method is not available.
					</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
							This method completely disables ACPI; some computers do not boot correctly if ACPI is completely disabled. Use this method <span class="emphasis"><em>only</em></span> if the other methods are not effective for your cluster.
						</p></div></rh-alert></li></ul></div><section class="section" id="s2-bios-setting-CA"><div class="titlepage"><div><div><h4 class="title">10.14.1. Disabling ACPI Soft-Off with the BIOS</h4></div></div></div><p>
					You can disable ACPI Soft-Off by configuring the BIOS of each cluster node with the following procedure.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						The procedure for disabling ACPI Soft-Off with the BIOS may differ among server systems. You should verify this procedure with your hardware documentation.
					</p></div></rh-alert><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Reboot the node and start the <code class="literal command">BIOS CMOS Setup Utility</code> program.
						</li><li class="listitem">
							Navigate to the Power menu (or equivalent power management menu).
						</li><li class="listitem"><p class="simpara">
							At the Power menu, set the <code class="literal">Soft-Off by PWR-BTTN</code> function (or equivalent) to <code class="literal">Instant-Off</code> (or the equivalent setting that turns off the node by means of the power button without delay). The <code class="literal">BIOS CMOS Setup Utiliy</code> example below shows a Power menu with <code class="literal">ACPI Function</code> set to <code class="literal">Enabled</code> and <code class="literal">Soft-Off by PWR-BTTN</code> set to <code class="literal">Instant-Off</code>.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								The equivalents to <code class="literal">ACPI Function</code>, <code class="literal">Soft-Off by PWR-BTTN</code>, and <code class="literal">Instant-Off</code> may vary among computers. However, the objective of this procedure is to configure the BIOS so that the computer is turned off by means of the power button without delay.
							</p></div></rh-alert></li><li class="listitem">
							Exit the <code class="literal command">BIOS CMOS Setup Utility</code> program, saving the BIOS configuration.
						</li><li class="listitem">
							Verify that the node turns off immediately when fenced. For information about testing a fence device, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters#proc_testing-fence-devices-configuring-fencing">Testing a fence device</a>.
						</li></ol></div><div id="ex-bios-acpi-off-CA" class="formalpara"><p class="title"><strong><code class="literal command">BIOS CMOS Setup Utility</code>:</strong></p><p>

</p><pre class="literallayout">`Soft-Off by PWR-BTTN` set to
`Instant-Off`</pre>

					<p></p></div><div class="informalexample"><pre class="literallayout">+---------------------------------------------|-------------------+
|    ACPI Function             [Enabled]      |    Item Help      |
|    ACPI Suspend Type         [S1(POS)]      |-------------------|
|  x Run VGABIOS if S3 Resume   Auto          |   Menu Level   *  |
|    Suspend Mode              [Disabled]     |                   |
|    HDD Power Down            [Disabled]     |                   |
|    Soft-Off by PWR-BTTN      [Instant-Off   |                   |
|    CPU THRM-Throttling       [50.0%]        |                   |
|    Wake-Up by PCI card       [Enabled]      |                   |
|    Power On by Ring          [Enabled]      |                   |
|    Wake Up On LAN            [Enabled]      |                   |
|  x USB KB Wake-Up From S3     Disabled      |                   |
|    Resume by Alarm           [Disabled]     |                   |
|  x  Date(of Month) Alarm       0            |                   |
|  x  Time(hh:mm:ss) Alarm       0 :  0 :     |                   |
|    POWER ON Function         [BUTTON ONLY   |                   |
|  x KB Power ON Password       Enter         |                   |
|  x Hot Key Power ON           Ctrl-F1       |                   |
|                                             |                   |
|                                             |                   |
+---------------------------------------------|-------------------+</pre><p>
					This example shows <code class="literal">ACPI Function</code> set to <code class="literal">Enabled</code>, and <code class="literal">Soft-Off by PWR-BTTN</code> set to <code class="literal">Instant-Off</code>.
				</p></div></section><section class="section" id="s2-acpi-disable-logind-CA"><div class="titlepage"><div><div><h4 class="title">10.14.2. Disabling ACPI Soft-Off in the logind.conf file</h4></div></div></div><p>
					To disable power-key handing in the <code class="literal">/etc/systemd/logind.conf</code> file, use the following procedure.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Define the following configuration in the <code class="literal">/etc/systemd/logind.conf</code> file:
						</p><pre class="literallayout">HandlePowerKey=ignore</pre></li><li class="listitem"><p class="simpara">
							Restart the <code class="literal">systemd-logind</code> service:
						</p><pre class="literallayout"># <span class="strong strong"><strong>systemctl restart systemd-logind.service</strong></span></pre></li><li class="listitem">
							Verify that the node turns off immediately when fenced. For information about testing a fence device, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters#proc_testing-fence-devices-configuring-fencing">Testing a fence device</a>.
						</li></ol></div></section><section class="section" id="s2-acpi-disable-boot-CA"><div class="titlepage"><div><div><h4 class="title">10.14.3. Disabling ACPI completely in the GRUB 2 file</h4></div></div></div><p>
					You can disable ACPI Soft-Off by appending <code class="literal">acpi=off</code> to the GRUB menu entry for a kernel.
				</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
						This method completely disables ACPI; some computers do not boot correctly if ACPI is completely disabled. Use this method <span class="emphasis"><em>only</em></span> if the other methods are not effective for your cluster.
					</p></div></rh-alert><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						Use the following procedure to disable ACPI in the GRUB 2 file:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Use the <code class="literal option">--args</code> option in combination with the <code class="literal option">--update-kernel</code> option of the <code class="literal command">grubby</code> tool to change the <code class="literal">grub.cfg</code> file of each cluster node as follows:
						</p><pre class="literallayout"># <span class="strong strong"><strong>grubby --args=acpi=off --update-kernel=ALL</strong></span></pre></li><li class="listitem">
							Reboot the node.
						</li><li class="listitem">
							Verify that the node turns off immediately when fenced. For information about testing a fence device, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters#proc_testing-fence-devices-configuring-fencing">Testing a fence device</a>.
						</li></ol></div></section></section></section><section class="chapter" id="assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 11. Configuring cluster resources</h2></div></div></div><p class="_abstract _abstract">
			Create and delete cluster resources with the following commands.
		</p><p>
			The format for the command to create a cluster resource is as follows:
		</p><pre class="literallayout">pcs resource create <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>standard</em></span>:[<span class="emphasis"><em>provider</em></span>:]]<span class="emphasis"><em>type</em></span> [<span class="emphasis"><em>resource_options</em></span>] [op <span class="emphasis"><em>operation_action operation_options</em></span> [<span class="emphasis"><em>operation_action</em></span> <span class="emphasis"><em>operation options</em></span>]...] [meta <span class="emphasis"><em>meta_options</em></span>...] [clone [<span class="emphasis"><em>clone_id</em></span>] [<span class="emphasis"><em>clone_options</em></span>] | promotable [<span class="emphasis"><em>clone_id</em></span>] [<span class="emphasis"><em>clone_options</em></span>] [--wait[=<span class="emphasis"><em>n</em></span>]]</pre><p>
			Key cluster resource creation options include the following:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					The <code class="literal option">--before</code> and <code class="literal option">--after</code> options specify the position of the added resource relative to a resource that already exists in a resource group.
				</li><li class="listitem">
					Specifying the <code class="literal option">--disabled</code> option indicates that the resource is not started automatically.
				</li></ul></div><p>
			There is no limit to the number of resources you can create in a cluster.
		</p><p>
			You can determine the behavior of a resource in a cluster by configuring constraints for that resource.
		</p><h4 id="resource_creation_examples">Resource creation examples</h4><p>
			The following command creates a resource with the name <code class="literal">VirtualIP</code> of standard <code class="literal">ocf</code>, provider <code class="literal">heartbeat</code>, and type <code class="literal">IPaddr2</code>. The floating address of this resource is 192.168.0.120, and the system will check whether the resource is running every 30 seconds.
		</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.120 cidr_netmask=24 op monitor interval=30s</strong></span></pre><p>
			Alternately, you can omit the <span class="emphasis"><em>standard</em></span> and <span class="emphasis"><em>provider</em></span> fields and use the following command. This will default to a standard of <code class="literal">ocf</code> and a provider of <code class="literal">heartbeat</code>.
		</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create VirtualIP IPaddr2 ip=192.168.0.120 cidr_netmask=24 op monitor interval=30s</strong></span></pre><h4 id="deleting_a_configured_resource">Deleting a configured resource</h4><p>
			Delete a configured resource with the following command.
		</p><pre class="literallayout">pcs resource delete <span class="emphasis"><em>resource_id</em></span></pre><p>
			For example, the following command deletes an existing resource with a resource ID of <code class="literal">VirtualIP</code>.
		</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource delete VirtualIP</strong></span></pre><section class="section" id="ref_resource-properties.adoc-configuring-cluster-resources"><div class="titlepage"><div><div><h3 class="title">11.1. Resource agent identifiers</h3></div></div></div><p class="_abstract _abstract">
				The identifiers that you define for a resource tell the cluster which agent to use for the resource, where to find that agent and what standards it conforms to.
			</p><p>
				The following table describes these properties of a resource agent.
			</p><rh-table id="tb-resource-props-summary-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 11.1. Resource Agent Identifiers</caption><colgroup><col style="width: 33%; " class="col_1"><!--Empty--><col style="width: 67%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686141112880" scope="col">Field</th><th align="left" valign="top" id="idm140686141111792" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686141112880"> <p>
								standard
							</p>
							 </td><td align="left" valign="top" headers="idm140686141111792"> <p>
								The standard the agent conforms to. Allowed values and their meaning:
							</p>
							 <p>
								* <code class="literal">ocf</code> - The specified <span class="emphasis"><em>type</em></span> is the name of an executable file conforming to the Open Cluster Framework Resource Agent API and located beneath <code class="literal">/usr/lib/ocf/resource.d/<span class="emphasis"><em>provider</em></span></code>
							</p>
							 <p>
								* <code class="literal">lsb</code> - The specified <span class="emphasis"><em>type</em></span> is the name of an executable file conforming to Linux Standard Base Init Script Actions. If the type does not specify a full path, the system will look for it in the <code class="literal">/etc/init.d</code> directory.
							</p>
							 <p>
								* <code class="literal">systemd</code> - The specified <span class="emphasis"><em>type</em></span> is the name of an installed <code class="literal">systemd</code> unit
							</p>
							 <p>
								* <code class="literal">service</code> - Pacemaker will search for the specified <span class="emphasis"><em>type</em></span>, first as an <code class="literal">lsb</code> agent, then as a <code class="literal">systemd</code> agent
							</p>
							 <p>
								* <code class="literal">nagios</code> - The specified <span class="emphasis"><em>type</em></span> is the name of an executable file conforming to the Nagios Plugin API and located in the <code class="literal">/usr/libexec/nagios/plugins</code> directory, with OCF-style metadata stored separately in the <code class="literal">/usr/share/nagios/plugins-metadata</code> directory (available in the <code class="literal">nagios-agents-metadata</code> package for certain common plugins).
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686141112880"> <p>
								type
							</p>
							 </td><td align="left" valign="top" headers="idm140686141111792"> <p>
								The name of the resource agent you wish to use, for example <code class="literal">IPaddr</code> or <code class="literal">Filesystem</code>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686141112880"> <p>
								provider
							</p>
							 </td><td align="left" valign="top" headers="idm140686141111792"> <p>
								The OCF spec allows multiple vendors to supply the same resource agent. Most of the agents shipped by Red Hat use <code class="literal">heartbeat</code> as the provider.
							</p>
							 </td></tr></tbody></table></rh-table><p>
				The following table summarizes the commands that display the available resource properties.
			</p><rh-table id="tb-resource-displayopts-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 11.2. Commands to Display Resource Properties</caption><colgroup><col style="width: 50%; " class="col_1"><!--Empty--><col style="width: 50%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686136901488" scope="col">pcs Display Command</th><th align="left" valign="top" id="idm140686136900400" scope="col">Output</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686136901488"> <p>
								<code class="literal command">pcs resource list</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686136900400"> <p>
								Displays a list of all available resources.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686136901488"> <p>
								<code class="literal command">pcs resource standards</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686136900400"> <p>
								Displays a list of available resource agent standards.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686136901488"> <p>
								<code class="literal command">pcs resource providers</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686136900400"> <p>
								Displays a list of available resource agent providers.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686136901488"> <p>
								<code class="literal command">pcs resource list <span class="emphasis"><em>string</em></span></code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686136900400"> <p>
								Displays a list of available resources filtered by the specified string. You can use this command to display resources filtered by the name of a standard, a provider, or a type.
							</p>
							 </td></tr></tbody></table></rh-table></section><section class="section" id="proc_displaying-resource-specific-parameters-configuring-cluster-resources"><div class="titlepage"><div><div><h3 class="title">11.2. Displaying resource-specific parameters</h3></div></div></div><p class="_abstract _abstract">
				For any individual resource, you can use the following command to display a description of the resource, the parameters you can set for that resource, and the default values that are set for the resource.
			</p><pre class="literallayout">pcs resource describe [<span class="emphasis"><em>standard</em></span>:[<span class="emphasis"><em>provider</em></span>:]]<span class="emphasis"><em>type</em></span></pre><p>
				For example, the following command displays information for a resource of type <code class="literal">apache</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource describe ocf:heartbeat:apache</strong></span>
This is the resource agent for the Apache Web server.
This resource agent operates both version 1.x and version 2.x Apache
servers.

...</pre></section><section class="section" id="proc_configuring-resource-meta-options-configuring-cluster-resources"><div class="titlepage"><div><div><h3 class="title">11.3. Configuring resource meta options</h3></div></div></div><p class="_abstract _abstract">
				In addition to the resource-specific parameters, you can configure additional resource options for any resource. These options are used by the cluster to decide how your resource should behave.
			</p><p>
				The following table describes the resource meta options.
			</p><rh-table id="tb-resource-options-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 11.3. Resource Meta Options</caption><colgroup><col style="width: 29%; " class="col_1"><!--Empty--><col style="width: 29%; " class="col_2"><!--Empty--><col style="width: 43%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686141499600" scope="col">Field</th><th align="left" valign="top" id="idm140686141498512" scope="col">Default</th><th align="left" valign="top" id="idm140686141497424" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686141499600"> <p>
								<code class="literal">priority</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141498512"> <p>
								<code class="literal">0</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141497424"> <p>
								If not all resources can be active, the cluster will stop lower priority resources in order to keep higher priority ones active.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686141499600"> <p>
								<code class="literal">target-role</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141498512"> <p>
								<code class="literal">Started</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141497424"> <p>
								Indicates what state the cluster should attempt to keep this resource in. Allowed values:
							</p>
							 <p>
								* <code class="literal">Stopped</code> - Force the resource to be stopped
							</p>
							 <p>
								* <code class="literal">Started</code> - Allow the resource to be started (and in the case of promotable clones, promoted if appropriate)
							</p>
							 <p>
								* <code class="literal">Promoted</code> - Allow the resource to be started and, if appropriate, promoted
							</p>
							 <p>
								* <code class="literal">Unpromoted</code> - Allow the resource to be started, but only in unpromoted mode if the resource is promotable
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686141499600"> <p>
								<code class="literal">is-managed</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141498512"> <p>
								<code class="literal">true</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141497424"> <p>
								Indicates whether the cluster is allowed to start and stop the resource. Allowed values: <code class="literal">true</code>, <code class="literal">false</code>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686141499600"> <p>
								<code class="literal">resource-stickiness</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141498512"> <p>
								1
							</p>
							 </td><td align="left" valign="top" headers="idm140686141497424"> <p>
								Value to indicate how much the resource prefers to stay where it is.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686141499600"> <p>
								<code class="literal">requires</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141498512"> <p>
								Calculated
							</p>
							 </td><td align="left" valign="top" headers="idm140686141497424"> <p>
								Indicates under what conditions the resource can be started.
							</p>
							 <p>
								Defaults to <code class="literal">fencing</code> except under the conditions noted below. Possible values:
							</p>
							 <p>
								* <code class="literal">nothing</code> - The cluster can always start the resource.
							</p>
							 <p>
								* <code class="literal">quorum</code> - The cluster can only start this resource if a majority of the configured nodes are active. This is the default value if <code class="literal">stonith-enabled</code> is <code class="literal">false</code> or the resource’s <code class="literal">standard</code> is <code class="literal">stonith</code>.
							</p>
							 <p>
								* <code class="literal">fencing</code> - The cluster can only start this resource if a majority of the configured nodes are active <span class="emphasis"><em>and</em></span> any failed or unknown nodes have been fenced.
							</p>
							 <p>
								* <code class="literal">unfencing</code> - The cluster can only start this resource if a majority of the configured nodes are active <span class="emphasis"><em>and</em></span> any failed or unknown nodes have been fenced <span class="emphasis"><em>and</em></span> only on nodes that have been <span class="emphasis"><em>unfenced</em></span>. This is the default value if the <code class="literal">provides=unfencing</code> <code class="literal">stonith</code> meta option has been set for a fencing device.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686141499600"> <p>
								<code class="literal">migration-threshold</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141498512"> <p>
								<code class="literal">INFINITY</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141497424"> <p>
								How many failures may occur for this resource on a node before this node is marked ineligible to host this resource. A value of 0 indicates that this feature is disabled (the node will never be marked ineligible); by contrast, the cluster treats <code class="literal">INFINITY</code> (the default) as a very large but finite number. This option has an effect only if the failed operation has <code class="literal">on-fail=restart</code> (the default), and additionally for failed start operations if the cluster property <code class="literal">start-failure-is-fatal</code> is <code class="literal">false</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686141499600"> <p>
								<code class="literal">failure-timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141498512"> <p>
								<code class="literal">0</code> (disabled)
							</p>
							 </td><td align="left" valign="top" headers="idm140686141497424"> <p>
								Ignore previously failed resource actions after this much time has passed without new failures. This potentially allows the resource to move back to the node on which it failed, if it previously reached its migration threshold there. A value of 0 indicates that failures do not expire.
							</p>
							 <p>
								<span class="emphasis"><em>WARNING</em></span>: If this value is low, and pending cluster activity prevents the cluster from responding to a failure within that time, the failure is ignored completely and does not cause recovery of the resource, even if a recurring action continues to report failure. The value of this option should be at least greater than the longest action timeout for all resources in the cluster. A value in hours or days is reasonable.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686141499600"> <p>
								<code class="literal">multiple-active</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141498512"> <p>
								<code class="literal">stop_start</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141497424"> <p>
								Indicates what the cluster should do if it ever finds the resource active on more than one node. Allowed values:
							</p>
							 <p>
								* <code class="literal">block</code> - mark the resource as unmanaged
							</p>
							 <p>
								* <code class="literal">stop_only</code> - stop all active instances and leave them that way
							</p>
							 <p>
								* <code class="literal">stop_start</code> - stop all active instances and start the resource in one location only
							</p>
							 <p>
								* <code class="literal">stop_unexpected</code> - (RHEL 9.1 and later) stop only unexpected instances of the resource, without requiring a full restart. It is the user’s responsibility to verify that the service and its resource agent can function with extra active instances without requiring a full restart.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686141499600"> <p>
								<code class="literal">critical</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141498512"> <p>
								<code class="literal">true</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141497424"> <p>
								Sets the default value for the <code class="literal">influence</code> option for all colocation constraints involving the resource as a dependent resource (<span class="emphasis"><em>target_resource</em></span>), including implicit colocation constraints created when the resource is part of a resource group. The <code class="literal">influence</code> colocation constraint option determines whether the cluster will move both the primary and dependent resources to another node when the dependent resource reaches its migration threshold for failure, or whether the cluster will leave the dependent resource offline without causing a service switch. The <code class="literal">critical</code> resource meta option can have a value of <code class="literal">true</code> or <code class="literal">false</code>, with a default value of <code class="literal">true</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686141499600"> <p>
								<code class="literal">allow-unhealthy-nodes</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141498512"> <p>
								<code class="literal">false</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686141497424"> <p>
								(RHEL 9.1 and later) When set to <code class="literal">true</code>, the resource is not forced off a node due to degraded node health. When health resources have this attribute set, the cluster can automatically detect if the node’s health recovers and move resources back to it. A node’s health is determined by a combination of the health attributes set by health resource agents based on local conditions, and the strategy-related options that determine how the cluster reacts to those conditions.
							</p>
							 </td></tr></tbody></table></rh-table><section class="section" id="changing_the_default_value_of_a_resource_option"><div class="titlepage"><div><div><h4 class="title">11.3.1. Changing the default value of a resource option</h4></div></div></div><p>
					You can change the default value of a resource option for all resources with the <code class="literal">pcs resource defaults update</code> command. The following command resets the default value of <code class="literal">resource-stickiness</code> to 100.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource defaults update resource-stickiness=100</strong></span></pre><p>
					The original <code class="literal">pcs resource defaults <span class="emphasis"><em>name</em></span>=<span class="emphasis"><em>value</em></span></code> command, which set defaults for all resources in previous releases, remains supported unless there is more than one set of defaults configured. However, <code class="literal">pcs resource defaults update</code> is now the preferred version of the command.
				</p></section><section class="section" id="changing_the_default_value_of_a_resource_option_for_sets_of_resources"><div class="titlepage"><div><div><h4 class="title">11.3.2. Changing the default value of a resource option for sets of resources</h4></div></div></div><p>
					You can create multiple sets of resource defaults with the <code class="literal">pcs resource defaults set create</code> command, which allows you to specify a rule that contains <code class="literal">resource</code> expressions. Only <code class="literal">resource</code> and <code class="literal">date</code> expressions, including <code class="literal">and</code>, <code class="literal">or</code> and parentheses, are allowed in rules that you specify with this command.
				</p><p>
					With the <code class="literal">pcs resource defaults set create</code> command, you can configure a default resource value for all resources of a particular type. If, for example, you are running databases which take a long time to stop, you can increase the <code class="literal">resource-stickiness</code> default value for all resources of the database type to prevent those resources from moving to other nodes more often than you desire.
				</p><p>
					The following command sets the default value of <code class="literal">resource-stickiness</code> to 100 for all resources of type <code class="literal">pqsql</code>.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The <code class="literal">id</code> option, which names the set of resource defaults, is not mandatory. If you do not set this option <code class="literal">pcs</code> will generate an ID automatically. Setting this value allows you to provide a more descriptive name.
						</li><li class="listitem"><p class="simpara">
							In this example, <code class="literal">::pgsql</code> means a resource of any class, any provider, of type <code class="literal">pgsql</code>.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Specifying <code class="literal">ocf:heartbeat:pgsql</code> would indicate class <code class="literal">ocf</code>, provider <code class="literal">heartbeat</code>, type <code class="literal">pgsql</code>,
								</li><li class="listitem">
									Specifying <code class="literal">ocf:pacemaker:</code> would indicate all resources of class <code class="literal">ocf</code>, provider <code class="literal">pacemaker</code>, of any type.
								</li></ul></div></li></ul></div><pre class="literallayout"># <span class="strong strong"><strong>pcs resource defaults set create id=pgsql-stickiness meta resource-stickiness=100 rule resource ::pgsql</strong></span></pre><p>
					To change the default values in an existing set, use the <code class="literal">pcs resource defaults set update</code> command.
				</p></section><section class="section" id="displaying_currently_configured_resource_defaults"><div class="titlepage"><div><div><h4 class="title">11.3.3. Displaying currently configured resource defaults</h4></div></div></div><p>
					The <code class="literal">pcs resource defaults [config]</code> command displays a list of currently configured default values for resource options, including any rules that you specified. As of Red Hat Enterprise Linux 9.5, you can display the output of this command in text, JSON, and command formats.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Specifying <code class="literal">--output-format=text</code> displays the configured resource defaults in plain text format, which is the default value for this option.
						</li><li class="listitem">
							Specifying <code class="literal">--output-format=cmd</code> displays the <code class="literal">pcs resource defaults</code> commands created from the current cluster defaults configuration. You can use these commands to re-create configured resource defaults on a different system.
						</li><li class="listitem">
							Specifying <code class="literal">--output-format=json</code> displays the configured resource defaults in JSON format, which is suitable for machine parsing.
						</li></ul></div><p>
					The following examples show the three different output formats of the <code class="literal">pcs resource defaults config</code> command after the default values for any <code class="literal">ocf:pacemaker:pgsql</code> resource were reset with the following example command:
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource defaults set create id=set-1 score=100 meta resource-stickiness=10 rule resource ocf:pacemaker:pgsql</strong></span></pre><p>
					This example displays the configured resource default values in plain text.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource defaults config</strong></span>
Meta Attrs: build-resource-defaults
  resource-stickiness=1
Meta Attrs: set-1 score=100
  resource-stickiness=10
  Rule: boolean-op=and score=INFINITY
    Expression: resource ocf:pacemaker:pgsql</pre><p>
					This example displays the <code class="literal">pcs resource defaults</code> commands created from the current cluster defaults configuration.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource defaults config --output-format=cmd</strong></span>
pcs -- resource defaults set create id=build-resource-defaults \
  meta resource-stickiness=1;
pcs -- resource defaults set create id=set-1 score=100 \
  meta resource-stickiness=10 \
  rule 'resource ocf:pacemaker:pgsql'</pre><p>
					This example displays the configured resource default values in JSON format.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource defaults config --output-format=json</strong></span>
{"instance_attributes": [], "meta_attributes": [{"id": "build-resource-defaults", "options": {}, "rule": null, "nvpairs": [{"id": "build-resource-stickiness", "name": "resource-stickiness", "value": "1"}]}, {"id": "set-1", "options": {"score": "100"}, "rule": {"id": "set-1-rule", "type": "RULE", "in_effect": "UNKNOWN", "options": {"boolean-op": "and", "score": "INFINITY"}, "date_spec": null, "duration": null, "expressions": [{"id": "set-1-rule-rsc-ocf-pacemaker-pgsql", "type": "RSC_EXPRESSION", "in_effect": "UNKNOWN", "options": {"class": "ocf", "provider": "pacemaker", "type": "pgsql"}, "date_spec": null, "duration": null, "expressions": [], "as_string": "resource ocf:pacemaker:pgsql"}], "as_string": "resource ocf:pacemaker:pgsql"}, "nvpairs": [{"id": "set-1-resource-stickiness", "name": "resource-stickiness", "value": "10"}]}]}</pre></section><section class="section" id="setting_meta_options_on_resource_creation"><div class="titlepage"><div><div><h4 class="title">11.3.4. Setting meta options on resource creation</h4></div></div></div><p>
					Whether you have reset the default value of a resource meta option or not, you can set a resource option for a particular resource to a value other than the default when you create the resource. The following shows the format of the <code class="literal">pcs resource create</code> command you use when specifying a value for a resource meta option.
				</p><pre class="literallayout">pcs resource create <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>standard</em></span>:[<span class="emphasis"><em>provider</em></span>:]]<span class="emphasis"><em>type</em></span> [<span class="emphasis"><em>resource options</em></span>] [meta <span class="emphasis"><em>meta_options</em></span>...]</pre><p>
					For example, the following command creates a resource with a <code class="literal">resource-stickiness</code> value of 50.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.120 meta resource-stickiness=50</strong></span></pre><p>
					You can also set the value of a resource meta option for an existing resource, group, or cloned resource with the following command.
				</p><pre class="literallayout">pcs resource meta <span class="emphasis"><em>resource_id</em></span> | <span class="emphasis"><em>group_id</em></span> | <span class="emphasis"><em>clone_id</em></span> <span class="emphasis"><em>meta_options</em></span></pre><p>
					In the following example, there is an existing resource named <code class="literal">dummy_resource</code>. This command sets the <code class="literal">failure-timeout</code> meta option to 20 seconds, so that the resource can attempt to restart on the same node in 20 seconds.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource meta dummy_resource failure-timeout=20s</strong></span></pre><p>
					After executing this command, you can display the values for the resource to verify that <code class="literal">failure-timeout=20s</code> is set.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource config dummy_resource</strong></span>
 Resource: dummy_resource (class=ocf provider=heartbeat type=Dummy)
  Meta Attrs: failure-timeout=20s
  ...</pre></section></section><section class="section" id="proc_creating-resource-groups-configuring-cluster-resources"><div class="titlepage"><div><div><h3 class="title">11.4. Configuring resource groups</h3></div></div></div><p class="_abstract _abstract">
				One of the most common elements of a cluster is a set of resources that need to be located together, start sequentially, and stop in the reverse order. To simplify this configuration, Pacemaker supports the concept of resource groups.
			</p><section class="section" id="creating_a_resource_group"><div class="titlepage"><div><div><h4 class="title">11.4.1. Creating a resource group</h4></div></div></div><p>
					You create a resource group with the following command, specifying the resources to include in the group. If the group does not exist, this command creates the group. If the group exists, this command adds additional resources to the group. The resources will start in the order you specify them with this command, and will stop in the reverse order of their starting order.
				</p><pre class="literallayout">pcs resource group add <span class="emphasis"><em>group_name</em></span> <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>resource_id</em></span>] ... [<span class="emphasis"><em>resource_id</em></span>] [--before <span class="emphasis"><em>resource_id</em></span> | --after <span class="emphasis"><em>resource_id</em></span>]</pre><p>
					You can use the <code class="literal option">--before</code> and <code class="literal option">--after</code> options of this command to specify the position of the added resources relative to a resource that already exists in the group.
				</p><p>
					You can also add a new resource to an existing group when you create the resource, using the following command. The resource you create is added to the group named <span class="emphasis"><em>group_name</em></span>. If the group <span class="emphasis"><em>group_name</em></span> does not exist, it will be created.
				</p><pre class="literallayout">pcs resource create <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>standard</em></span>:[<span class="emphasis"><em>provider</em></span>:]]<span class="emphasis"><em>type</em></span> [resource_options] [op <span class="emphasis"><em>operation_action</em></span> <span class="emphasis"><em>operation_options</em></span>] --group <span class="emphasis"><em>group_name</em></span></pre><p>
					There is no limit to the number of resources a group can contain. The fundamental properties of a group are as follows.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Resources are colocated within a group.
						</li><li class="listitem">
							Resources are started in the order in which you specify them. If a resource in the group cannot run anywhere, then no resource specified after that resource is allowed to run.
						</li><li class="listitem">
							Resources are stopped in the reverse order in which you specify them.
						</li></ul></div><p>
					The following example creates a resource group named <code class="literal">shortcut</code> that contains the existing resources <code class="literal">IPaddr</code> and <code class="literal">Email</code>.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource group add shortcut IPaddr Email</strong></span></pre><p>
					In this example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The <code class="literal">IPaddr</code> is started first, then <code class="literal">Email</code>.
						</li><li class="listitem">
							The <code class="literal">Email</code> resource is stopped first, then <code class="literal">IPAddr</code>.
						</li><li class="listitem">
							If <code class="literal">IPaddr</code> cannot run anywhere, neither can <code class="literal">Email</code>.
						</li><li class="listitem">
							If <code class="literal">Email</code> cannot run anywhere, however, this does not affect <code class="literal">IPaddr</code> in any way.
						</li></ul></div></section><section class="section" id="removing_a_resource_group"><div class="titlepage"><div><div><h4 class="title">11.4.2. Removing a resource group</h4></div></div></div><p>
					You remove a resource from a group with the following command. If there are no remaining resources in the group, this command removes the group itself.
				</p><pre class="literallayout">pcs resource group remove <span class="emphasis"><em>group_name</em></span> <span class="emphasis"><em>resource_id</em></span>...</pre></section><section class="section" id="displaying_resource_groups"><div class="titlepage"><div><div><h4 class="title">11.4.3. Displaying resource groups</h4></div></div></div><p>
					The following command lists all currently configured resource groups.
				</p><pre class="literallayout">pcs resource group list</pre></section><section class="section" id="s2-group_options-HAAR"><div class="titlepage"><div><div><h4 class="title">11.4.4. Group options</h4></div></div></div><p>
					You can set the following options for a resource group, and they maintain the same meaning as when they are set for a single resource: <code class="literal">priority</code>, <code class="literal">target-role</code>, <code class="literal">is-managed</code>. For information about resource meta options, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters#proc_configuring-resource-meta-options-configuring-cluster-resources">Configuring resource meta options</a>.
				</p></section><section class="section" id="s2-group_stickiness-HAAR"><div class="titlepage"><div><div><h4 class="title">11.4.5. Group stickiness</h4></div></div></div><p>
					Stickiness, the measure of how much a resource wants to stay where it is, is additive in groups. Every active resource of the group will contribute its stickiness value to the group’s total. So if the default <code class="literal">resource-stickiness</code> is 100, and a group has seven members, five of which are active, then the group as a whole will prefer its current location with a score of 500.
				</p></section></section><section class="section" id="con_determining-resource-behavior-configuring-cluster-resources"><div class="titlepage"><div><div><h3 class="title">11.5. Determining resource behavior</h3></div></div></div><p class="_abstract _abstract">
				You can determine the behavior of a resource in a cluster by configuring constraints for that resource. You can configure the following categories of constraints:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">location</code> constraints — A location constraint determines which nodes a resource can run on. For information about configuring location constraints, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters">Determining which nodes a resource can run on</a>.
					</li><li class="listitem">
						<code class="literal">order</code> constraints — An ordering constraint determines the order in which the resources run. For information about configuring ordering constraints, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters">Determining the order in which cluster resources are run</a>.
					</li><li class="listitem">
						<code class="literal">colocation</code> constraints — A colocation constraint determines where resources will be placed relative to other resources. For information about colocation constraints, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters">Colocating cluster resources</a>.
					</li></ul></div><p>
				As a shorthand for configuring a set of constraints that will locate a set of resources together and ensure that the resources start sequentially and stop in reverse order, Pacemaker supports the concept of resource groups. After you have created a resource group, you can configure constraints on the group itself just as you configure constraints for individual resources.
			</p></section></section><section class="chapter" id="assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 12. Determining which nodes a resource can run on</h2></div></div></div><p class="_abstract _abstract">
			Location constraints determine which nodes a resource can run on. You can configure location constraints to determine whether a resource will prefer or avoid a specified node.
		</p><p>
			In addition to location constraints, the node on which a resource runs is influenced by the <code class="literal">resource-stickiness</code> value for that resource, which determines to what degree a resource prefers to remain on the node where it is currently running. For information about setting the <code class="literal">resource-stickiness</code> value, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/configuring_and_managing_high_availability_clusters/index#proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on">Configuring a resource to prefer its current node</a>.
		</p><section class="section" id="proc_configuring-location-constraints-determining-which-node-a-resource-runs-on"><div class="titlepage"><div><div><h3 class="title">12.1. Configuring location constraints</h3></div></div></div><p class="_abstract _abstract">
				You can configure a basic location constraint to specify whether a resource prefers or avoids a node, with an optional <code class="literal">score</code> value to indicate the relative degree of preference for the constraint.
			</p><p>
				The following command creates a location constraint for a resource to prefer the specified node or nodes. Note that it is possible to create constraints on a particular resource for more than one node with a single command.
			</p><pre class="literallayout">pcs constraint location <span class="emphasis"><em>rsc</em></span> prefers <span class="emphasis"><em>node</em></span>[=<span class="emphasis"><em>score</em></span>] [<span class="emphasis"><em>node</em></span>[=<span class="emphasis"><em>score</em></span>]] ...</pre><p>
				The following command creates a location constraint for a resource to avoid the specified node or nodes.
			</p><pre class="literallayout">pcs constraint location <span class="emphasis"><em>rsc</em></span> avoids <span class="emphasis"><em>node</em></span>[=<span class="emphasis"><em>score</em></span>] [<span class="emphasis"><em>node</em></span>[=<span class="emphasis"><em>score</em></span>]] ...</pre><p>
				The following table summarizes the meanings of the basic options for configuring location constraints.
			</p><rh-table id="tb-locationconstraint-options-HAAR-determining-which-node-a-resource-runs-on"><table class="lt-4-cols lt-7-rows"><caption>Table 12.1. Location Constraint Options</caption><colgroup><col style="width: 33%; " class="col_1"><!--Empty--><col style="width: 67%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686136405808" scope="col">Field</th><th align="left" valign="top" id="idm140686136404720" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686136405808"> <p>
								<code class="literal">rsc</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686136404720"> <p>
								A resource name
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686136405808"> <p>
								<code class="literal">node</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686136404720"> <p>
								A node’s name
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686136405808"> <p>
								<code class="literal">score</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686136404720"> <p>
								Positive integer value to indicate the degree of preference for whether the given resource should prefer or avoid the given node. <code class="literal">INFINITY</code> is the default <code class="literal">score</code> value for a resource location constraint.
							</p>
							 <p>
								A value of <code class="literal">INFINITY</code> for <code class="literal">score</code> in a <code class="literal">pcs constraint location <span class="emphasis"><em>rsc</em></span> prefers</code> command indicates that the resource will prefer that node if the node is available, but does not prevent the resource from running on another node if the specified node is unavailable.
							</p>
							 <p>
								A value of <code class="literal">INFINITY</code> for <code class="literal">score</code> in a <code class="literal">pcs constraint location <span class="emphasis"><em>rsc</em></span> avoids</code> command indicates that the resource will never run on that node, even if no other node is available. This is the equivalent of setting a <code class="literal">pcs constraint location add</code> command with a score of <code class="literal">-INFINITY</code>.
							</p>
							 <p>
								A numeric score (that is, not <code class="literal">INFINITY</code>) means the constraint is optional, and will be honored unless some other factor outweighs it. For example, if the resource is already placed on a different node, and its <code class="literal">resource-stickiness</code> score is higher than a <code class="literal">prefers</code> location constraint’s score, then the resource will be left where it is.
							</p>
							 </td></tr></tbody></table></rh-table><p>
				The following command creates a location constraint to specify that the resource <code class="literal">Webserver</code> prefers node <code class="literal">node1</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint location Webserver prefers node1</strong></span></pre><p>
				<code class="literal command">pcs</code> supports regular expressions in location constraints on the command line. These constraints apply to multiple resources based on the regular expression matching resource name. This allows you to configure multiple location constraints with a single command line.
			</p><p>
				The following command creates a location constraint to specify that resources <code class="literal">dummy0</code> to <code class="literal">dummy9</code> prefer <code class="literal">node1</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint location 'regexp%dummy[0-9]' prefers node1</strong></span></pre><p>
				Since Pacemaker uses POSIX extended regular expressions as documented at <a class="link" href="http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_04">http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_04</a>, you can specify the same constraint with the following command.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint location 'regexp%dummy[[:digit:]]' prefers node1</strong></span></pre></section><section class="section" id="proc_limiting-resource-discovery-to-a-subset-of-nodes-determining-which-node-a-resource-runs-on"><div class="titlepage"><div><div><h3 class="title">12.2. Limiting resource discovery to a subset of nodes</h3></div></div></div><p class="_abstract _abstract">
				Before Pacemaker starts a resource anywhere, it first runs a one-time monitor operation (often referred to as a "probe") on every node, to learn whether the resource is already running. This process of resource discovery can result in errors on nodes that are unable to execute the monitor.
			</p><p>
				When configuring a location constraint on a node, you can use the <code class="literal option">resource-discovery</code> option of the <code class="literal command">pcs constraint location</code> command to indicate a preference for whether Pacemaker should perform resource discovery on this node for the specified resource. Limiting resource discovery to a subset of nodes the resource is physically capable of running on can significantly boost performance when a large set of nodes is present. When <code class="literal">pacemaker_remote</code> is in use to expand the node count into the hundreds of nodes range, this option should be considered.
			</p><p>
				The following command shows the format for specifying the <code class="literal option">resource-discovery</code> option of the <code class="literal command">pcs constraint location</code> command. In this command, a positive value for <span class="emphasis"><em>score</em></span> corresponds to a basic location constraint that configures a resource to prefer a node, while a negative value for <span class="emphasis"><em>score</em></span> corresponds to a basic location`constraint that configures a resource to avoid a node. As with basic location constraints, you can use regular expressions for resources with these constraints as well.
			</p><pre class="literallayout">pcs constraint location add <span class="emphasis"><em>id</em></span> <span class="emphasis"><em>rsc</em></span> <span class="emphasis"><em>node</em></span> <span class="emphasis"><em>score</em></span> [resource-discovery=<span class="emphasis"><em>option</em></span>]</pre><p>
				The following table summarizes the meanings of the basic parameters for configuring constraints for resource discovery.
			</p><rh-table id="tb-resourcediscoveryconstraint-options-HAAR-determining-which-node-a-resource-runs-on"><table class="lt-4-cols lt-7-rows"><caption>Table 12.2. Resource Discovery Constraint Parameters</caption><colgroup><col style="width: 40%; " class="col_1"><!--Empty--><col style="width: 60%; " class="col_2"><!--Empty--></colgroup><tbody><tr><td align="left" valign="top"> <p>
								Field
							</p>
							 </td><td align="left" valign="top"> <p>
								Description
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								<code class="literal">id</code>
							</p>
							 </td><td align="left" valign="top"> <p>
								A user-chosen name for the constraint itself.
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								<code class="literal">rsc</code>
							</p>
							 </td><td align="left" valign="top"> <p>
								A resource name
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								<code class="literal">node</code>
							</p>
							 </td><td align="left" valign="top"> <p>
								A node’s name
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								<code class="literal">score</code>
							</p>
							 </td><td align="left" valign="top"> <p>
								Integer value to indicate the degree of preference for whether the given resource should prefer or avoid the given node. A positive value for score corresponds to a basic location constraint that configures a resource to prefer a node, while a negative value for score corresponds to a basic location constraint that configures a resource to avoid a node.
							</p>
							 <p>
								A value of <code class="literal">INFINITY</code> for <code class="literal">score</code> indicates that the resource will prefer that node if the node is available, but does not prevent the resource from running on another node if the specified node is unavailable. A value of <code class="literal">-INFINITY</code> for <code class="literal">score</code> indicates that the resource will never run on that node, even if no other node is available.
							</p>
							 <p>
								A numeric score (that is, not <code class="literal">INFINITY</code> or <code class="literal">-INFINITY</code>) means the constraint is optional, and will be honored unless some other factor outweighs it. For example, if the resource is already placed on a different node, and its <code class="literal">resource-stickiness</code> score is higher than a <code class="literal">prefers</code> location constraint’s score, then the resource will be left where it is.
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								<code class="literal">resource-discovery</code> options
							</p>
							 </td><td align="left" valign="top"> <p>
								* <code class="literal">always</code> - Always perform resource discovery for the specified resource on this node. This is the default <code class="literal">resource-discovery</code> value for a resource location constraint.
							</p>
							 <p>
								* <code class="literal">never</code> - Never perform resource discovery for the specified resource on this node.
							</p>
							 <p>
								* <code class="literal">exclusive</code> - Perform resource discovery for the specified resource only on this node (and other nodes similarly marked as <code class="literal">exclusive</code>). Multiple location constraints using <code class="literal">exclusive</code> discovery for the same resource across different nodes creates a subset of nodes <code class="literal">resource-discovery</code> is exclusive to. If a resource is marked for <code class="literal">exclusive</code> discovery on one or more nodes, that resource is only allowed to be placed within that subset of nodes.
							</p>
							 </td></tr></tbody></table></rh-table><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
					Setting <code class="literal">resource-discovery</code> to <code class="literal">never</code> or <code class="literal">exclusive</code> removes Pacemaker’s ability to detect and stop unwanted instances of a service running where it is not supposed to be. It is up to the system administrator to make sure that the service can never be active on nodes without resource discovery (such as by leaving the relevant software uninstalled).
				</p></div></rh-alert></section><section class="section" id="proc_configuring-location-constraint-strategy-determining-which-node-a-resource-runs-on"><div class="titlepage"><div><div><h3 class="title">12.3. Configuring a location constraint strategy</h3></div></div></div><p class="_abstract _abstract">
				When using location constraints, you can configure a general strategy for specifying which nodes a resource can run on:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Opt-in clusters — Configure a cluster in which, by default, no resource can run anywhere and then selectively enable allowed nodes for specific resources.
					</li><li class="listitem">
						Opt-out clusters — Configure a cluster in which, by default, all resources can run anywhere and then create location constraints for resources that are not allowed to run on specific nodes.
					</li></ul></div><p>
				Whether you should choose to configure your cluster as an opt-in or opt-out cluster depends on both your personal preference and the make-up of your cluster. If most of your resources can run on most of the nodes, then an opt-out arrangement is likely to result in a simpler configuration. On the other hand, if most resources can only run on a small subset of nodes an opt-in configuration might be simpler.
			</p><section class="section" id="s3-optin-clusters-HAAR"><div class="titlepage"><div><div><h4 class="title">12.3.1. Configuring an "Opt-In" cluster</h4></div></div></div><p>
					To create an opt-in cluster, set the <code class="literal">symmetric-cluster</code> cluster property to <code class="literal">false</code> to prevent resources from running anywhere by default.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs property set symmetric-cluster=false</strong></span></pre><p>
					Enable nodes for individual resources. The following commands configure location constraints so that the resource <code class="literal">Webserver</code> prefers node <code class="literal">example-1</code>, the resource <code class="literal">Database</code> prefers node <code class="literal">example-2</code>, and both resources can fail over to node <code class="literal">example-3</code> if their preferred node fails. When configuring location constraints for an opt-in cluster, setting a score of zero allows a resource to run on a node without indicating any preference to prefer or avoid the node.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint location Webserver prefers example-1=200</strong></span>
# <span class="strong strong"><strong>pcs constraint location Webserver prefers example-3=0</strong></span>
# <span class="strong strong"><strong>pcs constraint location Database prefers example-2=200</strong></span>
# <span class="strong strong"><strong>pcs constraint location Database prefers example-3=0</strong></span></pre></section><section class="section" id="s3-optout-clusters-HAAR"><div class="titlepage"><div><div><h4 class="title">12.3.2. Configuring an "Opt-Out" cluster</h4></div></div></div><p>
					To create an opt-out cluster, set the <code class="literal">symmetric-cluster</code> cluster property to <code class="literal">true</code> to allow resources to run everywhere by default. This is the default configuration if <code class="literal">symmetric-cluster</code> is not set explicitly.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs property set symmetric-cluster=true</strong></span></pre><p>
					The following commands will then yield a configuration that is equivalent to the example in "Configuring an "Opt-In" cluster". Both resources can fail over to node <code class="literal">example-3</code> if their preferred node fails, since every node has an implicit score of 0.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint location Webserver prefers example-1=200</strong></span>
# <span class="strong strong"><strong>pcs constraint location Webserver avoids example-2=INFINITY</strong></span>
# <span class="strong strong"><strong>pcs constraint location Database avoids example-1=INFINITY</strong></span>
# <span class="strong strong"><strong>pcs constraint location Database prefers example-2=200</strong></span></pre><p>
					Note that it is not necessary to specify a score of INFINITY in these commands, since that is the default value for the score.
				</p></section></section><section class="section" id="proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on"><div class="titlepage"><div><div><h3 class="title">12.4. Configuring a resource to prefer its current node</h3></div></div></div><p class="_abstract _abstract">
				Resources have a <code class="literal">resource-stickiness</code> value that you can set as a meta attribute when you create the resource, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters#proc_configuring-resource-meta-options-configuring-cluster-resources">Configuring resource meta options</a>. The <code class="literal">resource-stickiness</code> value determines how much a resource wants to remain on the node where it is currently running. Pacemaker considers the <code class="literal">resource-stickiness</code> value in conjunction with other settings (for example, the score values of location constraints) to determine whether to move a resource to another node or to leave it in place.
			</p><p>
				With a <code class="literal">resource-stickiness</code> value of 0, a cluster may move resources as needed to balance resources across nodes. This may result in resources moving when unrelated resources start or stop. With a positive stickiness, resources have a preference to stay where they are, and move only if other circumstances outweigh the stickiness. This may result in newly-added nodes not getting any resources assigned to them without administrator intervention.
			</p><p>
				Newly-created clusters in RHEL 9 set the default value for <code class="literal">resource-stickiness</code> to 1. This small value can easily be overridden by other constraints that you create, but it is enough to prevent Pacemaker from needlessly moving healthy resources around the cluster. If you prefer cluster behavior that results from a <code class="literal">resource-stickiness</code> value of 0, you can change the <code class="literal">resource-stickiness</code> default value to 0 with the following command:
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource defaults update resource-stickiness=0</strong></span></pre><p>
				If you upgrade an existing cluster to RHEL 9 and you have not explicitly set a default value for <code class="literal">resource-stickiness</code>, the <code class="literal">resource-stickiness</code> value remains 0 and the <code class="literal">pcs resource defaults</code> command will not show anything for stickiness.
			</p><p>
				With a positive <code class="literal">resource-stickiness</code> value, no resources will move to a newly-added node. If resource balancing is desired at that point, you can temporarily set the <code class="literal">resource-stickiness</code> value to 0.
			</p><p>
				Note that if a location constraint score is higher than the <code class="literal">resource-stickiness</code> value, the cluster may still move a healthy resource to the node where the location constraint points.
			</p><p>
				For further information about how Pacemaker determines where to place a resource, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters">Configuring a node placement strategy</a>.
			</p></section><section class="section" id="proc_exporting-constraints-determining-which-node-a-resource-runs-on"><div class="titlepage"><div><div><h3 class="title">12.5. Exporting resource constraints as <code class="literal">pcs</code> commands</h3></div></div></div><p>
				As of Red Hat Enterprise Linux 9.3, you can display the <code class="literal">pcs</code> commands that can be used to re-create configured resource constraints on a different system using the <code class="literal">--output-format=cmd</code> option of the <code class="literal">pcs constraint</code> command.
			</p><p>
				The following commands create an <code class="literal">IPaddr2</code> resource and an <code class="literal">apache</code> resource.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create VirtualIP IPaddr2 ip=198.51.100.3 cidr_netmask=24</strong></span>
Assumed agent name 'ocf:heartbeat:IPaddr2' (deduced from 'IPaddr2')
# <span class="strong strong"><strong>pcs resource create Website apache configfile="/etc/httpd/conf/httpd.conf" statusurl="http://127.0.0.1/server-status"</strong></span>
Assumed agent name 'ocf:heartbeat:apache' (deduced from 'apache')</pre><p>
				The following commands configure a location constraint, a colocation constraint, and an order constraint for the two resources.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint location Website avoids node1</strong></span>
# <span class="strong strong"><strong>pcs constraint colocation add Website with VirtualIP</strong></span>
# <span class="strong strong"><strong>pcs constraint order VirtualIP then Website</strong></span>
Adding VirtualIP Website (kind: Mandatory) (Options: first-action=start then-action=start)</pre><p>
				After you create the resources and the constraints, the following command displays the <code class="literal">pcs</code> commands you can use to re-create the constraints on a different system.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint --output-format=cmd</strong></span>
pcs -- constraint location add location-Website-node1--INFINITY resource%Website node1 -INFINITY;
pcs -- constraint colocation add Website with VirtualIP INFINITY \
  id=colocation-Website-VirtualIP-INFINITY;
pcs -- constraint order start VirtualIP then start Website \
  id=order-VirtualIP-Website-mandatory</pre></section></section><section class="chapter" id="assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 13. Determining the order in which cluster resources are run</h2></div></div></div><p class="_abstract _abstract">
			To determine the order in which the resources run, you configure an ordering constraint.
		</p><p>
			The following shows the format for the command to configure an ordering constraint.
		</p><pre class="literallayout">pcs constraint order [<span class="emphasis"><em>action</em></span>] <span class="emphasis"><em>resource_id</em></span> then [<span class="emphasis"><em>action</em></span>] <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>options</em></span>]</pre><p>
			The following table summarizes the properties and options for configuring ordering constraints.
		</p><rh-table id="idm140686137783136"><table class="lt-4-cols lt-7-rows"><caption>Table 13.1. Properties of an Order Constraint</caption><colgroup><col style="width: 40%; " class="col_1"><!--Empty--><col style="width: 60%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686137778288" scope="col">Field</th><th align="left" valign="top" id="idm140686137777200" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686137778288"> <p>
							resource_id
						</p>
						 </td><td align="left" valign="top" headers="idm140686137777200"> <p>
							The name of a resource on which an action is performed.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686137778288"> <p>
							action
						</p>
						 </td><td align="left" valign="top" headers="idm140686137777200"> <p>
							The action to be ordered on the resource. Possible values of the <span class="emphasis"><em>action</em></span> property are as follows:
						</p>
						 <p>
							* <code class="literal">start</code> - Order start actions of the resource.
						</p>
						 <p>
							* <code class="literal">stop</code> - Order stop actions of the resource.
						</p>
						 <p>
							* <code class="literal">promote</code> - Promote the resource from an unpromoted resource to a promoted resource.
						</p>
						 <p>
							* <code class="literal">demote</code> - Demote the resource from a promoted resource to an unpromoted resource.
						</p>
						 <p>
							If no action is specified, the default action is <code class="literal">start</code>.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686137778288"> <p>
							<code class="literal">kind</code> option
						</p>
						 </td><td align="left" valign="top" headers="idm140686137777200"> <p>
							How to enforce the constraint. The possible values of the <code class="literal">kind</code> option are as follows:
						</p>
						 <p>
							* <code class="literal">Optional</code> - Only applies if both resources are executing the specified action. For information about optional ordering, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/configuring_and_managing_high_availability_clusters/index#proc_configuring-advisory-ordering-determining-resource-order">Configuring advisory ordering</a>.
						</p>
						 <p>
							* <code class="literal">Mandatory</code> - Always enforce the constraint (default value). If the first resource you specified is stopping or cannot be started, the second resource you specified must be stopped. For information about mandatory ordering, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/configuring_and_managing_high_availability_clusters/index#con_configuring-mandatory-ordering-determining-resource-order">Configuring mandatory ordering</a>.
						</p>
						 <p>
							* <code class="literal">Serialize</code> - Ensure that no two stop/start actions occur concurrently for the resources you specify. The first and second resource you specify can start in either order, but one must complete starting before the other can be started. A typical use case is when resource startup puts a high load on the host.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686137778288"> <p>
							<code class="literal">symmetrical</code> option
						</p>
						 </td><td align="left" valign="top" headers="idm140686137777200"> <p>
							If true, the reverse of the constraint applies for the opposite action (for example, if B starts after A starts, then B stops before A stops). Ordering constraints for which <code class="literal">kind</code> is <code class="literal">Serialize</code> cannot be symmetrical. The default value is <code class="literal">true</code> for <code class="literal">Mandatory</code> and <code class="literal">Optional</code> kinds, <code class="literal">false</code> for <code class="literal">Serialize</code>.
						</p>
						 </td></tr></tbody></table></rh-table><p>
			Use the following command to remove resources from any ordering constraint.
		</p><pre class="literallayout">pcs constraint order remove <span class="emphasis"><em>resource1</em></span> [<span class="emphasis"><em>resourceN</em></span>]...</pre><section class="section" id="con_configuring-mandatory-ordering-determining-resource-order"><div class="titlepage"><div><div><h3 class="title">13.1. Configuring mandatory ordering</h3></div></div></div><p class="_abstract _abstract">
				A mandatory ordering constraint indicates that the second action should not be initiated for the second resource unless and until the first action successfully completes for the first resource. Actions that may be ordered are <code class="literal">stop</code>, <code class="literal">start</code>, and additionally for promotable clones, <code class="literal">demote</code> and <code class="literal">promote</code>. For example, "A then B" (which is equivalent to "start A then start B") means that B will not be started unless and until A successfully starts. An ordering constraint is mandatory if the <code class="literal">kind</code> option for the constraint is set to <code class="literal">Mandatory</code> or left as default.
			</p><p>
				If the <code class="literal">symmetrical</code> option is set to <code class="literal">true</code> or left to default, the opposite actions will be ordered in reverse. The <code class="literal">start</code> and <code class="literal">stop</code> actions are opposites, and <code class="literal">demote</code> and <code class="literal">promote</code> are opposites. For example, a symmetrical "promote A then start B" ordering implies "stop B then demote A", which means that A cannot be demoted until and unless B successfully stops. A symmetrical ordering means that changes in A’s state can cause actions to be scheduled for B. For example, given "A then B", if A restarts due to failure, B will be stopped first, then A will be stopped, then A will be started, then B will be started.
			</p><p>
				Note that the cluster reacts to each state change. If the first resource is restarted and is in a started state again before the second resource initiated a stop operation, the second resource will not need to be restarted.
			</p></section><section class="section" id="proc_configuring-advisory-ordering-determining-resource-order"><div class="titlepage"><div><div><h3 class="title">13.2. Configuring advisory ordering</h3></div></div></div><p class="_abstract _abstract">
				When the <code class="literal">kind=Optional</code> option is specified for an ordering constraint, the constraint is considered optional and only applies if both resources are executing the specified actions. Any change in state by the first resource you specify will have no effect on the second resource you specify.
			</p><p>
				The following command configures an advisory ordering constraint for the resources named <code class="literal">VirtualIP</code> and <code class="literal">dummy_resource</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint order VirtualIP then dummy_resource kind=Optional</strong></span></pre></section><section class="section" id="proc_configuring-ordered-resource-sets.adocdetermining-resource-order"><div class="titlepage"><div><div><h3 class="title">13.3. Configuring ordered resource sets</h3></div></div></div><p class="_abstract _abstract">
				A common situation is for an administrator to create a chain of ordered resources, where, for example, resource A starts before resource B which starts before resource C. If your configuration requires that you create a set of resources that is colocated and started in order, you can configure a resource group that contains those resources.
			</p><p>
				There are some situations, however, where configuring the resources that need to start in a specified order as a resource group is not appropriate:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You may need to configure resources to start in order and the resources are not necessarily colocated.
					</li><li class="listitem">
						You may have a resource C that must start after either resource A or B has started but there is no relationship between A and B.
					</li><li class="listitem">
						You may have resources C and D that must start after both resources A and B have started, but there is no relationship between A and B or between C and D.
					</li></ul></div><p>
				In these situations, you can create an ordering constraint on a set or sets of resources with the <code class="literal command">pcs constraint order set</code> command.
			</p><p>
				You can set the following options for a set of resources with the <code class="literal command">pcs constraint order set</code> command.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						<code class="literal">sequential</code>, which can be set to <code class="literal">true</code> or <code class="literal">false</code> to indicate whether the set of resources must be ordered relative to each other. The default value is <code class="literal">true</code>.
					</p><p class="simpara">
						Setting <code class="literal">sequential</code> to <code class="literal">false</code> allows a set to be ordered relative to other sets in the ordering constraint, without its members being ordered relative to each other. Therefore, this option makes sense only if multiple sets are listed in the constraint; otherwise, the constraint has no effect.
					</p></li><li class="listitem">
						<code class="literal">require-all</code>, which can be set to <code class="literal">true</code> or <code class="literal">false</code> to indicate whether all of the resources in the set must be active before continuing. Setting <code class="literal">require-all</code> to <code class="literal">false</code> means that only one resource in the set needs to be started before continuing on to the next set. Setting <code class="literal">require-all</code> to <code class="literal">false</code> has no effect unless used in conjunction with unordered sets, which are sets for which <code class="literal">sequential</code> is set to <code class="literal">false</code>. The default value is <code class="literal">true</code>.
					</li><li class="listitem">
						<code class="literal">action</code>, which can be set to <code class="literal">start</code>, <code class="literal">promote</code>, <code class="literal">demote</code> or <code class="literal">stop</code>, as described in the "Properties of an Order Constraint" table in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters">Determining the order in which cluster resources are run</a>.
					</li><li class="listitem">
						<code class="literal">role</code>, which can be set to <code class="literal">Stopped</code>, <code class="literal">Started</code>, <code class="literal">Promoted</code>, or <code class="literal">Unpromoted</code>.
					</li></ul></div><p>
				You can set the following constraint options for a set of resources following the <code class="literal">setoptions</code> parameter of the <code class="literal command">pcs constraint order set</code> command.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">id</code>, to provide a name for the constraint you are defining.
					</li><li class="listitem">
						<code class="literal">kind</code>, which indicates how to enforce the constraint, as described in the "Properties of an Order Constraint" table in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters">Determining the order in which cluster resources are run</a>.
					</li><li class="listitem">
						<code class="literal">symmetrical</code>, to set whether the reverse of the constraint applies for the opposite action, as described in in the "Properties of an Order Constraint" table in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters">Determining the order in which cluster resources are run</a>.
					</li></ul></div><pre class="literallayout">pcs constraint order set <span class="emphasis"><em>resource1</em></span> <span class="emphasis"><em>resource2</em></span> [<span class="emphasis"><em>resourceN</em></span>]... [<span class="emphasis"><em>options</em></span>] [set <span class="emphasis"><em>resourceX</em></span> <span class="emphasis"><em>resourceY</em></span> ... [<span class="emphasis"><em>options</em></span>]] [setoptions [<span class="emphasis"><em>constraint_options</em></span>]]</pre><p>
				If you have three resources named <code class="literal">D1</code>, <code class="literal">D2</code>, and <code class="literal">D3</code>, the following command configures them as an ordered resource set.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint order set D1 D2 D3</strong></span></pre><p>
				If you have six resources named <code class="literal">A</code>, <code class="literal">B</code>, <code class="literal">C</code>, <code class="literal">D</code>, <code class="literal">E</code>, and <code class="literal">F</code>, this example configures an ordering constraint for the set of resources that will start as follows:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">A</code> and <code class="literal">B</code> start independently of each other
					</li><li class="listitem">
						<code class="literal">C</code> starts once either <code class="literal">A</code> or <code class="literal">B</code> has started
					</li><li class="listitem">
						<code class="literal">D</code> starts once <code class="literal">C</code> has started
					</li><li class="listitem">
						<code class="literal">E</code> and <code class="literal">F</code> start independently of each other once <code class="literal">D</code> has started
					</li></ul></div><p>
				Stopping the resources is not influenced by this constraint since <code class="literal">symmetrical=false</code> is set.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint order set A B sequential=false require-all=false set C D set E F sequential=false setoptions symmetrical=false</strong></span></pre></section><section class="section" id="proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order"><div class="titlepage"><div><div><h3 class="title">13.4. Configuring startup order for resource dependencies not managed by Pacemaker</h3></div></div></div><p class="_abstract _abstract">
				It is possible for a cluster to include resources with dependencies that are not themselves managed by the cluster. In this case, you must ensure that those dependencies are started before Pacemaker is started and stopped after Pacemaker is stopped.
			</p><p>
				You can configure your startup order to account for this situation by means of the <code class="literal">systemd</code> <code class="literal">resource-agents-deps</code> target. You can create a <code class="literal">systemd</code> drop-in unit for this target and Pacemaker will order itself appropriately relative to this target.
			</p><p>
				For example, if a cluster includes a resource that depends on the external service <code class="literal">foo</code> that is not managed by the cluster, perform the following procedure.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create the drop-in unit <code class="literal">/etc/systemd/system/resource-agents-deps.target.d/foo.conf</code> that contains the following:
					</p><pre class="literallayout">[Unit]
Requires=foo.service
After=foo.service</pre></li><li class="listitem">
						Run the <code class="literal command">systemctl daemon-reload</code> command.
					</li></ol></div><p>
				A cluster dependency specified in this way can be something other than a service. For example, you may have a dependency on mounting a file system at <code class="literal">/srv</code>, in which case you would perform the following procedure:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Ensure that <code class="literal">/srv</code> is listed in the <code class="literal">/etc/fstab</code> file. This will be converted automatically to the <code class="literal">systemd</code> file <code class="literal">srv.mount</code> at boot when the configuration of the system manager is reloaded. For more information, see the <code class="literal">systemd.mount</code>(5) and the <code class="literal">systemd-fstab-generator</code>(8) man pages.
					</li><li class="listitem"><p class="simpara">
						To make sure that Pacemaker starts after the disk is mounted, create the drop-in unit <code class="literal">/etc/systemd/system/resource-agents-deps.target.d/srv.conf</code> that contains the following.
					</p><pre class="literallayout">[Unit]
Requires=srv.mount
After=srv.mount</pre></li><li class="listitem">
						Run the <code class="literal command">systemctl daemon-reload</code> command.
					</li></ol></div><p>
				If an LVM volume group used by a Pacemaker cluster contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, you can configure a <code class="literal">systemd resource-agents-deps</code> target and a <code class="literal">systemd</code> drop-in unit for the target to ensure that the service starts before Pacemaker starts.
			</p><p>
				The following procedure configures <code class="literal">blk-availability.service</code> as a dependency. The <code class="literal">blk-availability.service</code> service is a wrapper that includes <code class="literal">iscsi.service</code>, among other services. If your deployment requires it, you could configure <code class="literal">iscsi.service</code> (for iSCSI only) or <code class="literal">remote-fs.target</code> as the dependency instead of <code class="literal">blk-availability</code>.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create the drop-in unit <code class="literal">/etc/systemd/system/resource-agents-deps.target.d/blk-availability.conf</code> that contains the following:
					</p><pre class="literallayout">[Unit]
Requires=blk-availability.service
After=blk-availability.service</pre></li><li class="listitem">
						Run the <code class="literal command">systemctl daemon-reload</code> command.
					</li></ol></div></section></section><section class="chapter" id="assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 14. Colocating cluster resources</h2></div></div></div><p class="_abstract _abstract">
			To specify that the location of one resource depends on the location of another resource, you configure a colocation constraint.
		</p><p>
			There is an important side effect of creating a colocation constraint between two resources: it affects the order in which resources are assigned to a node. This is because you cannot place resource A relative to resource B unless you know where resource B is. So when you are creating colocation constraints, it is important to consider whether you should colocate resource A with resource B or resource B with resource A.
		</p><p>
			Another thing to keep in mind when creating colocation constraints is that, assuming resource A is colocated with resource B, the cluster will also take into account resource A’s preferences when deciding which node to choose for resource B.
		</p><p>
			The following command creates a colocation constraint.
		</p><pre class="literallayout">pcs constraint colocation add [promoted|unpromoted] <span class="emphasis"><em>source_resource</em></span> with [promoted|unpromoted] <span class="emphasis"><em>target_resource</em></span> [<span class="emphasis"><em>score</em></span>] [<span class="emphasis"><em>options</em></span>]</pre><p>
			The following table summarizes the properties and options for configuring colocation constraints.
		</p><rh-table id="idm140686139802688"><table class="lt-4-cols lt-7-rows"><caption>Table 14.1. Parameters of a Colocation Constraint</caption><colgroup><col style="width: 40%; " class="col_1"><!--Empty--><col style="width: 60%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686139797840" scope="col">Parameter</th><th align="left" valign="top" id="idm140686139796752" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686139797840"> <p>
							source_resource
						</p>
						 </td><td align="left" valign="top" headers="idm140686139796752"> <p>
							The colocation source. If the constraint cannot be satisfied, the cluster may decide not to allow the resource to run at all.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686139797840"> <p>
							target_resource
						</p>
						 </td><td align="left" valign="top" headers="idm140686139796752"> <p>
							The colocation target. The cluster will decide where to put this resource first and then decide where to put the source resource.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686139797840"> <p>
							score
						</p>
						 </td><td align="left" valign="top" headers="idm140686139796752"> <p>
							Positive values indicate the resource should run on the same node. Negative values indicate the resources should not run on the same node. A value of +<code class="literal">INFINITY</code>, the default value, indicates that the <span class="emphasis"><em>source_resource</em></span> must run on the same node as the <span class="emphasis"><em>target_resource</em></span>. A value of -<code class="literal">INFINITY</code> indicates that the <span class="emphasis"><em>source_resource</em></span> must not run on the same node as the <span class="emphasis"><em>target_resource</em></span>.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686139797840"> <p>
							<code class="literal">influence</code> option
						</p>
						 </td><td align="left" valign="top" headers="idm140686139796752"> <p>
							Determines whether the cluster will move both the primary resource (<span class="emphasis"><em>source_resource</em></span>) and dependent resources (<span class="emphasis"><em>target_resource)</em></span> to another node when the dependent resource reaches its migration threshold for failure, or whether the cluster will leave the dependent resource offline without causing a service switch.
						</p>
						 <p>
							The <code class="literal">influence</code> colocation constraint option can have a value of <code class="literal">true</code> or <code class="literal">false</code>. The default value for this option is determined by the value of the dependent resource’s <code class="literal">critical</code> resource meta option, which has a default value of <code class="literal">true</code>.
						</p>
						 <p>
							When this option has a value of <code class="literal">true</code>, Pacemaker will attempt to keep both the primary and dependent resource active. If the dependent resource reaches its migration threshold for failures, both resources will move to another node if possible.
						</p>
						 <p>
							When this option has a value of <code class="literal">false</code>, Pacemaker will avoid moving the primary resource as a result of the status of the dependent resource. In this case, if the dependent resource reaches its migration threshold for failures, it will stop if the primary resource is active and can remain on its current node.
						</p>
						 </td></tr></tbody></table></rh-table><section class="section" id="proc_specifying-mandatory-placement.adoc-colocating-cluster-resources"><div class="titlepage"><div><div><h3 class="title">14.1. Specifying mandatory placement of resources</h3></div></div></div><p class="_abstract _abstract">
				Mandatory placement occurs any time the constraint’s score is <code class="literal">+INFINITY</code> or <code class="literal">-INFINITY</code>. In such cases, if the constraint cannot be satisfied, then the <span class="emphasis"><em>source_resource</em></span> is not permitted to run. For <code class="literal">score=INFINITY</code>, this includes cases where the <span class="emphasis"><em>target_resource</em></span> is not active.
			</p><p>
				If you need <code class="literal">myresource1</code> to always run on the same machine as <code class="literal">myresource2</code>, you would add the following constraint:
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint colocation add myresource1 with myresource2 score=INFINITY</strong></span></pre><p>
				Because <code class="literal">INFINITY</code> was used, if <code class="literal">myresource2</code> cannot run on any of the cluster nodes (for whatever reason) then <code class="literal">myresource1</code> will not be allowed to run.
			</p><p>
				Alternatively, you may want to configure the opposite, a cluster in which <code class="literal">myresource1</code> cannot run on the same machine as <code class="literal">myresource2</code>. In this case use <code class="literal">score=-INFINITY</code>
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint colocation add myresource1 with myresource2 score=-INFINITY</strong></span></pre><p>
				Again, by specifying <code class="literal">-INFINITY</code>, the constraint is binding. So if the only place left to run is where <code class="literal">myresource2</code> already is, then <code class="literal">myresource1</code> may not run anywhere.
			</p></section><section class="section" id="con_specifying-advisory-placement-colocating-cluster-resources"><div class="titlepage"><div><div><h3 class="title">14.2. Specifying advisory placement of resources</h3></div></div></div><p class="_abstract _abstract">
				Advisory placement of resources indicates the placement of resources is a preference, but is not mandatory. For constraints with scores greater than <code class="literal">-INFINITY</code> and less than <code class="literal">INFINITY</code>, the cluster will try to accommodate your wishes but may ignore them if the alternative is to stop some of the cluster resources.
			</p></section><section class="section" id="proc_colocating-resource-sets.adoc-colocating-cluster-resources"><div class="titlepage"><div><div><h3 class="title">14.3. Colocating sets of resources</h3></div></div></div><p class="_abstract _abstract">
				If your configuration requires that you create a set of resources that are colocated and started in order, you can configure a resource group that contains those resources. There are some situations, however, where configuring the resources that need to be colocated as a resource group is not appropriate:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You may need to colocate a set of resources but the resources do not necessarily need to start in order.
					</li><li class="listitem">
						You may have a resource C that must be colocated with either resource A or B, but there is no relationship between A and B.
					</li><li class="listitem">
						You may have resources C and D that must be colocated with both resources A and B, but there is no relationship between A and B or between C and D.
					</li></ul></div><p>
				In these situations, you can create a colocation constraint on a set or sets of resources with the <code class="literal command">pcs constraint colocation set</code> command.
			</p><p>
				You can set the following options for a set of resources with the <code class="literal command">pcs constraint colocation set</code> command.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						<code class="literal">sequential</code>, which can be set to <code class="literal">true</code> or <code class="literal">false</code> to indicate whether the members of the set must be colocated with each other.
					</p><p class="simpara">
						Setting <code class="literal">sequential</code> to <code class="literal">false</code> allows the members of this set to be colocated with another set listed later in the constraint, regardless of which members of this set are active. Therefore, this option makes sense only if another set is listed after this one in the constraint; otherwise, the constraint has no effect.
					</p></li><li class="listitem">
						<code class="literal">role</code>, which can be set to <code class="literal">Stopped</code>, <code class="literal">Started</code>, <code class="literal">Promoted</code>, or <code class="literal">Unpromoted</code>.
					</li></ul></div><p>
				You can set the following constraint option for a set of resources following the <code class="literal">setoptions</code> parameter of the <code class="literal command">pcs constraint colocation set</code> command.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">id</code>, to provide a name for the constraint you are defining.
					</li><li class="listitem">
						<code class="literal">score</code>, to indicate the degree of preference for this constraint. For information about this option, see the "Location Constraint Options" table in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters#proc_configuring-location-constraints-determining-which-node-a-resource-runs-on">Configuring Location Constraints</a>
					</li></ul></div><p>
				When listing members of a set, each member is colocated with the one before it. For example, "set A B" means "B is colocated with A". However, when listing multiple sets, each set is colocated with the one after it. For example, "set C D sequential=false set A B" means "set C D (where C and D have no relation between each other) is colocated with set A B (where B is colocated with A)".
			</p><p>
				The following command creates a colocation constraint on a set or sets of resources.
			</p><pre class="literallayout">pcs constraint colocation set <span class="emphasis"><em>resource1</em></span> <span class="emphasis"><em>resource2</em></span>] [<span class="emphasis"><em>resourceN</em></span>]... [<span class="emphasis"><em>options</em></span>] [set <span class="emphasis"><em>resourceX</em></span> <span class="emphasis"><em>resourceY</em></span>] ... [<span class="emphasis"><em>options</em></span>]] [setoptions [<span class="emphasis"><em>constraint_options</em></span>]]</pre><p>
				Use the following command to remove colocation constraints with <span class="emphasis"><em>source_resource</em></span>.
			</p><pre class="literallayout">pcs constraint colocation remove <span class="emphasis"><em>source_resource</em></span> <span class="emphasis"><em>target_resource</em></span></pre></section></section><section class="chapter" id="proc_displaying-resource-constraints.adoc-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 15. Displaying resource constraints and resource dependencies</h2></div></div></div><p class="_abstract _abstract">
			There are a several commands you can use to display constraints that have been configured. You can display all configured resource constraints, or you can limit the display of esource constraints to specific types of resource constraints. Additionally, you can display configured resource dependencies.
		</p><div class="formalpara"><p class="title"><strong>Displaying all configured constraints</strong></p><p>
				The following command lists all current location, order, and colocation constraints. If the <code class="literal">--full</code> option is specified, show the internal constraint IDs.
			</p></div><pre class="literallayout">pcs constraint [list|show] [--full]</pre><p>
			By default, listing resource constraints does not display expired constraints. To include expired constaints in the listing, use the <code class="literal">--all</code> option of the <code class="literal">pcs constraint</code> command. This will list expired constraints, noting the constraints and their associated rules as <code class="literal">(expired)</code> in the display.
		</p><div class="formalpara"><p class="title"><strong>Displaying location constraints</strong></p><p>
				The following command lists all current location constraints.
			</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					If <code class="literal">resources</code> is specified, location constraints are displayed per resource. This is the default behavior.
				</li><li class="listitem">
					If <code class="literal">nodes</code> is specified, location constraints are displayed per node.
				</li><li class="listitem">
					If specific resources or nodes are specified, then only information about those resources or nodes is displayed.
				</li></ul></div><pre class="literallayout">pcs constraint location [show [resources [<span class="emphasis"><em>resource</em></span>...]] | [nodes [<span class="emphasis"><em>node</em></span>...]]] [--full]</pre><div class="formalpara"><p class="title"><strong>Displaying ordering constraints</strong></p><p>
				The following command lists all current ordering constraints.
			</p></div><pre class="literallayout">pcs constraint order [show]</pre><div class="formalpara"><p class="title"><strong>Displaying colocation constraints</strong></p><p>
				The following command lists all current colocation constraints.
			</p></div><pre class="literallayout">pcs constraint colocation [show]</pre><div class="formalpara"><p class="title"><strong>Displaying resource-specific constraints</strong></p><p>
				The following command lists the constraints that reference specific resources.
			</p></div><pre class="literallayout">pcs constraint ref <span class="emphasis"><em>resource</em></span> ...</pre><div class="formalpara"><p class="title"><strong>Displaying resource dependencies</strong></p><p>
				The following command displays the relations between cluster resources in a tree structure.
			</p></div><pre class="literallayout">pcs resource relations <span class="emphasis"><em>resource</em></span> [--full]</pre><p>
			If the <code class="literal">--full</code> option is used, the command displays additional information, including the constraint IDs and the resource types.
		</p><p>
			In the following example, there are 3 configured resources: C, D, and E.
		</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint order start C then start D</strong></span>
Adding C D (kind: Mandatory) (Options: first-action=start then-action=start)
# <span class="strong strong"><strong>pcs constraint order start D then start E</strong></span>
Adding D E (kind: Mandatory) (Options: first-action=start then-action=start)

# <span class="strong strong"><strong>pcs resource relations C</strong></span>
C
`- order
   |  start C then start D
   `- D
      `- order
         |  start D then start E
         `- E
# <span class="strong strong"><strong>pcs resource relations D</strong></span>
D
|- order
|  |  start C then start D
|  `- C
`- order
   |  start D then start E
   `- E
# pcs <span class="strong strong"><strong>resource relations E</strong></span>
E
`- order
   |  start D then start E
   `- D
      `- order
         |  start C then start D
         `- C</pre><p>
			In the following example, there are 2 configured resources: A and B. Resources A and B are part of resource group G.
		</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource relations A</strong></span>
A
`- outer resource
   `- G
      `- inner resource(s)
         |  members: A B
         `- B
# <span class="strong strong"><strong>pcs resource relations B</strong></span>
B
`- outer resource
   `- G
      `- inner resource(s)
         |  members: A B
         `- A
# <span class="strong strong"><strong>pcs resource relations G</strong></span>
G
`- inner resource(s)
   |  members: A B
   |- A
   `- B</pre></section><section class="chapter" id="assembly_determining-resource-location-with-rules-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 16. Determining resource location with rules</h2></div></div></div><p class="_abstract _abstract">
			For more complicated location constraints, you can use Pacemaker rules to determine a resource’s location.
		</p><section class="section" id="ref_pacemaker-rules.adoc-determining-resource-location-with-rules"><div class="titlepage"><div><div><h3 class="title">16.1. Pacemaker rules</h3></div></div></div><p class="_abstract _abstract">
				Pacemaker rules can be used to make your configuration more dynamic. One use of rules might be to assign machines to different processing groups (using a node attribute) based on time and to then use that attribute when creating location constraints.
			</p><p>
				Each rule can contain a number of expressions, date-expressions and even other rules. The results of the expressions are combined based on the rule’s <code class="literal">boolean-op</code> field to determine if the rule ultimately evaluates to <code class="literal">true</code> or <code class="literal">false</code>. What happens next depends on the context in which the rule is being used.
			</p><rh-table id="tb-rule-props-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 16.1. Properties of a Rule</caption><colgroup><col style="width: 50%; " class="col_1"><!--Empty--><col style="width: 50%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686137985360" scope="col">Field</th><th align="left" valign="top" id="idm140686137984272" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686137985360"> <p>
								<code class="literal">role</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137984272"> <p>
								Limits the rule to apply only when the resource is in that role. Allowed values: <code class="literal">Started</code>, <code class="literal">Unpromoted,</code> and <code class="literal">Promoted</code>. NOTE: A rule with <code class="literal">role="Promoted"</code> cannot determine the initial location of a clone instance. It will only affect which of the active instances will be promoted.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137985360"> <p>
								<code class="literal">score</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137984272"> <p>
								The score to apply if the rule evaluates to <code class="literal">true</code>. Limited to use in rules that are part of location constraints.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137985360"> <p>
								<code class="literal">score-attribute</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137984272"> <p>
								The node attribute to look up and use as a score if the rule evaluates to <code class="literal">true</code>. Limited to use in rules that are part of location constraints.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137985360"> <p>
								<code class="literal">boolean-op</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137984272"> <p>
								How to combine the result of multiple expression objects. Allowed values: <code class="literal">and</code> and <code class="literal">or</code>. The default value is <code class="literal">and</code>.
							</p>
							 </td></tr></tbody></table></rh-table><section class="section" id="node_attribute_expressions"><div class="titlepage"><div><div><h4 class="title">16.1.1. Node attribute expressions</h4></div></div></div><p>
					Node attribute expressions are used to control a resource based on the attributes defined by a node or nodes.
				</p><rh-table id="tb-expressions-props-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 16.2. Properties of an Expression</caption><colgroup><col style="width: 50%; " class="col_1"><!--Empty--><col style="width: 50%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686137044976" scope="col">Field</th><th align="left" valign="top" id="idm140686137043888" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686137044976"> <p>
									<code class="literal">attribute</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686137043888"> <p>
									The node attribute to test
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686137044976"> <p>
									<code class="literal">type</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686137043888"> <p>
									Determines how the value(s) should be tested. Allowed values: <code class="literal">string</code>, <code class="literal">integer</code>, <code class="literal">number</code>, <code class="literal">version</code>. The default value is <code class="literal">string</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686137044976"> <p>
									<code class="literal">operation</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686137043888"> <p>
									The comparison to perform. Allowed values:
								</p>
								 <p>
									* <code class="literal">lt</code> - True if the node attribute’s value is less than <code class="literal">value</code>
								</p>
								 <p>
									* <code class="literal">gt</code> - True if the node attribute’s value is greater than <code class="literal">value</code>
								</p>
								 <p>
									* <code class="literal">lte</code> - True if the node attribute’s value is less than or equal to <code class="literal">value</code>
								</p>
								 <p>
									* <code class="literal">gte</code> - True if the node attribute’s value is greater than or equal to <code class="literal">value</code>
								</p>
								 <p>
									* <code class="literal">eq</code> - True if the node attribute’s value is equal to <code class="literal">value</code>
								</p>
								 <p>
									* <code class="literal">ne</code> - True if the node attribute’s value is not equal to <code class="literal">value</code>
								</p>
								 <p>
									* <code class="literal">defined</code> - True if the node has the named attribute
								</p>
								 <p>
									* <code class="literal">not_defined</code> - True if the node does not have the named attribute
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686137044976"> <p>
									<code class="literal">value</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686137043888"> <p>
									User supplied value for comparison (required unless <code class="literal">operation</code> is <code class="literal">defined</code> or <code class="literal">not_defined</code>)
								</p>
								 </td></tr></tbody></table></rh-table><p>
					In addition to any attributes added by the administrator, the cluster defines special, built-in node attributes for each node that can also be used, as described in the following table.
				</p><rh-table id="tb-nodeattributes-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 16.3. Built-in Node Attributes</caption><colgroup><col style="width: 50%; " class="col_1"><!--Empty--><col style="width: 50%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686140515856" scope="col">Name</th><th align="left" valign="top" id="idm140686140514768" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686140515856"> <p>
									<code class="literal">#uname</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140514768"> <p>
									Node name
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686140515856"> <p>
									<code class="literal">#id</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140514768"> <p>
									Node ID
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686140515856"> <p>
									<code class="literal">#kind</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140514768"> <p>
									Node type. Possible values are <code class="literal">cluster</code>, <code class="literal">remote</code>, and <code class="literal">container</code>. The value of <code class="literal">kind</code> is <code class="literal">remote</code> for Pacemaker Remote nodes created with the <code class="literal">ocf:pacemaker:remote</code> resource, and <code class="literal">container</code> for Pacemaker Remote guest nodes and bundle nodes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686140515856"> <p>
									<code class="literal">#is_dc</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140514768"> <p>
									<code class="literal">true</code> if this node is a Designated Controller (DC), <code class="literal">false</code> otherwise
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686140515856"> <p>
									<code class="literal">#cluster_name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140514768"> <p>
									The value of the <code class="literal">cluster-name</code> cluster property, if set
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686140515856"> <p>
									<code class="literal">#site_name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140514768"> <p>
									The value of the <code class="literal">site-name</code> node attribute, if set, otherwise identical to <code class="literal">#cluster-name</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686140515856"> <p>
									<code class="literal">#role</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140514768"> <p>
									The role the relevant promotable clone has on this node. Valid only within a rule for a location constraint for a promotable clone.
								</p>
								 </td></tr></tbody></table></rh-table></section><section class="section" id="time_date_based_expressions"><div class="titlepage"><div><div><h4 class="title">16.1.2. Time/date based expressions</h4></div></div></div><p>
					Date expressions are used to control a resource or cluster option based on the current date/time. They can contain an optional date specification.
				</p><rh-table id="tb-dateexpress-props-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 16.4. Properties of a Date Expression</caption><colgroup><col style="width: 50%; " class="col_1"><!--Empty--><col style="width: 50%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686139006992" scope="col">Field</th><th align="left" valign="top" id="idm140686139005904" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686139006992"> <p>
									<code class="literal">start</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686139005904"> <p>
									A date/time conforming to the ISO8601 specification.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686139006992"> <p>
									<code class="literal">end</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686139005904"> <p>
									A date/time conforming to the ISO8601 specification.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686139006992"> <p>
									<code class="literal">operation</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686139005904"> <p>
									Compares the current date/time with the start or the end date or both the start and end date, depending on the context. Allowed values:
								</p>
								 <p>
									* <code class="literal">gt</code> - True if the current date/time is after <code class="literal">start</code>
								</p>
								 <p>
									* <code class="literal">lt</code> - True if the current date/time is before <code class="literal">end</code>
								</p>
								 <p>
									* <code class="literal">in_range</code> - True if the current date/time is after <code class="literal">start</code> and before <code class="literal">end</code>
								</p>
								 <p>
									* <code class="literal">date-spec</code> - performs a cron-like comparison to the current date/time
								</p>
								 </td></tr></tbody></table></rh-table></section><section class="section" id="date_specifications"><div class="titlepage"><div><div><h4 class="title">16.1.3. Date specifications</h4></div></div></div><p>
					Date specifications are used to create cron-like expressions relating to time. Each field can contain a single number or a single range. Instead of defaulting to zero, any field not supplied is ignored.
				</p><p>
					For example, <code class="literal">monthdays="1"</code> matches the first day of every month and <code class="literal">hours="09-17"</code> matches the hours between 9 am and 5 pm (inclusive). However, you cannot specify <code class="literal">weekdays="1,2"</code> or <code class="literal">weekdays="1-2,5-6"</code> since they contain multiple ranges.
				</p><rh-table id="tb-datespecs-props-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 16.5. Properties of a Date Specification</caption><colgroup><col style="width: 50%; " class="col_1"><!--Empty--><col style="width: 50%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686141555536" scope="col">Field</th><th align="left" valign="top" id="idm140686141554448" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686141555536"> <p>
									<code class="literal">id</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686141554448"> <p>
									A unique name for the date
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686141555536"> <p>
									<code class="literal">hours</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686141554448"> <p>
									Allowed values: 0-23
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686141555536"> <p>
									<code class="literal">monthdays</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686141554448"> <p>
									Allowed values: 0-31 (depending on month and year)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686141555536"> <p>
									<code class="literal">weekdays</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686141554448"> <p>
									Allowed values: 1-7 (1=Monday, 7=Sunday)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686141555536"> <p>
									<code class="literal">yeardays</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686141554448"> <p>
									Allowed values: 1-366 (depending on the year)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686141555536"> <p>
									<code class="literal">months</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686141554448"> <p>
									Allowed values: 1-12
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686141555536"> <p>
									<code class="literal">weeks</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686141554448"> <p>
									Allowed values: 1-53 (depending on <code class="literal">weekyear</code>)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686141555536"> <p>
									<code class="literal">years</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686141554448"> <p>
									Year according the Gregorian calendar
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686141555536"> <p>
									<code class="literal">weekyears</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686141554448"> <p>
									May differ from Gregorian years; for example, <code class="literal">2005-001 Ordinal</code> is also <code class="literal">2005-01-01 Gregorian</code> is also <code class="literal">2004-W53-6 Weekly</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686141555536"> <p>
									<code class="literal">moon</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686141554448"> <p>
									Allowed values: 0-7 (0 is new, 4 is full moon).
								</p>
								 </td></tr></tbody></table></rh-table></section></section><section class="section" id="ref_configuring-constraint-using-rules.adoc-determining-resource-location-with-rules"><div class="titlepage"><div><div><h3 class="title">16.2. Configuring a Pacemaker location constraint using rules</h3></div></div></div><p class="_abstract _abstract">
				Use the following command to configure a Pacemaker constraint that uses rules. If <code class="literal">score</code> is omitted, it defaults to INFINITY. If <code class="literal">resource-discovery</code> is omitted, it defaults to <code class="literal">always</code>.
			</p><p>
				For information about the <code class="literal">resource-discovery</code> option, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters#proc_limiting-resource-discovery-to-a-subset-of-nodes-determining-which-node-a-resource-runs-on">Limiting resource discovery to a subset of nodes</a>.
			</p><p>
				As with basic location constraints, you can use regular expressions for resources with these constraints as well.
			</p><p>
				When using rules to configure location constraints, the value of <code class="literal">score</code> can be positive or negative, with a positive value indicating "prefers" and a negative value indicating "avoids".
			</p><pre class="literallayout">pcs constraint location <span class="emphasis"><em>rsc</em></span> rule [resource-discovery=<span class="emphasis"><em>option</em></span>] [role=promoted|unpromoted] [score=<span class="emphasis"><em>score</em></span> | score-attribute=<span class="emphasis"><em>attribute</em></span>] <span class="emphasis"><em>expression</em></span></pre><p>
				The <span class="emphasis"><em>expression</em></span> option can be one of the following where <span class="emphasis"><em>duration_options</em></span> and <span class="emphasis"><em>date_spec_options</em></span> are: hours, monthdays, weekdays, yeardays, months, weeks, years, weekyears, and moon as described in the "Properties of a Date Specification" table in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-location-with-rules-configuring-and-managing-high-availability-clusters#date_specifications">Date specifications</a>.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">defined|not_defined <span class="emphasis"><em>attribute</em></span></code>
					</li><li class="listitem">
						<code class="literal"><span class="emphasis"><em>attribute</em></span> lt|gt|lte|gte|eq|ne [string|integer|number|version] <span class="emphasis"><em>value</em></span></code>
					</li><li class="listitem">
						<code class="literal">date gt|lt <span class="emphasis"><em>date</em></span></code>
					</li><li class="listitem">
						<code class="literal">date in_range <span class="emphasis"><em>date</em></span> to <span class="emphasis"><em>date</em></span></code>
					</li><li class="listitem">
						<code class="literal">date in_range <span class="emphasis"><em>date</em></span> to duration <span class="emphasis"><em>duration_options</em></span> …​</code>
					</li><li class="listitem">
						<code class="literal">date-spec <span class="emphasis"><em>date_spec_options</em></span></code>
					</li><li class="listitem">
						<code class="literal"><span class="emphasis"><em>expression</em></span> and|or <span class="emphasis"><em>expression</em></span></code>
					</li><li class="listitem">
						<code class="literal">(<span class="emphasis"><em>expression</em></span>)</code>
					</li></ul></div><p>
				Note that durations are an alternative way to specify an end for <code class="literal">in_range</code> operations by means of calculations. For example, you can specify a duration of 19 months.
			</p><p>
				The following location constraint configures an expression that is true if now is any time in the year 2018.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint location Webserver rule score=INFINITY date-spec years=2018</strong></span></pre><p>
				The following command configures an expression that is true from 9 am to 5 pm, Monday through Friday. Note that the hours value of 16 matches up to 16:59:59, as the numeric value (hour) still matches.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint location Webserver rule score=INFINITY date-spec hours="9-16" weekdays="1-5"</strong></span></pre><p>
				The following command configures an expression that is true when there is a full moon on Friday the thirteenth.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint location Webserver rule date-spec weekdays=5 monthdays=13 moon=4</strong></span></pre><p>
				To remove a rule, use the following command. If the rule that you are removing is the last rule in its constraint, the constraint will be removed.
			</p><pre class="literallayout">pcs constraint rule remove <span class="emphasis"><em>rule_id</em></span></pre></section></section><section class="chapter" id="assembly_managing-cluster-resources-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 17. Managing cluster resources</h2></div></div></div><p class="_abstract _abstract">
			There are a variety of commands you can use to display, modify, and administer cluster resources.
		</p><section class="section" id="proc_display-configured-resources-managing-cluster-resources"><div class="titlepage"><div><div><h3 class="title">17.1. Displaying configured resources</h3></div></div></div><p class="_abstract _abstract">
				To display a list of all configured resources, use the following command.
			</p><pre class="literallayout">pcs resource status</pre><p>
				For example, if your system is configured with a resource named <code class="literal">VirtualIP</code> and a resource named <code class="literal">WebSite</code>, the <code class="literal command">pcs resource status</code> command yields the following output.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource status</strong></span>
 VirtualIP	(ocf::heartbeat:IPaddr2):	Started
 WebSite	(ocf::heartbeat:apache):	Started</pre><p>
				To display the configured parameters for a resource, use the following command.
			</p><pre class="literallayout">pcs resource config <span class="emphasis"><em>resource_id</em></span></pre><p>
				For example, the following command displays the currently configured parameters for resource <code class="literal">VirtualIP</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource config VirtualIP</strong></span>
 Resource: VirtualIP (type=IPaddr2 class=ocf provider=heartbeat)
  Attributes: ip=192.168.0.120 cidr_netmask=24
  Operations: monitor interval=30s</pre><p>
				To display the status of an individual resource, use the following command.
			</p><pre class="literallayout">pcs resource status <span class="emphasis"><em>resource_id</em></span></pre><p>
				For example, if your system is configured with a resource named <code class="literal">VirtualIP</code> the <code class="literal">pcs resource status VirtualIP</code> command yields the following output.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource status VirtualIP</strong></span>
 VirtualIP      (ocf::heartbeat:IPaddr2):       Started</pre><p>
				To display the status of the resources running on a specific node, use the following command. You can use this command to display the status of resources on both cluster and remote nodes.
			</p><pre class="literallayout">pcs resource status node=<span class="emphasis"><em>node_id</em></span></pre><p>
				For example, if <code class="literal">node-01</code> is running resources named <code class="literal">VirtualIP</code> and <code class="literal">WebSite</code> the <code class="literal">pcs resource status node=node-01</code> command might yield the following output.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource status node=node-01</strong></span>
 VirtualIP      (ocf::heartbeat:IPaddr2):       Started
 WebSite        (ocf::heartbeat:apache):        Started</pre></section><section class="section" id="proc_exporting-resources-managing-cluster-resources"><div class="titlepage"><div><div><h3 class="title">17.2. Exporting cluster resources as <code class="literal">pcs</code> commands</h3></div></div></div><p class="_abstract _abstract">
				As of Red Hat Enterprise Linux 9.1, you can display the <code class="literal">pcs</code> commands that can be used to re-create configured cluster resources on a different system using the <code class="literal">--output-format=cmd</code> option of the <code class="literal">pcs resource config</code> command.
			</p><p>
				The following commands create four resources created for an active/passive Apache HTTP server in a Red Hat high availability cluster: an <code class="literal">LVM-activate</code> resource, a <code class="literal">Filesystem</code> resource, an <code class="literal">IPaddr2</code> resource, and an <code class="literal">Apache</code> resource.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create my_lvm ocf:heartbeat:LVM-activate vgname=my_vg vg_access_mode=system_id --group apachegroup</strong></span>
# <span class="strong strong"><strong>pcs resource create my_fs Filesystem device="/dev/my_vg/my_lv" directory="/var/www" fstype="xfs" --group apachegroup</strong></span>
# <span class="strong strong"><strong>pcs resource create VirtualIP IPaddr2 ip=198.51.100.3 cidr_netmask=24 --group apachegroup</strong></span>
# <span class="strong strong"><strong>pcs resource create Website apache configfile="/etc/httpd/conf/httpd.conf" statusurl="http://127.0.0.1/server-status" --group apachegroup</strong></span></pre><p>
				After you create the resources, the following command displays the <code class="literal">pcs</code> commands you can use to re-create those resources on a different system.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource config --output-format=cmd</strong></span>
pcs resource create --no-default-ops --force -- my_lvm ocf:heartbeat:LVM-activate \
  vg_access_mode=system_id vgname=my_vg \
  op \
    monitor interval=30s id=my_lvm-monitor-interval-30s timeout=90s \
    start interval=0s id=my_lvm-start-interval-0s timeout=90s \
    stop interval=0s id=my_lvm-stop-interval-0s timeout=90s;
pcs resource create --no-default-ops --force -- my_fs ocf:heartbeat:Filesystem \
  device=/dev/my_vg/my_lv directory=/var/www fstype=xfs \
  op \
    monitor interval=20s id=my_fs-monitor-interval-20s timeout=40s \
    start interval=0s id=my_fs-start-interval-0s timeout=60s \
    stop interval=0s id=my_fs-stop-interval-0s timeout=60s;
pcs resource create --no-default-ops --force -- VirtualIP ocf:heartbeat:IPaddr2 \
  cidr_netmask=24 ip=198.51.100.3 \
  op \
    monitor interval=10s id=VirtualIP-monitor-interval-10s timeout=20s \
    start interval=0s id=VirtualIP-start-interval-0s timeout=20s \
    stop interval=0s id=VirtualIP-stop-interval-0s timeout=20s;
pcs resource create --no-default-ops --force -- Website ocf:heartbeat:apache \
  configfile=/etc/httpd/conf/httpd.conf statusurl=http://127.0.0.1/server-status \
  op \
    monitor interval=10s id=Website-monitor-interval-10s timeout=20s \
    start interval=0s id=Website-start-interval-0s timeout=40s \
    stop interval=0s id=Website-stop-interval-0s timeout=60s;
pcs resource group add apachegroup \
  my_lvm my_fs VirtualIP Website</pre><p>
				To display the <code class="literal">pcs</code> command or commands you can use to re-create only one configured resource, specify the resource ID for that resource.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource config VirtualIP --output-format=cmd</strong></span>
pcs resource create --no-default-ops --force -- VirtualIP ocf:heartbeat:IPaddr2 \
  cidr_netmask=24 ip=198.51.100.3 \
  op \
    monitor interval=10s id=VirtualIP-monitor-interval-10s timeout=20s \
    start interval=0s id=VirtualIP-start-interval-0s timeout=20s \
    stop interval=0s id=VirtualIP-stop-interval-0s timeout=20s</pre></section><section class="section" id="proc_modify-resource-parameters-managing-cluster-resources"><div class="titlepage"><div><div><h3 class="title">17.3. Modifying resource parameters</h3></div></div></div><p class="_abstract _abstract">
				To modify the parameters of a configured resource, use the following command.
			</p><pre class="literallayout">pcs resource update <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>resource_options</em></span>]</pre><p>
				The following sequence of commands show the initial values of the configured parameters for resource <code class="literal">VirtualIP</code>, the command to change the value of the <code class="literal">ip</code> parameter, and the values following the update command.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource config VirtualIP</strong></span>
 Resource: VirtualIP (type=IPaddr2 class=ocf provider=heartbeat)
  Attributes: ip=192.168.0.120 cidr_netmask=24
  Operations: monitor interval=30s
# <span class="strong strong"><strong>pcs resource update VirtualIP ip=192.169.0.120</strong></span>
# <span class="strong strong"><strong>pcs resource config VirtualIP</strong></span>
 Resource: VirtualIP (type=IPaddr2 class=ocf provider=heartbeat)
  Attributes: ip=192.169.0.120 cidr_netmask=24
  Operations: monitor interval=30s</pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					When you update a resource’s operation with the <code class="literal">pcs resource update</code> command, any options you do not specifically call out are reset to their default values.
				</p></div></rh-alert></section><section class="section" id="proc_cleanup-cluster-resources-managing-cluster-resources"><div class="titlepage"><div><div><h3 class="title">17.4. Clearing failure status of cluster resources</h3></div></div></div><p>
				If a resource has failed, a failure message appears when you display the cluster status with the <code class="literal">pcs status</code> command. After attempting to resolve the cause of the failure, you can check the updated status of the resource by running the <code class="literal">pcs status</code> command again, and you can check the failure count for the cluster resources with the <code class="literal">pcs resource failcount show --full</code> command.
			</p><p>
				You can clear that failure status of a resource with the <code class="literal command">pcs resource cleanup</code> command. The <code class="literal">pcs resource cleanup</code> command resets the resource status and <code class="literal">failcount</code> value for the resource. This command also removes the operation history for the resource and re-detects its current state.
			</p><p>
				The following command resets the resource status and <code class="literal">failcount</code> value for the resource specified by <span class="emphasis"><em>resource_id</em></span>.
			</p><pre class="literallayout">pcs resource cleanup <span class="emphasis"><em>resource_id</em></span></pre><p>
				If you do not specify <span class="emphasis"><em>resource_id</em></span>, the <code class="literal">pcs resource cleanup</code> command resets the resource status and <code class="literal">failcount</code> value for all resources with a failure count.
			</p><p>
				In addition to the <code class="literal">pcs resource cleanup <span class="emphasis"><em>resource_id</em></span></code> command, you can also reset the resource status and clear the operation history of a resource with the <code class="literal">pcs resource refresh <span class="emphasis"><em>resource_id</em></span></code> command. As with the <code class="literal">pcs resource cleanup</code> command, you can run the <code class="literal">pcs resource refresh</code> command with no options specified to reset the resource status and <code class="literal">failcount</code> value for all resources.
			</p><p>
				Both the <code class="literal">pcs resource cleanup</code> and the <code class="literal">pcs resource refresh</code> commands clear the operation history for a resource and re-detect the current state of the resource. The <code class="literal">pcs resource cleanup</code> command operates only on resources with failed actions as shown in the cluster status, while the <code class="literal">pcs resource refresh</code> command operates on resources regardless of their current state.
			</p></section><section class="section" id="proc_moving-cluster-resources-managing-cluster-resources"><div class="titlepage"><div><div><h3 class="title">17.5. Moving resources in a cluster</h3></div></div></div><p class="_abstract _abstract">
				Pacemaker provides a variety of mechanisms for configuring a resource to move from one node to another and to manually move a resource when needed.
			</p><p>
				You can manually move resources in a cluster with the <code class="literal command">pcs resource move</code> and <code class="literal command">pcs resource relocate</code> commands, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_manually-move-resources-cluster-maintenance">Manually moving cluster resources</a>. In addition to these commands, you can also control the behavior of cluster resources by enabling, disabling, and banning resources, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_disabling-resources-cluster-maintenance">Disabling, enabling, and banning cluster resources</a>.
			</p><p>
				You can configure a resource so that it will move to a new node after a defined number of failures, and you can configure a cluster to move resources when external connectivity is lost.
			</p><section class="section" id="moving_resources_due_to_failure"><div class="titlepage"><div><div><h4 class="title">17.5.1. Moving resources due to failure</h4></div></div></div><p>
					When you create a resource, you can configure the resource so that it will move to a new node after a defined number of failures by setting the <code class="literal">migration-threshold</code> option for that resource. Once the threshold has been reached, this node will no longer be allowed to run the failed resource until:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The resource’s <code class="literal">failure-timeout</code> value is reached.
						</li><li class="listitem">
							The administrator manually resets the resource’s failure count by using the <code class="literal">pcs resource cleanup</code> command.
						</li></ul></div><p>
					The value of <code class="literal">migration-threshold</code> is set to <code class="literal">INFINITY</code> by default. <code class="literal">INFINITY</code> is defined internally as a very large but finite number. A value of 0 disables the <code class="literal">migration-threshold</code> feature.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						Setting a <code class="literal">migration-threshold</code> for a resource is not the same as configuring a resource for migration, in which the resource moves to another location without loss of state.
					</p></div></rh-alert><p>
					The following example adds a migration threshold of 10 to the resource named <code class="literal">dummy_resource</code>, which indicates that the resource will move to a new node after 10 failures.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource meta dummy_resource migration-threshold=10</strong></span></pre><p>
					You can add a migration threshold to the defaults for the whole cluster with the following command.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource defaults update migration-threshold=10</strong></span></pre><p>
					To determine the resource’s current failure status and limits, use the <code class="literal">pcs resource failcount show</code> command.
				</p><p>
					There are two exceptions to the migration threshold concept; they occur when a resource either fails to start or fails to stop. If the cluster property <code class="literal">start-failure-is-fatal</code> is set to <code class="literal">true</code> (which is the default), start failures cause the <code class="literal">failcount</code> to be set to <code class="literal">INFINITY</code> and always cause the resource to move immediately.
				</p><p>
					Stop failures are slightly different and crucial. If a resource fails to stop and STONITH is enabled, then the cluster will fence the node to be able to start the resource elsewhere. If STONITH is not enabled, then the cluster has no way to continue and will not try to start the resource elsewhere, but will try to stop it again after the failure timeout.
				</p></section><section class="section" id="moving_resources_due_to_connectivity_changes"><div class="titlepage"><div><div><h4 class="title">17.5.2. Moving resources due to connectivity changes</h4></div></div></div><p>
					Setting up the cluster to move resources when external connectivity is lost is a two step process.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Add a <code class="literal">ping</code> resource to the cluster. The <code class="literal">ping</code> resource uses the system utility of the same name to test if a list of machines (specified by DNS host name or IPv4/IPv6 address) are reachable and uses the results to maintain a node attribute called <code class="literal">pingd</code>.
						</li><li class="listitem">
							Configure a location constraint for the resource that will move the resource to a different node when connectivity is lost.
						</li></ol></div><p>
					The following table describes the properties you can set for a <code class="literal">ping</code> resource.
				</p><rh-table id="tb-pingoptions-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 17.1. Properties of a ping resources</caption><colgroup><col style="width: 50%; " class="col_1"><!--Empty--><col style="width: 50%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686140959024" scope="col">Field</th><th align="left" valign="top" id="idm140686140957936" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686140959024"> <p>
									<code class="literal">dampen</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140957936"> <p>
									The time to wait (dampening) for further changes to occur. This prevents a resource from bouncing around the cluster when cluster nodes notice the loss of connectivity at slightly different times.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686140959024"> <p>
									<code class="literal">multiplier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140957936"> <p>
									The number of connected ping nodes gets multiplied by this value to get a score. Useful when there are multiple ping nodes configured.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686140959024"> <p>
									<code class="literal">host_list</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140957936"> <p>
									The machines to contact to determine the current connectivity status. Allowed values include resolvable DNS host names, IPv4 and IPv6 addresses. The entries in the host list are space separated.
								</p>
								 </td></tr></tbody></table></rh-table><p>
					The following example command creates a <code class="literal">ping</code> resource that verifies connectivity to <code class="literal">gateway.example.com</code>. In practice, you would verify connectivity to your network gateway/router. You configure the <code class="literal">ping</code> resource as a clone so that the resource will run on all cluster nodes.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create ping ocf:pacemaker:ping dampen=5s multiplier=1000 host_list=gateway.example.com clone</strong></span></pre><p>
					The following example configures a location constraint rule for the existing resource named <code class="literal">Webserver</code>. This will cause the <code class="literal">Webserver</code> resource to move to a host that is able to ping <code class="literal">gateway.example.com</code> if the host that it is currently running on cannot ping <code class="literal">gateway.example.com</code>.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint location Webserver rule score=-INFINITY pingd lt 1 or not_defined pingd</strong></span></pre></section></section><section class="section" id="proc_disabling-monitor-operationmanaging-cluster-resources"><div class="titlepage"><div><div><h3 class="title">17.6. Disabling a monitor operation</h3></div></div></div><p class="_abstract _abstract">
				The easiest way to stop a recurring monitor is to delete it. However, there can be times when you only want to disable it temporarily. In such cases, add <code class="literal">enabled="false"</code> to the operation’s definition. When you want to reinstate the monitoring operation, set <code class="literal">enabled="true"</code> to the operation’s definition.
			</p><p>
				When you update a resource’s operation with the <code class="literal">pcs resource update</code> command, any options you do not specifically call out are reset to their default values. For example, if you have configured a monitoring operation with a custom timeout value of 600, running the following commands will reset the timeout value to the default value of 20 (or whatever you have set the default value to with the <code class="literal">pcs resource op defaults</code> command).
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource update resourceXZY op monitor enabled=false</strong></span>
# <span class="strong strong"><strong>pcs resource update resourceXZY op monitor enabled=true</strong></span></pre><p>
				In order to maintain the original value of 600 for this option, when you reinstate the monitoring operation you must specify that value, as in the following example.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource update resourceXZY op monitor timeout=600 enabled=true</strong></span></pre></section><section class="section" id="proc_tagging-cluster-resources-managing-cluster-resources"><div class="titlepage"><div><div><h3 class="title">17.7. Configuring and managing cluster resource tags</h3></div></div></div><p class="_abstract _abstract">
				You can use the <code class="literal">pcs</code> command to tag cluster resources. This allows you to enable, disable, manage, or unmanage a specified set of resources with a single command.
			</p><section class="section" id="tagging_cluster_resources_for_administration_by_category"><div class="titlepage"><div><div><h4 class="title">17.7.1. Tagging cluster resources for administration by category</h4></div></div></div><p>
					The following procedure tags two resources with a resource tag and disables the tagged resources. In this example, the existing resources to be tagged are named <code class="literal">d-01</code> and <code class="literal">d-02</code>.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a tag named <code class="literal">special-resources</code> for resources <code class="literal">d-01</code> and <code class="literal">d-02</code>.
						</p><pre class="literallayout">[root@node-01]# <span class="strong strong"><strong>pcs tag create special-resources d-01 d-02</strong></span></pre></li><li class="listitem"><p class="simpara">
							Display the resource tag configuration.
						</p><pre class="literallayout">[root@node-01]# <span class="strong strong"><strong>pcs tag config</strong></span>
special-resources
  d-01
  d-02</pre></li><li class="listitem"><p class="simpara">
							Disable all resources that are tagged with the <code class="literal">special-resources</code> tag.
						</p><pre class="literallayout">[root@node-01]# <span class="strong strong"><strong>pcs resource disable special-resources</strong></span></pre></li><li class="listitem"><p class="simpara">
							Display the status of the resources to confirm that resources <code class="literal">d-01</code> and <code class="literal">d-02</code> are disabled.
						</p><pre class="literallayout">[root@node-01]# <span class="strong strong"><strong>pcs resource</strong></span>
  * d-01        (ocf::pacemaker:Dummy): Stopped (disabled)
  * d-02        (ocf::pacemaker:Dummy): Stopped (disabled)</pre></li></ol></div><p>
					In addition to the <code class="literal">pcs resource disable</code> command, the <code class="literal">pcs resource enable</code>, <code class="literal">pcs resource manage</code>, and <code class="literal">pcs resource unmanage</code> commands support the administration of tagged resources.
				</p><p>
					After you have created a resource tag:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You can delete a resource tag with the <code class="literal">pcs tag delete</code> command.
						</li><li class="listitem">
							You can modify resource tag configuration for an existing resource tag with the <code class="literal">pcs tag update</code> command.
						</li></ul></div></section><section class="section" id="deleting_a_tagged_cluster_resource"><div class="titlepage"><div><div><h4 class="title">17.7.2. Deleting a tagged cluster resource</h4></div></div></div><p>
					You cannot delete a tagged cluster resource with the <code class="literal">pcs</code> command. To delete a tagged resource, use the following procedure.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Remove the resource tag.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									The following command removes the resource tag <code class="literal">special-resources</code> from all resources with that tag,
								</p><pre class="literallayout">[root@node-01]# <span class="strong strong"><strong>pcs tag remove special-resources</strong></span>
[root@node-01]# <span class="strong strong"><strong>pcs tag</strong></span>
 No tags defined</pre></li><li class="listitem"><p class="simpara">
									The following command removes the resource tag <code class="literal">special-resources</code> from the resource <code class="literal">d-01</code> only.
								</p><pre class="literallayout">[root@node-01]# <span class="strong strong"><strong>pcs tag update special-resources remove d-01</strong></span></pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Delete the resource.
						</p><pre class="literallayout">[root@node-01]# <span class="strong strong"><strong>pcs resource delete d-01</strong></span>
Attempting to stop: d-01... Stopped</pre></li></ol></div></section></section></section><section class="chapter" id="assembly_creating-multinode-resources-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 18. Creating cluster resources that are active on multiple nodes (cloned resources)</h2></div></div></div><p class="_abstract _abstract">
			You can clone a cluster resource so that the resource can be active on multiple nodes. For example, you can use cloned resources to configure multiple instances of an IP resource to distribute throughout a cluster for node balancing. You can clone any resource provided the resource agent supports it. A clone consists of one resource or one resource group.
		</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
				Only resources that can be active on multiple nodes at the same time are suitable for cloning. For example, a <code class="literal">Filesystem</code> resource mounting a non-clustered file system such as <code class="literal">ext4</code> from a shared memory device should not be cloned. Since the <code class="literal">ext4</code> partition is not cluster aware, this file system is not suitable for read/write operations occurring from multiple nodes at the same time.
			</p></div></rh-alert><section class="section" id="proc_creating-cloned-resource-creating-multinode-resources"><div class="titlepage"><div><div><h3 class="title">18.1. Creating and removing a cloned resource</h3></div></div></div><p class="_abstract _abstract">
				You can create a resource and a clone of that resource at the same time.
			</p><p>
				To create a resource and clone of the resource with the following single command.
			</p><pre class="literallayout">pcs resource create <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>standard</em></span>:[<span class="emphasis"><em>provider</em></span>:]]<span class="emphasis"><em>type</em></span> [<span class="emphasis"><em>resource options</em></span>] [meta <span class="emphasis"><em>resource meta options</em></span>] clone [<span class="emphasis"><em>clone_id</em></span>] [<span class="emphasis"><em>clone options</em></span>]</pre><p>
				You can set a custom name for the clone by specifying a value for the <span class="emphasis"><em>clone_id</em></span> option.
			</p><p>
				You cannot create a resource group and a clone of that resource group in a single command.
			</p><p>
				Alternately, you can create a clone of a previously-created resource or resource group with the following command.
			</p><pre class="literallayout">pcs resource clone <span class="emphasis"><em>resource_id</em></span> | <span class="emphasis"><em>group_id</em></span> [<span class="emphasis"><em>clone_id</em></span>][<span class="emphasis"><em>clone options</em></span>]...</pre><p>
				By default, the name of the clone will be <code class="literal"><span class="emphasis"><em>resource_id</em></span>-clone</code> or <code class="literal"><span class="emphasis"><em>group_name</em></span>-clone</code>. You can set a custom name for the clone by specifying a value for the <span class="emphasis"><em>clone_id</em></span> option.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					You need to configure resource configuration changes on one node only.
				</p></div></rh-alert><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					When configuring constraints, always use the name of the group or clone.
				</p></div></rh-alert><p>
				When you create a clone of a resource, by default the clone takes on the name of the resource with <code class="literal">-clone</code> appended to the name. The following command creates a resource of type <code class="literal">apache</code> named <code class="literal">webfarm</code> and a clone of that resource named <code class="literal">webfarm-clone</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create webfarm apache clone</strong></span></pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					When you create a resource or resource group clone that will be ordered after another clone, you should almost always set the <code class="literal">interleave=true</code> option. This ensures that copies of the dependent clone can stop or start when the clone it depends on has stopped or started on the same node. If you do not set this option, if a cloned resource B depends on a cloned resource A and a node leaves the cluster, when the node returns to the cluster and resource A starts on that node, then all of the copies of resource B on all of the nodes will restart. This is because when a dependent cloned resource does not have the <code class="literal">interleave</code> option set, all instances of that resource depend on any running instance of the resource it depends on.
				</p></div></rh-alert><p>
				Use the following command to remove a clone of a resource or a resource group. This does not remove the resource or resource group itself.
			</p><pre class="literallayout">pcs resource unclone <span class="emphasis"><em>resource_id</em></span> | <span class="emphasis"><em>clone_id</em></span> | <span class="emphasis"><em>group_name</em></span></pre><p>
				The following table describes the options you can specify for a cloned resource.
			</p><rh-table id="tb-resourcecloneoptions-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 18.1. Resource Clone Options</caption><colgroup><col style="width: 50%; " class="col_1"><!--Empty--><col style="width: 50%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686140682704" scope="col">Field</th><th align="left" valign="top" id="idm140686140681616" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686140682704"> <p>
								<code class="literal">priority, target-role, is-managed</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686140681616"> <p>
								Options inherited from resource that is being cloned, as described in the "Resource Meta Options" table in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters#proc_configuring-resource-meta-options-configuring-cluster-resources">Configuring resource meta options</a>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686140682704"> <p>
								<code class="literal">clone-max</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686140681616"> <p>
								How many copies of the resource to start. Defaults to the number of nodes in the cluster.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686140682704"> <p>
								<code class="literal">clone-node-max</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686140681616"> <p>
								How many copies of the resource can be started on a single node; the default value is <code class="literal">1</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686140682704"> <p>
								<code class="literal">notify</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686140681616"> <p>
								When stopping or starting a copy of the clone, tell all the other copies beforehand and when the action was successful. Allowed values: <code class="literal">false</code>, <code class="literal">true</code>. The default value is <code class="literal">false</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686140682704"> <p>
								<code class="literal">globally-unique</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686140681616"> <p>
								Does each copy of the clone perform a different function? Allowed values: <code class="literal">false</code>, <code class="literal">true</code>
							</p>
							 <p>
								If the value of this option is <code class="literal">false</code>, these resources behave identically everywhere they are running and thus there can be only one copy of the clone active per machine.
							</p>
							 <p>
								If the value of this option is <code class="literal">true</code>, a copy of the clone running on one machine is not equivalent to another instance, whether that instance is running on another node or on the same node. The default value is <code class="literal">true</code> if the value of <code class="literal">clone-node-max</code> is greater than one; otherwise the default value is <code class="literal">false</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686140682704"> <p>
								<code class="literal">ordered</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686140681616"> <p>
								Should the copies be started in series (instead of in parallel). Allowed values: <code class="literal">false</code>, <code class="literal">true</code>. The default value is <code class="literal">false</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686140682704"> <p>
								<code class="literal">interleave</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686140681616"> <p>
								Changes the behavior of ordering constraints (between clones) so that copies of the first clone can start or stop as soon as the copy on the same node of the second clone has started or stopped (rather than waiting until every instance of the second clone has started or stopped). Allowed values: <code class="literal">false</code>, <code class="literal">true</code>. The default value is <code class="literal">false</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686140682704"> <p>
								<code class="literal">clone-min</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686140681616"> <p>
								If a value is specified, any clones which are ordered after this clone will not be able to start until the specified number of instances of the original clone are running, even if the <code class="literal">interleave</code> option is set to <code class="literal">true</code>.
							</p>
							 </td></tr></tbody></table></rh-table><p>
				To achieve a stable allocation pattern, clones are slightly sticky by default, which indicates that they have a slight preference for staying on the node where they are running. If no value for <code class="literal">resource-stickiness</code> is provided, the clone will use a value of 1. Being a small value, it causes minimal disturbance to the score calculations of other resources but is enough to prevent Pacemaker from needlessly moving copies around the cluster. For information about setting the <code class="literal">resource-stickiness</code> resource meta-option, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters#proc_configuring-resource-meta-options-configuring-cluster-resources">Configuring resource meta options</a>.
			</p></section><section class="section" id="proc_configuring-clone-constraints-creating-multinode-resources"><div class="titlepage"><div><div><h3 class="title">18.2. Configuring clone resource constraints</h3></div></div></div><p class="_abstract _abstract">
				In most cases, a clone will have a single copy on each active cluster node. You can, however, set <code class="literal">clone-max</code> for the resource clone to a value that is less than the total number of nodes in the cluster. If this is the case, you can indicate which nodes the cluster should preferentially assign copies to with resource location constraints. These constraints are written no differently to those for regular resources except that the clone’s id must be used.
			</p><p>
				The following command creates a location constraint for the cluster to preferentially assign resource clone <code class="literal">webfarm-clone</code> to <code class="literal">node1</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint location webfarm-clone prefers node1</strong></span></pre><p>
				Ordering constraints behave slightly differently for clones. In the example below, because the <code class="literal">interleave</code> clone option is left to default as <code class="literal">false</code>, no instance of <code class="literal">webfarm-stats</code> will start until all instances of <code class="literal">webfarm-clone</code> that need to be started have done so. Only if no copies of <code class="literal">webfarm-clone</code> can be started then <code class="literal">webfarm-stats</code> will be prevented from being active. Additionally, <code class="literal">webfarm-clone</code> will wait for <code class="literal">webfarm-stats</code> to be stopped before stopping itself.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint order start webfarm-clone then webfarm-stats</strong></span></pre><p>
				Colocation of a regular (or group) resource with a clone means that the resource can run on any machine with an active copy of the clone. The cluster will choose a copy based on where the clone is running and the resource’s own location preferences.
			</p><p>
				Colocation between clones is also possible. In such cases, the set of allowed locations for the clone is limited to nodes on which the clone is (or will be) active. Allocation is then performed as normally.
			</p><p>
				The following command creates a colocation constraint to ensure that the resource <code class="literal">webfarm-stats</code> runs on the same node as an active copy of <code class="literal">webfarm-clone</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs constraint colocation add webfarm-stats with webfarm-clone</strong></span></pre></section><section class="section" id="proc_creating-promotable-clone-resources-creating-multinode-resources"><div class="titlepage"><div><div><h3 class="title">18.3. Promotable clone resources</h3></div></div></div><p class="_abstract _abstract">
				Promotable clone resources are clone resources with the <code class="literal">promotable</code> meta attribute set to <code class="literal">true</code>. They allow the instances to be in one of two operating modes; these are called <code class="literal">promoted</code> and <code class="literal">unpromoted</code>. The names of the modes do not have specific meanings, except for the limitation that when an instance is started, it must come up in the <code class="literal">Unpromoted</code> state. Note: The Promoted and Unpromoted role names are the functional equivalent of the Master and Slave Pacemaker roles in previous RHEL releases.
			</p><section class="section" id="creating_a_promotable_clone_resource"><div class="titlepage"><div><div><h4 class="title">18.3.1. Creating a promotable clone resource</h4></div></div></div><p>
					You can create a resource as a promotable clone with the following single command.
				</p><pre class="literallayout">pcs resource create <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>standard</em></span>:[<span class="emphasis"><em>provider</em></span>:]]<span class="emphasis"><em>type</em></span> [<span class="emphasis"><em>resource options</em></span>] promotable [<span class="emphasis"><em>clone_id</em></span>] [<span class="emphasis"><em>clone options</em></span>]</pre><p>
					By default, the name of the promotable clone will be <code class="literal"><span class="emphasis"><em>resource_id</em></span>-clone</code>.
				</p><p>
					You can set a custom name for the clone by specifying a value for the <span class="emphasis"><em>clone_id</em></span> option.
				</p><p>
					Alternately, you can create a promotable resource from a previously-created resource or resource group with the following command.
				</p><pre class="literallayout">pcs resource promotable <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>clone_id</em></span>] [<span class="emphasis"><em>clone options</em></span>]</pre><p>
					By default, the name of the promotable clone will be <code class="literal"><span class="emphasis"><em>resource_id</em></span>-clone</code> or <code class="literal"><span class="emphasis"><em>group_name</em></span>-clone</code>.
				</p><p>
					You can set a custom name for the clone by specifying a value for the <span class="emphasis"><em>clone_id</em></span> option.
				</p><p>
					The following table describes the extra clone options you can specify for a promotable resource.
				</p><rh-table id="tb-promotablecloneoptions-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 18.2. Extra Clone Options Available for Promotable Clones</caption><colgroup><col style="width: 50%; " class="col_1"><!--Empty--><col style="width: 50%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686140565952" scope="col">Field</th><th align="left" valign="top" id="idm140686140564864" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686140565952"> <p>
									<code class="literal">promoted-max</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140564864"> <p>
									How many copies of the resource can be promoted; default 1.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686140565952"> <p>
									<code class="literal">promoted-node-max</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140564864"> <p>
									How many copies of the resource can be promoted on a single node; default 1.
								</p>
								 </td></tr></tbody></table></rh-table></section><section class="section" id="configuring_promotable_resource_constraints"><div class="titlepage"><div><div><h4 class="title">18.3.2. Configuring promotable resource constraints</h4></div></div></div><p>
					In most cases, a promotable resource will have a single copy on each active cluster node. If this is not the case, you can indicate which nodes the cluster should preferentially assign copies to with resource location constraints. These constraints are written no differently than those for regular resources.
				</p><p>
					You can create a colocation constraint which specifies whether the resources are operating in a promoted or unpromoted role. The following command creates a resource colocation constraint.
				</p><pre class="literallayout">pcs constraint colocation add [promoted|unpromoted] <span class="emphasis"><em>source_resource</em></span> with [promoted|unpromoted] <span class="emphasis"><em>target_resource</em></span> [<span class="emphasis"><em>score</em></span>] [<span class="emphasis"><em>options</em></span>]</pre><p>
					For information about colocation constraints, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters">Colocating cluster resources</a>.
				</p><p>
					When configuring an ordering constraint that includes promotable resources, one of the actions that you can specify for the resources is <code class="literal">promote</code>, indicating that the resource be promoted from unpromoted role to promoted role. Additionally, you can specify an action of <code class="literal">demote</code>, indicated that the resource be demoted from promoted role to unpromoted role.
				</p><p>
					The command for configuring an order constraint is as follows.
				</p><pre class="literallayout">pcs constraint order [<span class="emphasis"><em>action</em></span>] <span class="emphasis"><em>resource_id</em></span> then [<span class="emphasis"><em>action</em></span>] <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>options</em></span>]</pre><p>
					For information about resource order constraints, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters">Determining the order in which cluster resources are run</a>.
				</p></section></section><section class="section" id="proc_recovering-promoted-node-creating-multinode-resources"><div class="titlepage"><div><div><h3 class="title">18.4. Demoting a promoted resource on failure</h3></div></div></div><p class="_abstract _abstract">
				You can configure a promotable resource so that when a <code class="literal">promote</code> or <code class="literal">monitor</code> action fails for that resource, or the partition in which the resource is running loses quorum, the resource will be demoted but will not be fully stopped. This can prevent the need for manual intervention in situations where fully stopping the resource would require it.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To configure a promotable resource to be demoted when a <code class="literal">promote</code> action fails, set the <code class="literal">on-fail</code> operation meta option to <code class="literal">demote</code>, as in the following example.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource op add my-rsc promote on-fail="demote"</strong></span></pre></li><li class="listitem"><p class="simpara">
						To configure a promotable resource to be demoted when a <code class="literal">monitor</code> action fails, set <code class="literal">interval</code> to a nonzero value, set the <code class="literal">on-fail</code> operation meta option to <code class="literal">demote</code>, and set <code class="literal">role</code> to <code class="literal">Promoted</code>, as in the following example.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource op add my-rsc monitor interval="10s" on-fail="demote" role="Promoted"</strong></span></pre></li><li class="listitem">
						To configure a cluster so that when a cluster partition loses quorum any promoted resources will be demoted but left running and all other resources will be stopped, set the <code class="literal">no-quorum-policy</code> cluster property to <code class="literal">demote</code>
					</li></ul></div><p>
				Setting the <code class="literal">on-fail</code> meta-attribute to <code class="literal">demote</code> for an operation does not affect how promotion of a resource is determined. If the affected node still has the highest promotion score, it will be selected to be promoted again.
			</p></section></section><section class="chapter" id="assembly_clusternode-management-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 19. Managing cluster nodes</h2></div></div></div><p class="_abstract _abstract">
			There are a variety of <code class="literal">pcs</code> commands you can use to manage cluster nodes, including commands to start and stop cluster services and to add and remove cluster nodes.
		</p><section class="section" id="proc_cluster-stop-clusternode-management"><div class="titlepage"><div><div><h3 class="title">19.1. Stopping cluster services</h3></div></div></div><p class="_abstract _abstract">
				The following command stops cluster services on the specified node or nodes. As with the <code class="literal command">pcs cluster start</code>, the <code class="literal">--all</code> option stops cluster services on all nodes and if you do not specify any nodes, cluster services are stopped on the local node only.
			</p><pre class="literallayout">pcs cluster stop [--all | <span class="emphasis"><em>node</em></span>] [...]</pre><p>
				You can force a stop of cluster services on the local node with the following command, which performs a <code class="literal command">kill -9</code> command.
			</p><pre class="literallayout">pcs cluster kill</pre></section><section class="section" id="proc_cluster-enable-clusternode-management"><div class="titlepage"><div><div><h3 class="title">19.2. Enabling and disabling cluster services</h3></div></div></div><p class="_abstract _abstract">
				Enable the cluster services with the following command. This configures the cluster services to run on startup on the specified node or nodes.
			</p><p>
				Enabling allows nodes to automatically rejoin the cluster after they have been fenced, minimizing the time the cluster is at less than full strength. If the cluster services are not enabled, an administrator can manually investigate what went wrong before starting the cluster services manually, so that, for example, a node with hardware issues in not allowed back into the cluster when it is likely to fail again.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						If you specify the <code class="literal">--all</code> option, the command enables cluster services on all nodes.
					</li><li class="listitem">
						If you do not specify any nodes, cluster services are enabled on the local node only.
					</li></ul></div><pre class="literallayout">pcs cluster enable [--all | <span class="emphasis"><em>node</em></span>] [...]</pre><p>
				Use the following command to configure the cluster services not to run on startup on the specified node or nodes.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						If you specify the <code class="literal">--all</code> option, the command disables cluster services on all nodes.
					</li><li class="listitem">
						If you do not specify any nodes, cluster services are disabled on the local node only.
					</li></ul></div><pre class="literallayout">pcs cluster disable [--all | <span class="emphasis"><em>node</em></span>] [...]</pre></section><section class="section" id="proc_cluster-nodeadd-clusternode-management"><div class="titlepage"><div><div><h3 class="title">19.3. Adding cluster nodes</h3></div></div></div><p class="_abstract _abstract">
				Add a new node to an existing cluster with the following procedure.
			</p><p>
				This procedure adds standard clusters nodes running <code class="literal">corosync</code>. For information about integrating non-corosync nodes into a cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_remote-node-management-configuring-and-managing-high-availability-clusters">Integrating non-corosync nodes into a cluster: the pacemaker_remote service</a>.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					It is recommended that you add nodes to existing clusters only during a production maintenance window. This allows you to perform appropriate resource and deployment testing for the new node and its fencing configuration.
				</p></div></rh-alert><p>
				In this example, the existing cluster nodes are <code class="literal">clusternode-01.example.com</code>, <code class="literal">clusternode-02.example.com</code>, and <code class="literal">clusternode-03.example.com</code>. The new node is <code class="literal">newnode.example.com</code>.
			</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					On the new node to add to the cluster, perform the following tasks.
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Install the cluster packages. If the cluster uses SBD, the Booth ticket manager, or a quorum device, you must manually install the respective packages (<code class="literal">sbd</code>, <code class="literal">booth-site</code>, <code class="literal">corosync-qdevice</code>) on the new node as well.
					</p><pre class="literallayout">[root@newnode ~]# <span class="strong strong"><strong>dnf install -y pcs fence-agents-all</strong></span></pre><p class="simpara">
						In addition to the cluster packages, you will also need to install and configure all of the services that you are running in the cluster, which you have installed on the existing cluster nodes. For example, if you are running an Apache HTTP server in a Red Hat high availability cluster, you will need to install the server on the node you are adding, as well as the <code class="literal">wget</code> tool that checks the status of the server.
					</p></li><li class="listitem"><p class="simpara">
						If you are running the <code class="literal command">firewalld</code> daemon, execute the following commands to enable the ports that are required by the Red Hat High Availability Add-On.
					</p><pre class="literallayout"># <span class="strong strong"><strong>firewall-cmd --permanent --add-service=high-availability</strong></span>
# <span class="strong strong"><strong>firewall-cmd --add-service=high-availability</strong></span></pre></li><li class="listitem"><p class="simpara">
						Set a password for the user ID <code class="literal">hacluster</code>. It is recommended that you use the same password for each node in the cluster.
					</p><pre class="literallayout">[root@newnode ~]# <span class="strong strong"><strong>passwd hacluster</strong></span>
Changing password for user hacluster.
New password:
Retype new password:
passwd: all authentication tokens updated successfully.</pre></li><li class="listitem"><p class="simpara">
						Execute the following commands to start the <code class="literal">pcsd</code> service and to enable <code class="literal">pcsd</code> at system start.
					</p><pre class="literallayout"># <span class="strong strong"><strong>systemctl start pcsd.service</strong></span>
# <span class="strong strong"><strong>systemctl enable pcsd.service</strong></span></pre></li></ol></div><p>
				On a node in the existing cluster, perform the following tasks.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Authenticate user <code class="literal">hacluster</code> on the new cluster node.
					</p><pre class="literallayout">[root@clusternode-01 ~]# <span class="strong strong"><strong>pcs host auth newnode.example.com</strong></span>
Username: hacluster
Password:
newnode.example.com: Authorized</pre></li><li class="listitem"><p class="simpara">
						Add the new node to the existing cluster. This command also syncs the cluster configuration file <code class="literal">corosync.conf</code> to all nodes in the cluster, including the new node you are adding.
					</p><pre class="literallayout">[root@clusternode-01 ~]# <span class="strong strong"><strong>pcs cluster node add newnode.example.com</strong></span></pre></li></ol></div><p>
				On the new node to add to the cluster, perform the following tasks.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Start and enable cluster services on the new node.
					</p><pre class="literallayout">[root@newnode ~]# <span class="strong strong"><strong>pcs cluster start</strong></span>
Starting Cluster...
[root@newnode ~]# <span class="strong strong"><strong>pcs cluster enable</strong></span></pre></li><li class="listitem">
						Ensure that you configure and test a fencing device for the new cluster node.
					</li></ol></div></section><section class="section" id="proc_cluster-noderemove-clusternode-management"><div class="titlepage"><div><div><h3 class="title">19.4. Removing cluster nodes</h3></div></div></div><p class="_abstract _abstract">
				The following command shuts down the specified node and removes it from the cluster configuration file, <code class="literal">corosync.conf</code>, on all of the other nodes in the cluster.
			</p><pre class="literallayout">pcs cluster node remove <span class="emphasis"><em>node</em></span></pre></section><section class="section" id="proc_add-nodes-to-multiple-ip-cluster-clusternode-management"><div class="titlepage"><div><div><h3 class="title">19.5. Adding a node to a cluster with multiple links</h3></div></div></div><p class="_abstract _abstract">
				When adding a node to a cluster with multiple links, you must specify addresses for all links.
			</p><p>
				The following example adds the node <code class="literal">rh80-node3</code> to a cluster, specifying IP address 192.168.122.203 for the first link and 192.168.123.203 as the second link.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster node add rh80-node3 addr=192.168.122.203 addr=192.168.123.203</strong></span></pre></section><section class="section" id="proc_changing-links-in-multiple-ip-cluster-clusternode-management"><div class="titlepage"><div><div><h3 class="title">19.6. Adding and modifying links in an existing cluster</h3></div></div></div><p class="_abstract _abstract">
				In most cases, you can add or modify the links in an existing cluster without restarting the cluster.
			</p><section class="section" id="adding_and_removing_links_in_an_existing_cluster"><div class="titlepage"><div><div><h4 class="title">19.6.1. Adding and removing links in an existing cluster</h4></div></div></div><p>
					To add a new link to a running cluster, use the <code class="literal">pcs cluster link add</code> command.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							When adding a link, you must specify an address for each node.
						</li><li class="listitem">
							Adding and removing a link is only possible when you are using the <code class="literal">knet</code> transport protocol.
						</li><li class="listitem">
							At least one link in the cluster must be defined at any time.
						</li><li class="listitem">
							The maximum number of links in a cluster is 8, numbered 0-7. It does not matter which links are defined, so, for example, you can define only links 3, 6 and 7.
						</li><li class="listitem">
							When you add a link without specifying its link number, <code class="literal">pcs</code> uses the lowest link available.
						</li><li class="listitem">
							The link numbers of currently configured links are contained in the <code class="literal">corosync.conf</code> file. To display the <code class="literal">corosync.conf</code> file, run the <code class="literal">pcs cluster corosync</code> command or the <code class="literal">pcs cluster config show</code> command.
						</li></ul></div><p>
					The following command adds link number 5 to a three node cluster.
				</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link add node1=10.0.5.11 node2=10.0.5.12 node3=10.0.5.31 options linknumber=5</strong></span></pre><p>
					To remove an existing link, use the <code class="literal">pcs cluster link delete</code> or <code class="literal">pcs cluster link remove</code> command. Either of the following commands will remove link number 5 from the cluster.
				</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link delete 5</strong></span>

[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link remove 5</strong></span></pre></section><section class="section" id="modifying_a_link_in_a_cluster_with_multiple_links"><div class="titlepage"><div><div><h4 class="title">19.6.2. Modifying a link in a cluster with multiple links</h4></div></div></div><p>
					If there are multiple links in the cluster and you want to change one of them, perform the following procedure.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Remove the link you want to change.
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link remove 2</strong></span></pre></li><li class="listitem"><p class="simpara">
							Add the link back to the cluster with the updated addresses and options.
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link add node1=10.0.5.11 node2=10.0.5.12 node3=10.0.5.31 options linknumber=2</strong></span></pre></li></ol></div></section><section class="section" id="modifying_the_link_addresses_in_a_cluster_with_a_single_link"><div class="titlepage"><div><div><h4 class="title">19.6.3. Modifying the link addresses in a cluster with a single link</h4></div></div></div><p>
					If your cluster uses only one link and you want to modify that link to use different addresses, perform the following procedure. In this example, the original link is link 1.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a new link with the new addresses and options.
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link add node1=10.0.5.11 node2=10.0.5.12 node3=10.0.5.31 options linknumber=2</strong></span></pre></li><li class="listitem"><p class="simpara">
							Remove the original link.
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link remove 1</strong></span></pre></li></ol></div><p>
					Note that you cannot specify addresses that are currently in use when adding links to a cluster. This means, for example, that if you have a two-node cluster with one link and you want to change the address for one node only, you cannot use the above procedure to add a new link that specifies one new address and one existing address. Instead, you can add a temporary link before removing the existing link and adding it back with the updated address, as in the following example.
				</p><p>
					In this example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The link for the existing cluster is link 1, which uses the address 10.0.5.11 for node 1 and the address 10.0.5.12 for node 2.
						</li><li class="listitem">
							You would like to change the address for node 2 to 10.0.5.31.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To update only one of the addresses for a two-node cluster with a single link, use the following procedure.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a new temporary link to the existing cluster, using addresses that are not currently in use.
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link add node1=10.0.5.13 node2=10.0.5.14 options linknumber=2</strong></span></pre></li><li class="listitem"><p class="simpara">
							Remove the original link.
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link remove 1</strong></span></pre></li><li class="listitem"><p class="simpara">
							Add the new, modified link.
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link add node1=10.0.5.11 node2=10.0.5.31 options linknumber=1</strong></span></pre></li><li class="listitem"><p class="simpara">
							Remove the temporary link you created
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link remove 2</strong></span></pre></li></ol></div></section><section class="section" id="modifying_the_link_options_for_a_link_in_a_cluster_with_a_single_link"><div class="titlepage"><div><div><h4 class="title">19.6.4. Modifying the link options for a link in a cluster with a single link</h4></div></div></div><p>
					If your cluster uses only one link and you want to modify the options for that link but you do not want to change the address to use, you can add a temporary link before removing and updating the link to modify.
				</p><p>
					In this example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The link for the existing cluster is link 1, which uses the address 10.0.5.11 for node 1 and the address 10.0.5.12 for node 2.
						</li><li class="listitem">
							You would like to change the link option <code class="literal">link_priority</code> to 11.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						Modify the link option in a cluster with a single link with the following procedure.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a new temporary link to the existing cluster, using addresses that are not currently in use.
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link add node1=10.0.5.13 node2=10.0.5.14 options linknumber=2</strong></span></pre></li><li class="listitem"><p class="simpara">
							Remove the original link.
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link remove 1</strong></span></pre></li><li class="listitem"><p class="simpara">
							Add back the original link with the updated options.
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link add node1=10.0.5.11 node2=10.0.5.12 options linknumber=1 link_priority=11</strong></span></pre></li><li class="listitem"><p class="simpara">
							Remove the temporary link.
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link remove 2</strong></span></pre></li></ol></div></section><section class="section" id="modifying_a_link_when_adding_a_new_link_is_not_possible"><div class="titlepage"><div><div><h4 class="title">19.6.5. Modifying a link when adding a new link is not possible</h4></div></div></div><p>
					If for some reason adding a new link is not possible in your configuration and your only option is to modify a single existing link, you can use the following procedure, which requires that you shut your cluster down.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						The following example procedure updates link number 1 in the cluster and sets the <code class="literal">link_priority</code> option for the link to 11.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Stop the cluster services for the cluster.
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster stop --all</strong></span></pre></li><li class="listitem"><p class="simpara">
							Update the link addresses and options.
						</p><p class="simpara">
							The <code class="literal">pcs cluster link update</code> command does not require that you specify all of the node addresses and options. Instead, you can specify only the addresses to change. This example modifies the addresses for <code class="literal">node1</code> and <code class="literal">node3</code> and the <code class="literal">link_priority</code> option only.
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster link update 1 node1=10.0.5.11 node3=10.0.5.31 options link_priority=11</strong></span></pre><p class="simpara">
							To remove an option, you can set the option to a null value with the <code class="literal"><span class="emphasis"><em>option</em></span>=</code> format.
						</p></li><li class="listitem"><p class="simpara">
							Restart the cluster
						</p><pre class="literallayout">[root@node1 ~] # <span class="strong strong"><strong>pcs cluster start --all</strong></span></pre></li></ol></div></section></section><section class="section" id="proc_tracking-node-health-clusternode-management"><div class="titlepage"><div><div><h3 class="title">19.7. Configuring a node health strategy</h3></div></div></div><p>
				A node might be functioning well enough to maintain its cluster membership and yet be unhealthy in some respect that makes it an undesirable location for resources. For example, a disk drive might be reporting SMART errors, or the CPU might be highly loaded. As of RHEL 9.1, You can use a node health strategy in Pacemaker to automatically move resources off unhealthy nodes.
			</p><p>
				You can monitor a node’s health with the the following health node resource agents, which set node attributes based on CPU and disk status:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">ocf:pacemaker:HealthCPU</code>, which monitors CPU idling
					</li><li class="listitem">
						<code class="literal">ocf:pacemaker:HealthIOWait</code>, which monitors the CPU I/O wait
					</li><li class="listitem">
						<code class="literal">ocf:pacemaker:HealthSMART</code>, which monitors SMART status of a disk drive
					</li><li class="listitem">
						<code class="literal">ocf:pacemaker:SysInfo</code>, which sets a variety of node attributes with local system information and also functions as a health agent monitoring disk space usage
					</li></ul></div><p>
				Additionally, any resource agent might provide node attributes that can be used to define a health node strategy.
			</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					The following procedure configures a health node strategy for a cluster that will move resources off of any node whose CPU I/O wait goes above 15%.
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Set the <code class="literal">health-node-strategy</code> cluster property to define how Pacemaker responds to changes in node health.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs property set node-health-strategy=migrate-on-red</strong></span></pre></li><li class="listitem"><p class="simpara">
						Create a cloned cluster resource that uses a health node resource agent, setting the <code class="literal">allow-unhealthy-nodes</code> resource meta option to define whether the cluster will detect if the node’s health recovers and move resources back to the node. Configure this resource with a recurring monitor action, to continually check the health of all nodes.
					</p><p class="simpara">
						This example creates a <code class="literal">HealthIOWait</code> resource agent to monitor the CPU I/O wait, setting a red limit for moving resources off a node to 15%. This command sets the <code class="literal">allow-unhealthy-nodes</code> resource meta option to <code class="literal">true</code> and configures a recurring monitor interval of 10 seconds.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create io-monitor ocf:pacemaker:HealthIOWait red_limit=15 op monitor interval=10s meta allow-unhealthy-nodes=true clone</strong></span></pre></li></ol></div></section><section class="section" id="proc_configuring-large-clusters-clusternode-management"><div class="titlepage"><div><div><h3 class="title">19.8. Configuring a large cluster with many resources</h3></div></div></div><p class="_abstract _abstract">
				If the cluster you are deploying consists of a large number of nodes and many resources, you may need to modify the default values of the following parameters for your cluster.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">The <code class="literal">cluster-ipc-limit</code> cluster property</span></dt><dd><p class="simpara">
							The <code class="literal">cluster-ipc-limit</code> cluster property is the maximum IPC message backlog before one cluster daemon will disconnect another. When a large number of resources are cleaned up or otherwise modified simultaneously in a large cluster, a large number of CIB updates arrive at once. This could cause slower clients to be evicted if the Pacemaker service does not have time to process all of the configuration updates before the CIB event queue threshold is reached.
						</p><p class="simpara">
							The recommended value of <code class="literal">cluster-ipc-limit</code> for use in large clusters is the number of resources in the cluster multiplied by the number of nodes. This value can be raised if you see "Evicting client" messages for cluster daemon PIDs in the logs.
						</p><p class="simpara">
							You can increase the value of <code class="literal">cluster-ipc-limit</code> from its default value of 500 with the <code class="literal">pcs property set</code> command. For example, for a ten-node cluster with 200 resources you can set the value of <code class="literal">cluster-ipc-limit</code> to 2000 with the following command.
						</p><pre class="literallayout"># <span class="strong strong"><strong>pcs property set cluster-ipc-limit=2000</strong></span></pre></dd><dt><span class="term">The <code class="literal">PCMK_ipc_buffer</code> Pacemaker parameter</span></dt><dd><p class="simpara">
							On very large deployments, internal Pacemaker messages may exceed the size of the message buffer. When this occurs, you will see a message in the system logs of the following format:
						</p><pre class="literallayout">Compressed message exceeds <span class="emphasis"><em>X</em></span>% of configured IPC limit (<span class="emphasis"><em>X</em></span> bytes); consider setting PCMK_ipc_buffer to <span class="emphasis"><em>X</em></span> or higher</pre><p class="simpara">
							When you see this message, you can increase the value of <code class="literal">PCMK_ipc_buffer</code> in the <code class="literal">/etc/sysconfig/pacemaker</code> configuration file on each node. For example, to increase the value of <code class="literal">PCMK_ipc_buffer</code> from its default value to 13396332 bytes, change the uncommented <code class="literal">PCMK_ipc_buffer</code> field in the <code class="literal">/etc/sysconfig/pacemaker</code> file on each node in the cluster as follows.
						</p><pre class="literallayout">PCMK_ipc_buffer=13396332</pre><p class="simpara">
							To apply this change, run the following command.
						</p><pre class="literallayout"># <span class="strong strong"><strong>systemctl restart pacemaker</strong></span></pre></dd></dl></div></section></section><section class="chapter" id="assembly_cluster-permissions-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 20. Setting user permissions for a Pacemaker cluster</h2></div></div></div><p class="_abstract _abstract">
			You can grant permission for specific users other than user <code class="literal">hacluster</code> to manage a Pacemaker cluster. There are two sets of permissions that you can grant to individual users:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Permissions that allow individual users to manage the cluster through the Web UI and to run <code class="literal command">pcs</code> commands that connect to nodes over a network. Commands that connect to nodes over a network include commands to set up a cluster, or to add or remove nodes from a cluster.
				</li><li class="listitem">
					Permissions for local users to allow read-only or read-write access to the cluster configuration. Commands that do not require connecting over a network include commands that edit the cluster configuration, such as those that create resources and configure constraints.
				</li></ul></div><p>
			In situations where both sets of permissions have been assigned, the permissions for commands that connect over a network are applied first, and then permissions for editing the cluster configuration on the local node are applied. Most <code class="literal command">pcs</code> commands do not require network access and in those cases the network permissions will not apply.
		</p><section class="section" id="proc_setting-cluster-access-over-network-cluster-permissions"><div class="titlepage"><div><div><h3 class="title">20.1. Setting permissions for node access over a network</h3></div></div></div><p class="_abstract _abstract">
				To grant permission for specific users to manage the cluster through the Web UI and to run <code class="literal command">pcs</code> commands that connect to nodes over a network, add those users to the group <code class="literal">haclient</code>. This must be done on every node in the cluster.
			</p></section><section class="section" id="proc_setting-local-cluster-permissions-cluster-permissions"><div class="titlepage"><div><div><h3 class="title">20.2. Setting local permissions using ACLs</h3></div></div></div><p class="_abstract _abstract">
				You can use the <code class="literal command">pcs acl</code> command to set permissions for local users to allow read-only or read-write access to the cluster configuration by using access control lists (ACLs).
			</p><p>
				By default, ACLs are not enabled. When ACLs are not enabled, any user who is a member of the group <code class="literal">haclient</code> on all nodes has full local read/write access to the cluster configuration while users who are not members of <code class="literal">haclient</code> have no access. When ACLs are enabled, however, even users who are members of the <code class="literal">haclient</code> group have access only to what has been granted to that user by the ACLs. The root and <code class="literal">hacluster</code> user accounts always have full access to the cluster configuration, even when ACLs are enabled.
			</p><p>
				Setting permissions for local users is a two step process:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Execute the <code class="literal command">pcs acl role create…​</code> command to create a <span class="emphasis"><em>role</em></span> which defines the permissions for that role.
					</li><li class="listitem">
						Assign the role you created to a user with the <code class="literal command">pcs acl user create</code> command. If you assign multiple roles to the same user, any <code class="literal">deny</code> permission takes precedence, then <code class="literal">write</code>, then <code class="literal">read</code>.
					</li></ol></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					The following example procedure provides read-only access for a cluster configuration to a local user named <code class="literal">rouser</code>. Note that it is also possible to restrict access to certain portions of the configuration only.
				</p></div><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
					It is important to perform this procedure as root or to save all of the configuration updates to a working file which you can then push to the active CIB when you are finished. Otherwise, you can lock yourself out of making any further changes. For information on saving configuration updates to a working file, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_pcs-operation-configuring-and-managing-high-availability-clusters#proc_configure-testfile-pcs-operation">Saving a configuration change to a working file</a>.
				</p></div></rh-alert><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						This procedure requires that the user <code class="literal">rouser</code> exists on the local system and that the user <code class="literal">rouser</code> is a member of the group <code class="literal">haclient</code>.
					</p><pre class="literallayout"># <span class="strong strong"><strong>adduser rouser</strong></span>
# <span class="strong strong"><strong>usermod -a -G haclient rouser</strong></span></pre></li><li class="listitem"><p class="simpara">
						Enable Pacemaker ACLs with the <code class="literal command">pcs acl enable</code> command.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs acl enable</strong></span></pre></li><li class="listitem"><p class="simpara">
						Create a role named <code class="literal">read-only</code> with read-only permissions for the cib.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs acl role create read-only description="Read access to cluster" read xpath /cib</strong></span></pre></li><li class="listitem"><p class="simpara">
						Create the user <code class="literal">rouser</code> in the pcs ACL system and assign that user the <code class="literal">read-only</code> role.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs acl user create rouser read-only</strong></span></pre></li><li class="listitem"><p class="simpara">
						View the current ACLs.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs acl</strong></span>
User: rouser
  Roles: read-only
Role: read-only
  Description: Read access to cluster
  Permission: read xpath /cib (read-only-read)</pre></li><li class="listitem"><p class="simpara">
						On each node where <code class="literal">rouser</code> will run <code class="literal">pcs</code> commands, log in as <code class="literal">rouser</code> and authenticate to the local <code class="literal">pcsd</code> service. This is required in order to run certain <code class="literal">pcs</code> commands, such as <code class="literal">pcs status</code>, as the ACL user.
					</p><pre class="literallayout">[rouser ~]$ <span class="strong strong"><strong>pcs client local-auth</strong></span></pre></li></ol></div></section></section><section class="chapter" id="assembly_resource-monitoring-operations-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 21. Resource monitoring operations</h2></div></div></div><p class="_abstract _abstract">
			To ensure that resources remain healthy, you can add a monitoring operation to a resource’s definition. If you do not specify a monitoring operation for a resource, by default the <code class="literal command">pcs</code> command will create a monitoring operation, with an interval that is determined by the resource agent. If the resource agent does not provide a default monitoring interval, the pcs command will create a monitoring operation with an interval of 60 seconds.
		</p><p>
			The following table summarizes the properties of a resource monitoring operation.
		</p><rh-table id="idm140686144862608"><table class="lt-4-cols lt-7-rows"><caption>Table 21.1. Properties of an Operation</caption><colgroup><col style="width: 25%; " class="col_1"><!--Empty--><col style="width: 75%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686144857760" scope="col">Field</th><th align="left" valign="top" id="idm140686144856672" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686144857760"> <p>
							<code class="literal">id</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686144856672"> <p>
							Unique name for the action. The system assigns this when you configure an operation.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686144857760"> <p>
							<code class="literal">name</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686144856672"> <p>
							The action to perform. Common values: <code class="literal">monitor</code>, <code class="literal">start</code>, <code class="literal">stop</code>
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686144857760"> <p>
							<code class="literal">interval</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686144856672"> <p>
							If set to a nonzero value, a recurring operation is created that repeats at this frequency, in seconds. A nonzero value makes sense only when the action <code class="literal">name</code> is set to <code class="literal">monitor</code>. A recurring monitor action will be executed immediately after a resource start completes, and subsequent monitor actions are scheduled starting at the time the previous monitor action completed. For example, if a monitor action with <code class="literal">interval=20s</code> is executed at 01:00:00, the next monitor action does not occur at 01:00:20, but at 20 seconds after the first monitor action completes.
						</p>
						 <p>
							If set to zero, which is the default value, this parameter allows you to provide values to be used for operations created by the cluster. For example, if the <code class="literal">interval</code> is set to zero, the <code class="literal">name</code> of the operation is set to <code class="literal">start</code>, and the <code class="literal">timeout</code> value is set to 40, then Pacemaker will use a timeout of 40 seconds when starting this resource. A <code class="literal">monitor</code> operation with a zero interval allows you to set the <code class="literal">timeout</code>/<code class="literal">on-fail</code>/<code class="literal">enabled</code> values for the probes that Pacemaker does at startup to get the current status of all resources when the defaults are not desirable.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686144857760"> <p>
							<code class="literal">timeout</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686144856672"> <p>
							If the operation does not complete in the amount of time set by this parameter, abort the operation and consider it failed. The default value is the value of <code class="literal">timeout</code> if set with the <code class="literal command">pcs resource op defaults</code> command, or 20 seconds if it is not set. If you find that your system includes a resource that requires more time than the system allows to perform an operation (such as <code class="literal">start</code>, <code class="literal">stop</code>, or <code class="literal">monitor</code>), investigate the cause and if the lengthy execution time is expected you can increase this value.
						</p>
						 <p>
							The <code class="literal">timeout</code> value is not a delay of any kind, nor does the cluster wait the entire timeout period if the operation returns before the timeout period has completed.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686144857760"> <p>
							<code class="literal">on-fail</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686144856672"> <p>
							The action to take if this action ever fails. Allowed values:
						</p>
						 <p>
							* <code class="literal">ignore</code> - Pretend the resource did not fail
						</p>
						 <p>
							* <code class="literal">block</code> - Do not perform any further operations on the resource
						</p>
						 <p>
							* <code class="literal">stop</code> - Stop the resource and do not start it elsewhere
						</p>
						 <p>
							* <code class="literal">restart</code> - Stop the resource and start it again (possibly on a different node)
						</p>
						 <p>
							* <code class="literal">fence</code> - STONITH the node on which the resource failed
						</p>
						 <p>
							* <code class="literal">standby</code> - Move <span class="emphasis"><em>all</em></span> resources away from the node on which the resource failed
						</p>
						 <p>
							* <code class="literal">demote</code> - When a <code class="literal">promote</code> action fails for the resource, the resource will be demoted but will not be fully stopped. When a <code class="literal">monitor</code> action fails for a resource, if <code class="literal">interval</code> is set to a nonzero value and <code class="literal">role</code> is set to <code class="literal">Promoted</code> the resource will be demoted but will not be fully stopped.
						</p>
						 <p>
							The default for the <code class="literal">stop</code> operation is <code class="literal">fence</code> when STONITH is enabled and <code class="literal">block</code> otherwise. All other operations default to <code class="literal">restart</code>.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686144857760"> <p>
							<code class="literal">enabled</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686144856672"> <p>
							If <code class="literal">false</code>, the operation is treated as if it does not exist. Allowed values: <code class="literal">true</code>, <code class="literal">false</code>
						</p>
						 </td></tr></tbody></table></rh-table><section class="section" id="proc_configuring-resource-monitoring-operations-resource-monitoring-operations"><div class="titlepage"><div><div><h3 class="title">21.1. Configuring resource monitoring operations</h3></div></div></div><p class="_abstract _abstract">
				You can configure monitoring operations when you create a resource with the following command.
			</p><pre class="literallayout">pcs resource create <span class="emphasis"><em>resource_id</em></span> <span class="emphasis"><em>standard:provider:type|type</em></span> [<span class="emphasis"><em>resource_options</em></span>] [op <span class="emphasis"><em>operation_action</em></span> <span class="emphasis"><em>operation_options</em></span> [<span class="emphasis"><em>operation_type</em></span> <span class="emphasis"><em>operation_options</em></span>]...]</pre><p>
				For example, the following command creates an <code class="literal">IPaddr2</code> resource with a monitoring operation. The new resource is called <code class="literal">VirtualIP</code> with an IP address of 192.168.0.99 and a netmask of 24 on <code class="literal">eth2</code>. A monitoring operation will be performed every 30 seconds.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.99 cidr_netmask=24 nic=eth2 op monitor interval=30s</strong></span></pre><p>
				Alternately, you can add a monitoring operation to an existing resource with the following command.
			</p><pre class="literallayout">pcs resource op add <span class="emphasis"><em>resource_id</em></span> <span class="emphasis"><em>operation_action</em></span> [<span class="emphasis"><em>operation_properties</em></span>]</pre><p>
				Use the following command to delete a configured resource operation.
			</p><pre class="literallayout">pcs resource op remove <span class="emphasis"><em>resource_id</em></span> <span class="emphasis"><em>operation_name</em></span> <span class="emphasis"><em>operation_properties</em></span></pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					You must specify the exact operation properties to properly remove an existing operation.
				</p></div></rh-alert><p>
				To change the values of a monitoring option, you can update the resource. For example, you can create a <code class="literal">VirtualIP</code> with the following command.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.99 cidr_netmask=24 nic=eth2</strong></span></pre><p>
				By default, this command creates these operations.
			</p><pre class="literallayout">Operations: start interval=0s timeout=20s (VirtualIP-start-timeout-20s)
            stop interval=0s timeout=20s (VirtualIP-stop-timeout-20s)
            monitor interval=10s timeout=20s (VirtualIP-monitor-interval-10s)</pre><p>
				To change the stop timeout operation, execute the following command.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource update VirtualIP op stop interval=0s timeout=40s</strong></span>

# <span class="strong strong"><strong>pcs resource config VirtualIP</strong></span>
 Resource: VirtualIP (class=ocf provider=heartbeat type=IPaddr2)
  Attributes: ip=192.168.0.99 cidr_netmask=24 nic=eth2
  Operations: start interval=0s timeout=20s (VirtualIP-start-timeout-20s)
              monitor interval=10s timeout=20s (VirtualIP-monitor-interval-10s)
              stop interval=0s timeout=40s (VirtualIP-name-stop-interval-0s-timeout-40s)</pre></section><section class="section" id="proc_configuring-global-resource-operation-defaults-resource-monitoring-operations"><div class="titlepage"><div><div><h3 class="title">21.2. Configuring global resource operation defaults</h3></div></div></div><p class="_abstract _abstract">
				You can change the default value of a resource operation for all resources with the <code class="literal">pcs resource op defaults update</code> command.
			</p><p>
				The following command sets a global default of a <code class="literal">timeout</code> value of 240 seconds for all monitoring operations.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource op defaults update timeout=240s</strong></span></pre><p>
				The original <code class="literal">pcs resource op defaults <span class="emphasis"><em>name</em></span>=<span class="emphasis"><em>value</em></span></code> command, which set resource operation defaults for all resources in previous releases, remains supported unless there is more than one set of defaults configured. However, <code class="literal">pcs resource op defaults update</code> is now the preferred version of the command.
			</p><section class="section" id="overriding_resource_specific_operation_values"><div class="titlepage"><div><div><h4 class="title">21.2.1. Overriding resource-specific operation values</h4></div></div></div><p>
					Note that a cluster resource will use the global default only when the option is not specified in the cluster resource definition. By default, resource agents define the <code class="literal">timeout</code> option for all operations. For the global operation timeout value to be honored, you must create the cluster resource without the <code class="literal">timeout</code> option explicitly or you must remove the <code class="literal">timeout</code> option by updating the cluster resource, as in the following command.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource update VirtualIP op monitor interval=10s</strong></span></pre><p>
					For example, after setting a global default of a <code class="literal">timeout</code> value of 240 seconds for all monitoring operations and updating the cluster resource <code class="literal">VirtualIP</code> to remove the timeout value for the <code class="literal">monitor</code> operation, the resource <code class="literal">VirtualIP</code> will then have timeout values for <code class="literal">start</code>, <code class="literal">stop</code>, and <code class="literal">monitor</code> operations of 20s, 40s and 240s, respectively. The global default value for timeout operations is applied here only on the <code class="literal">monitor</code> operation, where the default <code class="literal">timeout</code> option was removed by the previous command.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource config VirtualIP</strong></span>
 Resource: VirtualIP (class=ocf provider=heartbeat type=IPaddr2)
   Attributes: ip=192.168.0.99 cidr_netmask=24 nic=eth2
   Operations: start interval=0s timeout=20s (VirtualIP-start-timeout-20s)
               monitor interval=10s (VirtualIP-monitor-interval-10s)
               stop interval=0s timeout=40s (VirtualIP-name-stop-interval-0s-timeout-40s)</pre></section><section class="section" id="changing_the_default_value_of_a_resource_operation_for_sets_of_resources"><div class="titlepage"><div><div><h4 class="title">21.2.2. Changing the default value of a resource operation for sets of resources</h4></div></div></div><p>
					You can create multiple sets of resource operation defaults with the <code class="literal">pcs resource op defaults set create</code> command, which allows you to specify a rule that contains <code class="literal">resource</code> and operation expressions. All of the rule expressions supported by Pacemaker are allowed.
				</p><p>
					With this command, you can configure a default resource operation value for all resources of a particular type. For example, it is now possible to configure implicit <code class="literal">podman</code> resources created by Pacemaker when bundles are in use.
				</p><p>
					The following command sets a default timeout value of 90s for all operations for all <code class="literal">podman</code> resources. In this example, <code class="literal">::podman</code> means a resource of any class, any provider, of type <code class="literal">podman</code>.
				</p><p>
					The <code class="literal">id</code> option, which names the set of resource operation defaults, is not mandatory. If you do not set this option, <code class="literal">pcs</code> will generate an ID automatically. Setting this value allows you to provide a more descriptive name.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource op defaults set create id=podman-timeout meta timeout=90s rule resource ::podman</strong></span></pre><p>
					The following command sets a default timeout value of 120s for the <code class="literal">stop</code> operation for all resources.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource op defaults set create id=stop-timeout meta timeout=120s rule op stop</strong></span></pre><p>
					It is possible to set the default timeout value for a specific operation for all resources of a particular type. The following example sets a default timeout value of 120s for the <code class="literal">stop</code> operation for all <code class="literal">podman</code> resources.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource op defaults set create id=podman-stop-timeout meta timeout=120s rule resource ::podman and op stop</strong></span></pre></section><section class="section" id="displaying_currently_configured_resource_operation_default_values"><div class="titlepage"><div><div><h4 class="title">21.2.3. Displaying currently configured resource operation default values</h4></div></div></div><p>
					The <code class="literal">pcs resource op defaults [config]</code> command displays a list of currently configured default values for resource operations, including any rules you specified. As of Red Hat Enterprise Linux 9.5, you can display the output of this command in text, JSON, and command formats.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Specifying <code class="literal">--output-format=text</code> displays the configured resource operation defaults in plain text format, which is the default value for this option.
						</li><li class="listitem">
							Specifying <code class="literal">--output-format=cmd</code> displays the <code class="literal">pcs resource op defaults</code> commands created from the current cluster defaults configuration. You can use these commands to re-create configured resource operation defaults on a different system.
						</li><li class="listitem">
							Specifying <code class="literal">--output-format=json</code> displays the configured resource operation defaults in JSON format, which is suitable for machine parsing.
						</li></ul></div><p>
					The following examples show the three different output formats of the <code class="literal">pcs resource op defaults config</code> command after the default resource operation values for any <code class="literal">ocf:pacemaker:podman</code> resource were reset with this example command:
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource op defaults set create id=op-set-1 score=100 meta timeout=30s rule op monitor and resource ocf:pacemaker:podman</strong></span>
Warning: Defaults do not apply to resources which override them with their own defined values</pre><p>
					This example displays the configured resource operation default values in plain text.
				</p><pre class="literallayout"># pcs resource op defaults config
Meta Attrs: op-set-1 score=100
  timeout=30s
  Rule: boolean-op=and score=INFINITY
    Expression: op monitor
    Expression: resource ocf:pacemaker:podman</pre><p>
					This example displays the <code class="literal">pcs resource op defaults</code> commands created from the current cluster defaults configuration.
				</p><pre class="literallayout"># pcs resource op defaults config --output-format=cmd
pcs -- resource op defaults set create id=op-set-1 score=100 \
  meta timeout=30s \
  rule 'op monitor and resource ocf:pacemaker:podman'</pre><p>
					This example displays the configured resource operation default values in JSON format.
				</p><pre class="literallayout"># pcs resource op defaults config --output-format=json
{"instance_attributes": [], "meta_attributes": [{"id": "op-set-1", "options": {"score": "100"}, "rule": {"id": "op-set-1-rule", "type": "RULE", "in_effect": "UNKNOWN", "options": {"boolean-op": "and", "score": "INFINITY"}, "date_spec": null, "duration": null, "expressions": [{"id": "op-set-1-rule-op-monitor", "type": "OP_EXPRESSION", "in_effect": "UNKNOWN", "options": {"name": "monitor"}, "date_spec": null, "duration": null, "expressions": [], "as_string": "op monitor"}, {"id": "op-set-1-rule-rsc-ocf-pacemaker-podman", "type": "RSC_EXPRESSION", "in_effect": "UNKNOWN", "options": {"class": "ocf", "provider": "pacemaker", "type": "podman"}, "date_spec": null, "duration": null, "expressions": [], "as_string": "resource ocf:pacemaker:podman"}], "as_string": "op monitor and resource ocf:pacemaker:podman"}, "nvpairs": [{"id": "op-set-1-timeout", "name": "timeout", "value": "30s"}]}]}</pre></section></section><section class="section" id="proc_configuring-multiple-monitoring-operations-resource-monitoring-operations"><div class="titlepage"><div><div><h3 class="title">21.3. Configuring multiple monitoring operations</h3></div></div></div><p class="_abstract _abstract">
				You can configure a single resource with as many monitor operations as a resource agent supports. In this way you can do a superficial health check every minute and progressively more intense ones at higher intervals.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					When configuring multiple monitor operations, you must ensure that no two operations are performed at the same interval.
				</p></div></rh-alert><p>
				To configure additional monitoring operations for a resource that supports more in-depth checks at different levels, you add an <code class="literal">OCF_CHECK_LEVEL=<span class="emphasis"><em>n</em></span></code> option.
			</p><p>
				For example, if you configure the following <code class="literal">IPaddr2</code> resource, by default this creates a monitoring operation with an interval of 10 seconds and a timeout value of 20 seconds.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.99 cidr_netmask=24 nic=eth2</strong></span></pre><p>
				If the Virtual IP supports a different check with a depth of 10, the following command causes Pacemaker to perform the more advanced monitoring check every 60 seconds in addition to the normal Virtual IP check every 10 seconds. (As noted, you should not configure the additional monitoring operation with a 10-second interval as well.)
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource op add VirtualIP monitor interval=60s OCF_CHECK_LEVEL=10</strong></span></pre></section></section><section class="chapter" id="assembly_controlling-cluster-behavior-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 22. Pacemaker cluster properties</h2></div></div></div><p class="_abstract _abstract">
			Cluster properties control how the cluster behaves when confronted with situations that might occur during cluster operation.
		</p><section class="section" id="ref_cluster-properties-options-controlling-cluster-behavior"><div class="titlepage"><div><div><h3 class="title">22.1. Summary of cluster properties and options</h3></div></div></div><p class="_abstract _abstract">
				The following table summaries the Pacemaker cluster properties, showing the default values of the properties and the possible values you can set for those properties.
			</p><p>
				There are additional cluster properties that determine fencing behavior. For information about these properties, see the table of cluster properties that determine fencing behavior in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters#ref_general-fence-device-properties-configuring-fencing">General properties of fencing devices</a>.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					In addition to the properties described in this table, there are additional cluster properties that are exposed by the cluster software. For these properties, it is recommended that you not change their values from their defaults.
				</p></div></rh-alert><rh-table id="tb-clusterprops-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 22.1. Cluster Properties</caption><colgroup><col style="width: 29%; " class="col_1"><!--Empty--><col style="width: 29%; " class="col_2"><!--Empty--><col style="width: 43%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686137714352" scope="col">Option</th><th align="left" valign="top" id="idm140686137713264" scope="col">Default</th><th align="left" valign="top" id="idm140686137712176" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">batch-limit</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								0
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								The number of resource actions that the cluster is allowed to execute in parallel. The "correct" value will depend on the speed and load of your network and cluster nodes. The default value of 0 means that the cluster will dynamically impose a limit when any node has a high CPU load.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">migration-limit</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								-1 (unlimited)
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								The number of migration jobs that the cluster is allowed to execute in parallel on a node.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">no-quorum-policy</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								stop
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								What to do when the cluster does not have quorum. Allowed values:
							</p>
							 <p>
								* ignore - continue all resource management
							</p>
							 <p>
								* freeze - continue resource management, but do not recover resources from nodes not in the affected partition
							</p>
							 <p>
								* stop - stop all resources in the affected cluster partition
							</p>
							 <p>
								* suicide - fence all nodes in the affected cluster partition
							</p>
							 <p>
								* demote - if a cluster partition loses quorum, demote any promoted resources and stop all other resources
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">symmetric-cluster</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								true
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								Indicates whether resources can run on any node by default.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">cluster-delay</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								60s
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								Round trip delay over the network (excluding action execution). The "correct" value will depend on the speed and load of your network and cluster nodes.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">dc-deadtime</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								20s
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								How long to wait for a response from other nodes during startup. The "correct" value will depend on the speed and load of your network and the type of switches used.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">stop-orphan-resources</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								true
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								Indicates whether deleted resources should be stopped.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">stop-orphan-actions</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								true
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								Indicates whether deleted actions should be canceled.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">start-failure-is-fatal</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								true
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								Indicates whether a failure to start a resource on a particular node prevents further start attempts on that node. When set to <code class="literal">false</code>, the cluster will decide whether to try starting on the same node again based on the resource’s current failure count and migration threshold. For information about setting the <code class="literal">migration-threshold</code> option for a resource, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters#proc_configuring-resource-meta-options-configuring-cluster-resources">Configuring resource meta options</a>.
							</p>
							 <p>
								Setting <code class="literal">start-failure-is-fatal</code> to <code class="literal">false</code> incurs the risk that this will allow one faulty node that is unable to start a resource to hold up all dependent actions. This is why <code class="literal">start-failure-is-fatal</code> defaults to true. The risk of setting <code class="literal">start-failure-is-fatal=false</code> can be mitigated by setting a low migration threshold so that other actions can proceed after that many failures.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">pe-error-series-max</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								-1 (all)
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								The number of scheduler inputs resulting in ERRORs to save. Used when reporting problems.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">pe-warn-series-max</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								-1 (all)
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								The number of scheduler inputs resulting in WARNINGs to save. Used when reporting problems.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">pe-input-series-max</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								-1 (all)
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								The number of "normal" scheduler inputs to save. Used when reporting problems.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">cluster-infrastructure</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								The messaging stack on which Pacemaker is currently running. Used for informational and diagnostic purposes; not user-configurable.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">dc-version</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								Version of Pacemaker on the cluster’s Designated Controller (DC). Used for diagnostic purposes; not user-configurable.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">cluster-recheck-interval</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								15 minutes
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								Pacemaker is primarily event-driven, and looks ahead to know when to recheck the cluster for failure timeouts and most time-based rules. Pacemaker will also recheck the cluster after the duration of inactivity specified by this property. This cluster recheck has two purposes: rules with <code class="literal">date-spec</code> are guaranteed to be checked this often, and it serves as a fail-safe for some kinds of scheduler bugs. A value of 0 disables this polling; positive values indicate a time interval.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">maintenance-mode</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								false
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								Maintenance Mode tells the cluster to go to a "hands off" mode, and not start or stop any services until told otherwise. When maintenance mode is completed, the cluster does a sanity check of the current state of any services, and then stops or starts any that need it.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">shutdown-escalation</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								20min
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								The time after which to give up trying to shut down gracefully and just exit. Advanced use only.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">stop-all-resources</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								false
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								Should the cluster stop all resources.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">enable-acl</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								false
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								Indicates whether the cluster can use access control lists, as set with the <code class="literal command">pcs acl</code> command.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">placement-strategy</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								default
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								Indicates whether and how the cluster will take utilization attributes into account when determining resource placement on cluster nodes.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137714352"> <p>
								<code class="literal">node-health-strategy</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137713264"> <p>
								none
							</p>
							 </td><td align="left" valign="top" headers="idm140686137712176"> <p>
								When used in conjunction with a health resource agent, controls how Pacemaker responds to changes in node health. Allowed values:
							</p>
							 <p>
								* <code class="literal">none</code> - Do not track node health.
							</p>
							 <p>
								* <code class="literal">migrate-on-red</code> - Resources are moved off any node where a health agent has determined that the node’s status is <code class="literal">red</code>, based on the local conditions that the agent monitors.
							</p>
							 <p>
								* <code class="literal">only-green</code> - Resources are moved off any node where a health agent has determined that the node’s status is <code class="literal">yellow</code> or <code class="literal">red</code>, based on the local conditions that the agent monitors.
							</p>
							 <p>
								* <code class="literal">progressive</code>, <code class="literal">custom</code> - Advanced node health strategies that offer finer-grained control over the cluster’s response to health conditions according to the internal numeric values of health attributes.
							</p>
							 </td></tr></tbody></table></rh-table></section><section class="section" id="setting-cluster-properties-controlling-cluster-behavior"><div class="titlepage"><div><div><h3 class="title">22.2. Setting and removing cluster properties</h3></div></div></div><p class="_abstract _abstract">
				To set the value of a cluster property, use the following <span class="strong strong"><strong><span class="application application">pcs</span></strong></span> command.
			</p><pre class="literallayout">pcs property set <span class="emphasis"><em>property</em></span>=<span class="emphasis"><em>value</em></span></pre><p>
				For example, to set the value of <code class="literal">symmetric-cluster</code> to <code class="literal">false</code>, use the following command.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs property set symmetric-cluster=false</strong></span></pre><p>
				You can remove a cluster property from the configuration with the following command.
			</p><pre class="literallayout">pcs property unset <span class="emphasis"><em>property</em></span></pre><p>
				Alternately, you can remove a cluster property from a configuration by leaving the value field of the <code class="literal command">pcs property set</code> command blank. This restores that property to its default value. For example, if you have previously set the <code class="literal">symmetric-cluster</code> property to <code class="literal">false</code>, the following command removes the value you have set from the configuration and restores the value of <code class="literal">symmetric-cluster</code> to <code class="literal">true</code>, which is its default value.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs property set symmetic-cluster=</strong></span></pre></section><section class="section" id="proc_querying-cluster-property-settings-controlling-cluster-behavior"><div class="titlepage"><div><div><h3 class="title">22.3. Querying cluster property settings</h3></div></div></div><p class="_abstract _abstract">
				In most cases, when you use the <code class="literal command">pcs</code> command to display values of the various cluster components, you can use <code class="literal command">pcs list</code> or <code class="literal command">pcs show</code> interchangeably. In the following examples, <code class="literal command">pcs list</code> is the format used to display an entire list of all settings for more than one property, while <code class="literal command">pcs show</code> is the format used to display the values of a specific property.
			</p><p>
				To display the values of the property settings that have been set for the cluster, use the following <span class="strong strong"><strong><span class="application application">pcs</span></strong></span> command.
			</p><pre class="literallayout">pcs property list</pre><p>
				To display all of the values of the property settings for the cluster, including the default values of the property settings that have not been explicitly set, use the following command.
			</p><pre class="literallayout">pcs property list --all</pre><p>
				To display the current value of a specific cluster property, use the following command.
			</p><pre class="literallayout">pcs property show <span class="emphasis"><em>property</em></span></pre><p>
				For example, to display the current value of the <code class="literal">cluster-infrastructure</code> property, execute the following command:
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs property show cluster-infrastructure</strong></span>
Cluster Properties:
 cluster-infrastructure: cman</pre><p>
				For informational purposes, you can display a list of all of the default values for the properties, whether they have been set to a value other than the default or not, by using the following command.
			</p><pre class="literallayout">pcs property [list|show] --defaults</pre></section><section class="section" id="proc_exporting-properties-controlling-cluster-behavior"><div class="titlepage"><div><div><h3 class="title">22.4. Exporting cluster properties as <code class="literal">pcs</code> commands</h3></div></div></div><p class="_abstract _abstract">
				As of Red Hat Enterprise Linux 9.3, you can display the <code class="literal">pcs</code> commands that can be used to re-create configured cluster properties on a different system using the <code class="literal">--output-format=cmd</code> option of the <code class="literal">pcs property config</code> command.
			</p><p>
				The following command sets the <code class="literal">migration-limit</code> cluster property to 10.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs property set migration-limit=10</strong></span></pre><p>
				After you set the cluster property, the following command displays the <code class="literal">pcs</code> command you can use to set the cluster property on a different system.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs property config --output-format=cmd</strong></span>
pcs property set --force -- \
 migration-limit=10 \
 placement-strategy=minimal</pre></section></section><section class="chapter" id="assembly_configuring-resources-to-remain-stopped-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 23. Configuring resources to remain stopped on clean node shutdown</h2></div></div></div><p class="_abstract _abstract">
			When a cluster node shuts down, Pacemaker’s default response is to stop all resources running on that node and recover them elsewhere, even if the shutdown is a clean shutdown. You can configure Pacemaker so that when a node shuts down cleanly, the resources attached to the node will be locked to the node and unable to start elsewhere until they start again when the node that has shut down rejoins the cluster. This allows you to power down nodes during maintenance windows when service outages are acceptable without causing that node’s resources to fail over to other nodes in the cluster.
		</p><section class="section" id="ref_cluster-properties-shutdown-lock-configuring-resources-to-remain-stopped"><div class="titlepage"><div><div><h3 class="title">23.1. Cluster properties to configure resources to remain stopped on clean node shutdown</h3></div></div></div><p class="_abstract _abstract">
				The ability to prevent resources from failing over on a clean node shutdown is implemented by means of the following cluster properties.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">shutdown-lock</code></span></dt><dd><p class="simpara">
							When this cluster property is set to the default value of <code class="literal">false</code>, the cluster will recover resources that are active on nodes being cleanly shut down. When this property is set to <code class="literal">true</code>, resources that are active on the nodes being cleanly shut down are unable to start elsewhere until they start on the node again after it rejoins the cluster.
						</p><p class="simpara">
							The <code class="literal">shutdown-lock</code> property will work for either cluster nodes or remote nodes, but not guest nodes.
						</p><p class="simpara">
							If <code class="literal">shutdown-lock</code> is set to <code class="literal">true</code>, you can remove the lock on one cluster resource when a node is down so that the resource can start elsewhere by performing a manual refresh on the node with the following command.
						</p><p class="simpara">
							<code class="literal command">pcs resource refresh <span class="emphasis"><em>resource</em></span> node=<span class="emphasis"><em>nodename</em></span></code>
						</p><p class="simpara">
							Note that once the resources are unlocked, the cluster is free to move the resources elsewhere. You can control the likelihood of this occurring by using stickiness values or location preferences for the resource.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								A manual refresh will work with remote nodes only if you first run the following commands:
							</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
										Run the <code class="literal command">systemctl stop pacemaker_remote</code> command on the remote node to stop the node.
									</li><li class="listitem">
										Run the <code class="literal command">pcs resource disable <span class="emphasis"><em>remote-connection-resource</em></span></code> command.
									</li></ol></div><p>
								You can then perform a manual refresh on the remote node.
							</p></div></rh-alert></dd><dt><span class="term"><code class="literal">shutdown-lock-limit</code></span></dt><dd><p class="simpara">
							When this cluster property is set to a time other than the default value of 0, resources will be available for recovery on other nodes if the node does not rejoin within the specified time since the shutdown was initiated.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								The <code class="literal">shutdown-lock-limit</code> property will work with remote nodes only if you first run the following commands:
							</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
										Run the <code class="literal command">systemctl stop pacemaker_remote</code> command on the remote node to stop the node.
									</li><li class="listitem">
										Run the <code class="literal command">pcs resource disable <span class="emphasis"><em>remote-connection-resource</em></span></code> command.
									</li></ol></div><p>
								After you run these commands, the resources that had been running on the remote node will be available for recovery on other nodes when the amount of time specified as the <code class="literal">shutdown-lock-limit</code> has passed.
							</p></div></rh-alert></dd></dl></div></section><section class="section" id="proc_setting-shutdown-lock-configuring-resources-to-remain-stopped"><div class="titlepage"><div><div><h3 class="title">23.2. Setting the shutdown-lock cluster property</h3></div></div></div><p class="_abstract _abstract">
				The following example sets the <code class="literal">shutdown-lock</code> cluster property to <code class="literal">true</code> in an example cluster and shows the effect this has when the node is shut down and started again. This example cluster consists of three nodes: <code class="literal">z1.example.com</code>, <code class="literal">z2.example.com</code>, and <code class="literal">z3.example.com</code>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Set the <code class="literal">shutdown-lock</code> property to to <code class="literal">true</code> and verify its value. In this example the <code class="literal">shutdown-lock-limit</code> property maintains its default value of 0.
					</p><pre class="literallayout">[root@z3 ~]# <span class="strong strong"><strong>pcs property set shutdown-lock=true</strong></span>
[root@z3 ~]# <span class="strong strong"><strong>pcs property list --all | grep shutdown-lock</strong></span>
 shutdown-lock: true
 shutdown-lock-limit: 0</pre></li><li class="listitem"><p class="simpara">
						Check the status of the cluster. In this example, resources <code class="literal">third</code> and <code class="literal">fifth</code> are running on <code class="literal">z1.example.com</code>.
					</p><pre class="literallayout">[root@z3 ~]# <span class="strong strong"><strong>pcs status</strong></span>
...
Full List of Resources:
...
 * first	(ocf::pacemaker:Dummy):	Started z3.example.com
 * second	(ocf::pacemaker:Dummy):	Started z2.example.com
 * third	(ocf::pacemaker:Dummy):	Started z1.example.com
 * fourth	(ocf::pacemaker:Dummy):	Started z2.example.com
 * fifth	(ocf::pacemaker:Dummy):	Started z1.example.com
...</pre></li><li class="listitem"><p class="simpara">
						Shut down <code class="literal">z1.example.com</code>, which will stop the resources that are running on that node.
					</p><pre class="literallayout">[root@z3 ~] <span class="strong strong"><strong># pcs cluster stop z1.example.com</strong></span>
Stopping Cluster (pacemaker)...
Stopping Cluster (corosync)...</pre></li><li class="listitem"><p class="simpara">
						Running the <code class="literal">pcs status</code> command shows that node <code class="literal">z1.example.com</code> is offline and that the resources that had been running on <code class="literal">z1.example.com</code> are <code class="literal">LOCKED</code> while the node is down.
					</p><pre class="literallayout">[root@z3 ~]# <span class="strong strong"><strong>pcs status</strong></span>
...

Node List:
 * Online: [ z2.example.com z3.example.com ]
 * OFFLINE: [ z1.example.com ]

Full List of Resources:
...
 * first	(ocf::pacemaker:Dummy):	Started z3.example.com
 * second	(ocf::pacemaker:Dummy):	Started z2.example.com
 * third	(ocf::pacemaker:Dummy):	Stopped z1.example.com (LOCKED)
 * fourth	(ocf::pacemaker:Dummy):	Started z3.example.com
 * fifth	(ocf::pacemaker:Dummy):	Stopped z1.example.com (LOCKED)

...</pre></li><li class="listitem"><p class="simpara">
						Start cluster services again on <code class="literal">z1.example.com</code> so that it rejoins the cluster. Locked resources should get started on that node, although once they start they will not not necessarily remain on the same node.
					</p><pre class="literallayout">[root@z3 ~]# <span class="strong strong"><strong>pcs cluster start z1.example.com</strong></span>
Starting Cluster...</pre></li><li class="listitem"><p class="simpara">
						In this example, resouces <code class="literal">third</code> and <code class="literal">fifth</code> are recovered on node <code class="literal">z1.example.com</code>.
					</p><pre class="literallayout">[root@z3 ~]# <span class="strong strong"><strong>pcs status</strong></span>
...

Node List:
 * Online: [ z1.example.com z2.example.com z3.example.com ]

Full List of Resources:
..
 * first	(ocf::pacemaker:Dummy):	Started z3.example.com
 * second	(ocf::pacemaker:Dummy):	Started z2.example.com
 * third	(ocf::pacemaker:Dummy):	Started z1.example.com
 * fourth	(ocf::pacemaker:Dummy):	Started z3.example.com
 * fifth	(ocf::pacemaker:Dummy):	Started z1.example.com

...</pre></li></ol></div></section></section><section class="chapter" id="assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 24. Configuring a node placement strategy</h2></div></div></div><p class="_abstract _abstract">
			Pacemaker decides where to place a resource according to the resource allocation scores on every node. The resource will be allocated to the node where the resource has the highest score. This allocation score is derived from a combination of factors, including resource constraints, <code class="literal">resource-stickiness</code> settings, prior failure history of a resource on each node, and utilization of each node.
		</p><p>
			If the resource allocation scores on all the nodes are equal, by the default placement strategy Pacemaker will choose a node with the least number of allocated resources for balancing the load. If the number of resources on each node is equal, the first eligible node listed in the CIB will be chosen to run the resource.
		</p><p>
			Often, however, different resources use significantly different proportions of a node’s capacities (such as memory or I/O). You cannot always balance the load ideally by taking into account only the number of resources allocated to a node. In addition, if resources are placed such that their combined requirements exceed the provided capacity, they may fail to start completely or they may run with degraded performance. To take these factors into account, Pacemaker allows you to configure the following components:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					the capacity a particular node provides
				</li><li class="listitem">
					the capacity a particular resource requires
				</li><li class="listitem">
					an overall strategy for placement of resources
				</li></ul></div><section class="section" id="configuring-utilization-attributes-configuring-node-placement-strategy"><div class="titlepage"><div><div><h3 class="title">24.1. Utilization attributes and placement strategy</h3></div></div></div><p class="_abstract _abstract">
				To configure the capacity that a node provides or a resource requires, you can use <span class="emphasis"><em>utilization attributes</em></span> for nodes and resources. You do this by setting a utilization variable for a resource and assigning a value to that variable to indicate what the resource requires, and then setting that same utilization variable for a node and assigning a value to that variable to indicate what that node provides.
			</p><p>
				You can name utilization attributes according to your preferences and define as many name and value pairs as your configuration needs. The values of utilization attributes must be integers.
			</p><section class="section" id="configuring_node_and_resource_capacity"><div class="titlepage"><div><div><h4 class="title">24.1.1. Configuring node and resource capacity</h4></div></div></div><p>
					The following example configures a utilization attribute of CPU capacity for two nodes, setting this attribute as the variable <code class="literal">cpu</code>. It also configures a utilization attribute of RAM capacity, setting this attribute as the variable <code class="literal">memory</code>. In this example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Node 1 is defined as providing a CPU capacity of two and a RAM capacity of 2048
						</li><li class="listitem">
							Node 2 is defined as providing a CPU capacity of four and a RAM capacity of 2048
						</li></ul></div><pre class="literallayout"># <span class="strong strong"><strong>pcs node utilization node1 cpu=2 memory=2048</strong></span>
# <span class="strong strong"><strong>pcs node utilization node2 cpu=4 memory=2048</strong></span></pre><p>
					The following example specifies the same utilization attributes that three different resources require. In this example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							resource <code class="literal">dummy-small</code> requires a CPU capacity of 1 and a RAM capacity of 1024
						</li><li class="listitem">
							resource <code class="literal">dummy-medium</code> requires a CPU capacity of 2 and a RAM capacity of 2048
						</li><li class="listitem">
							resource <code class="literal">dummy-large</code> requires a CPU capacity of 1 and a RAM capacity of 3072
						</li></ul></div><pre class="literallayout"># <span class="strong strong"><strong>pcs resource utilization dummy-small cpu=1 memory=1024</strong></span>
# <span class="strong strong"><strong>pcs resource utilization dummy-medium cpu=2 memory=2048</strong></span>
# <span class="strong strong"><strong>pcs resource utilization dummy-large cpu=3 memory=3072</strong></span></pre><p>
					A node is considered eligible for a resource if it has sufficient free capacity to satisfy the resource’s requirements, as defined by the utilization attributes.
				</p></section><section class="section" id="configuring_placement_strategy"><div class="titlepage"><div><div><h4 class="title">24.1.2. Configuring placement strategy</h4></div></div></div><p>
					After you have configured the capacities your nodes provide and the capacities your resources require, you need to set the <code class="literal">placement-strategy</code> cluster property, otherwise the capacity configurations have no effect.
				</p><p>
					Four values are available for the <code class="literal">placement-strategy</code> cluster property:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">default</code> — Utilization values are not taken into account at all. Resources are allocated according to allocation scores. If scores are equal, resources are evenly distributed across nodes.
						</li><li class="listitem">
							<code class="literal">utilization</code> — Utilization values are taken into account only when deciding whether a node is considered eligible (that is, whether it has sufficient free capacity to satisfy the resource’s requirements). Load-balancing is still done based on the number of resources allocated to a node.
						</li><li class="listitem">
							<code class="literal">balanced</code> — Utilization values are taken into account when deciding whether a node is eligible to serve a resource and when load-balancing, so an attempt is made to spread the resources in a way that optimizes resource performance.
						</li><li class="listitem">
							<code class="literal">minimal</code> — Utilization values are taken into account only when deciding whether a node is eligible to serve a resource. For load-balancing, an attempt is made to concentrate the resources on as few nodes as possible, thereby enabling possible power savings on the remaining nodes.
						</li></ul></div><p>
					The following example command sets the value of <code class="literal">placement-strategy</code> to <code class="literal">balanced</code>. After running this command, Pacemaker will ensure the load from your resources will be distributed evenly throughout the cluster, without the need for complicated sets of colocation constraints.
				</p><pre class="literallayout"># <span class="strong strong"><strong>pcs property set placement-strategy=balanced</strong></span></pre></section></section><section class="section" id="pacemaker-resource-allocation-configuring-node-placement-strategy"><div class="titlepage"><div><div><h3 class="title">24.2. Pacemaker resource allocation</h3></div></div></div><p class="_abstract _abstract">
				Pacemaker allocates resources according to node preference, node capacity, and resource allocation preference.
			</p><section class="section" id="node_preference"><div class="titlepage"><div><div><h4 class="title">24.2.1. Node preference</h4></div></div></div><p>
					Pacemaker determines which node is preferred when allocating resources according to the following strategy.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The node with the highest node weight gets consumed first. Node weight is a score maintained by the cluster to represent node health.
						</li><li class="listitem"><p class="simpara">
							If multiple nodes have the same node weight:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
									If the <code class="literal">placement-strategy</code> cluster property is <code class="literal">default</code> or <code class="literal">utilization</code>:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="square"><li class="listitem">
											The node that has the least number of allocated resources gets consumed first.
										</li><li class="listitem">
											If the numbers of allocated resources are equal, the first eligible node listed in the CIB gets consumed first.
										</li></ul></div></li><li class="listitem"><p class="simpara">
									If the <code class="literal">placement-strategy</code> cluster property is <code class="literal">balanced</code>:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="square"><li class="listitem">
											The node that has the most free capacity gets consumed first.
										</li><li class="listitem">
											If the free capacities of the nodes are equal, the node that has the least number of allocated resources gets consumed first.
										</li><li class="listitem">
											If the free capacities of the nodes are equal and the number of allocated resources is equal, the first eligible node listed in the CIB gets consumed first.
										</li></ul></div></li><li class="listitem">
									If the <code class="literal">placement-strategy</code> cluster property is <code class="literal">minimal</code>, the first eligible node listed in the CIB gets consumed first.
								</li></ul></div></li></ul></div></section><section class="section" id="node_capacity"><div class="titlepage"><div><div><h4 class="title">24.2.2. Node capacity</h4></div></div></div><p>
					Pacemaker determines which node has the most free capacity according to the following strategy.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If only one type of utilization attribute has been defined, free capacity is a simple numeric comparison.
						</li><li class="listitem"><p class="simpara">
							If multiple types of utilization attributes have been defined, then the node that is numerically highest in the most attribute types has the most free capacity. For example:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									If NodeA has more free CPUs, and NodeB has more free memory, then their free capacities are equal.
								</li><li class="listitem">
									If NodeA has more free CPUs, while NodeB has more free memory and storage, then NodeB has more free capacity.
								</li></ul></div></li></ul></div></section><section class="section" id="resource_allocation_preference"><div class="titlepage"><div><div><h4 class="title">24.2.3. Resource allocation preference</h4></div></div></div><p>
					Pacemaker determines which resource is allocated first according to the following strategy.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The resource that has the highest priority gets allocated first. You can set a resource’s priority when you create the resource.
						</li><li class="listitem">
							If the priorities of the resources are equal, the resource that has the highest score on the node where it is running gets allocated first, to prevent resource shuffling.
						</li><li class="listitem">
							If the resource scores on the nodes where the resources are running are equal or the resources are not running, the resource that has the highest score on the preferred node gets allocated first. If the resource scores on the preferred node are equal in this case, the first runnable resource listed in the CIB gets allocated first.
						</li></ul></div></section></section><section class="section" id="resource-placement-strategy-guidelines-configuring-node-placement-strategy"><div class="titlepage"><div><div><h3 class="title">24.3. Resource placement strategy guidelines</h3></div></div></div><p class="_abstract _abstract">
				To ensure that Pacemaker’s placement strategy for resources works most effectively, you should take the following considerations into account when configuring your system.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Make sure that you have sufficient physical capacity.
					</p><p class="simpara">
						If the physical capacity of your nodes is being used to near maximum under normal conditions, then problems could occur during failover. Even without the utilization feature, you may start to experience timeouts and secondary failures.
					</p></li><li class="listitem"><p class="simpara">
						Build some buffer into the capabilities you configure for the nodes.
					</p><p class="simpara">
						Advertise slightly more node resources than you physically have, on the assumption the that a Pacemaker resource will not use 100% of the configured amount of CPU, memory, and so forth all the time. This practice is sometimes called overcommit.
					</p></li><li class="listitem"><p class="simpara">
						Specify resource priorities.
					</p><p class="simpara">
						If the cluster is going to sacrifice services, it should be the ones you care about least. Ensure that resource priorities are properly set so that your most important resources are scheduled first.
					</p></li></ul></div></section><section class="section" id="node-utilization-resource-agent-configuring-node-placement-strategy"><div class="titlepage"><div><div><h3 class="title">24.4. The NodeUtilization resource agent</h3></div></div></div><p class="_abstract _abstract">
				The <code class="literal">NodeUtilization</code> resoure agent can detect the system parameters of available CPU, host memory availability, and hypervisor memory availability and add these parameters into the CIB. You can run the agent as a clone resource to have it automatically populate these parameters on each node.
			</p><p>
				For information about the <code class="literal">NodeUtilization</code> resource agent and the resource options for this agent, run the <code class="literal command">pcs resource describe NodeUtilization</code> command.
			</p></section></section><section class="chapter" id="assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 25. Configuring a virtual domain as a resource</h2></div></div></div><p class="_abstract _abstract">
			You can configure a virtual domain that is managed by the <code class="literal">libvirt</code> virtualization framework as a cluster resource with the <code class="literal command">pcs resource create</code> command, specifying <code class="literal">VirtualDomain</code> as the resource type.
		</p><p>
			When configuring a virtual domain as a resource, take the following considerations into account:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					A virtual domain should be stopped before you configure it as a cluster resource.
				</li><li class="listitem">
					Once a virtual domain is a cluster resource, it should not be started, stopped, or migrated except through the cluster tools.
				</li><li class="listitem">
					Do not configure a virtual domain that you have configured as a cluster resource to start when its host boots.
				</li><li class="listitem">
					All nodes allowed to run a virtual domain must have access to the necessary configuration files and storage devices for that virtual domain.
				</li></ul></div><p>
			If you want the cluster to manage services within the virtual domain itself, you can configure the virtual domain as a guest node.
		</p><section class="section" id="ref_virtual-domain-resource-options-configuring-virtual-domain-as-a-resource"><div class="titlepage"><div><div><h3 class="title">25.1. Virtual domain resource options</h3></div></div></div><p class="_abstract _abstract">
				The following table describes the resource options you can configure for a <code class="literal">VirtualDomain</code> resource.
			</p><rh-table id="tb-virtdomain-options-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 25.1. Resource Options for Virtual Domain Resources</caption><colgroup><col style="width: 38%; " class="col_1"><!--Empty--><col style="width: 25%; " class="col_2"><!--Empty--><col style="width: 38%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686139550576" scope="col">Field</th><th align="left" valign="top" id="idm140686139549488" scope="col">Default</th><th align="left" valign="top" id="idm140686139548400" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686139550576"> <p>
								<code class="literal">config</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686139549488"> </td><td align="left" valign="top" headers="idm140686139548400"> <p>
								(required) Absolute path to the <code class="literal">libvirt</code> configuration file for this virtual domain.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686139550576"> <p>
								<code class="literal">hypervisor</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686139549488"> <p>
								System dependent
							</p>
							 </td><td align="left" valign="top" headers="idm140686139548400"> <p>
								Hypervisor URI to connect to. You can determine the system’s default URI by running the <code class="literal command">virsh --quiet uri</code> command.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686139550576"> <p>
								<code class="literal">force_stop</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686139549488"> <p>
								<code class="literal">0</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686139548400"> <p>
								Always forcefully shut down ("destroy") the domain on stop. The default behavior is to resort to a forceful shutdown only after a graceful shutdown attempt has failed. You should set this to <code class="literal">true</code> only if your virtual domain (or your virtualization back end) does not support graceful shutdown.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686139550576"> <p>
								<code class="literal">migration_transport</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686139549488"> <p>
								System dependent
							</p>
							 </td><td align="left" valign="top" headers="idm140686139548400"> <p>
								Transport used to connect to the remote hypervisor while migrating. If this parameter is omitted, the resource will use <code class="literal">libvirt</code>'s default transport to connect to the remote hypervisor.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686139550576"> <p>
								<code class="literal">migration_network_suffix</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686139549488"> </td><td align="left" valign="top" headers="idm140686139548400"> <p>
								Use a dedicated migration network. The migration URI is composed by adding this parameter’s value to the end of the node name. If the node name is a fully qualified domain name (FQDN), insert the suffix immediately prior to the first period (.) in the FQDN. Ensure that this composed host name is locally resolvable and the associated IP address is reachable through the favored network.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686139550576"> <p>
								<code class="literal">monitor_scripts</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686139549488"> </td><td align="left" valign="top" headers="idm140686139548400"> <p>
								To additionally monitor services within the virtual domain, add this parameter with a list of scripts to monitor. <span class="emphasis"><em>Note</em></span>: When monitor scripts are used, the <code class="literal">start</code> and <code class="literal">migrate_from</code> operations will complete only when all monitor scripts have completed successfully. Be sure to set the timeout of these operations to accommodate this delay
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686139550576"> <p>
								<code class="literal">autoset_utilization_cpu</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686139549488"> <p>
								<code class="literal">true</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686139548400"> <p>
								If set to <code class="literal">true</code>, the agent will detect the number of <code class="literal">domainU</code>'s <code class="literal">vCPU</code>s from <code class="literal">virsh</code>, and put it into the CPU utilization of the resource when the monitor is executed.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686139550576"> <p>
								<code class="literal">autoset_utilization_hv_memory</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686139549488"> <p>
								<code class="literal">true</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686139548400"> <p>
								If set it true, the agent will detect the number of <code class="literal">Max memory</code> from <code class="literal">virsh</code>, and put it into the <code class="literal">hv_memory</code> utilization of the source when the monitor is executed.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686139550576"> <p>
								<code class="literal">migrateport</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686139549488"> <p>
								random highport
							</p>
							 </td><td align="left" valign="top" headers="idm140686139548400"> <p>
								This port will be used in the <code class="literal">qemu</code> migrate URI. If unset, the port will be a random highport.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686139550576"> <p>
								<code class="literal">snapshot</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686139549488"> </td><td align="left" valign="top" headers="idm140686139548400"> <p>
								Path to the snapshot directory where the virtual machine image will be stored. When this parameter is set, the virtual machine’s RAM state will be saved to a file in the snapshot directory when stopped. If on start a state file is present for the domain, the domain will be restored to the same state it was in right before it stopped last. This option is incompatible with the <code class="literal">force_stop</code> option.
							</p>
							 </td></tr></tbody></table></rh-table><p>
				In addition to the <code class="literal">VirtualDomain</code> resource options, you can configure the <code class="literal">allow-migrate</code> metadata option to allow live migration of the resource to another node. When this option is set to <code class="literal">true</code>, the resource can be migrated without loss of state. When this option is set to <code class="literal">false</code>, which is the default state, the virtual domain will be shut down on the first node and then restarted on the second node when it is moved from one node to the other.
			</p></section><section class="section" id="proc_creating-virtual-domain-resource-configuring-virtual-domain-as-a-resource"><div class="titlepage"><div><div><h3 class="title">25.2. Creating the virtual domain resource</h3></div></div></div><p class="_abstract _abstract">
				The following procedure creates a <code class="literal">VirtualDomain</code> resource in a cluster for a virtual machine you have previously created.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To create the <code class="literal">VirtualDomain</code> resource agent for the management of the virtual machine, Pacemaker requires the virtual machine’s <code class="literal">xml</code> configuration file to be dumped to a file on disk. For example, if you created a virtual machine named <code class="literal">guest1</code>, dump the <code class="literal">xml</code> file to a file somewhere on one of the cluster nodes that will be allowed to run the guest. You can use a file name of your choosing; this example uses <code class="literal">/etc/pacemaker/guest1.xml</code>.
					</p><pre class="literallayout"># <span class="strong strong"><strong>virsh dumpxml guest1 &gt; /etc/pacemaker/guest1.xml</strong></span></pre></li><li class="listitem">
						Copy the virtual machine’s <code class="literal">xml</code> configuration file to all of the other cluster nodes that will be allowed to run the guest, in the same location on each node.
					</li><li class="listitem">
						Ensure that all of the nodes allowed to run the virtual domain have access to the necessary storage devices for that virtual domain.
					</li><li class="listitem">
						Separately test that the virtual domain can start and stop on each node that will run the virtual domain.
					</li><li class="listitem">
						If it is running, shut down the guest node. Pacemaker will start the node when it is configured in the cluster. The virtual machine should not be configured to start automatically when the host boots.
					</li><li class="listitem"><p class="simpara">
						Configure the <code class="literal">VirtualDomain</code> resource with the <code class="literal command">pcs resource create</code> command. For example, the following command configures a <code class="literal">VirtualDomain</code> resource named <code class="literal">VM</code>. Since the <code class="literal">allow-migrate</code> option is set to <code class="literal">true</code> a <code class="literal">pcs resource move VM <span class="emphasis"><em>nodeX</em></span></code> command would be done as a live migration.
					</p><p class="simpara">
						In this example <code class="literal">migration_transport</code> is set to <code class="literal">ssh</code>. Note that for SSH migration to work properly, keyless logging must work between nodes.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create VM VirtualDomain config=/etc/pacemaker/guest1.xml migration_transport=ssh meta allow-migrate=true</strong></span></pre></li></ol></div></section></section><section class="chapter" id="assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 26. Configuring cluster quorum</h2></div></div></div><p class="_abstract _abstract">
			A Red Hat Enterprise Linux High Availability Add-On cluster uses the <code class="literal">votequorum</code> service, in conjunction with fencing, to avoid split brain situations. A number of votes is assigned to each system in the cluster, and cluster operations are allowed to proceed only when a majority of votes is present. The service must be loaded into all nodes or none; if it is loaded into a subset of cluster nodes, the results will be unpredictable. For information about the configuration and operation of the <code class="literal">votequorum</code> service, see the <code class="literal command">votequorum</code>(5) man page.
		</p><section class="section" id="ref_quorum-options-configuring-cluster-quorum"><div class="titlepage"><div><div><h3 class="title">26.1. Configuring quorum options</h3></div></div></div><p class="_abstract _abstract">
				There are some special features of quorum configuration that you can set when you create a cluster with the <code class="literal command">pcs cluster setup</code> command. The following table summarizes these options.
			</p><rh-table id="tb-quorumoptions-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 26.1. Quorum Options</caption><colgroup><col style="width: 44%; " class="col_1"><!--Empty--><col style="width: 56%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686137345104" scope="col">Option</th><th align="left" valign="top" id="idm140686137344016" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686137345104"> <p>
								<code class="literal">auto_tie_breaker</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137344016"> <p>
								When enabled, the cluster can suffer up to 50% of the nodes failing at the same time, in a deterministic fashion. The cluster partition, or the set of nodes that are still in contact with the <code class="literal">nodeid</code> configured in <code class="literal">auto_tie_breaker_node</code> (or lowest <code class="literal">nodeid</code> if not set), will remain quorate. The other nodes will be inquorate.
							</p>
							 <p>
								The <code class="literal option">auto_tie_breaker</code> option is principally used for clusters with an even number of nodes, as it allows the cluster to continue operation with an even split. For more complex failures, such as multiple, uneven splits, it is recommended that you use a quorum device.
							</p>
							 <p>
								The <code class="literal option">auto_tie_breaker</code> option is incompatible with quorum devices.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137345104"> <p>
								<code class="literal">wait_for_all</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137344016"> <p>
								When enabled, the cluster will be quorate for the first time only after all nodes have been visible at least once at the same time.
							</p>
							 <p>
								The <code class="literal option">wait_for_all</code> option is primarily used for two-node clusters and for even-node clusters using the quorum device <code class="literal">lms</code> (last man standing) algorithm.
							</p>
							 <p>
								The <code class="literal option">wait_for_all</code> option is automatically enabled when a cluster has two nodes, does not use a quorum device, and <code class="literal option">auto_tie_breaker</code> is disabled. You can override this by explicitly setting <code class="literal option">wait_for_all</code> to 0.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137345104"> <p>
								<code class="literal">last_man_standing</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137344016"> <p>
								When enabled, the cluster can dynamically recalculate <code class="literal">expected_votes</code> and quorum under specific circumstances. You must enable <code class="literal">wait_for_all</code> when you enable this option. The <code class="literal">last_man_standing</code> option is incompatible with quorum devices.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137345104"> <p>
								<code class="literal">last_man_standing_window</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137344016"> <p>
								The time, in milliseconds, to wait before recalculating <code class="literal">expected_votes</code> and quorum after a cluster loses nodes.
							</p>
							 </td></tr></tbody></table></rh-table><p>
				For further information about configuring and using these options, see the <code class="literal command">votequorum</code>(5) man page.
			</p></section><section class="section" id="proc_modifying-quorum-options-configuring-cluster-quorum"><div class="titlepage"><div><div><h3 class="title">26.2. Modifying quorum options</h3></div></div></div><p class="_abstract _abstract">
				You can modify general quorum options for your cluster with the <code class="literal command">pcs quorum update</code> command. Executing this command requires that the cluster be stopped. For information on the quorum options, see the <code class="literal command">votequorum</code>(5) man page.
			</p><p>
				The format of the <code class="literal command">pcs quorum update</code> command is as follows.
			</p><pre class="literallayout">pcs quorum update [auto_tie_breaker=[0|1]] [last_man_standing=[0|1]] [last_man_standing_window=[<span class="emphasis"><em>time-in-ms</em></span>] [wait_for_all=[0|1]]</pre><p>
				The following series of commands modifies the <code class="literal">wait_for_all</code> quorum option and displays the updated status of the option. Note that the system does not allow you to execute this command while the cluster is running.
			</p><pre class="literallayout">[root@node1:~]# <span class="strong strong"><strong>pcs quorum update wait_for_all=1</strong></span>
Checking corosync is not running on nodes...
Error: node1: corosync is running
Error: node2: corosync is running

[root@node1:~]# <span class="strong strong"><strong>pcs cluster stop --all</strong></span>
node2: Stopping Cluster (pacemaker)...
node1: Stopping Cluster (pacemaker)...
node1: Stopping Cluster (corosync)...
node2: Stopping Cluster (corosync)...

[root@node1:~]# <span class="strong strong"><strong>pcs quorum update wait_for_all=1</strong></span>
Checking corosync is not running on nodes...
node2: corosync is not running
node1: corosync is not running
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded

[root@node1:~]# <span class="strong strong"><strong>pcs quorum config</strong></span>
Options:
  wait_for_all: 1</pre></section><section class="section" id="proc_displaying-quorum-configuration-status-configuring-cluster-quorum"><div class="titlepage"><div><div><h3 class="title">26.3. Displaying quorum configuration and status</h3></div></div></div><p class="_abstract _abstract">
				Once a cluster is running, you can enter the following cluster quorum commands to display the quorum configuration and status.
			</p><p>
				The following command shows the quorum configuration.
			</p><pre class="literallayout">pcs quorum [config]</pre><p>
				The following command shows the quorum runtime status.
			</p><pre class="literallayout">pcs quorum status</pre></section><section class="section" id="proc_running-inquorate-clusters-configuring-cluster-quorum"><div class="titlepage"><div><div><h3 class="title">26.4. Running inquorate clusters</h3></div></div></div><p class="_abstract _abstract">
				If you take nodes out of a cluster for a long period of time and the loss of those nodes would cause quorum loss, you can change the value of the <code class="literal">expected_votes</code> parameter for the live cluster with the <code class="literal">pcs quorum expected-votes</code> command. This allows the cluster to continue operation when it does not have quorum.
			</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
					Changing the expected votes in a live cluster should be done with extreme caution. If less than 50% of the cluster is running because you have manually changed the expected votes, then the other nodes in the cluster could be started separately and run cluster services, causing data corruption and other unexpected results. If you change this value, you should ensure that the <code class="literal">wait_for_all</code> parameter is enabled.
				</p></div></rh-alert><p>
				The following command sets the expected votes in the live cluster to the specified value. This affects the live cluster only and does not change the configuration file; the value of <code class="literal">expected_votes</code> is reset to the value in the configuration file in the event of a reload.
			</p><pre class="literallayout">pcs quorum expected-votes <span class="emphasis"><em>votes</em></span></pre><p>
				In a situation in which you know that the cluster is inquorate but you want the cluster to proceed with resource management, you can use the <code class="literal command">pcs quorum unblock</code> command to prevent the cluster from waiting for all nodes when establishing quorum.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					This command should be used with extreme caution. Before issuing this command, it is imperative that you ensure that nodes that are not currently in the cluster are switched off and have no access to shared resources.
				</p></div></rh-alert><pre class="literallayout"># <span class="strong strong"><strong>pcs quorum unblock</strong></span></pre></section></section><section class="chapter" id="assembly_configuring-quorum-devices-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 27. Configuring quorum devices</h2></div></div></div><p class="_abstract _abstract">
			You can allow a cluster to sustain more node failures than standard quorum rules allows by configuring a separate quorum device which acts as a third-party arbitration device for the cluster. A quorum device is recommended for clusters with an even number of nodes. With two-node clusters, the use of a quorum device can better determine which node survives in a split-brain situation.
		</p><p>
			You must take the following into account when configuring a quorum device.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					It is recommended that a quorum device be run on a different physical network at the same site as the cluster that uses the quorum device. Ideally, the quorum device host should be in a separate rack than the main cluster, or at least on a separate PSU and not on the same network segment as the corosync ring or rings.
				</li><li class="listitem">
					You cannot use more than one quorum device in a cluster at the same time.
				</li><li class="listitem">
					Although you cannot use more than one quorum device in a cluster at the same time, a single quorum device may be used by several clusters at the same time. Each cluster using that quorum device can use different algorithms and quorum options, as those are stored on the cluster nodes themselves. For example, a single quorum device can be used by one cluster with an <code class="literal">ffsplit</code> (fifty/fifty split) algorithm and by a second cluster with an <code class="literal">lms</code> (last man standing) algorithm.
				</li><li class="listitem">
					A quorum device should not be run on an existing cluster node.
				</li></ul></div><section class="section" id="proc_installing-quorum-device-packages-configuring-quorum-devices"><div class="titlepage"><div><div><h3 class="title">27.1. Installing quorum device packages</h3></div></div></div><p class="_abstract _abstract">
				Configuring a quorum device for a cluster requires that you install the following packages:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Install <code class="literal">corosync-qdevice</code> on the nodes of an existing cluster.
					</p><pre class="literallayout">[root@node1:~]# <span class="strong strong"><strong>dnf install corosync-qdevice</strong></span>
[root@node2:~]# <span class="strong strong"><strong>dnf install corosync-qdevice</strong></span></pre></li><li class="listitem"><p class="simpara">
						Install <code class="literal">pcs</code> and <code class="literal">corosync-qnetd</code> on the quorum device host.
					</p><pre class="literallayout">[root@qdevice:~]# <span class="strong strong"><strong>dnf install pcs corosync-qnetd</strong></span></pre></li><li class="listitem"><p class="simpara">
						Start the <code class="literal">pcsd</code> service and enable <code class="literal">pcsd</code> at system start on the quorum device host.
					</p><pre class="literallayout">[root@qdevice:~]# <span class="strong strong"><strong>systemctl start pcsd.service</strong></span>
[root@qdevice:~]# <span class="strong strong"><strong>systemctl enable pcsd.service</strong></span></pre></li></ul></div></section><section class="section" id="proc_configuring-quorum-device-configuring-quorum-devices"><div class="titlepage"><div><div><h3 class="title">27.2. Configuring a quorum device</h3></div></div></div><p class="_abstract _abstract">
				Configure a quorum device and add it to the cluster with the following procedure.
			</p><p>
				In this example:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The node used for a quorum device is <code class="literal">qdevice</code>.
					</li><li class="listitem"><p class="simpara">
						The quorum device model is <code class="literal">net</code>, which is currently the only supported model. The <code class="literal">net</code> model supports the following algorithms:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								<code class="literal">ffsplit</code>: fifty-fifty split. This provides exactly one vote to the partition with the highest number of active nodes.
							</li><li class="listitem"><p class="simpara">
								<code class="literal">lms</code>: last-man-standing. If the node is the only one left in the cluster that can see the <code class="literal">qnetd</code> server, then it returns a vote.
							</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
									The LMS algorithm allows the cluster to remain quorate even with only one remaining node, but it also means that the voting power of the quorum device is great since it is the same as number_of_nodes - 1. Losing connection with the quorum device means losing number_of_nodes - 1 votes, which means that only a cluster with all nodes active can remain quorate (by overvoting the quorum device); any other cluster becomes inquorate.
								</p></div></rh-alert><p class="simpara">
								For more detailed information about the implementation of these algorithms, see the <code class="literal">corosync-qdevice</code>(8) man page.
							</p></li></ul></div></li><li class="listitem">
						The cluster nodes are <code class="literal">node1</code> and <code class="literal">node2</code>.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On the node that you will use to host your quorum device, configure the quorum device with the following command. This command configures and starts the quorum device model <code class="literal">net</code> and configures the device to start on boot.
					</p><pre class="literallayout">[root@qdevice:~]# <span class="strong strong"><strong>pcs qdevice setup model net --enable --start</strong></span>
Quorum device 'net' initialized
quorum device enabled
Starting quorum device...
quorum device started</pre><p class="simpara">
						After configuring the quorum device, you can check its status. This should show that the <code class="literal">corosync-qnetd</code> daemon is running and, at this point, there are no clients connected to it. The <code class="literal option">--full</code> command option provides detailed output.
					</p><pre class="literallayout">[root@qdevice:~]# <span class="strong strong"><strong>pcs qdevice status net --full</strong></span>
QNetd address:                  *:5403
TLS:                            Supported (client certificate required)
Connected clients:              0
Connected clusters:             0
Maximum send/receive size:      32768/32768 bytes</pre></li><li class="listitem"><p class="simpara">
						Enable the ports on the firewall needed by the <code class="literal">pcsd</code> daemon and the <code class="literal">net</code> quorum device by enabling the <code class="literal">high-availability</code> service on <code class="literal">firewalld</code> with following commands.
					</p><pre class="literallayout">[root@qdevice:~]# <span class="strong strong"><strong>firewall-cmd --permanent --add-service=high-availability</strong></span>
[root@qdevice:~]# <span class="strong strong"><strong>firewall-cmd --add-service=high-availability</strong></span></pre></li><li class="listitem"><p class="simpara">
						From one of the nodes in the existing cluster, authenticate user <code class="literal">hacluster</code> on the node that is hosting the quorum device. This allows <code class="literal">pcs</code> on the cluster to connect to <code class="literal">pcs</code> on the <code class="literal">qdevice</code> host, but does not allow <code class="literal">pcs</code> on the <code class="literal">qdevice</code> host to connect to <code class="literal">pcs</code> on the cluster.
					</p><pre class="literallayout">[root@node1:~] # <span class="strong strong"><strong>pcs host auth qdevice</strong></span>
Username: hacluster
Password:
qdevice: Authorized</pre></li><li class="listitem"><p class="simpara">
						Add the quorum device to the cluster.
					</p><p class="simpara">
						Before adding the quorum device, you can check the current configuration and status for the quorum device for later comparison. The output for these commands indicates that the cluster is not yet using a quorum device, and the <code class="literal">Qdevice</code> membership status for each node is <code class="literal">NR</code> (Not Registered).
					</p><pre class="literallayout">[root@node1:~]# <span class="strong strong"><strong>pcs quorum config</strong></span>
Options:</pre><pre class="literallayout">[root@node1:~]# <span class="strong strong"><strong>pcs quorum status</strong></span>
Quorum information
------------------
Date:             Wed Jun 29 13:15:36 2016
Quorum provider:  corosync_votequorum
Nodes:            2
Node ID:          1
Ring ID:          1/8272
Quorate:          Yes

Votequorum information
----------------------
Expected votes:   2
Highest expected: 2
Total votes:      2
Quorum:           1
Flags:            2Node Quorate

Membership information
----------------------
    Nodeid      Votes    Qdevice Name
         1          1         NR node1 (local)
         2          1         NR node2</pre><p class="simpara">
						The following command adds the quorum device that you have previously created to the cluster. You cannot use more than one quorum device in a cluster at the same time. However, one quorum device can be used by several clusters at the same time. This example command configures the quorum device to use the <code class="literal">ffsplit</code> algorithm. For information about the configuration options for the quorum device, see the <code class="literal">corosync-qdevice</code>(8) man page.
					</p><pre class="literallayout">[root@node1:~]# <span class="strong strong"><strong>pcs quorum device add model net host=qdevice algorithm=ffsplit</strong></span>
Setting up qdevice certificates on nodes...
node2: Succeeded
node1: Succeeded
Enabling corosync-qdevice...
node1: corosync-qdevice enabled
node2: corosync-qdevice enabled
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded
Corosync configuration reloaded
Starting corosync-qdevice...
node1: corosync-qdevice started
node2: corosync-qdevice started</pre></li><li class="listitem"><p class="simpara">
						Check the configuration status of the quorum device.
					</p><p class="simpara">
						From the cluster side, you can execute the following commands to see how the configuration has changed.
					</p><p class="simpara">
						The <code class="literal command">pcs quorum config</code> shows the quorum device that has been configured.
					</p><pre class="literallayout">[root@node1:~]# <span class="strong strong"><strong>pcs quorum config</strong></span>
Options:
Device:
  Model: net
    algorithm: ffsplit
    host: qdevice</pre><p class="simpara">
						The <code class="literal command">pcs quorum status</code> command shows the quorum runtime status, indicating that the quorum device is in use. The meanings of of the <code class="literal">Qdevice</code> membership information status values for each cluster node are as follows:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">A/NA</code> — The quorum device is alive or not alive, indicating whether there is a heartbeat between <code class="literal">qdevice</code> and <code class="literal">corosync</code>. This should always indicate that the quorum device is alive.
							</li><li class="listitem">
								<code class="literal">V/NV</code> — <code class="literal">V</code> is set when the quorum device has given a vote to a node. In this example, both nodes are set to <code class="literal">V</code> since they can communicate with each other. If the cluster were to split into two single-node clusters, one of the nodes would be set to <code class="literal">V</code> and the other node would be set to <code class="literal">NV</code>.
							</li><li class="listitem"><p class="simpara">
								<code class="literal">MW/NMW</code> — The internal quorum device flag is set (<code class="literal">MW</code>) or not set (<code class="literal">NMW</code>). By default the flag is not set and the value is <code class="literal">NMW</code>.
							</p><pre class="literallayout">[root@node1:~]# <span class="strong strong"><strong>pcs quorum status</strong></span>
Quorum information
------------------
Date:             Wed Jun 29 13:17:02 2016
Quorum provider:  corosync_votequorum
Nodes:            2
Node ID:          1
Ring ID:          1/8272
Quorate:          Yes

Votequorum information
----------------------
Expected votes:   3
Highest expected: 3
Total votes:      3
Quorum:           2
Flags:            Quorate Qdevice

Membership information
----------------------
    Nodeid      Votes    Qdevice Name
         1          1    A,V,NMW node1 (local)
         2          1    A,V,NMW node2
         0          1            Qdevice</pre><p class="simpara">
								The <code class="literal command">pcs quorum device status</code> shows the quorum device runtime status.
							</p><pre class="literallayout">[root@node1:~]# <span class="strong strong"><strong>pcs quorum device status</strong></span>
Qdevice information
-------------------
Model:                  Net
Node ID:                1
Configured node list:
    0   Node ID = 1
    1   Node ID = 2
Membership node list:   1, 2

Qdevice-net information
----------------------
Cluster name:           mycluster
QNetd host:             qdevice:5403
Algorithm:              ffsplit
Tie-breaker:            Node with lowest node ID
State:                  Connected</pre><p class="simpara">
								From the quorum device side, you can execute the following status command, which shows the status of the <code class="literal">corosync-qnetd</code> daemon.
							</p><pre class="literallayout">[root@qdevice:~]# <span class="strong strong"><strong>pcs qdevice status net --full</strong></span>
QNetd address:                  *:5403
TLS:                            Supported (client certificate required)
Connected clients:              2
Connected clusters:             1
Maximum send/receive size:      32768/32768 bytes
Cluster "mycluster":
    Algorithm:          ffsplit
    Tie-breaker:        Node with lowest node ID
    Node ID 2:
        Client address:         ::ffff:192.168.122.122:50028
        HB interval:            8000ms
        Configured node list:   1, 2
        Ring ID:                1.2050
        Membership node list:   1, 2
        TLS active:             Yes (client certificate verified)
        Vote:                   ACK (ACK)
    Node ID 1:
        Client address:         ::ffff:192.168.122.121:48786
        HB interval:            8000ms
        Configured node list:   1, 2
        Ring ID:                1.2050
        Membership node list:   1, 2
        TLS active:             Yes (client certificate verified)
        Vote:                   ACK (ACK)</pre></li></ul></div></li></ol></div></section><section class="section" id="proc_managing-quorum-device-service-configuring-quorum-devices"><div class="titlepage"><div><div><h3 class="title">27.3. Managing the quorum device service</h3></div></div></div><p class="_abstract _abstract">
				PCS provides the ability to manage the quorum device service on the local host (<code class="literal command">corosync-qnetd</code>), as shown in the following example commands. Note that these commands affect only the <code class="literal">corosync-qnetd</code> service.
			</p><pre class="literallayout">[root@qdevice:~]# <span class="strong strong"><strong>pcs qdevice start net</strong></span>
[root@qdevice:~]# <span class="strong strong"><strong>pcs qdevice stop net</strong></span>
[root@qdevice:~]# <span class="strong strong"><strong>pcs qdevice enable net</strong></span>
[root@qdevice:~]# <span class="strong strong"><strong>pcs qdevice disable net</strong></span>
[root@qdevice:~]# <span class="strong strong"><strong>pcs qdevice kill net</strong></span></pre></section><section class="section" id="proc_managing-quorum-device-settings_configuring-quorum-devices"><div class="titlepage"><div><div><h3 class="title">27.4. Managing a quorum device in a cluster</h3></div></div></div><p class="_abstract _abstract">
				There are a variety of <code class="literal">pcs</code> commands that you can use to change the quorum device settings in a cluster, disable a quorum device, and remove a quorum device.
			</p><section class="section" id="changing_quorum_device_settings"><div class="titlepage"><div><div><h4 class="title">27.4.1. Changing quorum device settings</h4></div></div></div><p>
					You can change the setting of a quorum device with the <code class="literal command">pcs quorum device update</code> command.
				</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
						To change the <code class="literal">host</code> option of quorum device model <code class="literal">net</code>, use the <code class="literal command">pcs quorum device remove</code> and the <code class="literal command">pcs quorum device add</code> commands to set up the configuration properly, unless the old and the new host are the same machine.
					</p></div></rh-alert><p>
					The following command changes the quorum device algorithm to <code class="literal">lms</code>.
				</p><pre class="literallayout">[root@node1:~]# <span class="strong strong"><strong>pcs quorum device update model algorithm=lms</strong></span>
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded
Corosync configuration reloaded
Reloading qdevice configuration on nodes...
node1: corosync-qdevice stopped
node2: corosync-qdevice stopped
node1: corosync-qdevice started
node2: corosync-qdevice started</pre></section><section class="section" id="removing_a_quorum_device"><div class="titlepage"><div><div><h4 class="title">27.4.2. Removing a quorum device</h4></div></div></div><p>
					The following command removes a quorum device configured on a cluster node.
				</p><pre class="literallayout">[root@node1:~]# <span class="strong strong"><strong>pcs quorum device remove</strong></span>
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded
Corosync configuration reloaded
Disabling corosync-qdevice...
node1: corosync-qdevice disabled
node2: corosync-qdevice disabled
Stopping corosync-qdevice...
node1: corosync-qdevice stopped
node2: corosync-qdevice stopped
Removing qdevice certificates from nodes...
node1: Succeeded
node2: Succeeded</pre><p>
					After you have removed a quorum device, you should see the following error message when displaying the quorum device status.
				</p><pre class="literallayout">[root@node1:~]# <span class="strong strong"><strong>pcs quorum device status</strong></span>
Error: Unable to get quorum status: corosync-qdevice-tool: Can't connect to QDevice socket (is QDevice running?): No such file or directory</pre></section><section class="section" id="destroying_a_quorum_device"><div class="titlepage"><div><div><h4 class="title">27.4.3. Destroying a quorum device</h4></div></div></div><p>
					The following command disables and stops a quorum device on the quorum device host and deletes all of its configuration files.
				</p><pre class="literallayout">[root@qdevice:~]# <span class="strong strong"><strong>pcs qdevice destroy net</strong></span>
Stopping quorum device...
quorum device stopped
quorum device disabled
Quorum device 'net' configuration files removed</pre></section></section></section><section class="chapter" id="assembly_configuring-pacemaker-alert-agents_configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 28. Triggering scripts for cluster events</h2></div></div></div><p class="_abstract _abstract">
			A Pacemaker cluster is an event-driven system, where an event might be a resource or node failure, a configuration change, or a resource starting or stopping. You can configure Pacemaker cluster alerts to take some external action when a cluster event occurs by means of alert agents, which are external programs that the cluster calls in the same manner as the cluster calls resource agents to handle resource configuration and operation.
		</p><p>
			The cluster passes information about the event to the agent by means of environment variables. Agents can do anything with this information, such as send an email message or log to a file or update a monitoring system.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Pacemaker provides several sample alert agents, which are installed in <code class="literal">/usr/share/pacemaker/alerts</code> by default. These sample scripts may be copied and used as is, or they may be used as templates to be edited to suit your purposes. Refer to the source code of the sample agents for the full set of attributes they support.
				</li><li class="listitem">
					If the sample alert agents do not meet your needs, you can write your own alert agents for a Pacemaker alert to call.
				</li></ul></div><section class="section" id="using-sample-alert-agents-configuring-pacemaker-alert-agents"><div class="titlepage"><div><div><h3 class="title">28.1. Installing and configuring sample alert agents</h3></div></div></div><p class="_abstract _abstract">
				When you use one of the sample alert agents, you should review the script to ensure that it suits your needs. These sample agents are provided as a starting point for custom scripts for specific cluster environments. Note that while Red Hat supports the interfaces that the alert agents scripts use to communicate with Pacemaker, Red Hat does not provide support for the custom agents themselves.
			</p><p>
				To use one of the sample alert agents, you must install the agent on each node in the cluster. For example, the following command installs the <code class="literal">alert_file.sh.sample</code> script as <code class="literal">alert_file.sh</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>install --mode=0755 /usr/share/pacemaker/alerts/alert_file.sh.sample /var/lib/pacemaker/alert_file.sh</strong></span></pre><p>
				After you have installed the script, you can create an alert that uses the script.
			</p><p>
				The following example configures an alert that uses the installed <code class="literal">alert_file.sh</code> alert agent to log events to a file. Alert agents run as the user <code class="literal">hacluster</code>, which has a minimal set of permissions.
			</p><p>
				This example creates the log file <code class="literal">pcmk_alert_file.log</code> that will be used to record the events. It then creates the alert agent and adds the path to the log file as its recipient.
			</p><pre class="literallayout"># <span class="strong strong"><strong>touch /var/log/pcmk_alert_file.log</strong></span>
# <span class="strong strong"><strong>chown hacluster:haclient /var/log/pcmk_alert_file.log</strong></span>
# <span class="strong strong"><strong>chmod 600 /var/log/pcmk_alert_file.log</strong></span>
# <span class="strong strong"><strong>pcs alert create id=alert_file description="Log events to a file." path=/var/lib/pacemaker/alert_file.sh</strong></span>
# <span class="strong strong"><strong>pcs alert recipient add alert_file id=my-alert_logfile value=/var/log/pcmk_alert_file.log</strong></span></pre><p>
				The following example installs the <code class="literal">alert_snmp.sh.sample</code> script as <code class="literal">alert_snmp.sh</code> and configures an alert that uses the installed <code class="literal">alert_snmp.sh</code> alert agent to send cluster events as SNMP traps. By default, the script will send all events except successful monitor calls to the SNMP server. This example configures the timestamp format as a meta option. After configuring the alert, this example configures a recipient for the alert and displays the alert configuration.
			</p><pre class="literallayout"># <span class="strong strong"><strong>install --mode=0755 /usr/share/pacemaker/alerts/alert_snmp.sh.sample /var/lib/pacemaker/alert_snmp.sh</strong></span>
# <span class="strong strong"><strong>pcs alert create id=snmp_alert path=/var/lib/pacemaker/alert_snmp.sh meta timestamp-format="%Y-%m-%d,%H:%M:%S.%01N"</strong></span>
# <span class="strong strong"><strong>pcs alert recipient add snmp_alert value=192.168.1.2</strong></span>
# <span class="strong strong"><strong>pcs alert</strong></span>
Alerts:
 Alert: snmp_alert (path=/var/lib/pacemaker/alert_snmp.sh)
  Meta options: timestamp-format=%Y-%m-%d,%H:%M:%S.%01N.
  Recipients:
   Recipient: snmp_alert-recipient (value=192.168.1.2)</pre><p>
				The following example installs the <code class="literal">alert_smtp.sh</code> agent and then configures an alert that uses the installed alert agent to send cluster events as email messages. After configuring the alert, this example configures a recipient and displays the alert configuration.
			</p><pre class="literallayout"># <span class="strong strong"><strong>install --mode=0755 /usr/share/pacemaker/alerts/alert_smtp.sh.sample /var/lib/pacemaker/alert_smtp.sh</strong></span>
# <span class="strong strong"><strong>pcs alert create id=smtp_alert path=/var/lib/pacemaker/alert_smtp.sh options email_sender=donotreply@example.com</strong></span>
# <span class="strong strong"><strong>pcs alert recipient add smtp_alert value=admin@example.com</strong></span>
# <span class="strong strong"><strong>pcs alert</strong></span>
Alerts:
 Alert: smtp_alert (path=/var/lib/pacemaker/alert_smtp.sh)
  Options: email_sender=donotreply@example.com
  Recipients:
   Recipient: smtp_alert-recipient (value=admin@example.com)</pre></section><section class="section" id="creating-cluster-alert-configuring-pacemaker-alert-agents"><div class="titlepage"><div><div><h3 class="title">28.2. Creating a cluster alert</h3></div></div></div><p class="_abstract _abstract">
				The following command creates a cluster alert. The options that you configure are agent-specific configuration values that are passed to the alert agent script at the path you specify as additional environment variables. If you do not specify a value for <code class="literal">id</code>, one will be generated.
			</p><pre class="literallayout">pcs alert create path=<span class="emphasis"><em>path</em></span> [id=<span class="emphasis"><em>alert-id</em></span>] [description=<span class="emphasis"><em>description</em></span>] [options [<span class="emphasis"><em>option</em></span>=<span class="emphasis"><em>value</em></span>]...] [meta [<span class="emphasis"><em>meta-option</em></span>=<span class="emphasis"><em>value</em></span>]...]</pre><p>
				Multiple alert agents may be configured; the cluster will call all of them for each event. Alert agents will be called only on cluster nodes. They will be called for events involving Pacemaker Remote nodes, but they will never be called on those nodes.
			</p><p>
				The following example creates a simple alert that will call <code class="literal">myscript.sh</code> for each event.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs alert create id=my_alert path=/path/to/myscript.sh</strong></span></pre></section><section class="section" id="managing-cluster-alert-configuring-pacemaker-alert-agents"><div class="titlepage"><div><div><h3 class="title">28.3. Displaying, modifying, and removing cluster alerts</h3></div></div></div><p class="_abstract _abstract">
				There are a variety of <code class="literal">pcs</code> commands you can use to display, modify, and remove cluster alerts.
			</p><p>
				The following command shows all configured alerts along with the values of the configured options.
			</p><pre class="literallayout">pcs alert [config|show]</pre><p>
				The following command updates an existing alert with the specified <span class="emphasis"><em>alert-id</em></span> value.
			</p><pre class="literallayout">pcs alert update <span class="emphasis"><em>alert-id</em></span> [path=<span class="emphasis"><em>path</em></span>] [description=<span class="emphasis"><em>description</em></span>] [options [<span class="emphasis"><em>option</em></span>=<span class="emphasis"><em>value</em></span>]...] [meta [<span class="emphasis"><em>meta-option</em></span>=<span class="emphasis"><em>value</em></span>]...]</pre><p>
				The following command removes an alert with the specified <span class="emphasis"><em>alert-id</em></span> value.
			</p><pre class="literallayout">pcs alert remove <span class="emphasis"><em>alert-id</em></span></pre><p>
				Alternately, you can run the <code class="literal">pcs alert delete</code> command, which is identical to the <code class="literal">pcs alert remove</code> command. Both the <code class="literal">pcs alert delete</code> and the <code class="literal">pcs alert remove</code> commands allow you to specify more than one alert to be deleted.
			</p></section><section class="section" id="configuring-alert-recipients-configuring-pacemaker-alert-agents"><div class="titlepage"><div><div><h3 class="title">28.4. Configuring cluster alert recipients</h3></div></div></div><p class="_abstract _abstract">
				Usually alerts are directed towards a recipient. Thus each alert may be additionally configured with one or more recipients. The cluster will call the agent separately for each recipient.
			</p><p>
				The recipient may be anything the alert agent can recognize: an IP address, an email address, a file name, or whatever the particular agent supports.
			</p><p>
				The following command adds a new recipient to the specified alert.
			</p><pre class="literallayout">pcs alert recipient add <span class="emphasis"><em>alert-id</em></span> value=<span class="emphasis"><em>recipient-value</em></span> [id=<span class="emphasis"><em>recipient-id</em></span>] [description=<span class="emphasis"><em>description</em></span>] [options [<span class="emphasis"><em>option</em></span>=<span class="emphasis"><em>value</em></span>]...] [meta [<span class="emphasis"><em>meta-option</em></span>=<span class="emphasis"><em>value</em></span>]...]</pre><p>
				The following command updates an existing alert recipient.
			</p><pre class="literallayout">pcs alert recipient update <span class="emphasis"><em>recipient-id</em></span> [value=<span class="emphasis"><em>recipient-value</em></span>] [description=<span class="emphasis"><em>description</em></span>] [options [<span class="emphasis"><em>option</em></span>=<span class="emphasis"><em>value</em></span>]...] [meta [<span class="emphasis"><em>meta-option</em></span>=<span class="emphasis"><em>value</em></span>]...]</pre><p>
				The following command removes the specified alert recipient.
			</p><pre class="literallayout">pcs alert recipient remove <span class="emphasis"><em>recipient-id</em></span></pre><p>
				Alternately, you can run the <code class="literal">pcs alert recipient delete</code> command, which is identical to the <code class="literal">pcs alert recipient remove</code> command. Both the <code class="literal">pcs alert recipient remove</code> and the <code class="literal">pcs alert recipient delete</code> commands allow you to remove more than one alert recipient.
			</p><p>
				The following example command adds the alert recipient <code class="literal">my-alert-recipient</code> with a recipient ID of <code class="literal">my-recipient-id</code> to the alert <code class="literal">my-alert</code>. This will configure the cluster to call the alert script that has been configured for <code class="literal">my-alert</code> for each event, passing the recipient <code class="literal">some-address</code> as an environment variable.
			</p><pre class="literallayout">#  <span class="strong strong"><strong>pcs alert recipient add my-alert value=my-alert-recipient id=my-recipient-id options value=some-address</strong></span></pre></section><section class="section" id="cluster-alert-meta-options-configuring-pacemaker-alert-agents"><div class="titlepage"><div><div><h3 class="title">28.5. Alert meta options</h3></div></div></div><p class="_abstract _abstract">
				As with resource agents, meta options can be configured for alert agents to affect how Pacemaker calls them. The following table describes the alert meta options. Meta options can be configured per alert agent as well as per recipient.
			</p><rh-table id="tb-alert-meta-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 28.1. Alert Meta Options</caption><colgroup><col style="width: 33%; " class="col_1"><!--Empty--><col style="width: 33%; " class="col_2"><!--Empty--><col style="width: 33%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686140229520" scope="col">Meta-Attribute</th><th align="left" valign="top" id="idm140686140228432" scope="col">Default</th><th align="left" valign="top" id="idm140686140227344" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686140229520"> <p>
								<code class="literal">enabled</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686140228432"> <p>
								<code class="literal">true</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686140227344"> <p>
								(RHEL 9.3 and later) If set to <code class="literal">false</code> for an alert, the alert will not be used. If set to <code class="literal">true</code> for an alert and <code class="literal">false</code> for a particular recipient of that alert, that recipient will not be used.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686140229520"> <p>
								<code class="literal">timestamp-format</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686140228432"> <p>
								%H:%M:%S.%06N
							</p>
							 </td><td align="left" valign="top" headers="idm140686140227344"> <p>
								Format the cluster will use when sending the event’s timestamp to the agent. This is a string as used with the <code class="literal">date</code>(1) command.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686140229520"> <p>
								<code class="literal">timeout</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686140228432"> <p>
								30s
							</p>
							 </td><td align="left" valign="top" headers="idm140686140227344"> <p>
								If the alert agent does not complete within this amount of time, it will be terminated.
							</p>
							 </td></tr></tbody></table></rh-table><p>
				The following example configures an alert that calls the script <code class="literal">myscript.sh</code> and then adds two recipients to the alert. The first recipient has an ID of <code class="literal">my-alert-recipient1</code> and the second recipient has an ID of <code class="literal">my-alert-recipient2</code>. The script will get called twice for each event, with each call using a 15-second timeout. One call will be passed to the recipient <code class="literal">someuser@example.com</code> with a timestamp in the format %D %H:%M, while the other call will be passed to the recipient <code class="literal">otheruser@example.com</code> with a timestamp in the format %c.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs alert create id=my-alert path=/path/to/myscript.sh meta timeout=15s</strong></span>
# <span class="strong strong"><strong>pcs alert recipient add my-alert value=someuser@example.com id=my-alert-recipient1 meta timestamp-format="%D %H:%M"</strong></span>
# <span class="strong strong"><strong>pcs alert recipient add my-alert value=otheruser@example.com id=my-alert-recipient2 meta timestamp-format="%c"</strong></span></pre></section><section class="section" id="cluster-alert-configuration-configuring-pacemaker-alert-agents"><div class="titlepage"><div><div><h3 class="title">28.6. Cluster alert configuration command examples</h3></div></div></div><p class="_abstract _abstract">
				The following sequential examples show some basic alert configuration commands to show the format to use to create alerts, add recipients, and display the configured alerts.
			</p><p>
				Note that while you must install the alert agents themselves on each node in a cluster, you need to run the <code class="literal">pcs</code> commands only once.
			</p><p>
				The following commands create a simple alert, add two recipients to the alert, and display the configured values.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Since no alert ID value is specified, the system creates an alert ID value of <code class="literal">alert</code>.
					</li><li class="listitem">
						The first recipient creation command specifies a recipient of <code class="literal">rec_value</code>. Since this command does not specify a recipient ID, the value of <code class="literal">alert-recipient</code> is used as the recipient ID.
					</li><li class="listitem">
						The second recipient creation command specifies a recipient of <code class="literal">rec_value2</code>. This command specifies a recipient ID of <code class="literal">my-recipient</code> for the recipient.
					</li></ul></div><pre class="literallayout"># <span class="strong strong"><strong>pcs alert create path=/my/path</strong></span>
# <span class="strong strong"><strong>pcs alert recipient add alert value=rec_value</strong></span>
# <span class="strong strong"><strong>pcs alert recipient add alert value=rec_value2 id=my-recipient</strong></span>
# <span class="strong strong"><strong>pcs alert config</strong></span>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)
   Recipient: my-recipient (value=rec_value2)</pre><p>
				This following commands add a second alert and a recipient for that alert. The alert ID for the second alert is <code class="literal">my-alert</code> and the recipient value is <code class="literal">my-other-recipient</code>. Since no recipient ID is specified, the system provides a recipient id of <code class="literal">my-alert-recipient</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs alert create id=my-alert path=/path/to/script description=alert_description options option1=value1 opt=val meta timeout=50s timestamp-format="%H%B%S"</strong></span>
# <span class="strong strong"><strong>pcs alert recipient add my-alert value=my-other-recipient</strong></span>
# <span class="strong strong"><strong>pcs alert</strong></span>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)
   Recipient: my-recipient (value=rec_value2)
 Alert: my-alert (path=/path/to/script)
  Description: alert_description
  Options: opt=val option1=value1
  Meta options: timestamp-format=%H%B%S timeout=50s
  Recipients:
   Recipient: my-alert-recipient (value=my-other-recipient)</pre><p>
				The following commands modify the alert values for the alert <code class="literal">my-alert</code> and for the recipient <code class="literal">my-alert-recipient</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs alert update my-alert options option1=newvalue1 meta timestamp-format="%H%M%S"</strong></span>
# <span class="strong strong"><strong>pcs alert recipient update my-alert-recipient options option1=new meta timeout=60s</strong></span>
# <span class="strong strong"><strong>pcs alert</strong></span>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)
   Recipient: my-recipient (value=rec_value2)
 Alert: my-alert (path=/path/to/script)
  Description: alert_description
  Options: opt=val option1=newvalue1
  Meta options: timestamp-format=%H%M%S timeout=50s
  Recipients:
   Recipient: my-alert-recipient (value=my-other-recipient)
    Options: option1=new
    Meta options: timeout=60s</pre><p>
				The following command removes the recipient <code class="literal">my-alert-recipient</code> from <code class="literal">alert</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs alert recipient remove my-recipient</strong></span>
# <span class="strong strong"><strong>pcs alert</strong></span>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)
 Alert: my-alert (path=/path/to/script)
  Description: alert_description
  Options: opt=val option1=newvalue1
  Meta options: timestamp-format="%M%B%S" timeout=50s
  Recipients:
   Recipient: my-alert-recipient (value=my-other-recipient)
    Options: option1=new
    Meta options: timeout=60s</pre><p>
				The following command removes <code class="literal">myalert</code> from the configuration.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs alert remove myalert</strong></span>
# <span class="strong strong"><strong>pcs alert</strong></span>
Alerts:
 Alert: alert (path=/my/path)
  Recipients:
   Recipient: alert-recipient (value=rec_value)</pre></section><section class="section" id="writing-cluster-alert-agent-configuring-pacemaker-alert-agents"><div class="titlepage"><div><div><h3 class="title">28.7. Writing a cluster alert agent</h3></div></div></div><p class="_abstract _abstract">
				There are three types of Pacemaker cluster alerts: node alerts, fencing alerts, and resource alerts. The environment variables that are passed to the alert agents can differ, depending on the type of alert. The following table describes the environment variables that are passed to alert agents and specifies when the environment variable is associated with a specific alert type.
			</p><rh-table id="tb-alert-environmentvariables-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 28.2. Environment Variables Passed to Alert Agents</caption><colgroup><col style="width: 44%; " class="col_1"><!--Empty--><col style="width: 56%; " class="col_2"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686137395072" scope="col">Environment Variable</th><th align="left" valign="top" id="idm140686137393984" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686137395072"> <p>
								<code class="literal">CRM_alert_kind</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137393984"> <p>
								The type of alert (node, fencing, or resource)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137395072"> <p>
								<code class="literal">CRM_alert_version</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137393984"> <p>
								The version of Pacemaker sending the alert
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137395072"> <p>
								<code class="literal">CRM_alert_recipient</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137393984"> <p>
								The configured recipient
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137395072"> <p>
								<code class="literal">CRM_alert_node_sequence</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137393984"> <p>
								A sequence number increased whenever an alert is being issued on the local node, which can be used to reference the order in which alerts have been issued by Pacemaker. An alert for an event that happened later in time reliably has a higher sequence number than alerts for earlier events. Be aware that this number has no cluster-wide meaning.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137395072"> <p>
								<code class="literal">CRM_alert_timestamp</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137393984"> <p>
								A timestamp created prior to executing the agent, in the format specified by the <code class="literal">timestamp-format</code> meta option. This allows the agent to have a reliable, high-precision time of when the event occurred, regardless of when the agent itself was invoked (which could potentially be delayed due to system load or other circumstances).
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137395072"> <p>
								<code class="literal">CRM_alert_node</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137393984"> <p>
								Name of affected node
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137395072"> <p>
								<code class="literal">CRM_alert_desc</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137393984"> <p>
								Detail about event. For node alerts, this is the node’s current state (member or lost). For fencing alerts, this is a summary of the requested fencing operation, including origin, target, and fencing operation error code, if any. For resource alerts, this is a readable string equivalent of <code class="literal">CRM_alert_status</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137395072"> <p>
								<code class="literal">CRM_alert_nodeid</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137393984"> <p>
								ID of node whose status changed (provided with node alerts only)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137395072"> <p>
								<code class="literal">CRM_alert_task</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137393984"> <p>
								The requested fencing or resource operation (provided with fencing and resource alerts only)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137395072"> <p>
								<code class="literal">CRM_alert_rc</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137393984"> <p>
								The numerical return code of the fencing or resource operation (provided with fencing and resource alerts only)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137395072"> <p>
								<code class="literal">CRM_alert_rsc</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137393984"> <p>
								The name of the affected resource (resource alerts only)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137395072"> <p>
								<code class="literal">CRM_alert_interval</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137393984"> <p>
								The interval of the resource operation (resource alerts only)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137395072"> <p>
								<code class="literal">CRM_alert_target_rc</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137393984"> <p>
								The expected numerical return code of the operation (resource alerts only)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140686137395072"> <p>
								<code class="literal">CRM_alert_status</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140686137393984"> <p>
								A numerical code used by Pacemaker to represent the operation result (resource alerts only)
							</p>
							 </td></tr></tbody></table></rh-table><p>
				When writing an alert agent, you must take the following concerns into account.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Alert agents may be called with no recipient (if none is configured), so the agent must be able to handle this situation, even if it only exits in that case. Users may modify the configuration in stages, and add a recipient later.
					</li><li class="listitem">
						If more than one recipient is configured for an alert, the alert agent will be called once per recipient. If an agent is not able to run concurrently, it should be configured with only a single recipient. The agent is free, however, to interpret the recipient as a list.
					</li><li class="listitem">
						When a cluster event occurs, all alerts are fired off at the same time as separate processes. Depending on how many alerts and recipients are configured and on what is done within the alert agents, a significant load burst may occur. The agent could be written to take this into consideration, for example by queueing resource-intensive actions into some other instance, instead of directly executing them.
					</li><li class="listitem">
						Alert agents are run as the <code class="literal">hacluster</code> user, which has a minimal set of permissions. If an agent requires additional privileges, it is recommended to configure <code class="literal command">sudo</code> to allow the agent to run the necessary commands as another user with the appropriate privileges.
					</li><li class="listitem">
						Take care to validate and sanitize user-configured parameters, such as <code class="literal">CRM_alert_timestamp</code> (whose content is specified by the user-configured <code class="literal">timestamp-format</code>), <code class="literal">CRM_alert_recipient</code>, and all alert options. This is necessary to protect against configuration errors. In addition, if some user can modify the CIB without having <code class="literal">hacluster</code>-level access to the cluster nodes, this is a potential security concern as well, and you should avoid the possibility of code injection.
					</li><li class="listitem">
						If a cluster contains resources with operations for which the <code class="literal">on-fail</code> parameter is set to <code class="literal">fence</code>, there will be multiple fence notifications on failure, one for each resource for which this parameter is set plus one additional notification. Both the <code class="literal">pacemaker-fenced</code> and <code class="literal">pacemaker-controld</code> will send notifications. Pacemaker performs only one actual fence operation in this case, however, no matter how many notifications are sent.
					</li></ul></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					The alerts interface is designed to be backward compatible with the external scripts interface used by the <code class="literal">ocf:pacemaker:ClusterMon</code> resource. To preserve this compatibility, the environment variables passed to alert agents are available prepended with <code class="literal">CRM_notify_</code> as well as <code class="literal">CRM_alert_</code>. One break in compatibility is that the <code class="literal">ClusterMon</code> resource ran external scripts as the root user, while alert agents are run as the <code class="literal">hacluster</code> user.
				</p></div></rh-alert></section></section><section class="chapter" id="assembly_configuring-multisite-cluster-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 29. Multi-site Pacemaker clusters</h2></div></div></div><p class="_abstract _abstract">
			When a cluster spans more than one site, issues with network connectivity between the sites can lead to split-brain situations. When connectivity drops, there is no way for a node on one site to determine whether a node on another site has failed or is still functioning with a failed site interlink. In addition, it can be problematic to provide high availability services across two sites which are too far apart to keep synchronous. To address these issues, Pacemaker provides full support for the ability to configure high availability clusters that span multiple sites through the use of a Booth cluster ticket manager.
		</p><section class="section" id="con_booth-cluster-ticket-manager-configuring-multisite-cluster"><div class="titlepage"><div><div><h3 class="title">29.1. Overview of Booth cluster ticket manager</h3></div></div></div><p class="_abstract _abstract">
				The Booth <span class="emphasis"><em>ticket manager</em></span> is a distributed service that is meant to be run on a different physical network than the networks that connect the cluster nodes at particular sites. It yields another, loose cluster, a <span class="emphasis"><em>Booth formation</em></span>, that sits on top of the regular clusters at the sites. This aggregated communication layer facilitates consensus-based decision processes for individual Booth tickets.
			</p><p>
				A Booth <span class="emphasis"><em>ticket</em></span> is a singleton in the Booth formation and represents a time-sensitive, movable unit of authorization. Resources can be configured to require a certain ticket to run. This can ensure that resources are run at only one site at a time, for which a ticket or tickets have been granted.
			</p><p>
				You can think of a Booth formation as an overlay cluster consisting of clusters running at different sites, where all the original clusters are independent of each other. It is the Booth service which communicates to the clusters whether they have been granted a ticket, and it is Pacemaker that determines whether to run resources in a cluster based on a Pacemaker ticket constraint. This means that when using the ticket manager, each of the clusters can run its own resources as well as shared resources. For example there can be resources A, B and C running only in one cluster, resources D, E, and F running only in the other cluster, and resources G and H running in either of the two clusters as determined by a ticket. It is also possible to have an additional resource J that could run in either of the two clusters as determined by a separate ticket.
			</p></section><section class="section" id="proc-configuring-multisite-with-booth-configuring-multisite-cluster"><div class="titlepage"><div><div><h3 class="title">29.2. Configuring multi-site clusters with Pacemaker</h3></div></div></div><p class="_abstract _abstract">
				You can configure a multi-site configuration that uses the Booth ticket manager with the following procedure.
			</p><p>
				These example commands use the following arrangement:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Cluster 1 consists of the nodes <code class="literal">cluster1-node1</code> and <code class="literal">cluster1-node2</code>
					</li><li class="listitem">
						Cluster 1 has a floating IP address assigned to it of 192.168.11.100
					</li><li class="listitem">
						Cluster 2 consists of <code class="literal">cluster2-node1</code> and <code class="literal">cluster2-node2</code>
					</li><li class="listitem">
						Cluster 2 has a floating IP address assigned to it of 192.168.22.100
					</li><li class="listitem">
						The arbitrator node is <code class="literal">arbitrator-node</code> with an ip address of 192.168.99.100
					</li><li class="listitem">
						The name of the Booth ticket that this configuration uses is <code class="literal">apacheticket</code>
					</li></ul></div><p>
				These example commands assume that the cluster resources for an Apache service have been configured as part of the resource group <code class="literal">apachegroup</code> for each cluster. It is not required that the resources and resource groups be the same on each cluster to configure a ticket constraint for those resources, since the Pacemaker instance for each cluster is independent, but that is a common failover scenario.
			</p><p>
				Note that at any time in the configuration procedure you can enter the <code class="literal command">pcs booth config</code> command to display the booth configuration for the current node or cluster or the <code class="literal command">pcs booth status</code> command to display the current status of booth on the local node.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Install the <code class="literal">booth-site</code> Booth ticket manager package on each node of both clusters.
					</p><pre class="literallayout">[root@cluster1-node1 ~]# <span class="strong strong"><strong>dnf install -y booth-site</strong></span>
[root@cluster1-node2 ~]# <span class="strong strong"><strong>dnf install -y booth-site</strong></span>
[root@cluster2-node1 ~]# <span class="strong strong"><strong>dnf install -y booth-site</strong></span>
[root@cluster2-node2 ~]# <span class="strong strong"><strong>dnf install -y booth-site</strong></span></pre></li><li class="listitem"><p class="simpara">
						Install the <code class="literal">pcs</code>, <code class="literal">booth-core</code>, and <code class="literal">booth-arbitrator</code> packages on the arbitrator node.
					</p><pre class="literallayout">[root@arbitrator-node ~]# <span class="strong strong"><strong>dnf install -y pcs booth-core booth-arbitrator</strong></span></pre></li><li class="listitem"><p class="simpara">
						If you are running the <code class="literal">firewalld</code> daemon, execute the following commands on all nodes in both clusters as well as on the arbitrator node to enable the ports that are required by the Red Hat High Availability Add-On.
					</p><pre class="literallayout"># <span class="strong strong"><strong>firewall-cmd --permanent --add-service=high-availability</strong></span>
# <span class="strong strong"><strong>firewall-cmd --add-service=high-availability</strong></span></pre><p class="simpara">
						You may need to modify which ports are open to suit local conditions. For more information about the ports that are required by the Red Hat High-Availability Add-On, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters#proc_enabling-ports-for-high-availability-creating-high-availability-cluster">Enabling ports for the High Availability Add-On</a>.
					</p></li><li class="listitem"><p class="simpara">
						Create a Booth configuration on one node of one cluster. The addresses you specify for each cluster and for the arbitrator must be IP addresses. For each cluster, you specify a floating IP address.
					</p><pre class="literallayout">[cluster1-node1 ~] # <span class="strong strong"><strong>pcs booth setup sites 192.168.11.100 192.168.22.100 arbitrators 192.168.99.100</strong></span></pre><p class="simpara">
						This command creates the configuration files <code class="literal">/etc/booth/booth.conf</code> and <code class="literal">/etc/booth/booth.key</code> on the node from which it is run.
					</p></li><li class="listitem"><p class="simpara">
						Create a ticket for the Booth configuration. This is the ticket that you will use to define the resource constraint that will allow resources to run only when this ticket has been granted to the cluster.
					</p><p class="simpara">
						This basic failover configuration procedure uses only one ticket, but you can create additional tickets for more complicated scenarios where each ticket is associated with a different resource or resources.
					</p><pre class="literallayout">[cluster1-node1 ~] # <span class="strong strong"><strong>pcs booth ticket add apacheticket</strong></span></pre></li><li class="listitem"><p class="simpara">
						Synchronize the Booth configuration to all nodes in the current cluster.
					</p><pre class="literallayout">[cluster1-node1 ~] # <span class="strong strong"><strong>pcs booth sync</strong></span></pre></li><li class="listitem"><p class="simpara">
						From the arbitrator node, pull the Booth configuration to the arbitrator. If you have not previously done so, you must first authenticate <code class="literal">pcs</code> to the node from which you are pulling the configuration.
					</p><pre class="literallayout">[arbitrator-node ~] # <span class="strong strong"><strong>pcs host auth cluster1-node1</strong></span>
[arbitrator-node ~] # <span class="strong strong"><strong>pcs booth pull cluster1-node1</strong></span></pre></li><li class="listitem"><p class="simpara">
						Pull the Booth configuration to the other cluster and synchronize to all the nodes of that cluster. As with the arbitrator node, if you have not previously done so, you must first authenticate <code class="literal">pcs</code> to the node from which you are pulling the configuration.
					</p><pre class="literallayout">[cluster2-node1 ~] # <span class="strong strong"><strong>pcs host auth cluster1-node1</strong></span>
[cluster2-node1 ~] # <span class="strong strong"><strong>pcs booth pull cluster1-node1</strong></span>
[cluster2-node1 ~] # <span class="strong strong"><strong>pcs booth sync</strong></span></pre></li><li class="listitem"><p class="simpara">
						Start and enable Booth on the arbitrator.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							You must not manually start or enable Booth on any of the nodes of the clusters since Booth runs as a Pacemaker resource in those clusters.
						</p></div></rh-alert><pre class="literallayout">[arbitrator-node ~] # <span class="strong strong"><strong>pcs booth start</strong></span>
[arbitrator-node ~] # <span class="strong strong"><strong>pcs booth enable</strong></span></pre></li><li class="listitem"><p class="simpara">
						Configure Booth to run as a cluster resource on both cluster sites, using the floating IP addresses assigned to each cluster. This creates a resource group with <code class="literal">booth-ip</code> and <code class="literal">booth-service</code> as members of that group.
					</p><pre class="literallayout">[cluster1-node1 ~] # <span class="strong strong"><strong>pcs booth create ip 192.168.11.100</strong></span>
[cluster2-node1 ~] # <span class="strong strong"><strong>pcs booth create ip 192.168.22.100</strong></span></pre></li><li class="listitem"><p class="simpara">
						Add a ticket constraint to the resource group you have defined for each cluster.
					</p><pre class="literallayout">[cluster1-node1 ~] # <span class="strong strong"><strong>pcs constraint ticket add apacheticket apachegroup</strong></span>
[cluster2-node1 ~] # <span class="strong strong"><strong>pcs constraint ticket add apacheticket apachegroup</strong></span></pre><p class="simpara">
						You can enter the following command to display the currently configured ticket constraints.
					</p><pre class="literallayout">pcs constraint ticket [show]</pre></li><li class="listitem"><p class="simpara">
						Grant the ticket you created for this setup to the first cluster.
					</p><p class="simpara">
						Note that it is not necessary to have defined ticket constraints before granting a ticket. Once you have initially granted a ticket to a cluster, then Booth takes over ticket management unless you override this manually with the <code class="literal command">pcs booth ticket revoke</code> command. For information about the <code class="literal command">pcs booth</code> administration commands, see the PCS help screen for the <code class="literal command">pcs booth</code> command.
					</p><pre class="literallayout">[cluster1-node1 ~] # <span class="strong strong"><strong>pcs booth ticket grant apacheticket</strong></span></pre></li></ol></div><p>
				It is possible to add or remove tickets at any time, even after completing this procedure. After adding or removing a ticket, however, you must synchronize the configuration files to the other nodes and clusters as well as to the arbitrator and grant the ticket as is shown in this procedure.
			</p><p>
				For information about additional Booth administration commands that you can use for cleaning up and removing Booth configuration files, tickets, and resources, see the PCS help screen for the <code class="literal command">pcs booth</code> command.
			</p></section></section><section class="chapter" id="assembly_remote-node-management-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 30. Integrating non-corosync nodes into a cluster: the pacemaker_remote service</h2></div></div></div><p class="_abstract _abstract">
			The <code class="literal">pacemaker_remote</code> service allows nodes not running <code class="literal">corosync</code> to integrate into the cluster and have the cluster manage their resources just as if they were real cluster nodes.
		</p><p>
			Among the capabilities that the <code class="literal">pacemaker_remote</code> service provides are the following:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					The <code class="literal">pacemaker_remote</code> service allows you to scale beyond the Red Hat support limit of 32 nodes.
				</li><li class="listitem">
					The <code class="literal">pacemaker_remote</code> service allows you to manage a virtual environment as a cluster resource and also to manage individual services within the virtual environment as cluster resources.
				</li></ul></div><p>
			The following terms are used to describe the <code class="literal">pacemaker_remote</code> service.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<span class="emphasis"><em>cluster node</em></span> — A node running the High Availability services (<code class="literal">pacemaker</code> and <code class="literal">corosync</code>).
				</li><li class="listitem">
					<span class="emphasis"><em>remote node</em></span> — A node running <code class="literal">pacemaker_remote</code> to remotely integrate into the cluster without requiring <code class="literal">corosync</code> cluster membership. A remote node is configured as a cluster resource that uses the <code class="literal">ocf:pacemaker:remote</code> resource agent.
				</li><li class="listitem">
					<span class="emphasis"><em>guest node</em></span> — A virtual guest node running the <code class="literal">pacemaker_remote</code> service. The virtual guest resource is managed by the cluster; it is both started by the cluster and integrated into the cluster as a remote node.
				</li><li class="listitem">
					<span class="emphasis"><em>pacemaker_remote</em></span> — A service daemon capable of performing remote application management within remote nodes and KVM guest nodes in a Pacemaker cluster environment. This service is an enhanced version of Pacemaker’s local executor daemon (<code class="literal">pacemaker-execd</code>) that is capable of managing resources remotely on a node not running corosync.
				</li></ul></div><p>
			A Pacemaker cluster running the <code class="literal">pacemaker_remote</code> service has the following characteristics.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Remote nodes and guest nodes run the <code class="literal">pacemaker_remote</code> service (with very little configuration required on the virtual machine side).
				</li><li class="listitem">
					The cluster stack (<code class="literal">pacemaker</code> and <code class="literal">corosync</code>), running on the cluster nodes, connects to the <code class="literal">pacemaker_remote</code> service on the remote nodes, allowing them to integrate into the cluster.
				</li><li class="listitem">
					The cluster stack (<code class="literal">pacemaker</code> and <code class="literal">corosync</code>), running on the cluster nodes, launches the guest nodes and immediately connects to the <code class="literal">pacemaker_remote</code> service on the guest nodes, allowing them to integrate into the cluster.
				</li></ul></div><p>
			The key difference between the cluster nodes and the remote and guest nodes that the cluster nodes manage is that the remote and guest nodes are not running the cluster stack. This means the remote and guest nodes have the following limitations:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					they do not take place in quorum
				</li><li class="listitem">
					they do not execute fencing device actions
				</li><li class="listitem">
					they are not eligible to be the cluster’s Designated Controller (DC)
				</li><li class="listitem">
					they do not themselves run the full range of <code class="literal command">pcs</code> commands
				</li></ul></div><p>
			On the other hand, remote nodes and guest nodes are not bound to the scalability limits associated with the cluster stack.
		</p><p>
			Other than these noted limitations, the remote and guest nodes behave just like cluster nodes in respect to resource management, and the remote and guest nodes can themselves be fenced. The cluster is fully capable of managing and monitoring resources on each remote and guest node: You can build constraints against them, put them in standby, or perform any other action you perform on cluster nodes with the <code class="literal command">pcs</code> commands. Remote and guest nodes appear in cluster status output just as cluster nodes do.
		</p><section class="section" id="ref_host-and-guest-authentication-of-remote-nodes-remote-node-management"><div class="titlepage"><div><div><h3 class="title">30.1. Host and guest authentication of pacemaker_remote nodes</h3></div></div></div><p class="_abstract _abstract">
				The connection between cluster nodes and pacemaker_remote is secured using Transport Layer Security (TLS) with pre-shared key (PSK) encryption and authentication over TCP (using port 3121 by default). This means both the cluster node and the node running <code class="literal">pacemaker_remote</code> must share the same private key. By default this key must be placed at <code class="literal">/etc/pacemaker/authkey</code> on both cluster nodes and remote nodes.
			</p><p>
				The <code class="literal command">pcs cluster node add-guest</code> command sets up the <code class="literal">authkey</code> for guest nodes and the <code class="literal command">pcs cluster node add-remote</code> command sets up the <code class="literal">authkey</code> for remote nodes.
			</p></section><section class="section" id="proc_configuring-kvm-guest-nodes-remote-node-management"><div class="titlepage"><div><div><h3 class="title">30.2. Configuring KVM guest nodes</h3></div></div></div><p class="_abstract _abstract">
				A Pacemaker guest node is a virtual guest node running the <code class="literal">pacemaker_remote</code> service. The virtual guest node is managed by the cluster.
			</p><section class="section" id="guest_node_resource_options"><div class="titlepage"><div><div><h4 class="title">30.2.1. Guest node resource options</h4></div></div></div><p>
					When configuring a virtual machine to act as a guest node, you create a <code class="literal">VirtualDomain</code> resource, which manages the virtual machine. For descriptions of the options you can set for a <code class="literal">VirtualDomain</code> resource, see the "Resource Options for Virtual Domain Resources" table in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters#ref_virtual-domain-resource-options-configuring-virtual-domain-as-a-resource">Virtual domain resource options</a>.
				</p><p>
					In addition to the <code class="literal">VirtualDomain</code> resource options, metadata options define the resource as a guest node and define the connection parameters. You set these resource options with the <code class="literal command">pcs cluster node add-guest</code> command. The following table describes these metadata options.
				</p><rh-table id="tb-remoteklm-options-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 30.1. Metadata Options for Configuring KVM Resources as Remote Nodes</caption><colgroup><col style="width: 33%; " class="col_1"><!--Empty--><col style="width: 33%; " class="col_2"><!--Empty--><col style="width: 33%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686138489296" scope="col">Field</th><th align="left" valign="top" id="idm140686138488208" scope="col">Default</th><th align="left" valign="top" id="idm140686138487120" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686138489296"> <p>
									<code class="literal">remote-node</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686138488208"> <p>
									&lt;none&gt;
								</p>
								 </td><td align="left" valign="top" headers="idm140686138487120"> <p>
									The name of the guest node this resource defines. This both enables the resource as a guest node and defines the unique name used to identify the guest node. <span class="emphasis"><em>WARNING</em></span>: This value cannot overlap with any resource or node IDs.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686138489296"> <p>
									<code class="literal">remote-port</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686138488208"> <p>
									3121
								</p>
								 </td><td align="left" valign="top" headers="idm140686138487120"> <p>
									Configures a custom port to use for the guest connection to <code class="literal">pacemaker_remote</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686138489296"> <p>
									<code class="literal">remote-addr</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686138488208"> <p>
									The address provided in the <code class="literal">pcs host auth</code> command
								</p>
								 </td><td align="left" valign="top" headers="idm140686138487120"> <p>
									The IP address or host name to connect to
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686138489296"> <p>
									<code class="literal">remote-connect-timeout</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686138488208"> <p>
									60s
								</p>
								 </td><td align="left" valign="top" headers="idm140686138487120"> <p>
									Amount of time before a pending guest connection will time out
								</p>
								 </td></tr></tbody></table></rh-table></section><section class="section" id="integrating_a_virtual_machine_as_a_guest_node"><div class="titlepage"><div><div><h4 class="title">30.2.2. Integrating a virtual machine as a guest node</h4></div></div></div><p>
					The following procedure is a high-level summary overview of the steps to perform to have Pacemaker launch a virtual machine and to integrate that machine as a guest node, using <code class="literal">libvirt</code> and KVM virtual guests.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Configure the <code class="literal">VirtualDomain</code> resources.
						</li><li class="listitem"><p class="simpara">
							Enter the following commands on every virtual machine to install <code class="literal">pacemaker_remote</code> packages, start the <code class="literal">pcsd</code> service and enable it to run on startup, and allow TCP port 3121 through the firewall.
						</p><pre class="literallayout"># <span class="strong strong"><strong>dnf install pacemaker-remote resource-agents pcs</strong></span>
# <span class="strong strong"><strong>systemctl start pcsd.service</strong></span>
# <span class="strong strong"><strong>systemctl enable pcsd.service</strong></span>
# <span class="strong strong"><strong>firewall-cmd --add-port 3121/tcp --permanent</strong></span>
# <span class="strong strong"><strong>firewall-cmd --add-port 2224/tcp --permanent</strong></span>
# <span class="strong strong"><strong>firewall-cmd --reload</strong></span></pre></li><li class="listitem">
							Give each virtual machine a static network address and unique host name, which should be known to all nodes.
						</li><li class="listitem"><p class="simpara">
							If you have not already done so, authenticate <code class="literal">pcs</code> to the node you will be integrating as a quest node.
						</p><pre class="literallayout"># <span class="strong strong"><strong>pcs host auth <span class="emphasis"><em>nodename</em></span></strong></span></pre></li><li class="listitem"><p class="simpara">
							Use the following command to convert an existing <code class="literal">VirtualDomain</code> resource into a guest node. This command must be run on a cluster node and not on the guest node which is being added. In addition to converting the resource, this command copies the <code class="literal">/etc/pacemaker/authkey</code> to the guest node and starts and enables the <code class="literal">pacemaker_remote</code> daemon on the guest node. The node name for the guest node, which you can define arbitrarily, can differ from the host name for the node.
						</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster node add-guest <span class="emphasis"><em>nodename</em></span> <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>options</em></span>]</strong></span></pre></li><li class="listitem"><p class="simpara">
							After creating the <code class="literal">VirtualDomain</code> resource, you can treat the guest node just as you would treat any other node in the cluster. For example, you can create a resource and place a resource constraint on the resource to run on the guest node as in the following commands, which are run from a cluster node. You can include guest nodes in groups, which allows you to group a storage device, file system, and VM.
						</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create webserver apache configfile=/etc/httpd/conf/httpd.conf op monitor interval=30s</strong></span>
# <span class="strong strong"><strong>pcs constraint location webserver prefers <span class="emphasis"><em>nodename</em></span></strong></span></pre></li></ol></div></section></section><section class="section" id="proc_configuring-remote-nodes-remote-node-management"><div class="titlepage"><div><div><h3 class="title">30.3. Configuring Pacemaker remote nodes</h3></div></div></div><p class="_abstract _abstract">
				A remote node is defined as a cluster resource with <code class="literal">ocf:pacemaker:remote</code> as the resource agent. You create this resource with the <code class="literal command">pcs cluster node add-remote</code> command.
			</p><section class="section" id="remote_node_resource_options"><div class="titlepage"><div><div><h4 class="title">30.3.1. Remote node resource options</h4></div></div></div><p>
					The following table describes the resource options you can configure for a <code class="literal">remote</code> resource.
				</p><rh-table id="tb-remotenode-options-HAAR"><table class="lt-4-cols lt-7-rows"><caption>Table 30.2. Resource Options for Remote Nodes</caption><colgroup><col style="width: 38%; " class="col_1"><!--Empty--><col style="width: 25%; " class="col_2"><!--Empty--><col style="width: 38%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686140461792" scope="col">Field</th><th align="left" valign="top" id="idm140686140460704" scope="col">Default</th><th align="left" valign="top" id="idm140686140459616" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686140461792"> <p>
									<code class="literal">reconnect_interval</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140460704"> <p>
									0
								</p>
								 </td><td align="left" valign="top" headers="idm140686140459616"> <p>
									Time in seconds to wait before attempting to reconnect to a remote node after an active connection to the remote node has been severed. This wait is recurring. If reconnect fails after the wait period, a new reconnect attempt will be made after observing the wait time. When this option is in use, Pacemaker will keep attempting to reach out and connect to the remote node indefinitely after each wait interval.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686140461792"> <p>
									<code class="literal">server</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140460704"> <p>
									Address specified with <code class="literal">pcs host auth</code> command
								</p>
								 </td><td align="left" valign="top" headers="idm140686140459616"> <p>
									Server to connect to. This can be an IP address or host name.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140686140461792"> <p>
									<code class="literal">port</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140686140460704"> </td><td align="left" valign="top" headers="idm140686140459616"> <p>
									TCP port to connect to.
								</p>
								 </td></tr></tbody></table></rh-table></section><section class="section" id="remote_node_configuration_overview"><div class="titlepage"><div><div><h4 class="title">30.3.2. Remote node configuration overview</h4></div></div></div><p>
					The following procedure provides a high-level summary overview of the steps to perform to configure a Pacemaker Remote node and to integrate that node into an existing Pacemaker cluster environment.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On the node that you will be configuring as a remote node, allow cluster-related services through the local firewall.
						</p><pre class="literallayout"># <span class="strong strong"><strong>firewall-cmd --permanent --add-service=high-availability</strong></span>
success
# <span class="strong strong"><strong>firewall-cmd --reload</strong></span>
success</pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								If you are using <code class="literal">iptables</code> directly, or some other firewall solution besides <code class="literal">firewalld</code>, simply open the following ports: TCP ports 2224 and 3121.
							</p></div></rh-alert></li><li class="listitem"><p class="simpara">
							Install the <code class="literal command">pacemaker_remote</code> daemon on the remote node.
						</p><pre class="literallayout"># <span class="strong strong"><strong>dnf install -y pacemaker-remote resource-agents pcs</strong></span></pre></li><li class="listitem"><p class="simpara">
							Start and enable <code class="literal">pcsd</code> on the remote node.
						</p><pre class="literallayout"># <span class="strong strong"><strong>systemctl start pcsd.service</strong></span>
# <span class="strong strong"><strong>systemctl enable pcsd.service</strong></span></pre></li><li class="listitem"><p class="simpara">
							If you have not already done so, authenticate <code class="literal">pcs</code> to the node you will be adding as a remote node.
						</p><pre class="literallayout"># <span class="strong strong"><strong>pcs host auth remote1</strong></span></pre></li><li class="listitem"><p class="simpara">
							Add the remote node resource to the cluster with the following command. This command also syncs all relevant configuration files to the new node, starts the node, and configures it to start <code class="literal">pacemaker_remote</code> on boot. This command must be run on a cluster node and not on the remote node which is being added.
						</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster node add-remote remote1</strong></span></pre></li><li class="listitem"><p class="simpara">
							After adding the <code class="literal">remote</code> resource to the cluster, you can treat the remote node just as you would treat any other node in the cluster. For example, you can create a resource and place a resource constraint on the resource to run on the remote node as in the following commands, which are run from a cluster node.
						</p><pre class="literallayout"># <span class="strong strong"><strong>pcs resource create webserver apache configfile=/etc/httpd/conf/httpd.conf op monitor interval=30s</strong></span>
# <span class="strong strong"><strong>pcs constraint location webserver prefers remote1</strong></span></pre><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
								Never involve a remote node connection resource in a resource group, colocation constraint, or order constraint.
							</p></div></rh-alert></li><li class="listitem">
							Configure fencing resources for the remote node. Remote nodes are fenced the same way as cluster nodes. Configure fencing resources for use with remote nodes the same as you would with cluster nodes. Note, however, that remote nodes can never initiate a fencing action. Only cluster nodes are capable of actually executing a fencing operation against another node.
						</li></ol></div></section></section><section class="section" id="proc_changing-default-port-location-remote-node-management"><div class="titlepage"><div><div><h3 class="title">30.4. Changing the default port location</h3></div></div></div><p class="_abstract _abstract">
				If you need to change the default port location for either Pacemaker or <code class="literal">pacemaker_remote</code>, you can set the <code class="literal">PCMK_remote_port</code> environment variable that affects both of these daemons. This environment variable can be enabled by placing it in the <code class="literal">/etc/sysconfig/pacemaker</code> file as follows.
			</p><pre class="literallayout">\#==#==# Pacemaker Remote
...
#
# Specify a custom port for Pacemaker Remote connections
PCMK_remote_port=3121</pre><p>
				When changing the default port used by a particular guest node or remote node, the <code class="literal">PCMK_remote_port</code> variable must be set in that node’s <code class="literal">/etc/sysconfig/pacemaker</code> file, and the cluster resource creating the guest node or remote node connection must also be configured with the same port number (using the <code class="literal">remote-port</code> metadata option for guest nodes, or the <code class="literal">port</code> option for remote nodes).
			</p></section><section class="section" id="proc_upgrading-systems-with-pacemaker-remote-remote-node-management"><div class="titlepage"><div><div><h3 class="title">30.5. Upgrading systems with pacemaker_remote nodes</h3></div></div></div><p class="_abstract _abstract">
				If the <code class="literal">pacemaker_remote</code> service is stopped on an active Pacemaker Remote node, the cluster will gracefully migrate resources off the node before stopping the node. This allows you to perform software upgrades and other routine maintenance procedures without removing the node from the cluster. Once <code class="literal">pacemaker_remote</code> is shut down, however, the cluster will immediately try to reconnect. If <code class="literal">pacemaker_remote</code> is not restarted within the resource’s monitor timeout, the cluster will consider the monitor operation as failed.
			</p><p>
				If you wish to avoid monitor failures when the <code class="literal">pacemaker_remote</code> service is stopped on an active Pacemaker Remote node, you can use the following procedure to take the node out of the cluster before performing any system administration that might stop <code class="literal">pacemaker_remote</code>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Stop the node’s connection resource with the <code class="literal command">pcs resource disable <span class="emphasis"><em>resourcename</em></span></code> command, which will move all services off the node. The connection resource would be the <code class="literal">ocf:pacemaker:remote</code> resource for a remote node or, commonly, the <code class="literal">ocf:heartbeat:VirtualDomain</code> resource for a guest node. For guest nodes, this command will also stop the VM, so the VM must be started outside the cluster (for example, using <code class="literal command">virsh</code>) to perform any maintenance.
					</p><pre class="literallayout">pcs resource disable <span class="emphasis"><em>resourcename</em></span></pre></li><li class="listitem">
						Perform the required maintenance.
					</li><li class="listitem"><p class="simpara">
						When ready to return the node to the cluster, re-enable the resource with the <code class="literal command">pcs resource enable</code> command.
					</p><pre class="literallayout">pcs resource enable <span class="emphasis"><em>resourcename</em></span></pre></li></ol></div></section></section><section class="chapter" id="assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 31. Performing cluster maintenance</h2></div></div></div><p class="_abstract _abstract">
			In order to perform maintenance on the nodes of your cluster, you may need to stop or move the resources and services running on that cluster. Or you may need to stop the cluster software while leaving the services untouched. Pacemaker provides a variety of methods for performing system maintenance.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					If you need to stop a node in a cluster while continuing to provide the services running on that cluster on another node, you can put the cluster node in standby mode. A node that is in standby mode is no longer able to host resources. Any resource currently active on the node will be moved to another node, or stopped if no other node is eligible to run the resource. For information about standby mode, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_stopping-individual-node-cluster-maintenance">Putting a node into standby mode</a>.
				</li><li class="listitem"><p class="simpara">
					If you need to move an individual resource off the node on which it is currently running without stopping that resource, you can use the <code class="literal command">pcs resource move</code> command to move the resource to a different node.
				</p><p class="simpara">
					When you execute the <code class="literal command">pcs resource move</code> command, this adds a constraint to the resource to prevent it from running on the node on which it is currently running. When you are ready to move the resource back, you can execute the <code class="literal command">pcs resource clear</code> or the <code class="literal command">pcs constraint delete</code> command to remove the constraint. This does not necessarily move the resources back to the original node, however, since where the resources can run at that point depends on how you have configured your resources initially. You can relocate a resource to its preferred node with the <code class="literal command">pcs resource relocate run</code> command.
				</p></li><li class="listitem">
					If you need to stop a running resource entirely and prevent the cluster from starting it again, you can use the <code class="literal command">pcs resource disable</code> command. For information on the <code class="literal command">pcs resource disable</code> command, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_disabling-resources-cluster-maintenance">Disabling, enabling, and banning cluster resources</a>.
				</li><li class="listitem">
					If you want to prevent Pacemaker from taking any action for a resource (for example, if you want to disable recovery actions while performing maintenance on the resource, or if you need to reload the <code class="literal">/etc/sysconfig/pacemaker</code> settings), use the <code class="literal command">pcs resource unmanage</code> command, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_unmanaging-resources-cluster-maintenance">Setting a resource to unmanaged mode</a>. Pacemaker Remote connection resources should never be unmanaged.
				</li><li class="listitem">
					If you need to put the cluster in a state where no services will be started or stopped, you can set the <code class="literal">maintenance-mode</code> cluster property. Putting the cluster into maintenance mode automatically unmanages all resources. For information about putting the cluster in maintenance mode, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_setting-maintenance-mode-cluster-maintenance">Putting a cluster in maintenance mode</a>.
				</li><li class="listitem">
					If you need to update the packages that make up the RHEL High Availability and Resilient Storage Add-Ons, you can update the packages on one node at a time or on the entire cluster as a whole, as summarized in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_updating-cluster-packages-cluster-maintenance">Updating a RHEL high availability cluster</a>.
				</li><li class="listitem">
					If you need to perform maintenance on a Pacemaker remote node, you can remove that node from the cluster by disabling the remote node resource, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_upgrading-remote-nodes-cluster-maintenance">Upgrading remote nodes and guest nodes</a>.
				</li><li class="listitem">
					If you need to migrate a VM in a RHEL cluster, you will first need to stop the cluster services on the VM to remove the node from the cluster and then start the cluster back up after performing the migration. as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_migrating-cluster-vms-cluster-maintenance">Migrating VMs in a RHEL cluster</a>.
				</li></ul></div><section class="section" id="proc_stopping-individual-node-cluster-maintenance"><div class="titlepage"><div><div><h3 class="title">31.1. Putting a node into standby mode</h3></div></div></div><p class="_abstract _abstract">
				When a cluster node is in standby mode, the node is no longer able to host resources. Any resources currently active on the node will be moved to another node.
			</p><p>
				The following command puts the specified node into standby mode. If you specify the <code class="literal option">--all</code>, this command puts all nodes into standby mode.
			</p><p>
				You can use this command when updating a resource’s packages. You can also use this command when testing a configuration, to simulate recovery without actually shutting down a node.
			</p><pre class="literallayout">pcs node standby <span class="emphasis"><em>node</em></span> | --all</pre><p>
				The following command removes the specified node from standby mode. After running this command, the specified node is then able to host resources. If you specify the <code class="literal option">--all</code>, this command removes all nodes from standby mode.
			</p><pre class="literallayout">pcs node unstandby <span class="emphasis"><em>node</em></span> | --all</pre><p>
				Note that when you execute the <code class="literal command">pcs node standby</code> command, this prevents resources from running on the indicated node. When you execute the <code class="literal command">pcs node unstandby</code> command, this allows resources to run on the indicated node. This does not necessarily move the resources back to the indicated node; where the resources can run at that point depends on how you have configured your resources initially.
			</p></section><section class="section" id="proc_manually-move-resources-cluster-maintenance"><div class="titlepage"><div><div><h3 class="title">31.2. Manually moving cluster resources</h3></div></div></div><p class="_abstract _abstract">
				You can override the cluster and force resources to move from their current location. There are two occasions when you would want to do this:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						When a node is under maintenance, and you need to move all resources running on that node to a different node
					</li><li class="listitem">
						When individually specified resources needs to be moved
					</li></ul></div><p>
				To move all resources running on a node to a different node, you put the node in standby mode.
			</p><p>
				You can move individually specified resources in either of the following ways.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You can use the <code class="literal command">pcs resource move</code> command to move a resource off a node on which it is currently running.
					</li><li class="listitem">
						You can use the <code class="literal command">pcs resource relocate run</code> command to move a resource to its preferred node, as determined by current cluster status, constraints, location of resources and other settings.
					</li></ul></div><section class="section" id="moving_a_resource_from_its_current_node"><div class="titlepage"><div><div><h4 class="title">31.2.1. Moving a resource from its current node</h4></div></div></div><p>
					To move a resource off the node on which it is currently running, use the following command, specifying the <span class="emphasis"><em>resource_id</em></span> of the resource as defined. Specify the <code class="literal">destination_node</code> if you want to indicate on which node to run the resource that you are moving.
				</p><pre class="literallayout">pcs resource move <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>destination_node</em></span>] [--promoted] [--strict] [--wait[=<span class="emphasis"><em>n</em></span>]]</pre><p>
					When you execute the <code class="literal command">pcs resource move</code> command, this adds a constraint to the resource to prevent it from running on the node on which it is currently running. By default, the location constraint that the command creates is automatically removed once the resource has been moved. If removing the constraint would cause the resource to move back to the original node, as might happen if the <code class="literal">resource-stickiness</code> value for the resource is 0, the <code class="literal">pcs resource move</code> command fails. If you would like to move a resource and leave the resulting constraint in place, use the <code class="literal">pcs resource move-with-constraint</code> command.
				</p><p>
					If you specify the <code class="literal">--promoted</code> parameter of the <code class="literal command">pcs resource move</code> command, the constraint applies only to promoted instances of the resource.
				</p><p>
					If you specify the <code class="literal">--strict</code> parameter of the <code class="literal command">pcs resource move</code> command, the command will fail if other resources than the one specified in the command would be affected.
				</p><p>
					You can optionally configure a <code class="literal">--wait[=<span class="emphasis"><em>n</em></span>]</code> parameter for the <code class="literal">pcs resource move</code> command to indicate the number of seconds to wait for the resource to start on the destination node before returning 0 if the resource is started or 1 if the resource has not yet started. If you do not specify n, it defaults to a value of 60 minutes.
				</p></section><section class="section" id="moving_a_resource_to_its_preferred_node"><div class="titlepage"><div><div><h4 class="title">31.2.2. Moving a resource to its preferred node</h4></div></div></div><p>
					After a resource has moved, either due to a failover or to an administrator manually moving the node, it will not necessarily move back to its original node even after the circumstances that caused the failover have been corrected. To relocate resources to their preferred node, use the following command. A preferred node is determined by the current cluster status, constraints, resource location, and other settings and may change over time.
				</p><pre class="literallayout">pcs resource relocate run [<span class="emphasis"><em>resource1</em></span>] [<span class="emphasis"><em>resource2</em></span>] ...</pre><p>
					If you do not specify any resources, all resource are relocated to their preferred nodes.
				</p><p>
					This command calculates the preferred node for each resource while ignoring resource stickiness. After calculating the preferred node, it creates location constraints which will cause the resources to move to their preferred nodes. Once the resources have been moved, the constraints are deleted automatically. To remove all constraints created by the <code class="literal command">pcs resource relocate run</code> command, you can enter the <code class="literal command">pcs resource relocate clear</code> command. To display the current status of resources and their optimal node ignoring resource stickiness, enter the <code class="literal command">pcs resource relocate show</code> command.
				</p></section></section><section class="section" id="proc_disabling-resources-cluster-maintenance"><div class="titlepage"><div><div><h3 class="title">31.3. Disabling, enabling, and banning cluster resources</h3></div></div></div><p class="_abstract _abstract">
				In addition to the <code class="literal command">pcs resource move</code> and <code class="literal command">pcs resource relocate</code> commands, there are a variety of other commands you can use to control the behavior of cluster resources.
			</p><h5 id="disabling_a_cluster_resource">Disabling a cluster resource</h5><p>
				You can manually stop a running resource and prevent the cluster from starting it again with the following command. Depending on the rest of the configuration (constraints, options, failures, and so on), the resource may remain started. If you specify the <code class="literal option">--wait</code> option, <span class="strong strong"><strong><span class="application application">pcs</span></strong></span> will wait up to 'n' seconds for the resource to stop and then return 0 if the resource is stopped or 1 if the resource has not stopped. If 'n' is not specified it defaults to 60 minutes.
			</p><pre class="literallayout">pcs resource disable <span class="emphasis"><em>resource_id</em></span> [--wait[=<span class="emphasis"><em>n</em></span>]]</pre><p>
				You can specify that a resource be disabled only if disabling the resource would not have an effect on other resources. Ensuring that this would be the case can be impossible to do by hand when complex resource relations are set up.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The <code class="literal command">pcs resource disable --simulate</code> command shows the effects of disabling a resource while not changing the cluster configuration.
					</li><li class="listitem">
						The <code class="literal command">pcs resource disable --safe</code> command disables a resource only if no other resources would be affected in any way, such as being migrated from one node to another. The <code class="literal command">pcs resource safe-disable</code> command is an alias for the <code class="literal">pcs resource disable --safe</code> command.
					</li><li class="listitem">
						The <code class="literal command">pcs resource disable --safe --no-strict</code> command disables a resource only if no other resources would be stopped or demoted
					</li></ul></div><p>
				You can specify the <code class="literal">--brief</code> option for the <code class="literal">pcs resource disable --safe</code> command to print errors only. The error report that the <code class="literal">pcs resource disable --safe</code> command generates if the safe disable operation fails contains the affected resource IDs. If you need to know only the resource IDs of resources that would be affected by disabling a resource, use the <code class="literal">--brief</code> option, which does not provide the full simulation result.
			</p><h5 id="enabling_a_cluster_resource">Enabling a cluster resource</h5><p>
				Use the following command to allow the cluster to start a resource. Depending on the rest of the configuration, the resource may remain stopped. If you specify the <code class="literal option">--wait</code> option, <span class="strong strong"><strong><span class="application application">pcs</span></strong></span> will wait up to 'n' seconds for the resource to start and then return 0 if the resource is started or 1 if the resource has not started. If 'n' is not specified it defaults to 60 minutes.
			</p><pre class="literallayout">pcs resource enable <span class="emphasis"><em>resource_id</em></span> [--wait[=<span class="emphasis"><em>n</em></span>]]</pre><h5 id="preventing_a_resource_from_running_on_a_particular_node">Preventing a resource from running on a particular node</h5><p>
				Use the following command to prevent a resource from running on a specified node, or on the current node if no node is specified.
			</p><pre class="literallayout">pcs resource ban <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>node</em></span>] [--promoted] [lifetime=<span class="emphasis"><em>lifetime</em></span>] [--wait[=<span class="emphasis"><em>n</em></span>]]</pre><p>
				Note that when you execute the <code class="literal command">pcs resource ban</code> command, this adds a -INFINITY location constraint to the resource to prevent it from running on the indicated node. You can execute the <code class="literal command">pcs resource clear</code> or the <code class="literal command">pcs constraint delete</code> command to remove the constraint. This does not necessarily move the resources back to the indicated node; where the resources can run at that point depends on how you have configured your resources initially.
			</p><p>
				If you specify the <code class="literal">--promoted</code> parameter of the <code class="literal command">pcs resource ban</code> command, the scope of the constraint is limited to the promoted role and you must specify <span class="emphasis"><em>promotable_id</em></span> rather than <span class="emphasis"><em>resource_id</em></span>.
			</p><p>
				You can optionally configure a <code class="literal">lifetime</code> parameter for the <code class="literal">pcs resource ban</code> command to indicate a period of time the constraint should remain.
			</p><p>
				You can optionally configure a <code class="literal">--wait[=<span class="emphasis"><em>n</em></span>]</code> parameter for the <code class="literal">pcs resource ban</code> command to indicate the number of seconds to wait for the resource to start on the destination node before returning 0 if the resource is started or 1 if the resource has not yet started. If you do not specify n, the default resource timeout will be used.
			</p><h5 id="forcing_a_resource_to_start_on_the_current_node">Forcing a resource to start on the current node</h5><p>
				Use the <code class="literal command">debug-start</code> parameter of the <code class="literal command">pcs resource</code> command to force a specified resource to start on the current node, ignoring the cluster recommendations and printing the output from starting the resource. This is mainly used for debugging resources; starting resources on a cluster is (almost) always done by Pacemaker and not directly with a <code class="literal command">pcs</code> command. If your resource is not starting, it is usually due to either a misconfiguration of the resource (which you debug in the system log), constraints that prevent the resource from starting, or the resource being disabled. You can use this command to test resource configuration, but it should not normally be used to start resources in a cluster.
			</p><p>
				The format of the <code class="literal command">debug-start</code> command is as follows.
			</p><pre class="literallayout">pcs resource debug-start <span class="emphasis"><em>resource_id</em></span></pre></section><section class="section" id="proc_unmanaging-resources-cluster-maintenance"><div class="titlepage"><div><div><h3 class="title">31.4. Setting a resource to unmanaged mode</h3></div></div></div><p class="_abstract _abstract">
				When a resource is in <code class="literal">unmanaged</code> mode, the resource is still in the configuration but Pacemaker does not manage the resource.
			</p><p>
				The following command sets the indicated resources to <code class="literal">unmanaged</code> mode.
			</p><pre class="literallayout">pcs resource unmanage <span class="emphasis"><em>resource1</em></span>  [<span class="emphasis"><em>resource2</em></span>] ...</pre><p>
				The following command sets resources to <code class="literal">managed</code> mode, which is the default state.
			</p><pre class="literallayout">pcs resource manage <span class="emphasis"><em>resource1</em></span>  [<span class="emphasis"><em>resource2</em></span>] ...</pre><p>
				You can specify the name of a resource group with the <code class="literal command">pcs resource manage</code> or <code class="literal command">pcs resource unmanage</code> command. The command will act on all of the resources in the group, so that you can set all of the resources in a group to <code class="literal">managed</code> or <code class="literal">unmanaged</code> mode with a single command and then manage the contained resources individually.
			</p></section><section class="section" id="proc_setting-maintenance-mode-cluster-maintenance"><div class="titlepage"><div><div><h3 class="title">31.5. Putting a cluster in maintenance mode</h3></div></div></div><p class="_abstract _abstract">
				When a cluster is in maintenance mode, the cluster does not start or stop any services until told otherwise. When maintenance mode is completed, the cluster does a sanity check of the current state of any services, and then stops or starts any that need it.
			</p><p>
				To put a cluster in maintenance mode, use the following command to set the <code class="literal">maintenance-mode</code> cluster property to <code class="literal">true</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs property set maintenance-mode=true</strong></span></pre><p>
				To remove a cluster from maintenance mode, use the following command to set the <code class="literal">maintenance-mode</code> cluster property to <code class="literal">false</code>.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs property set maintenance-mode=false</strong></span></pre><p>
				You can remove a cluster property from the configuration with the following command.
			</p><pre class="literallayout">pcs property unset <span class="emphasis"><em>property</em></span></pre><p>
				Alternately, you can remove a cluster property from a configuration by leaving the value field of the <code class="literal command">pcs property set</code> command blank. This restores that property to its default value. For example, if you have previously set the <code class="literal">symmetric-cluster</code> property to <code class="literal">false</code>, the following command removes the value you have set from the configuration and restores the value of <code class="literal">symmetric-cluster</code> to <code class="literal">true</code>, which is its default value.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs property set symmetric-cluster=</strong></span></pre></section><section class="section" id="proc_updating-cluster-packages-cluster-maintenance"><div class="titlepage"><div><div><h3 class="title">31.6. Updating a RHEL high availability cluster</h3></div></div></div><p class="_abstract _abstract">
				Updating packages that make up the RHEL High Availability and Resilient Storage Add-Ons, either individually or as a whole, can be done in one of two general ways:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<span class="emphasis"><em>Rolling Updates</em></span>: Remove one node at a time from service, update its software, then integrate it back into the cluster. This allows the cluster to continue providing service and managing resources while each node is updated.
					</li><li class="listitem">
						<span class="emphasis"><em>Entire Cluster Update</em></span>: Stop the entire cluster, apply updates to all nodes, then start the cluster back up.
					</li></ul></div><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
					It is critical that when performing software update procedures for Red Hat Enterprise Linux High Availability and Resilient Storage clusters, you ensure that any node that will undergo updates is not an active member of the cluster before those updates are initiated.
				</p></div></rh-alert><p>
				For a full description of each of these methods and the procedures to follow for the updates, see <a class="link" href="https://access.redhat.com/articles/2059253/">Recommended Practices for Applying Software Updates to a RHEL High Availability or Resilient Storage Cluster</a>.
			</p></section><section class="section" id="proc_upgrading-remote-nodes-cluster-maintenance"><div class="titlepage"><div><div><h3 class="title">31.7. Upgrading remote nodes and guest nodes</h3></div></div></div><p class="_abstract _abstract">
				If the <code class="literal">pacemaker_remote</code> service is stopped on an active remote node or guest node, the cluster will gracefully migrate resources off the node before stopping the node. This allows you to perform software upgrades and other routine maintenance procedures without removing the node from the cluster. Once <code class="literal">pacemaker_remote</code> is shut down, however, the cluster will immediately try to reconnect. If <code class="literal">pacemaker_remote</code> is not restarted within the resource’s monitor timeout, the cluster will consider the monitor operation as failed.
			</p><p>
				If you wish to avoid monitor failures when the <code class="literal">pacemaker_remote</code> service is stopped on an active Pacemaker Remote node, you can use the following procedure to take the node out of the cluster before performing any system administration that might stop <code class="literal">pacemaker_remote</code>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Stop the node’s connection resource with the <code class="literal command">pcs resource disable <span class="emphasis"><em>resourcename</em></span></code> command, which will move all services off the node. The connection resource would be the <code class="literal">ocf:pacemaker:remote</code> resource for a remote node or, commonly, the <code class="literal">ocf:heartbeat:VirtualDomain</code> resource for a guest node. For guest nodes, this command will also stop the VM, so the VM must be started outside the cluster (for example, using <code class="literal command">virsh</code>) to perform any maintenance.
					</p><pre class="literallayout">pcs resource disable <span class="emphasis"><em>resourcename</em></span></pre></li><li class="listitem">
						Perform the required maintenance.
					</li><li class="listitem"><p class="simpara">
						When ready to return the node to the cluster, re-enable the resource with the <code class="literal command">pcs resource enable</code> command.
					</p><pre class="literallayout">pcs resource enable <span class="emphasis"><em>resourcename</em></span></pre></li></ol></div></section><section class="section" id="proc_migrating-cluster-vms-cluster-maintenance"><div class="titlepage"><div><div><h3 class="title">31.8. Migrating VMs in a RHEL cluster</h3></div></div></div><p class="_abstract _abstract">
				Red Hat does not support live migration of active cluster nodes across hypervisors or hosts, as noted in <a class="link" href="https://access.redhat.com/articles/3131111/">Support Policies for RHEL High Availability Clusters - General Conditions with Virtualized Cluster Members</a>. If you need to perform a live migration, you will first need to stop the cluster services on the VM to remove the node from the cluster, and then start the cluster back up after performing the migration. The following steps outline the procedure for removing a VM from a cluster, migrating the VM, and restoring the VM to the cluster.
			</p><p>
				The following steps outline the procedure for removing a VM from a cluster, migrating the VM, and restoring the VM to the cluster.
			</p><p>
				This procedure applies to VMs that are used as full cluster nodes, not to VMs managed as cluster resources (including VMs used as guest nodes) which can be live-migrated without special precautions. For general information about the fuller procedure required for updating packages that make up the RHEL High Availability and Resilient Storage Add-Ons, either individually or as a whole, see <a class="link" href="https://access.redhat.com/articles/2059253/">Recommended Practices for Applying Software Updates to a RHEL High Availability or Resilient Storage Cluster</a>.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					Before performing this procedure, consider the effect on cluster quorum of removing a cluster node. For example, if you have a three-node cluster and you remove one node, your cluster can not withstand any node failure. This is because if one node of a three-node cluster is already down, removing a second node will lose quorum.
				</p></div></rh-alert><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						If any preparations need to be made before stopping or moving the resources or software running on the VM to migrate, perform those steps.
					</li><li class="listitem"><p class="simpara">
						Run the following command on the VM to stop the cluster software on the VM.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster stop</strong></span></pre></li><li class="listitem">
						Perform the live migration of the VM.
					</li><li class="listitem"><p class="simpara">
						Start cluster services on the VM.
					</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster start</strong></span></pre></li></ol></div></section><section class="section" id="identifying-cluster-by-uuid-cluster-maintenance"><div class="titlepage"><div><div><h3 class="title">31.9. Identifying clusters by UUID</h3></div></div></div><p class="_abstract _abstract">
				As of Red Hat Enterprise Linux 9.1, when you create a cluster it has an associated UUID. Since a cluster name is not a unique cluster identifier, a third-party tool such as a configuration management database that manages multiple clusters with the same name can uniquely identify a cluster by means of its UUID. You can display the current cluster UUID with the <code class="literal">pcs cluster config [show]</code> command, which includes the cluster UUID in its output.
			</p><p>
				To add a UUID to an existing cluster, run the following command.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster config uuid generate</strong></span></pre><p>
				To regenerate a UUID for a cluster with an existing UUID, run the following command.
			</p><pre class="literallayout"># <span class="strong strong"><strong>pcs cluster config uuid generate --force</strong></span></pre></section></section><section class="chapter" id="assembly_configuring-disaster-recovery-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 32. Configuring disaster recovery clusters</h2></div></div></div><p class="_abstract _abstract">
			One method of providing disaster recovery for a high availability cluster is to configure two clusters. You can then configure one cluster as your primary site cluster, and the second cluster as your disaster recovery cluster.
		</p><p>
			In normal circumstances, the primary cluster is running resources in production mode. The disaster recovery cluster has all the resources configured as well and is either running them in demoted mode or not at all. For example, there may be a database running in the primary cluster in promoted mode and running in the disaster recovery cluster in demoted mode. The database in this setup would be configured so that data is synchronized from the primary to disaster recovery site. This is done through the database configuration itself rather than through the <code class="literal">pcs</code> command interface.
		</p><p>
			When the primary cluster goes down, users can use the <code class="literal">pcs</code> command interface to manually fail the resources over to the disaster recovery site. They can then log in to the disaster site and promote and start the resources there. Once the primary cluster has recovered, users can use the <code class="literal">pcs</code> command interface to manually move resources back to the primary site.
		</p><p>
			You can use the <code class="literal">pcs</code> command to display the status of both the primary and the disaster recovery site cluster from a single node on either site.
		</p><section class="section" id="ref_recovery-considerations-configuring-disaster-recovery"><div class="titlepage"><div><div><h3 class="title">32.1. Considerations for disaster recovery clusters</h3></div></div></div><p class="_abstract _abstract">
				When planning and configuring a disaster recovery site that you will manage and monitor with the <code class="literal">pcs</code> command interface, note the following considerations.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The disaster recovery site must be a cluster. This makes it possible to configure it with same tools and similar procedures as the primary site.
					</li><li class="listitem">
						The primary and disaster recovery clusters are created by independent <code class="literal">pcs cluster setup</code> commands.
					</li><li class="listitem">
						The clusters and their resources must be configured so that that the data is synchronized and failover is possible.
					</li><li class="listitem">
						The cluster nodes in the recovery site can not have the same names as the nodes in the primary site.
					</li><li class="listitem">
						The pcs user <code class="literal">hacluster</code> must be authenticated for each node in both clusters on the node from which you will be running <code class="literal">pcs</code> commands.
					</li></ul></div></section><section class="section" id="proc_disaster-recovery-display-configuring-disaster-recovery"><div class="titlepage"><div><div><h3 class="title">32.2. Displaying status of recovery clusters</h3></div></div></div><p class="_abstract _abstract">
				To configure a primary and a disaster recovery cluster so that you can display the status of both clusters, perform the following procedure.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					Setting up a disaster recovery cluster does not automatically configure resources or replicate data. Those items must be configured manually by the user.
				</p></div></rh-alert><p>
				In this example:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The primary cluster will be named <code class="literal">PrimarySite</code> and will consist of the nodes <code class="literal">z1.example.com</code>. and <code class="literal">z2.example.com</code>.
					</li><li class="listitem">
						The disaster recovery site cluster will be named <code class="literal">DRsite</code> and will consist of the nodes <code class="literal">z3.example.com</code> and <code class="literal">z4.example.com</code>.
					</li></ul></div><p>
				This example sets up a basic cluster with no resources or fencing configured.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Authenticate all of the nodes that will be used for both clusters.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs host auth z1.example.com z2.example.com z3.example.com z4.example.com -u hacluster -p password</strong></span>
z1.example.com: Authorized
z2.example.com: Authorized
z3.example.com: Authorized
z4.example.com: Authorized</pre></li><li class="listitem"><p class="simpara">
						Create the cluster that will be used as the primary cluster and start cluster services for the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs cluster setup PrimarySite z1.example.com z2.example.com --start</strong></span>
{...}
Cluster has been successfully set up.
Starting cluster on hosts: 'z1.example.com', 'z2.example.com'...</pre></li><li class="listitem"><p class="simpara">
						Create the cluster that will be used as the disaster recovery cluster and start cluster services for the cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs cluster setup DRSite z3.example.com z4.example.com --start</strong></span>
{...}
Cluster has been successfully set up.
Starting cluster on hosts: 'z3.example.com', 'z4.example.com'...</pre></li><li class="listitem"><p class="simpara">
						From a node in the primary cluster, set up the second cluster as the recovery site. The recovery site is defined by a name of one of its nodes.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs dr set-recovery-site z3.example.com</strong></span>
Sending 'disaster-recovery config' to 'z3.example.com', 'z4.example.com'
z3.example.com: successful distribution of the file 'disaster-recovery config'
z4.example.com: successful distribution of the file 'disaster-recovery config'
Sending 'disaster-recovery config' to 'z1.example.com', 'z2.example.com'
z1.example.com: successful distribution of the file 'disaster-recovery config'
z2.example.com: successful distribution of the file 'disaster-recovery config'</pre></li><li class="listitem"><p class="simpara">
						Check the disaster recovery configuration.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs dr config</strong></span>
Local site:
  Role: Primary
Remote site:
  Role: Recovery
  Nodes:
    z3.example.com
    z4.example.com</pre></li><li class="listitem"><p class="simpara">
						Check the status of the primary cluster and the disaster recovery cluster from a node in the primary cluster.
					</p><pre class="literallayout">[root@z1 ~]# <span class="strong strong"><strong>pcs dr status</strong></span>
--- Local cluster - Primary site ---
Cluster name: PrimarySite

WARNINGS:
No stonith devices and stonith-enabled is not false

Cluster Summary:
  * Stack: corosync
  * Current DC: z2.example.com (version 2.0.3-2.el8-2c9cea563e) - partition with quorum
  * Last updated: Mon Dec  9 04:10:31 2019
  * Last change:  Mon Dec  9 04:06:10 2019 by hacluster via crmd on z2.example.com
  * 2 nodes configured
  * 0 resource instances configured

Node List:
  * Online: [ z1.example.com z2.example.com ]

Full List of Resources:
  * No resources

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled


--- Remote cluster - Recovery site ---
Cluster name: DRSite

WARNINGS:
No stonith devices and stonith-enabled is not false

Cluster Summary:
  * Stack: corosync
  * Current DC: z4.example.com (version 2.0.3-2.el8-2c9cea563e) - partition with quorum
  * Last updated: Mon Dec  9 04:10:34 2019
  * Last change:  Mon Dec  9 04:09:55 2019 by hacluster via crmd on z4.example.com
  * 2 nodes configured
  * 0 resource instances configured

Node List:
  * Online: [ z3.example.com z4.example.com ]

Full List of Resources:
  * No resources

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled</pre></li></ol></div><p>
				For additional display options for a disaster recovery configuration, see the help screen for the <code class="literal">pcs dr</code> command.
			</p></section></section><section class="chapter" id="ref_interpreting-resource-exit-codes-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 33. Interpreting resource agent OCF return codes</h2></div></div></div><p class="_abstract _abstract">
			Pacemaker resource agents conform to the Open Cluster Framework (OCF) Resource Agent API. This following tables describe the OCF return codes and how they are interpreted by Pacemaker.
		</p><p>
			The first thing the cluster does when an agent returns a code is to check the return code against the expected result. If the result does not match the expected value, then the operation is considered to have failed, and recovery action is initiated.
		</p><p>
			For any invocation, resource agents must exit with a defined return code that informs the caller of the outcome of the invoked action.
		</p><p>
			There are three types of failure recovery, as described in the following table.
		</p><rh-table id="idm140686139848096"><table class="lt-4-cols lt-7-rows"><caption>Table 33.1. Types of Recovery Performed by the Cluster</caption><colgroup><col style="width: 14%; " class="col_1"><!--Empty--><col style="width: 43%; " class="col_2"><!--Empty--><col style="width: 43%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686139842304" scope="col">Type</th><th align="left" valign="top" id="idm140686139841216" scope="col">Description</th><th align="left" valign="top" id="idm140686139840128" scope="col">Action Taken by the Cluster</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686139842304"> <p>
							soft
						</p>
						 </td><td align="left" valign="top" headers="idm140686139841216"> <p>
							A transient error occurred.
						</p>
						 </td><td align="left" valign="top" headers="idm140686139840128"> <p>
							Restart the resource or move it to a new location .
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686139842304"> <p>
							hard
						</p>
						 </td><td align="left" valign="top" headers="idm140686139841216"> <p>
							A non-transient error that may be specific to the current node occurred.
						</p>
						 </td><td align="left" valign="top" headers="idm140686139840128"> <p>
							Move the resource elsewhere and prevent it from being retried on the current node.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686139842304"> <p>
							fatal
						</p>
						 </td><td align="left" valign="top" headers="idm140686139841216"> <p>
							A non-transient error that will be common to all cluster nodes occurred (for example, a bad configuration was specified).
						</p>
						 </td><td align="left" valign="top" headers="idm140686139840128"> <p>
							Stop the resource and prevent it from being started on any cluster node.
						</p>
						 </td></tr></tbody></table></rh-table><p>
			The following table provides The OCF return codes and the type of recovery the cluster will initiate when a failure code is received.Note that even actions that return 0 (OCF alias <code class="literal">OCF_SUCCESS</code>) can be considered to have failed, if 0 was not the expected return value.
		</p><rh-table id="idm140686136972976"><table class="lt-4-cols lt-7-rows"><caption>Table 33.2. OCF Return Codes</caption><colgroup><col style="width: 11%; " class="col_1"><!--Empty--><col style="width: 33%; " class="col_2"><!--Empty--><col style="width: 56%; " class="col_3"><!--Empty--></colgroup><thead><tr><th align="left" valign="top" id="idm140686136967216" scope="col">Return Code</th><th align="left" valign="top" id="idm140686136966128" scope="col">OCF Label</th><th align="left" valign="top" id="idm140686136965040" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140686136967216"> <p>
							0
						</p>
						 </td><td align="left" valign="top" headers="idm140686136966128"> <p>
							<code class="literal">OCF_SUCCESS</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686136965040"> <p>
							* The action completed successfully. This is the expected return code for any successful start, stop, promote, and demote command.
						</p>
						 <p>
							* Type if unexpected: soft
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686136967216"> <p>
							1
						</p>
						 </td><td align="left" valign="top" headers="idm140686136966128"> <p>
							<code class="literal">OCF_ERR_GENERIC</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686136965040"> <p>
							* The action returned a generic error.
						</p>
						 <p>
							* Type: soft
						</p>
						 <p>
							* The resource manager will attempt to recover the resource or move it to a new location.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686136967216"> <p>
							2
						</p>
						 </td><td align="left" valign="top" headers="idm140686136966128"> <p>
							<code class="literal">OCF_ERR_ARGS</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686136965040"> <p>
							* The resource’s configuration is not valid on this machine. For example, it refers to a location not found on the node.
						</p>
						 <p>
							* Type: hard
						</p>
						 <p>
							* The resource manager will move the resource elsewhere and prevent it from being retried on the current node
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686136967216"> <p>
							3
						</p>
						 </td><td align="left" valign="top" headers="idm140686136966128"> <p>
							<code class="literal">OCF_ERR_UNIMPLEMENTED</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686136965040"> <p>
							* The requested action is not implemented.
						</p>
						 <p>
							* Type: hard
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686136967216"> <p>
							4
						</p>
						 </td><td align="left" valign="top" headers="idm140686136966128"> <p>
							<code class="literal">OCF_ERR_PERM</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686136965040"> <p>
							* The resource agent does not have sufficient privileges to complete the task. This may be due, for example, to the agent not being able to open a certain file, to listen on a specific socket, or to write to a directory.
						</p>
						 <p>
							* Type: hard
						</p>
						 <p>
							* Unless specifically configured otherwise, the resource manager will attempt to recover a resource which failed with this error by restarting the resource on a different node (where the permission problem may not exist).
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686136967216"> <p>
							5
						</p>
						 </td><td align="left" valign="top" headers="idm140686136966128"> <p>
							<code class="literal">OCF_ERR_INSTALLED</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686136965040"> <p>
							* A required component is missing on the node where the action was executed. This may be due to a required binary not being executable, or a vital configuration file being unreadable.
						</p>
						 <p>
							* Type: hard
						</p>
						 <p>
							* Unless specifically configured otherwise, the resource manager will attempt to recover a resource which failed with this error by restarting the resource on a different node (where the required files or binaries may be present).
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686136967216"> <p>
							6
						</p>
						 </td><td align="left" valign="top" headers="idm140686136966128"> <p>
							<code class="literal">OCF_ERR_CONFIGURED</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686136965040"> <p>
							* The resource’s configuration on the local node is invalid.
						</p>
						 <p>
							* Type: fatal
						</p>
						 <p>
							* When this code is returned, Pacemaker will prevent the resource from running on any node in the cluster, even if the service configuraiton is valid on some other node.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686136967216"> <p>
							7
						</p>
						 </td><td align="left" valign="top" headers="idm140686136966128"> <p>
							<code class="literal">OCF_NOT_RUNNING</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686136965040"> <p>
							* The resource is safely stopped. This implies that the resource has either gracefully shut down, or has never been started.
						</p>
						 <p>
							* Type if unexpected: soft
						</p>
						 <p>
							* The cluster will not attempt to stop a resource that returns this for any action.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686136967216"> <p>
							8
						</p>
						 </td><td align="left" valign="top" headers="idm140686136966128"> <p>
							<code class="literal">OCF_RUNNING_PROMOTED</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686136965040"> <p>
							* The resource is running in promoted role.
						</p>
						 <p>
							* Type if unexpected: soft
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686136967216"> <p>
							9
						</p>
						 </td><td align="left" valign="top" headers="idm140686136966128"> <p>
							<code class="literal">OCF_FAILED_PROMOTED</code>
						</p>
						 </td><td align="left" valign="top" headers="idm140686136965040"> <p>
							* The resource is (or might be) in promoted role but has failed.
						</p>
						 <p>
							* Type: soft
						</p>
						 <p>
							* The resource will be demoted, stopped and then started (and possibly promoted) again.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686136967216"> <p>
							190
						</p>
						 </td><td align="left" valign="top" headers="idm140686136966128"> </td><td align="left" valign="top" headers="idm140686136965040"> <p>
							* The service is found to be properly active, but in such a condition that future failures are more likely.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686136967216"> <p>
							191
						</p>
						 </td><td align="left" valign="top" headers="idm140686136966128"> </td><td align="left" valign="top" headers="idm140686136965040"> <p>
							* The resource agent supports roles and the service is found to be properly active in the promoted role, but in such a condition that future failures are more likely.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm140686136967216"> <p>
							other
						</p>
						 </td><td align="left" valign="top" headers="idm140686136966128"> <p>
							N/A
						</p>
						 </td><td align="left" valign="top" headers="idm140686136965040"> <p>
							Custom error code.
						</p>
						 </td></tr></tbody></table></rh-table></section><section class="chapter" id="ref_ibmz-configuring-and-managing-high-availability-clusters"><div class="titlepage"><div><div><h2 class="title">Chapter 34. Configuring a Red Hat High Availability cluster with IBM z/VM instances as cluster members</h2></div></div></div><p class="_abstract _abstract">
			Red Hat provides several articles that may be useful when designing, configuring, and administering a Red Hat High Availability cluster running on z/VM virtual machines.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<a class="link" href="https://access.redhat.com/articles/1543363">Design Guidance for RHEL High Availability Clusters - IBM z/VM Instances as Cluster Members</a>
				</li><li class="listitem">
					<a class="link" href="https://access.redhat.com/articles/3331981">Administrative Procedures for RHEL High Availability Clusters - Configuring z/VM SMAPI Fencing with fence_zvmip for RHEL 7 or 8 IBM z Systems Cluster Members</a>
				</li><li class="listitem">
					<a class="link" href="https://access.redhat.com/solutions/3555071">RHEL High Availability cluster nodes on IBM z Systems experience STONITH-device timeouts around midnight on a nightly basis</a> (Red Hat Knowledgebase)
				</li><li class="listitem">
					<a class="link" href="https://access.redhat.com/articles/3332491">Administrative Procedures for RHEL High Availability Clusters - Preparing a dasd Storage Device for Use by a Cluster of IBM z Systems Members</a>
				</li></ul></div><p>
			You may also find the following articles useful when designing a Red Hat High Availability cluster in general.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<a class="link" href="https://access.redhat.com/articles/2912891">Support Policies for RHEL High Availability Clusters</a>
				</li><li class="listitem">
					<a class="link" href="https://access.redhat.com/articles/3099541">Exploring Concepts of RHEL High Availability Clusters - Fencing/STONITH</a>
				</li></ul></div></section><div><div xml:lang="en-US" class="legalnotice" id="idm140686156711152"><h2 class="legalnotice">Legal Notice</h2><div class="para">
		Copyright <span class="trademark"><!--Empty--></span>© 2024 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></body></section><!----></div></article><aside id="layout" class="span-xs-12 span-sm-2 span-md-2 content-format-selectors" aria-label="Select page format" data-v-8589d091><div class="sticky-top page-layout-options" data-v-8589d091><label for="page-format" data-v-8589d091>Format</label><select id="page-format" class="page-format-dropdown" data-v-8589d091><option class="page-type" value="html" data-v-8589d091>Multi-page</option><option selected class="page-type" value="html-single" data-v-8589d091>Single-page</option><option class="page-type" value="pdf" data-v-8589d091>View full doc as PDF</option></select></div><!----></aside></div><div class="btn-container hidden" data-v-8589d091><pf-button class="top-scroll-btn" icon="angle-up" icon-set="fas" icon-position="right" data-v-8589d091>Back to top</pf-button></div><!--]--><!--]--></main><rh-footer data-analytics-region="page-footer" data-v-97dd2752><a slot="logo" href="/en" data-analytics-category="Footer" data-analytics-text="Logo" data-v-97dd2752><img alt="Red Hat logo" src="/Logo-Red_Hat-Documentation-A-Reverse-RGB.svg" loading="lazy" width="222" height="40" data-v-97dd2752></a><rh-footer-social-link slot="social-links" icon="github" data-v-97dd2752><a href="https://github.com/redhat-documentation" data-analytics-region="social-links-exit" data-analytics-category="Footer|social-links" data-analytics-text="LinkedIn" data-v-97dd2752>Github</a></rh-footer-social-link><rh-footer-social-link slot="social-links" icon="reddit" data-v-97dd2752><a href="https://www.reddit.com/r/redhat/" data-analytics-region="social-links-exit" data-analytics-category="Footer|social-links" data-analytics-text="YouTube" data-v-97dd2752>Reddit</a></rh-footer-social-link><rh-footer-social-link slot="social-links" icon="youtube" data-v-97dd2752><a href="https://www.youtube.com/@redhat" data-analytics-region="social-links-exit" data-analytics-category="Footer|social-links" data-analytics-text="Facebook" data-v-97dd2752>Youtube</a></rh-footer-social-link><rh-footer-social-link slot="social-links" icon="twitter" data-v-97dd2752><a href="https://twitter.com/RedHat" data-analytics-region="social-links-exit" data-analytics-category="Footer|social-links" data-analytics-text="Twitter" data-v-97dd2752>Twitter</a></rh-footer-social-link><h3 slot="links" data-analytics-text="Learn" data-v-97dd2752>Learn</h3><ul slot="links" data-v-97dd2752><li data-v-97dd2752><a href="https://developers.redhat.com/learn" data-analytics-category="Footer|Learn" data-analytics-text="Developer resources" data-v-97dd2752>Developer resources</a></li><li data-v-97dd2752><a href="https://cloud.redhat.com/learn" data-analytics-category="Footer|Learn" data-analytics-text="Cloud learning hub" data-v-97dd2752>Cloud learning hub</a></li><li data-v-97dd2752><a href="https://www.redhat.com/en/interactive-labs" data-analytics-category="Footer|Learn" data-analytics-text="Interactive labs" data-v-97dd2752>Interactive labs</a></li><li data-v-97dd2752><a href="https://www.redhat.com/services/training-and-certification" data-analytics-category="Footer|Learn" data-analytics-text="Training and certification" data-v-97dd2752>Training and certification</a></li><li data-v-97dd2752><a href="https://access.redhat.com/support" data-analytics-category="Footer|Learn" data-analytics-text="Customer support" data-v-97dd2752>Customer support</a></li><li data-v-97dd2752><a href="/products" data-analytics-category="Footer|Learn" data-analytics-text="See all documentation" data-v-97dd2752>See all documentation</a></li></ul><h3 slot="links" data-analytics-text="Try buy sell" data-v-97dd2752>Try, buy, &amp; sell</h3><ul slot="links" data-v-97dd2752><li data-v-97dd2752><a href="https://redhat.com/en/products/trials" data-analytics-category="Footer|Try buy sell" data-analytics-text="Product trial center" data-v-97dd2752>Product trial center</a></li><li data-v-97dd2752><a href="https://marketplace.redhat.com" data-analytics-category="Footer|Try buy sell" data-analytics-text="Red Hat Marketplace" data-v-97dd2752>Red Hat Marketplace</a></li><li data-v-97dd2752><a href="https://catalog.redhat.com/" data-analytics-category="Footer|Try buy sell" data-analytics-text="Red Hat Ecosystem Catalog" data-v-97dd2752>Red Hat Ecosystem Catalog</a></li><li data-v-97dd2752><a href="https://www.redhat.com/en/store" data-analytics-category="Footer|Try buy sell" data-analytics-text="Red Hat Store" data-v-97dd2752>Red Hat Store</a></li><li data-v-97dd2752><a href="https://www.redhat.com/about/japan-buy" data-analytics-category="Footer|Try buy sell" data-analytics-text="Buy online (Japan)" data-v-97dd2752>Buy online (Japan)</a></li></ul><h3 slot="links" data-analytics-text="Communities" data-v-97dd2752>Communities</h3><ul slot="links" data-v-97dd2752><li data-v-97dd2752><a href="https://access.redhat.com/community" data-analytics-category="Footer|Communities" data-analytics-text="Customer Portal Community" data-v-97dd2752>Customer Portal Community</a></li><li data-v-97dd2752><a href="https://www.redhat.com/events" data-analytics-category="Footer|Communities" data-analytics-text="Events" data-v-97dd2752>Events</a></li><li data-v-97dd2752><a href="https://www.redhat.com/about/our-community-contributions" data-analytics-category="Footer|Communities" data-analytics-text="How we contribute" data-v-97dd2752>How we contribute</a></li></ul><rh-footer-block slot="main-secondary" data-v-97dd2752><h3 slot="header" data-analytics-text="About Red Hat Documentation" data-v-97dd2752>About Red Hat Documentation</h3><p data-v-97dd2752>We help Red Hat users innovate and achieve their goals with our products and services with content they can trust.</p></rh-footer-block><rh-footer-block slot="main-secondary" data-v-97dd2752><h3 slot="header" data-analytics-text="Making open source more inclusive" data-v-97dd2752>Making open source more inclusive</h3><p data-v-97dd2752>Red Hat is committed to replacing problematic language in our code, documentation, and web properties. For more details, see the <a href=" https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language" data-analytics-category="Footer|Making open source more inclusive" data-analytics-text="Red Hat Blog" data-v-97dd2752>Red Hat Blog</a>.</p></rh-footer-block><rh-footer-block slot="main-secondary" data-v-97dd2752><h3 slot="header" data-analytics-text="About Red Hat" data-v-97dd2752>About Red Hat</h3><p data-v-97dd2752>We deliver hardened solutions that make it easier for enterprises to work across platforms and environments, from the core datacenter to the network edge.</p></rh-footer-block><rh-footer-universal slot="universal" data-v-97dd2752><h3 slot="links-primary" data-analytics-text="Red Hat legal and privacy links" hidden data-v-97dd2752>Red Hat legal and privacy links</h3><ul slot="links-primary" data-analytics-region="page-footer-bottom-primary" data-v-97dd2752><li data-v-97dd2752><a href="https://redhat.com/en/about/company" data-analytics-category="Footer|Corporate" data-analytics-text="About Red Hat" data-v-97dd2752>About Red Hat</a></li><li data-v-97dd2752><a href="https://redhat.com/en/jobs" data-analytics-category="Footer|Corporate" data-analytics-text="Jobs" data-v-97dd2752>Jobs</a></li><li data-v-97dd2752><a href="https://redhat.com/en/events" data-analytics-category="Footer|Corporate" data-analytics-text="Events" data-v-97dd2752>Events</a></li><li data-v-97dd2752><a href="https://redhat.com/en/about/office-locations" data-analytics-category="Footer|Corporate" data-analytics-text="Locations" data-v-97dd2752>Locations</a></li><li data-v-97dd2752><a href="https://redhat.com/en/contact" data-analytics-category="Footer|Corporate" data-analytics-text="Contact Red Hat" data-v-97dd2752>Contact Red Hat</a></li><li data-v-97dd2752><a href="https://redhat.com/en/blog" data-analytics-category="Footer|Corporate" data-analytics-text="Red Hat Blog" data-v-97dd2752>Red Hat Blog</a></li><li data-v-97dd2752><a href="https://redhat.com/en/about/our-culture/diversity-equity-inclusion" data-analytics-category="Footer|Corporate" data-analytics-text="Diversity equity and inclusion" data-v-97dd2752>Diversity, equity, and inclusion</a></li><li data-v-97dd2752><a href="https://coolstuff.redhat.com/" data-analytics-category="Footer|Corporate" data-analytics-text="Cool Stuff Store" data-v-97dd2752>Cool Stuff Store</a></li><li data-v-97dd2752><a href="https://www.redhat.com/en/summit" data-analytics-category="Footer|Corporate" data-analytics-text="Red Hat Summit" data-v-97dd2752>Red Hat Summit</a></li></ul><span data-v-97dd2752 data-v-5f538988></span><rh-footer-copyright slot="links-secondary" data-v-97dd2752>© 2024 Red Hat, Inc.</rh-footer-copyright><h3 slot="links-secondary" data-analytics-text="Red Hat legal and privacy links" hidden data-v-97dd2752>Red Hat legal and privacy links</h3><ul slot="links-secondary" data-analytics-region="page-footer-bottom-secondary" data-v-97dd2752><li data-v-97dd2752><a href="https://redhat.com/en/about/privacy-policy" data-analytics-category="Footer|Red Hat legal and privacy links" data-analytics-text="Privacy statement" data-v-97dd2752>Privacy statement</a></li><li data-v-97dd2752><a href="https://redhat.com/en/about/terms-use" data-analytics-category="Footer|Red Hat legal and privacy links" data-analytics-text="Terms of use" data-v-97dd2752>Terms of use</a></li><li data-v-97dd2752><a href="https://redhat.com/en/about/all-policies-guidelines" data-analytics-category="Footer|Red Hat legal and privacy links" data-analytics-text="All policies and guidelines" data-v-97dd2752>All policies and guidelines</a></li><li data-v-97dd2752><a href="https://redhat.com/en/about/digital-accessibility" data-analytics-category="Footer|Red Hat legal and privacy links" data-analytics-text="Digital accessibility" class="active" data-v-97dd2752>Digital accessibility</a></li><li data-v-97dd2752><span id="teconsent" data-v-97dd2752></span></li></ul></rh-footer-universal></rh-footer><div id="consent_blackbar" style="position:fixed;bottom:0;width:100%;z-index:5;padding:10px;"></div><!--]--><!--]--></div><div id="teleports"></div><script type="application/json" id="__NUXT_DATA__" data-ssr="true">[["ShallowReactive",1],{"data":2,"state":1217,"once":1221,"_errors":1222,"serverRendered":15,"path":1224},["ShallowReactive",3],{"s8LoCEfG4A":4,"uUstF4AIyn":10,"Pn02PlJOas":1145,"rFVLKcOK8e":1216},[5,6,7,8,9],"fr-fr","zh-cn","en-us","ko-kr","ja-jp",{"name":11,"html":12,"type":-1,"toc":13,"breadcrumbs":1042,"error":18,"title":1050,"productName":1044,"productVersions":1059,"pagination":1088,"redirect":1126,"canonicalLinks":1127,"openShiftProducts":1129,"tocFromVolume":-1,"jumpLinks":1144},"Configuring and managing high availability clusters","\u003Cbody>\u003Cdiv xml:lang=\"en-US\" class=\"book\" id=\"idm140686152248144\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv class=\"producttitle\">\u003Cspan class=\"productname\">Red Hat Enterprise Linux\u003C/span> \u003Cspan class=\"productnumber\">9\u003C/span>\u003C/div>\u003Cdiv>\u003Ch3 class=\"subtitle\">Using the Red Hat High Availability Add-On to create and maintain Pacemaker clusters\u003C/h3>\u003C/div>\u003Cdiv>\u003Cdiv xml:lang=\"en-US\" class=\"authorgroup\">\u003Cspan class=\"orgname\">Red Hat\u003C/span> \u003Cspan class=\"orgdiv\">Customer Content Services\u003C/span>\u003C/div>\u003C/div>\u003Cdiv>\u003Ca href=\"#idm140686156711152\">Legal Notice\u003C/a>\u003C/div>\u003Cdiv>\u003Cdiv class=\"abstract\">\u003Cp class=\"title\">\u003Cstrong>Abstract\u003C/strong>\u003C/p>\u003Cdiv class=\"para\">\n\t\t\t\tThe Red Hat High Availability Add-On configures high availability clusters that use the Pacemaker cluster resource manager. This title provides procedures to familiarize you with Pacemaker cluster configuration as well as example procedures for configuring active/active and active/passive clusters.\n\t\t\t\u003C/div>\u003C/div>\u003C/div>\u003C/div>\u003Chr/>\u003C/div>\u003Csection class=\"preface\" id=\"proc_providing-feedback-on-red-hat-documentation_configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Providing feedback on Red Hat documentation\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tWe appreciate your feedback on our documentation. Let us know how we can improve it.\n\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Submitting feedback through Jira (account required)\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\tLog in to the \u003Ca class=\"link\" href=\"https://issues.redhat.com/projects/RHELDOCS/issues\">Jira\u003C/a> website.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tClick \u003Cspan class=\"strong strong\">\u003Cstrong>Create\u003C/strong>\u003C/span> in the top navigation bar\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tEnter a descriptive title in the \u003Cspan class=\"strong strong\">\u003Cstrong>Summary\u003C/strong>\u003C/span> field.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tEnter your suggestion for improvement in the \u003Cspan class=\"strong strong\">\u003Cstrong>Description\u003C/strong>\u003C/span> field. Include links to the relevant parts of the documentation.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tClick \u003Cspan class=\"strong strong\">\u003Cstrong>Create\u003C/strong>\u003C/span> at the bottom of the dialogue.\n\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_overview-of-high-availability-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 1. High Availability Add-On overview\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tThe High Availability Add-On is a clustered system that provides reliability, scalability, and availability to critical production services.\n\t\t\u003C/p>\u003Cp>\n\t\t\tA cluster is two or more computers (called \u003Cspan class=\"emphasis\">\u003Cem>nodes\u003C/em>\u003C/span> or \u003Cspan class=\"emphasis\">\u003Cem>members\u003C/em>\u003C/span>) that work together to perform a task. Clusters can be used to provide highly available services or resources. The redundancy of multiple machines is used to guard against failures of many types.\n\t\t\u003C/p>\u003Cp>\n\t\t\tHigh availability clusters provide highly available services by eliminating single points of failure and by failing over services from one cluster node to another in case a node becomes inoperative. Typically, services in a high availability cluster read and write data (by means of read-write mounted file systems). Therefore, a high availability cluster must maintain data integrity as one cluster node takes over control of a service from another cluster node. Node failures in a high availability cluster are not visible from clients outside the cluster. (High availability clusters are sometimes referred to as failover clusters.) The High Availability Add-On provides high availability clustering through its high availability service management component, \u003Ccode class=\"literal command\">Pacemaker\u003C/code>.\n\t\t\u003C/p>\u003Cp>\n\t\t\tRed Hat provides a variety of documentation for planning, configuring, and maintaining a Red Hat high availability cluster. For a listing of articles that provide guided indexes to the various areas of Red Hat cluster documentation, see the \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/6565761\"> Red Hat High Availability Add-On Documentation Guide\u003C/a>.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"con_high-availability-add-on-components-overview-of-high-availability\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">1.1. High Availability Add-On components\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe Red Hat High Availability Add-On consists of several components that provide the high availability service.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe major components of the High Availability Add-On are as follows:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tCluster infrastructure — Provides fundamental functions for nodes to work together as a cluster: configuration file management, membership management, lock management, and fencing.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tHigh availability service management — Provides failover of services from one cluster node to another in case a node becomes inoperative.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tCluster administration tools — Configuration and management tools for setting up, configuring, and managing the High Availability Add-On. The tools are for use with the cluster infrastructure components, the high availability and service management components, and storage.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tYou can supplement the High Availability Add-On with the following components:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tRed Hat GFS2 (Global File System 2) — Part of the Resilient Storage Add-On, this provides a cluster file system for use with the High Availability Add-On. GFS2 allows multiple nodes to share storage at a block level as if the storage were connected locally to each cluster node. GFS2 cluster file system requires a cluster infrastructure.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tLVM Locking Daemon (\u003Ccode class=\"literal\">lvmlockd\u003C/code>) — Part of the Resilient Storage Add-On, this provides volume management of cluster storage. \u003Ccode class=\"literal\">lvmlockd\u003C/code> support also requires cluster infrastructure.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tHAProxy — Routing software that provides high availability load balancing and failover in layer 4 (TCP) and layer 7 (HTTP, HTTPS) services.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"con_high-availability-add-on-concepts-overview-of-high-availability\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">1.2. High Availability Add-On concepts\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tSome of the key concepts of a Red Hat High Availability Add-On cluster are as follows.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">1.2.1. Fencing\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tIf communication with a single node in the cluster fails, then other nodes in the cluster must be able to restrict or release access to resources that the failed cluster node may have access to. his cannot be accomplished by contacting the cluster node itself as the cluster node may not be responsive. Instead, you must provide an external method, which is called fencing with a fence agent. A fence device is an external device that can be used by the cluster to restrict access to shared resources by an errant node, or to issue a hard reboot on the cluster node.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tWithout a fence device configured you do not have a way to know that the resources previously used by the disconnected cluster node have been released, and this could prevent the services from running on any of the other cluster nodes. Conversely, the system may assume erroneously that the cluster node has released its resources and this can lead to data corruption and data loss. Without a fence device configured data integrity cannot be guaranteed and the cluster configuration will be unsupported.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tWhen the fencing is in progress no other cluster operation is allowed to run. Normal operation of the cluster cannot resume until fencing has completed or the cluster node rejoins the cluster after the cluster node has been rebooted.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tFor more information about fencing, see the Red Hat Knowledgebase solution \u003Ca class=\"link\" href=\"https://access.redhat.com/solutions/15575\">Fencing in a Red Hat High Availability Cluster\u003C/a>.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"quorum\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">1.2.2. Quorum\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tIn order to maintain cluster integrity and availability, cluster systems use a concept known as \u003Cspan class=\"emphasis\">\u003Cem>quorum\u003C/em>\u003C/span> to prevent data corruption and loss. A cluster has quorum when more than half of the cluster nodes are online. To mitigate the chance of data corruption due to failure, Pacemaker by default stops all resources if the cluster does not have quorum.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tQuorum is established using a voting system. When a cluster node does not function as it should or loses communication with the rest of the cluster, the majority working nodes can vote to isolate and, if needed, fence the node for servicing.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tFor example, in a 6-node cluster, quorum is established when at least 4 cluster nodes are functioning. If the majority of nodes go offline or become unavailable, the cluster no longer has quorum and Pacemaker stops clustered services.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe quorum features in Pacemaker prevent what is also known as \u003Cspan class=\"emphasis\">\u003Cem>split-brain\u003C/em>\u003C/span>, a phenomenon where the cluster is separated from communication but each part continues working as separate clusters, potentially writing to the same data and possibly causing corruption or loss. For more information about what it means to be in a split-brain state, and on quorum concepts in general, see the Red Hat Knowledgebase article \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/2824071\">Exploring Concepts of RHEL High Availability Clusters - Quorum\u003C/a>.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tA Red Hat Enterprise Linux High Availability Add-On cluster uses the \u003Ccode class=\"literal\">votequorum\u003C/code> service, in conjunction with fencing, to avoid split brain situations. A number of votes is assigned to each system in the cluster, and cluster operations are allowed to proceed only when a majority of votes is present.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"cluster_resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">1.2.3. Cluster resources\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tA \u003Cspan class=\"emphasis\">\u003Cem>cluster resource\u003C/em>\u003C/span> is an instance of program, data, or application to be managed by the cluster service. These resources are abstracted by \u003Cspan class=\"emphasis\">\u003Cem>agents\u003C/em>\u003C/span> that provide a standard interface for managing the resource in a cluster environment.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tTo ensure that resources remain healthy, you can add a monitoring operation to a resource’s definition. If you do not specify a monitoring operation for a resource, one is added by default.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tYou can determine the behavior of a resource in a cluster by configuring \u003Cspan class=\"emphasis\">\u003Cem>constraints\u003C/em>\u003C/span>. You can configure the following categories of constraints:\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tlocation constraints — A location constraint determines which nodes a resource can run on.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tordering constraints — An ordering constraint determines the order in which the resources run.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tcolocation constraints — A colocation constraint determines where resources will be placed relative to other resources.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\t\tOne of the most common elements of a cluster is a set of resources that need to be located together, start sequentially, and stop in the reverse order. To simplify this configuration, Pacemaker supports the concept of \u003Cspan class=\"emphasis\">\u003Cem>groups\u003C/em>\u003C/span>.\n\t\t\t\t\u003C/p>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"con_pacemaker-overview-overview-of-high-availability\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">1.3. Pacemaker overview\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tPacemaker is a cluster resource manager. It achieves maximum availability for your cluster services and resources by making use of the cluster infrastructure’s messaging and membership capabilities to deter and recover from node and resource-level failure.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"pacemaker_architecture_components\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">1.3.1. Pacemaker architecture components\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tA cluster configured with Pacemaker comprises separate component daemons that monitor cluster membership, scripts that manage the services, and resource management subsystems that monitor the disparate resources.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe following components form the Pacemaker architecture:\n\t\t\t\t\u003C/p>\u003Cdiv class=\"variablelist\">\u003Cdl class=\"variablelist\">\u003Cdt>\u003Cspan class=\"term\">Cluster Information Base (CIB)\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\t\tThe Pacemaker information daemon, which uses XML internally to distribute and synchronize current configuration and status information from the Designated Coordinator (DC) — a node assigned by Pacemaker to store and distribute cluster state and actions by means of the CIB — to all other cluster nodes.\n\t\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">Cluster Resource Management Daemon (CRMd)\u003C/span>\u003C/dt>\u003Cdd>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tPacemaker cluster resource actions are routed through this daemon. Resources managed by CRMd can be queried by client systems, moved, instantiated, and changed when needed.\n\t\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tEach cluster node also includes a local resource manager daemon (LRMd) that acts as an interface between CRMd and resources. LRMd passes commands from CRMd to agents, such as starting and stopping and relaying status information.\n\t\t\t\t\t\t\t\u003C/p>\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">Shoot the Other Node in the Head (STONITH)\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\t\tSTONITH is the Pacemaker fencing implementation. It acts as a cluster resource in Pacemaker that processes fence requests, forcefully shutting down nodes and removing them from the cluster to ensure data integrity. STONITH is configured in the CIB and can be monitored as a normal cluster resource.\n\t\t\t\t\t\t\t\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">corosync\u003C/span>\u003C/dt>\u003Cdd>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">corosync\u003C/code> is the component - and a daemon of the same name - that serves the core membership and member-communication needs for high availability clusters. It is required for the High Availability Add-On to function.\n\t\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tIn addition to those membership and messaging functions, \u003Ccode class=\"literal\">corosync\u003C/code> also:\n\t\t\t\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\tManages quorum rules and determination.\n\t\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\tProvides messaging capabilities for applications that coordinate or operate across multiple members of the cluster and thus must communicate stateful or other information between instances.\n\t\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\tUses the \u003Ccode class=\"literal\">kronosnet\u003C/code> library as its network transport to provide multiple redundant links and automatic failover.\n\t\t\t\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/dd>\u003C/dl>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"pacemaker_configuration_and_management_tools\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">1.3.2. Pacemaker configuration and management tools\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe High Availability Add-On features two configuration tools for cluster deployment, monitoring, and management.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"variablelist\">\u003Cdl class=\"variablelist\">\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal command\">pcs\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tThe \u003Ccode class=\"literal command\">pcs\u003C/code> command-line interface controls and configures Pacemaker and the \u003Ccode class=\"literal\">corosync\u003C/code> heartbeat daemon. A command-line based program, \u003Ccode class=\"literal command\">pcs\u003C/code> can perform the following cluster management tasks:\n\t\t\t\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\tCreate and configure a Pacemaker/Corosync cluster\n\t\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\tModify configuration of the cluster while it is running\n\t\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\tRemotely configure both Pacemaker and Corosync as well as start, stop, and display status information of the cluster\n\t\t\t\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal command\">pcsd\u003C/code> Web UI\u003C/span>\u003C/dt>\u003Cdd>\n\t\t\t\t\t\t\t\tA graphical user interface to create and configure Pacemaker/Corosync clusters.\n\t\t\t\t\t\t\t\u003C/dd>\u003C/dl>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"the_cluster_and_pacemaker_configuration_files\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">1.3.3. The cluster and Pacemaker configuration files\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe configuration files for the Red Hat High Availability Add-On are \u003Ccode class=\"literal\">corosync.conf\u003C/code> and \u003Ccode class=\"literal\">cib.xml\u003C/code>.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe \u003Ccode class=\"literal\">corosync.conf\u003C/code> file provides the cluster parameters used by \u003Ccode class=\"literal\">corosync\u003C/code>, the cluster manager that Pacemaker is built on. In general, you should not edit the \u003Ccode class=\"literal\">corosync.conf\u003C/code> directly but, instead, use the \u003Ccode class=\"literal command\">pcs\u003C/code> or \u003Ccode class=\"literal command\">pcsd\u003C/code> interface.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe \u003Ccode class=\"literal\">cib.xml\u003C/code> file is an XML file that represents both the cluster’s configuration and the current state of all resources in the cluster. This file is used by Pacemaker’s Cluster Information Base (CIB). The contents of the CIB are automatically kept in sync across the entire cluster. Do not edit the \u003Ccode class=\"literal\">cib.xml\u003C/code> file directly; use the \u003Ccode class=\"literal command\">pcs\u003C/code> or \u003Ccode class=\"literal command\">pcsd\u003C/code> interface instead.\n\t\t\t\t\u003C/p>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"con_HA-lvm-shared-volumes-overview-of-high-availability\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">1.4. LVM logical volumes in a Red Hat high availability cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe Red Hat High Availability Add-On provides support for LVM volumes in two distinct cluster configurations.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe cluster configurations you can choose are as follows:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tHigh availability LVM volumes (HA-LVM) in active/passive failover configurations in which only a single node of the cluster accesses the storage at any one time.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tLVM volumes that use the \u003Ccode class=\"literal\">lvmlockd\u003C/code> daemon to manage storage devices in active/active configurations in which more than one node of the cluster requires access to the storage at the same time. The \u003Ccode class=\"literal\">lvmlockd\u003C/code> daemon is part of the Resilient Storage Add-On.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Csection class=\"section\" id=\"choosing_ha_lvm_or_shared_volumes\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">1.4.1. Choosing HA-LVM or shared volumes\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tWhen to use HA-LVM or shared logical volumes managed by the \u003Ccode class=\"literal\">lvmlockd\u003C/code> daemon should be based on the needs of the applications or services being deployed.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tIf multiple nodes of the cluster require simultaneous read/write access to LVM volumes in an active/active system, then you must use the \u003Ccode class=\"literal\">lvmlockd\u003C/code> daemon and configure your volumes as shared volumes. The \u003Ccode class=\"literal\">lvmlockd\u003C/code> daemon provides a system for coordinating activation of and changes to LVM volumes across nodes of a cluster concurrently. The \u003Ccode class=\"literal\">lvmlockd\u003C/code> daemon’s locking service provides protection to LVM metadata as various nodes of the cluster interact with volumes and make changes to their layout. This protection is contingent upon configuring any volume group that will be activated simultaneously across multiple cluster nodes as a shared volume.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tIf the high availability cluster is configured to manage shared resources in an active/passive manner with only one single member needing access to a given LVM volume at a time, then you can use HA-LVM without the \u003Ccode class=\"literal\">lvmlockd\u003C/code> locking service.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\t\tMost applications will run better in an active/passive configuration, as they are not designed or optimized to run concurrently with other instances. Choosing to run an application that is not cluster-aware on shared logical volumes can result in degraded performance. This is because there is cluster communication overhead for the logical volumes themselves in these instances. A cluster-aware application must be able to achieve performance gains above the performance losses introduced by cluster file systems and cluster-aware logical volumes. This is achievable for some applications and workloads more easily than others. Determining what the requirements of the cluster are and whether the extra effort toward optimizing for an active/active cluster will pay dividends is the way to choose between the two LVM variants. Most users will achieve the best HA results from using HA-LVM.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tHA-LVM and shared logical volumes using \u003Ccode class=\"literal\">lvmlockd\u003C/code> are similar in the fact that they prevent corruption of LVM metadata and its logical volumes, which could otherwise occur if multiple machines are allowed to make overlapping changes. HA-LVM imposes the restriction that a logical volume can only be activated exclusively; that is, active on only one machine at a time. This means that only local (non-clustered) implementations of the storage drivers are used. Avoiding the cluster coordination overhead in this way increases performance. A shared volume using \u003Ccode class=\"literal\">lvmlockd\u003C/code> does not impose these restrictions and a user is free to activate a logical volume on all machines in a cluster; this forces the use of cluster-aware storage drivers, which allow for cluster-aware file systems and applications to be put on top.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"configuring_lvm_volumes_in_a_cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">1.4.2. Configuring LVM volumes in a cluster\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tClusters are managed through Pacemaker. Both HA-LVM and shared logical volumes are supported only in conjunction with Pacemaker clusters, and must be configured as cluster resources.\n\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\tIf an LVM volume group used by a Pacemaker cluster contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you configure a \u003Ccode class=\"literal\">systemd resource-agents-deps\u003C/code> target and a \u003Ccode class=\"literal\">systemd\u003C/code> drop-in unit for the target to ensure that the service starts before Pacemaker starts. For information on configuring a \u003Ccode class=\"literal\">systemd resource-agents-deps\u003C/code> target, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order\">Configuring startup order for resource dependencies not managed by Pacemaker\u003C/a>.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tFor examples of procedures for configuring an HA-LVM volume as part of a Pacemaker cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters\">Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster\u003C/a> and \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters\">Configuring an active/passive NFS server in a Red Hat High Availability cluster\u003C/a>.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tNote that these procedures include the following steps:\n\t\t\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"circle\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\tEnsuring that only the cluster is capable of activating the volume group\n\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\tConfiguring an LVM logical volume\n\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\tConfiguring the LVM volume as a cluster resource\n\t\t\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tFor procedures for configuring shared LVM volumes that use the \u003Ccode class=\"literal\">lvmlockd\u003C/code> daemon to manage storage devices in active/active configurations, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters\">GFS2 file systems in a cluster\u003C/a> and \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-active-samba-in-a-cluster-configuring-and-managing-high-availability-clusters\">Configuring an active/active Samba server in a Red Hat High Availability cluster\u003C/a>.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_getting-started-with-pacemaker-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 2. Getting started with Pacemaker\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tTo familiarize yourself with the tools and processes you use to create a Pacemaker cluster, you can run the following procedures. They are intended for users who are interested in seeing what the cluster software looks like and how it is administered, without needing to configure a working cluster.\n\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\tThese procedures do not create a supported Red Hat cluster, which requires at least two nodes and the configuration of a fencing device. For full information about Red Hat’s support policies, requirements, and limitations for RHEL High Availability clusters, see \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/2912891/\">Support Policies for RHEL High Availability Clusters\u003C/a>.\n\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Csection class=\"section\" id=\"proc_learning-to-use-pacemaker-getting-started-with-pacemaker\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">2.1. Learning to use Pacemaker\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tBy working through this procedure, you will learn how to use Pacemaker to set up a cluster, how to display cluster status, and how to configure a cluster service. This example creates an Apache HTTP server as a cluster resource and shows how the cluster responds when the resource fails.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIn this example:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe node is \u003Ccode class=\"literal\">z1.example.com\u003C/code>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe floating IP address is 192.168.122.120.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Prerequisites\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tA single node running RHEL 9\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tA floating IP address that resides on the same network as one of the node’s statically assigned IP addresses\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe name of the node on which you are running is in your \u003Ccode class=\"literal\">/etc/hosts\u003C/code> file\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tInstall the Red Hat High Availability Add-On software packages from the High Availability channel, and start and enable the \u003Ccode class=\"literal\">pcsd\u003C/code> service.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install pcs pacemaker fence-agents-all\u003C/strong>\u003C/span>\n...\n# \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl start pcsd.service\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl enable pcsd.service\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf you are running the \u003Ccode class=\"literal\">firewalld\u003C/code> daemon, enable the ports that are required by the Red Hat High Availability Add-On.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --permanent --add-service=high-availability\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --reload\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSet a password for user \u003Ccode class=\"literal\">hacluster\u003C/code> on each node in the cluster and authenticate user \u003Ccode class=\"literal\">hacluster\u003C/code> for each node in the cluster on the node from which you will be running the \u003Ccode class=\"literal\">pcs\u003C/code> commands. This example is using only a single node, the node from which you are running the commands, but this step is included here since it is a necessary step in configuring a supported Red Hat High Availability multi-node cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>passwd hacluster\u003C/strong>\u003C/span>\n...\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs host auth z1.example.com\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate a cluster named \u003Ccode class=\"literal\">my_cluster\u003C/code> with one member and check the status of the cluster. This command creates and starts the cluster in one step.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster setup my_cluster --start z1.example.com\u003C/strong>\u003C/span>\n...\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster status\u003C/strong>\u003C/span>\nCluster Status:\n Stack: corosync\n Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum\n Last updated: Thu Oct 11 16:11:18 2018\n Last change: Thu Oct 11 16:11:00 2018 by hacluster via crmd on z1.example.com\n 1 node configured\n 0 resources configured\n\nPCSD Status:\n  z1.example.com: Online\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tA Red Hat High Availability cluster requires that you configure fencing for the cluster. The reasons for this requirement are described in the Red Hat Knowledgebase solution \u003Ca class=\"link\" href=\"https://access.redhat.com/solutions/15575\">Fencing in a Red Hat High Availability Cluster\u003C/a>. For this introduction, however, which is intended to show only how to use the basic Pacemaker commands, disable fencing by setting the \u003Ccode class=\"literal\">stonith-enabled\u003C/code> cluster option to \u003Ccode class=\"literal\">false\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tThe use of \u003Ccode class=\"literal\">stonith-enabled=false\u003C/code> is completely inappropriate for a production cluster. It tells the cluster to simply pretend that failed nodes are safely fenced.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set stonith-enabled=false\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tConfigure a web browser on your system and create a web page to display a simple text message. If you are running the \u003Ccode class=\"literal\">firewalld\u003C/code> daemon, enable the ports that are required by \u003Ccode class=\"literal\">httpd\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tDo not use \u003Ccode class=\"literal command\">systemctl enable\u003C/code> to enable any services that will be managed by the cluster to start at system boot.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install -y httpd wget\u003C/strong>\u003C/span>\n...\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --permanent --add-service=http\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --reload\u003C/strong>\u003C/span>\n\n# \u003Cspan class=\"strong strong\">\u003Cstrong>cat &lt;&lt;-END &gt;/var/www/html/index.html\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;html&gt;\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;body&gt;My Test Site - $(hostname)&lt;/body&gt;\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;/html&gt;\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>END\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIn order for the Apache resource agent to get the status of Apache, create the following addition to the existing configuration to enable the status server URL.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>cat &lt;&lt;-END &gt; /etc/httpd/conf.d/status.conf\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;Location /server-status&gt;\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>SetHandler server-status\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>Order deny,allow\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>Deny from all\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>Allow from 127.0.0.1\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>Allow from ::1\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;/Location&gt;\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>END\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate \u003Ccode class=\"literal\">IPaddr2\u003C/code> and \u003Ccode class=\"literal\">apache\u003C/code> resources for the cluster to manage. The 'IPaddr2' resource is a floating IP address that must not be one already associated with a physical node. If the 'IPaddr2' resource’s NIC device is not specified, the floating IP must reside on the same network as the statically assigned IP address used by the node.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tYou can display a list of all available resource types with the \u003Ccode class=\"literal command\">pcs resource list\u003C/code> command. You can use the \u003Ccode class=\"literal command\">pcs resource describe \u003Cspan class=\"emphasis\">\u003Cem>resourcetype\u003C/em>\u003C/span>\u003C/code> command to display the parameters you can set for the specified resource type. For example, the following command displays the parameters you can set for a resource of type \u003Ccode class=\"literal\">apache\u003C/code>:\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource describe apache\u003C/strong>\u003C/span>\n...\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIn this example, the IP address resource and the apache resource are both configured as part of a group named \u003Ccode class=\"literal\">apachegroup\u003C/code>, which ensures that the resources are kept together to run on the same node when you are configuring a working multi-node cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create ClusterIP ocf:heartbeat:IPaddr2 ip=192.168.122.120 --group apachegroup\u003C/strong>\u003C/span>\n\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create WebSite ocf:heartbeat:apache configfile=/etc/httpd/conf/httpd.conf statusurl=\"http://localhost/server-status\" --group apachegroup\u003C/strong>\u003C/span>\n\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\nCluster name: my_cluster\nStack: corosync\nCurrent DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum\nLast updated: Fri Oct 12 09:54:33 2018\nLast change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com\n\n1 node configured\n2 resources configured\n\nOnline: [ z1.example.com ]\n\nFull list of resources:\n\nResource Group: apachegroup\n    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com\n    WebSite    (ocf::heartbeat:apache):        Started z1.example.com\n\nPCSD Status:\n  z1.example.com: Online\n...\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAfter you have configured a cluster resource, you can use the \u003Ccode class=\"literal command\">pcs resource config\u003C/code> command to display the options that are configured for that resource.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource config WebSite\u003C/strong>\u003C/span>\nResource: WebSite (class=ocf provider=heartbeat type=apache)\n Attributes: configfile=/etc/httpd/conf/httpd.conf statusurl=http://localhost/server-status\n Operations: start interval=0s timeout=40s (WebSite-start-interval-0s)\n             stop interval=0s timeout=60s (WebSite-stop-interval-0s)\n             monitor interval=1min (WebSite-monitor-interval-1min)\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tPoint your browser to the website you created using the floating IP address you configured. This should display the text message you defined.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tStop the apache web service and check the cluster status. Using \u003Ccode class=\"literal command\">killall -9\u003C/code> simulates an application-level crash.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>killall -9 httpd\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCheck the cluster status. You should see that stopping the web service caused a failed action, but that the cluster software restarted the service and you should still be able to access the website.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\nCluster name: my_cluster\n...\nCurrent DC: z1.example.com (version 1.1.13-10.el7-44eb2dd) - partition with quorum\n1 node and 2 resources configured\n\nOnline: [ z1.example.com ]\n\nFull list of resources:\n\nResource Group: apachegroup\n    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com\n    WebSite    (ocf::heartbeat:apache):        Started z1.example.com\n\nFailed Resource Actions:\n* WebSite_monitor_60000 on z1.example.com 'not running' (7): call=13, status=complete, exitreason='none',\n    last-rc-change='Thu Oct 11 23:45:50 2016', queued=0ms, exec=0ms\n\nPCSD Status:\n    z1.example.com: Online\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tYou can clear the failure status on the resource that failed once the service is up and running again and the failed action notice will no longer appear when you view the cluster status.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource cleanup WebSite\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tWhen you are finished looking at the cluster and the cluster status, stop the cluster services on the node. Even though you have only started services on one node for this introduction, the \u003Ccode class=\"literal\">--all\u003C/code> parameter is included since it would stop cluster services on all nodes on an actual multi-node cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster stop --all\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_learning-to-configure-failover-getting-started-with-pacemaker\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">2.2. Learning to configure failover\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe following procedure provides an introduction to creating a Pacemaker cluster running a service that will fail over from one node to another when the node on which the service is running becomes unavailable. By working through this procedure, you can learn how to create a service in a two-node cluster and you can then observe what happens to that service when it fails on the node on which it running.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThis example procedure configures a two-node Pacemaker cluster running an Apache HTTP server. You can then stop the Apache service on one node to see how the service remains available.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIn this example:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe nodes are \u003Ccode class=\"literal\">z1.example.com\u003C/code> and \u003Ccode class=\"literal\">z2.example.com\u003C/code>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe floating IP address is 192.168.122.120.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Prerequisites\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tTwo nodes running RHEL 9 that can communicate with each other\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tA floating IP address that resides on the same network as one of the node’s statically assigned IP addresses\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe name of the node on which you are running is in your \u003Ccode class=\"literal\">/etc/hosts\u003C/code> file\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes, install the Red Hat High Availability Add-On software packages from the High Availability channel, and start and enable the \u003Ccode class=\"literal\">pcsd\u003C/code> service.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install pcs pacemaker fence-agents-all\u003C/strong>\u003C/span>\n...\n# \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl start pcsd.service\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl enable pcsd.service\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf you are running the \u003Ccode class=\"literal\">firewalld\u003C/code> daemon, on both nodes enable the ports that are required by the Red Hat High Availability Add-On.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --permanent --add-service=high-availability\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --reload\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes in the cluster, set a password for user \u003Ccode class=\"literal\">hacluster\u003C/code> .\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>passwd hacluster\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAuthenticate user \u003Ccode class=\"literal\">hacluster\u003C/code> for each node in the cluster on the node from which you will be running the \u003Ccode class=\"literal\">pcs\u003C/code> commands.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs host auth z1.example.com z2.example.com\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate a cluster named \u003Ccode class=\"literal\">my_cluster\u003C/code> with both nodes as cluster members. This command creates and starts the cluster in one step. You only need to run this from one node in the cluster because \u003Ccode class=\"literal\">pcs\u003C/code> configuration commands take effect for the entire cluster.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn one node in cluster, run the following command.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster setup my_cluster --start z1.example.com z2.example.com\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\tA Red Hat High Availability cluster requires that you configure fencing for the cluster. The reasons for this requirement are described in the Red Hat Knowledgebase solution \u003Ca class=\"link\" href=\"https://access.redhat.com/solutions/15575\">Fencing in a Red Hat High Availability Cluster\u003C/a>. For this introduction, however, to show only how failover works in this configuration, disable fencing by setting the \u003Ccode class=\"literal\">stonith-enabled\u003C/code> cluster option to \u003Ccode class=\"literal\">false\u003C/code>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t+\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tThe use of \u003Ccode class=\"literal\">stonith-enabled=false\u003C/code> is completely inappropriate for a production cluster. It tells the cluster to simply pretend that failed nodes are safely fenced.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\t+\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set stonith-enabled=false\u003C/strong>\u003C/span>\u003C/pre>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAfter creating a cluster and disabling fencing, check the status of the cluster.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tWhen you run the \u003Ccode class=\"literal command\">pcs cluster status\u003C/code> command, it may show output that temporarily differs slightly from the examples as the system components start up.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster status\u003C/strong>\u003C/span>\nCluster Status:\n Stack: corosync\n Current DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum\n Last updated: Thu Oct 11 16:11:18 2018\n Last change: Thu Oct 11 16:11:00 2018 by hacluster via crmd on z1.example.com\n 2 nodes configured\n 0 resources configured\n\nPCSD Status:\n  z1.example.com: Online\n  z2.example.com: Online\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes, configure a web browser and create a web page to display a simple text message. If you are running the \u003Ccode class=\"literal\">firewalld\u003C/code> daemon, enable the ports that are required by \u003Ccode class=\"literal\">httpd\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tDo not use \u003Ccode class=\"literal command\">systemctl enable\u003C/code> to enable any services that will be managed by the cluster to start at system boot.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install -y httpd wget\u003C/strong>\u003C/span>\n...\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --permanent --add-service=http\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --reload\u003C/strong>\u003C/span>\n\n# \u003Cspan class=\"strong strong\">\u003Cstrong>cat &lt;&lt;-END &gt;/var/www/html/index.html\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;html&gt;\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;body&gt;My Test Site - $(hostname)&lt;/body&gt;\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;/html&gt;\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>END\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIn order for the Apache resource agent to get the status of Apache, on each node in the cluster create the following addition to the existing configuration to enable the status server URL.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>cat &lt;&lt;-END &gt; /etc/httpd/conf.d/status.conf\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;Location /server-status&gt;\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>SetHandler server-status\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>Order deny,allow\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>Deny from all\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>Allow from 127.0.0.1\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>Allow from ::1\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;/Location&gt;\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>END\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate \u003Ccode class=\"literal\">IPaddr2\u003C/code> and \u003Ccode class=\"literal\">apache\u003C/code> resources for the cluster to manage. The 'IPaddr2' resource is a floating IP address that must not be one already associated with a physical node. If the 'IPaddr2' resource’s NIC device is not specified, the floating IP must reside on the same network as the statically assigned IP address used by the node.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tYou can display a list of all available resource types with the \u003Ccode class=\"literal command\">pcs resource list\u003C/code> command. You can use the \u003Ccode class=\"literal command\">pcs resource describe \u003Cspan class=\"emphasis\">\u003Cem>resourcetype\u003C/em>\u003C/span>\u003C/code> command to display the parameters you can set for the specified resource type. For example, the following command displays the parameters you can set for a resource of type \u003Ccode class=\"literal\">apache\u003C/code>:\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource describe apache\u003C/strong>\u003C/span>\n...\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIn this example, the IP address resource and the apache resource are both configured as part of a group named \u003Ccode class=\"literal\">apachegroup\u003C/code>, which ensures that the resources are kept together to run on the same node.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tRun the following commands from one node in the cluster:\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create ClusterIP ocf:heartbeat:IPaddr2 ip=192.168.122.120 --group apachegroup\u003C/strong>\u003C/span>\n\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create WebSite ocf:heartbeat:apache configfile=/etc/httpd/conf/httpd.conf statusurl=\"http://localhost/server-status\" --group apachegroup\u003C/strong>\u003C/span>\n\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\nCluster name: my_cluster\nStack: corosync\nCurrent DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum\nLast updated: Fri Oct 12 09:54:33 2018\nLast change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com\n\n2 nodes configured\n2 resources configured\n\nOnline: [ z1.example.com z2.example.com ]\n\nFull list of resources:\n\nResource Group: apachegroup\n    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com\n    WebSite    (ocf::heartbeat:apache):        Started z1.example.com\n\nPCSD Status:\n  z1.example.com: Online\n  z2.example.com: Online\n...\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tNote that in this instance, the \u003Ccode class=\"literal\">apachegroup\u003C/code> service is running on node z1.example.com.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAccess the website you created, stop the service on the node on which it is running, and note how the service fails over to the second node.\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\tPoint a browser to the website you created using the floating IP address you configured. This should display the text message you defined, displaying the name of the node on which the website is running.\n\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tStop the apache web service. Using \u003Ccode class=\"literal command\">killall -9\u003C/code> simulates an application-level crash.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>killall -9 httpd\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCheck the cluster status. You should see that stopping the web service caused a failed action, but that the cluster software restarted the service on the node on which it had been running and you should still be able to access the web browser.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\nCluster name: my_cluster\nStack: corosync\nCurrent DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum\nLast updated: Fri Oct 12 09:54:33 2018\nLast change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com\n\n2 nodes configured\n2 resources configured\n\nOnline: [ z1.example.com z2.example.com ]\n\nFull list of resources:\n\nResource Group: apachegroup\n    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z1.example.com\n    WebSite    (ocf::heartbeat:apache):        Started z1.example.com\n\nFailed Resource Actions:\n* WebSite_monitor_60000 on z1.example.com 'not running' (7): call=31, status=complete, exitreason='none',\n    last-rc-change='Fri Feb  5 21:01:41 2016', queued=0ms, exec=0ms\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tClear the failure status once the service is up and running again.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource cleanup WebSite\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tPut the node on which the service is running into standby mode. Note that since we have disabled fencing we can not effectively simulate a node-level failure (such as pulling a power cable) because fencing is required for the cluster to recover from such situations.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node standby z1.example.com\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCheck the status of the cluster and note where the service is now running.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\nCluster name: my_cluster\nStack: corosync\nCurrent DC: z1.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum\nLast updated: Fri Oct 12 09:54:33 2018\nLast change: Fri Oct 12 09:54:30 2018 by root via cibadmin on z1.example.com\n\n2 nodes configured\n2 resources configured\n\nNode z1.example.com: standby\nOnline: [ z2.example.com ]\n\nFull list of resources:\n\nResource Group: apachegroup\n    ClusterIP  (ocf::heartbeat:IPaddr2):       Started z2.example.com\n    WebSite    (ocf::heartbeat:apache):        Started z2.example.com\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\tAccess the website. There should be no loss of service, although the display message should indicate the node on which the service is now running.\n\t\t\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tTo restore cluster services to the first node, take the node out of standby mode. This will not necessarily move the service back to that node.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node unstandby z1.example.com\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tFor final cleanup, stop the cluster services on both nodes.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster stop --all\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_pcs-operation-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 3. The pcs command-line interface\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tThe \u003Ccode class=\"literal command\">pcs\u003C/code> command-line interface controls and configures cluster services such as \u003Ccode class=\"literal\">corosync\u003C/code>, \u003Ccode class=\"literal\">pacemaker\u003C/code>,\u003Ccode class=\"literal\">booth\u003C/code>, and \u003Ccode class=\"literal\">sbd\u003C/code> by providing an easier interface to their configuration files.\n\t\t\u003C/p>\u003Cp>\n\t\t\tNote that you should not edit the \u003Ccode class=\"literal\">cib.xml\u003C/code> configuration file directly. In most cases, Pacemaker will reject a directly modified \u003Ccode class=\"literal\">cib.xml\u003C/code> file.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_pcs-help-pcs-operation\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">3.1. pcs help display\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou use the \u003Ccode class=\"literal\">-h\u003C/code> option of \u003Ccode class=\"literal command\">pcs\u003C/code> to display the parameters of a \u003Ccode class=\"literal command\">pcs\u003C/code> command and a description of those parameters.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command displays the parameters of the \u003Ccode class=\"literal command\">pcs resource\u003C/code> command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource -h\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_raw-config-pcs-operation\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">3.2. Viewing the raw cluster configuration\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tAlthough you should not edit the cluster configuration file directly, you can view the raw cluster configuration with the \u003Ccode class=\"literal command\">pcs cluster cib\u003C/code> command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can save the raw cluster configuration to a specified file with the \u003Ccode class=\"literal command\">pcs cluster cib \u003Cspan class=\"emphasis\">\u003Cem>filename\u003C/em>\u003C/span>\u003C/code> command. If you have previously configured a cluster and there is already an active CIB, you use the following command to save the raw xml file.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs cluster cib \u003Cspan class=\"emphasis\">\u003Cem>filename\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tFor example, the following command saves the raw xml from the CIB into a file named \u003Ccode class=\"literal\">testfile\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster cib testfile\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_configure-testfile-pcs-operation\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">3.3. Saving a configuration change to a working file\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen configuring a cluster, you can save configuration changes to a specified file without affecting the active CIB. This allows you to specify configuration updates without immediately updating the currently running cluster configuration with each individual update.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFor information about saving the CIB to a file, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_pcs-operation-configuring-and-managing-high-availability-clusters#proc_raw-config-pcs-operation\">Viewing the raw cluster configuration\u003C/a>. Once you have created that file, you can save configuration changes to that file rather than to the active CIB by using the \u003Ccode class=\"literal\">-f\u003C/code> option of the \u003Ccode class=\"literal\">pcs\u003C/code> command. When you have completed the changes and are ready to update the active CIB file, you can push those file updates with the \u003Ccode class=\"literal command\">pcs cluster cib-push\u003C/code> command.\n\t\t\t\u003C/p>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\tThe following is the recommended procedure for pushing changes to the CIB file. This procedure creates a copy of the original saved CIB file and makes changes to that copy. When pushing those changes to the active CIB, this procedure specifies the \u003Ccode class=\"literal\">diff-against\u003C/code> option of the \u003Ccode class=\"literal command\">pcs cluster cib-push\u003C/code> command so that only the changes between the original file and the updated file are pushed to the CIB. This allows users to make changes in parallel that do not overwrite each other, and it reduces the load on Pacemaker which does not need to parse the entire configuration file.\n\t\t\t\t\u003C/p>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSave the active CIB to a file. This example saves the CIB to a file named \u003Ccode class=\"literal\">original.xml\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster cib original.xml\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCopy the saved file to the working file you will be using for the configuration updates.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>cp original.xml updated.xml\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tUpdate your configuration as needed. The following command creates a resource in the file \u003Ccode class=\"literal\">updated.xml\u003C/code> but does not add that resource to the currently running cluster configuration.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs -f updated.xml resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.120 op monitor interval=30s\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tPush the updated file to the active CIB, specifying that you are pushing only the changes you have made to the original file.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster cib-push updated.xml diff-against=original.xml\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\tAlternately, you can push the entire current content of a CIB file with the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs cluster cib-push \u003Cspan class=\"emphasis\">\u003Cem>filename\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tWhen pushing the entire CIB file, Pacemaker checks the version and does not allow you to push a CIB file which is older than the one already in a cluster. If you need to update the entire CIB file with a version that is older than the one currently in the cluster, you can use the \u003Ccode class=\"literal\">--config\u003C/code> option of the \u003Ccode class=\"literal command\">pcs cluster cib-push\u003C/code> command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs cluster cib-push --config \u003Cspan class=\"emphasis\">\u003Cem>filename\u003C/em>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_cluster-status-pcs-operation\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">3.4. Displaying cluster status\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThere are a variety of commands you can use to display the status of a cluster and its components.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can display the status of the cluster and the cluster resources with the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tYou can display the status of a particular cluster component with the \u003Cspan class=\"emphasis\">\u003Cem>commands\u003C/em>\u003C/span> parameter of the \u003Ccode class=\"literal command\">pcs status\u003C/code> command, specifying \u003Ccode class=\"literal\">resources\u003C/code>, \u003Ccode class=\"literal\">cluster\u003C/code>, \u003Ccode class=\"literal\">nodes\u003C/code>, or \u003Ccode class=\"literal\">pcsd\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs status \u003Cspan class=\"emphasis\">\u003Cem>commands\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tFor example, the following command displays the status of the cluster resources.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status resources\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe following command displays the status of the cluster, but not the cluster resources.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster status\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tIf you run the \u003Ccode class=\"literal\">pcs status\u003C/code> command before Pacemaker has completed any actions required by changes to the CIB, the cluster state at that time might not match the desired status. As of RHEL Hat Enterprixe Linux 9.5, you can ensure that Pacemaker does not need to take any further actions by running the \u003Ccode class=\"literal\">pcs status wait\u003C/code> command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal\">pcs status wait\u003C/code> command waits until the cluster has completed all current actions before returning a value. If any actions unrelated to your recent changes are in progress, the command waits until those are completed. The \u003Ccode class=\"literal\">pcs status wait\u003C/code> command returns a value of 0 as soon as Pacemaker completes pending actions.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can specify a period of time to wait. If the current actions have not completed after that time period, the command prints an error and returns a value of 1.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command waits until Pacemaker has applied configuration changes.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status wait\u003C/strong>\u003C/span>\nWaiting for the cluster to apply configuration changes...\u003C/pre>\u003Cp>\n\t\t\t\tThe following command waits up to one minute until Pacemaker has applied configuration changes.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status wait 1min\u003C/strong>\u003C/span>\nWaiting for the cluster to apply configuration changes (timeout: 60 seconds)...\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_cluster-config-display-pcs-operation\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">3.5. Displaying the full cluster configuration\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tUse the following command to display the full current cluster configuration.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs config\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_resource-status-pcs-operation\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">3.6. Displaying resource status\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tIn a complex cluster setup, you might need to determine the status of an individual resource in a cluster before performing a cluster or resource action. As of Red Hat Enterprise Linux 9.5, you can query various attributes of a single resource in a cluster with the \u003Ccode class=\"literal\">pcs status query resource\u003C/code> commands. You can use these commands for pcs-based scripting because there is no need to parse plain text outputs.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal\">pcs status query resource\u003C/code> commands query the following attributes:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tthe existence of the resource\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tthe type of the resource\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tthe state of the resource\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tvarious information about the members of a collective resource\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\ton which nodes the resource is running\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tThe following command queries whether a resource has started.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status query resource\u003C/strong>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>RESOURCE_ID\u003C/em>\u003C/span> \u003Cspan class=\"strong strong\">\u003Cstrong>is-state started\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tFor example, the following command queries whether the resource \u003Ccode class=\"literal\">resource1\u003C/code> has started.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status query resource resource1 is-state started\u003C/strong>\u003C/span>\nTrue\u003C/pre>\u003Cp>\n\t\t\t\tFor a full list of the \u003Ccode class=\"literal\">pcs status query resource\u003C/code> commands, see the \u003Ccode class=\"literal\">pcs\u003C/code>(8) man page on your system.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_pcs-corosync-manage-pcs-operation\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">3.7. Modifying the corosync.conf file with the pcs command\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can use the \u003Ccode class=\"literal\">pcs\u003C/code> command to modify the parameters in the \u003Ccode class=\"literal\">corosync.conf\u003C/code> file.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command modifies the parameters in the \u003Ccode class=\"literal\">corosync.conf\u003C/code> file.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs cluster config update [transport pass:quotes[\u003Cspan class=\"emphasis\">\u003Cem>transport options\u003C/em>\u003C/span>]] [compression pass:quotes[\u003Cspan class=\"emphasis\">\u003Cem>compression options\u003C/em>\u003C/span>]] [crypto pass:quotes[\u003Cspan class=\"emphasis\">\u003Cem>crypto options\u003C/em>\u003C/span>]] [totem pass:quotes[\u003Cspan class=\"emphasis\">\u003Cem>totem options\u003C/em>\u003C/span>]] [--corosync_conf pass:quotes[\u003Cspan class=\"emphasis\">\u003Cem>path\u003C/em>\u003C/span>]]\u003C/pre>\u003Cp>\n\t\t\t\tThe following example command udates the \u003Ccode class=\"literal\">knet_pmtud_interval\u003C/code> transport value and the \u003Ccode class=\"literal\">token\u003C/code> and \u003Ccode class=\"literal\">join\u003C/code> totem values.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster config update transport knet_pmtud_interval=35 totem token=10000 join=100\u003C/strong>\u003C/span>\u003C/pre>\u003Cdiv class=\"itemizedlist _additional-resources\">\u003Cp class=\"title\">\u003Cstrong>Additional resources\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist _additional-resources\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tFor information about adding and removing nodes from an existing cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_clusternode-management-configuring-and-managing-high-availability-clusters\">Managing cluster nodes\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tFor information about adding and modifying links in an existing cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_clusternode-management-configuring-and-managing-high-availability-clusters#proc_changing-links-in-multiple-ip-cluster-clusternode-management\">Adding and modifying links in an existing cluster\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tFor information about modifyng quorum options and managing the quorum device settings in a cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters\">Configuring cluster quorum\u003C/a> and \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-quorum-devices-configuring-and-managing-high-availability-clusters\">Configuring quorum devices\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_pcs-corosync-display-pcs-operation\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">3.8. Displaying the corosync.conf file with the pcs command\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe following command displays the contents of the \u003Ccode class=\"literal\">corosync.conf\u003C/code> cluster configuration file.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster corosync\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tYou can print the contents of the \u003Ccode class=\"literal\">corosync.conf\u003C/code> file in a human-readable format with the \u003Ccode class=\"literal\">pcs cluster config\u003C/code> command, as in the following example.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe output for this command includes the UUID for the cluster if the cluster was created in RHEL 9.1 or later, or if the UUID was added manually as described in \u003Ca class=\"link\" href=\"#identifying-cluster-by-uuid-cluster-maintenance\" title=\"31.9. Identifying clusters by UUID\">Identifying clusters by UUID\u003C/a>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@r8-node-01 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster config\u003C/strong>\u003C/span>\nCluster Name: HACluster\nCluster UUID: ad4ae07dcafe4066b01f1cc9391f54f5\nTransport: knet\nNodes:\n  r8-node-01:\n    Link 0 address: r8-node-01\n    Link 1 address: 192.168.122.121\n    nodeid: 1\n  r8-node-02:\n    Link 0 address: r8-node-02\n    Link 1 address: 192.168.122.122\n    nodeid: 2\nLinks:\n  Link 1:\n    linknumber: 1\n    ping_interval: 1000\n    ping_timeout: 2000\n    pong_count: 5\nCompression Options:\n  level: 9\n  model: zlib\n  threshold: 150\nCrypto Options:\n  cipher: aes256\n  hash: sha256\nTotem Options:\n  downcheck: 2000\n  join: 50\n  token: 10000\nQuorum Device: net\n  Options:\n    sync_timeout: 2000\n    timeout: 3000\n  Model Options:\n    algorithm: lms\n    host: r8-node-03\n  Heuristics:\n    exec_ping: ping -c 1 127.0.0.1\u003C/pre>\u003Cp>\n\t\t\t\tYou can run the \u003Ccode class=\"literal\">pcs cluster config show\u003C/code> command with the \u003Ccode class=\"literal\">--output-format=cmd\u003C/code> option to display the \u003Ccode class=\"literal\">pcs\u003C/code> configuration commands that can be used to recreate the existing \u003Ccode class=\"literal\">corosync.conf\u003C/code> file, as in the following example.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@r8-node-01 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster config show --output-format=cmd\u003C/strong>\u003C/span>\npcs cluster setup HACluster \\\n  r8-node-01 addr=r8-node-01 addr=192.168.122.121 \\\n  r8-node-02 addr=r8-node-02 addr=192.168.122.122 \\\n  transport \\\n  knet \\\n    link \\\n      linknumber=1 \\\n      ping_interval=1000 \\\n      ping_timeout=2000 \\\n      pong_count=5 \\\n    compression \\\n      level=9 \\\n      model=zlib \\\n      threshold=150 \\\n    crypto \\\n      cipher=aes256 \\\n      hash=sha256 \\\n  totem \\\n    downcheck=2000 \\\n    join=50 \\\n    token=10000\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 4. Creating a Red Hat High-Availability cluster with Pacemaker\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tCreate a Red Hat High Availability two-node cluster using the \u003Ccode class=\"literal\">pcs\u003C/code> command-line interface with the following procedure.\n\t\t\u003C/p>\u003Cp>\n\t\t\tConfiguring the cluster in this example requires that your system include the following components:\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t2 nodes, which will be used to create the cluster. In this example, the nodes used are \u003Ccode class=\"literal\">z1.example.com\u003C/code> and \u003Ccode class=\"literal\">z2.example.com\u003C/code>.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tNetwork switches for the private network. We recommend but do not require a private network for communication among the cluster nodes and other cluster hardware such as network power switches and Fibre Channel switches.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tA fencing device for each node of the cluster. This example uses two ports of the APC power switch with a host name of \u003Ccode class=\"literal\">zapc.example.com\u003C/code>.\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\tYou must ensure that your configuration conforms to Red Hat’s support policies. For full information about Red Hat’s support policies, requirements, and limitations for RHEL High Availability clusters, see \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/2912891/\">Support Policies for RHEL High Availability Clusters\u003C/a>.\n\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Csection class=\"section\" id=\"proc_installing-cluster-software-creating-high-availability-cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">4.1. Installing cluster software\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tInstall the cluster software and configure your system for cluster creation with the following procedure.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn each node in the cluster, enable the repository for high availability that corresponds to your system architecture. For example, to enable the high availability repository for an x86_64 system, you can enter the following \u003Ccode class=\"literal\">subscription-manager\u003C/code> command:\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>subscription-manager repos --enable=rhel-9-for-x86_64-highavailability-rpms\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn each node in the cluster, install the Red Hat High Availability Add-On software packages along with all available fence agents from the High Availability channel.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install pcs pacemaker fence-agents-all\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAlternatively, you can install the Red Hat High Availability Add-On software packages along with only the fence agent that you require with the following command.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install pcs pacemaker fence-agents-\u003Cspan class=\"emphasis\">\u003Cem>model\u003C/em>\u003C/span>\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following command displays a list of the available fence agents.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>rpm -q -a | grep fence\u003C/strong>\u003C/span>\nfence-agents-rhevm-4.0.2-3.el7.x86_64\nfence-agents-ilo-mp-4.0.2-3.el7.x86_64\nfence-agents-ipmilan-4.0.2-3.el7.x86_64\n...\u003C/pre>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tAfter you install the Red Hat High Availability Add-On packages, you should ensure that your software update preferences are set so that nothing is installed automatically. Installation on a running cluster can cause unexpected behaviors. For more information, see \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/2059253/\">Recommended Practices for Applying Software Updates to a RHEL High Availability or Resilient Storage Cluster\u003C/a>.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf you are running the \u003Ccode class=\"literal command\">firewalld\u003C/code> daemon, execute the following commands to enable the ports that are required by the Red Hat High Availability Add-On.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tYou can determine whether the \u003Ccode class=\"literal command\">firewalld\u003C/code> daemon is installed on your system with the \u003Ccode class=\"literal command\">rpm -q firewalld\u003C/code> command. If it is installed, you can determine whether it is running with the \u003Ccode class=\"literal command\">firewall-cmd --state\u003C/code> command.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --permanent --add-service=high-availability\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --add-service=high-availability\u003C/strong>\u003C/span>\u003C/pre>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tThe ideal firewall configuration for cluster components depends on the local environment, where you may need to take into account such considerations as whether the nodes have multiple network interfaces or whether off-host firewalling is present. The example here, which opens the ports that are generally required by a Pacemaker cluster, should be modified to suit local conditions. \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters#proc_enabling-ports-for-high-availability-creating-high-availability-cluster\">Enabling ports for the High Availability Add-On\u003C/a> shows the ports to enable for the Red Hat High Availability Add-On and provides an explanation for what each port is used for.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIn order to use \u003Ccode class=\"literal\">pcs\u003C/code> to configure the cluster and communicate among the nodes, you must set a password on each node for the user ID \u003Ccode class=\"literal\">hacluster\u003C/code>, which is the \u003Ccode class=\"literal\">pcs\u003C/code> administration account. It is recommended that the password for user \u003Ccode class=\"literal\">hacluster\u003C/code> be the same on each node.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>passwd hacluster\u003C/strong>\u003C/span>\nChanging password for user hacluster.\nNew password:\nRetype new password:\npasswd: all authentication tokens updated successfully.\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tBefore the cluster can be configured, the \u003Ccode class=\"literal command\">pcsd\u003C/code> daemon must be started and enabled to start up on boot on each node. This daemon works with the \u003Ccode class=\"literal command\">pcs\u003C/code> command to manage configuration across the nodes in the cluster.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn each node in the cluster, execute the following commands to start the \u003Ccode class=\"literal\">pcsd\u003C/code> service and to enable \u003Ccode class=\"literal\">pcsd\u003C/code> at system start.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl start pcsd.service\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl enable pcsd.service\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_installing-pcp-zeroconf-creating-high-availability-cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">4.2. Installing the pcp-zeroconf package (recommended)\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen you set up your cluster, it is recommended that you install the \u003Ccode class=\"literal\">pcp-zeroconf\u003C/code> package for the Performance Co-Pilot (PCP) tool. PCP is Red Hat’s recommended resource-monitoring tool for RHEL systems. Installing the \u003Ccode class=\"literal\">pcp-zeroconf\u003C/code> package allows you to have PCP running and collecting performance-monitoring data for the benefit of investigations into fencing, resource failures, and other events that disrupt the cluster.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tCluster deployments where PCP is enabled will need sufficient space available for PCP’s captured data on the file system that contains \u003Ccode class=\"literal\">/var/log/pcp/\u003C/code>. Typical space usage by PCP varies across deployments, but 10Gb is usually sufficient when using the \u003Ccode class=\"literal\">pcp-zeroconf\u003C/code> default settings, and some environments may require less. Monitoring usage in this directory over a 14-day period of typical activity can provide a more accurate usage expectation.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\tTo install the \u003Ccode class=\"literal\">pcp-zeroconf\u003C/code> package, run the following command.\n\t\t\t\t\u003C/p>\u003C/div>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install pcp-zeroconf\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThis package enables \u003Ccode class=\"literal\">pmcd\u003C/code> and sets up data capture at a 10-second interval.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFor information about reviewing PCP data, see the Red Hat Knowledgebase solution \u003Ca class=\"link\" href=\"https://access.redhat.com/solutions/4545111\">Why did a RHEL High Availability cluster node reboot - and how can I prevent it from happening again?\u003C/a>.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_creating-high-availability-cluster-creating-high-availability-cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">4.3. Creating a high availability cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tCreate a Red Hat High Availability Add-On cluster with the following procedure. This example procedure creates a cluster that consists of the nodes \u003Ccode class=\"literal\">z1.example.com\u003C/code> and \u003Ccode class=\"literal\">z2.example.com\u003C/code>.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAuthenticate the \u003Ccode class=\"literal command\">pcs\u003C/code> user \u003Ccode class=\"literal\">hacluster\u003C/code> for each node in the cluster on the node from which you will be running \u003Ccode class=\"literal command\">pcs\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following command authenticates user \u003Ccode class=\"literal\">hacluster\u003C/code> on \u003Ccode class=\"literal\">z1.example.com\u003C/code> for both of the nodes in a two-node cluster that will consist of \u003Ccode class=\"literal\">z1.example.com\u003C/code> and \u003Ccode class=\"literal\">z2.example.com\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs host auth z1.example.com z2.example.com\u003C/strong>\u003C/span>\nUsername: \u003Cspan class=\"strong strong\">\u003Cstrong>hacluster\u003C/strong>\u003C/span>\nPassword:\nz1.example.com: Authorized\nz2.example.com: Authorized\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tExecute the following command from \u003Ccode class=\"literal\">z1.example.com\u003C/code> to create the two-node cluster \u003Ccode class=\"literal\">my_cluster\u003C/code> that consists of nodes \u003Ccode class=\"literal\">z1.example.com\u003C/code> and \u003Ccode class=\"literal\">z2.example.com\u003C/code>. This will propagate the cluster configuration files to both nodes in the cluster. This command includes the \u003Ccode class=\"literal\">--start\u003C/code> option, which will start the cluster services on both nodes in the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster setup my_cluster --start z1.example.com z2.example.com\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tEnable the cluster services to run on each node in the cluster when the node is booted.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tFor your particular environment, you may choose to leave the cluster services disabled by skipping this step. This allows you to ensure that if a node goes down, any issues with your cluster or your resources are resolved before the node rejoins the cluster. If you leave the cluster services disabled, you will need to manually start the services when you reboot a node by executing the \u003Ccode class=\"literal command\">pcs cluster start\u003C/code> command on that node.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster enable --all\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\tYou can display the current status of the cluster with the \u003Ccode class=\"literal command\">pcs cluster status\u003C/code> command. Because there may be a slight delay before the cluster is up and running when you start the cluster services with the \u003Ccode class=\"literal option\">--start\u003C/code> option of the \u003Ccode class=\"literal command\">pcs cluster setup\u003C/code> command, you should ensure that the cluster is up and running before performing any subsequent actions on the cluster and its configuration.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster status\u003C/strong>\u003C/span>\nCluster Status:\n Stack: corosync\n Current DC: z2.example.com (version 2.0.0-10.el8-b67d8d0de9) - partition with quorum\n Last updated: Thu Oct 11 16:11:18 2018\n Last change: Thu Oct 11 16:11:00 2018 by hacluster via crmd on z2.example.com\n 2 Nodes configured\n 0 Resources configured\n\n...\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_configure-multiple-ip-cluster-creating-high-availability-cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">4.4. Creating a high availability cluster with multiple links\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can use the \u003Ccode class=\"literal command\">pcs cluster setup\u003C/code> command to create a Red Hat High Availability cluster with multiple links by specifying all of the links for each node.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe format for the basic command to create a two-node cluster with two links is as follows.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs cluster setup pass:quotes[\u003Cspan class=\"emphasis\">\u003Cem>cluster_name\u003C/em>\u003C/span>] pass:quotes[\u003Cspan class=\"emphasis\">\u003Cem>node1_name\u003C/em>\u003C/span>] addr=pass:quotes[\u003Cspan class=\"emphasis\">\u003Cem>node1_link0_address\u003C/em>\u003C/span>] addr=pass:quotes[\u003Cspan class=\"emphasis\">\u003Cem>node1_link1_address\u003C/em>\u003C/span>] pass:quotes[\u003Cspan class=\"emphasis\">\u003Cem>node2_name\u003C/em>\u003C/span>] addr=pass:quotes[\u003Cspan class=\"emphasis\">\u003Cem>node2_link0_address\u003C/em>\u003C/span>] addr=pass:quotes[\u003Cspan class=\"emphasis\">\u003Cem>node2_link1_address\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\tFor the full syntax of this command, see the \u003Ccode class=\"literal\">pcs\u003C/code>(8) man page.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tWhen creating a cluster with multiple links, you should take the following into account.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe order of the \u003Ccode class=\"literal\">addr=\u003Cspan class=\"emphasis\">\u003Cem>address\u003C/em>\u003C/span>\u003C/code> parameters is important. The first address specified after a node name is for \u003Ccode class=\"literal\">link0\u003C/code>, the second one for \u003Ccode class=\"literal\">link1\u003C/code>, and so forth.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tBy default, if \u003Ccode class=\"literal\">link_priority\u003C/code> is not specified for a link, the link’s priority is equal to the link number. The link priorities are then 0, 1, 2, 3, and so forth, according to the order specified, with 0 being the highest link priority.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe default link mode is \u003Ccode class=\"literal\">passive\u003C/code>, meaning the active link with the lowest-numbered link priority is used.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tWith the default values of \u003Ccode class=\"literal\">link_mode\u003C/code> and \u003Ccode class=\"literal\">link_priority\u003C/code>, the first link specified will be used as the highest priority link, and if that link fails the next link specified will be used.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tIt is possible to specify up to eight links using the \u003Ccode class=\"literal\">knet\u003C/code> transport protocol, which is the default transport protocol.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tAll nodes must have the same number of \u003Ccode class=\"literal\">addr=\u003C/code> parameters.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tIt is possible to add, remove, and change links in an existing cluster using the \u003Ccode class=\"literal\">pcs cluster link add\u003C/code>, the \u003Ccode class=\"literal\">pcs cluster link remove\u003C/code>, the \u003Ccode class=\"literal\">pcs cluster link delete\u003C/code>, and the \u003Ccode class=\"literal\">pcs cluster link update\u003C/code> commands.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tAs with single-link clusters, do not mix IPv4 and IPv6 addresses in one link, although you can have one link running IPv4 and the other running IPv6.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tAs with single-link clusters, you can specify addresses as IP addresses or as names as long as the names resolve to IPv4 or IPv6 addresses for which IPv4 and IPv6 addresses are not mixed in one link.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tThe following example creates a two-node cluster named \u003Ccode class=\"literal\">my_twolink_cluster\u003C/code> with two nodes, \u003Ccode class=\"literal\">rh80-node1\u003C/code> and \u003Ccode class=\"literal\">rh80-node2\u003C/code>. \u003Ccode class=\"literal\">rh80-node1\u003C/code> has two interfaces, IP address 192.168.122.201 as \u003Ccode class=\"literal\">link0\u003C/code> and 192.168.123.201 as \u003Ccode class=\"literal\">link1\u003C/code>. \u003Ccode class=\"literal\">rh80-node2\u003C/code> has two interfaces, IP address 192.168.122.202 as \u003Ccode class=\"literal\">link0\u003C/code> and 192.168.123.202 as \u003Ccode class=\"literal\">link1\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster setup my_twolink_cluster rh80-node1 addr=192.168.122.201 addr=192.168.123.201 rh80-node2 addr=192.168.122.202 addr=192.168.123.202\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tTo set a link priority to a different value than the default value, which is the link number, you can set the link priority with the \u003Ccode class=\"literal\">link_priority\u003C/code> option of the \u003Ccode class=\"literal\">pcs cluster setup\u003C/code> command. Each of the following two example commands creates a two-node cluster with two interfaces where the first link, link 0, has a link priority of 1 and the second link, link 1, has a link priority of 0. Link 1 will be used first and link 0 will serve as the failover link. Since link mode is not specified, it defaults to passive.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThese two commands are equivalent. If you do not specify a link number following the \u003Ccode class=\"literal\">link\u003C/code> keyword, the \u003Ccode class=\"literal\">pcs\u003C/code> interface automatically adds a link number, starting with the lowest unused link number.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster setup my_twolink_cluster rh80-node1 addr=192.168.122.201 addr=192.168.123.201 rh80-node2 addr=192.168.122.202 addr=192.168.123.202 transport knet link link_priority=1 link link_priority=0\u003C/strong>\u003C/span>\n\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster setup my_twolink_cluster rh80-node1 addr=192.168.122.201 addr=192.168.123.201 rh80-node2 addr=192.168.122.202 addr=192.168.123.202 transport knet link linknumber=1 link_priority=0 link link_priority=1\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tYou can set the link mode to a different value than the default value of \u003Ccode class=\"literal\">passive\u003C/code> with the \u003Ccode class=\"literal\">link_mode\u003C/code> option of the \u003Ccode class=\"literal\">pcs cluster setup\u003C/code> command, as in the following example.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster setup my_twolink_cluster rh80-node1 addr=192.168.122.201 addr=192.168.123.201 rh80-node2 addr=192.168.122.202 addr=192.168.123.202 transport knet link_mode=active\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe following example sets both the link mode and the link priority.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster setup my_twolink_cluster rh80-node1 addr=192.168.122.201 addr=192.168.123.201 rh80-node2 addr=192.168.122.202 addr=192.168.123.202 transport knet link_mode=active link link_priority=1 link link_priority=0\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tFor information about adding nodes to an existing cluster with multiple links, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_clusternode-management-configuring-and-managing-high-availability-clusters#proc_add-nodes-to-multiple-ip-cluster-clusternode-management\">Adding a node to a cluster with multiple links\u003C/a>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFor information about changing the links in an existing cluster with multiple links, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_clusternode-management-configuring-and-managing-high-availability-clusters#proc_changing-links-in-multiple-ip-cluster-clusternode-management\">Adding and modifying links in an existing cluster\u003C/a>.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-fencing-creating-high-availability-cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">4.5. Configuring fencing\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou must configure a fencing device for each node in the cluster. For information about the fence configuration commands and options, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters\">Configuring fencing in a Red Hat High Availability cluster\u003C/a>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFor general information about fencing and its importance in a Red Hat High Availability cluster, see the Red Hat Knowledgebase solution \u003Ca class=\"link\" href=\"https://access.redhat.com/solutions/15575\">Fencing in a Red Hat High Availability Cluster\u003C/a>.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tWhen configuring a fencing device, attention should be given to whether that device shares power with any nodes or devices in the cluster. If a node and its fence device do share power, then the cluster may be at risk of being unable to fence that node if the power to it and its fence device should be lost. Such a cluster should either have redundant power supplies for fence devices and nodes, or redundant fence devices that do not share power. Alternative methods of fencing such as SBD or storage fencing may also bring redundancy in the event of isolated power losses.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\tThis example uses the APC power switch with a host name of \u003Ccode class=\"literal\">zapc.example.com\u003C/code> to fence the nodes, and it uses the \u003Ccode class=\"literal\">fence_apc_snmp\u003C/code> fencing agent. Because both nodes will be fenced by the same fencing agent, you can configure both fencing devices as a single resource, using the \u003Ccode class=\"literal\">pcmk_host_map\u003C/code> option.\n\t\t\t\t\u003C/p>\u003C/div>\u003Cp>\n\t\t\t\tYou create a fencing device by configuring the device as a \u003Ccode class=\"literal\">stonith\u003C/code> resource with the \u003Ccode class=\"literal command\">pcs stonith create\u003C/code> command. The following command configures a \u003Ccode class=\"literal\">stonith\u003C/code> resource named \u003Ccode class=\"literal\">myapc\u003C/code> that uses the \u003Ccode class=\"literal\">fence_apc_snmp\u003C/code> fencing agent for nodes \u003Ccode class=\"literal\">z1.example.com\u003C/code> and \u003Ccode class=\"literal\">z2.example.com\u003C/code>. The \u003Ccode class=\"literal\">pcmk_host_map\u003C/code> option maps \u003Ccode class=\"literal\">z1.example.com\u003C/code> to port 1, and \u003Ccode class=\"literal\">z2.example.com\u003C/code> to port 2. The login value and password for the APC device are both \u003Ccode class=\"literal\">apc\u003C/code>. By default, this device will use a monitor interval of sixty seconds for each node.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tNote that you can use an IP address when specifying the host name for the nodes.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith create myapc fence_apc_snmp ipaddr=\"zapc.example.com\" pcmk_host_map=\"z1.example.com:1;z2.example.com:2\" login=\"apc\" passwd=\"apc\"\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe following command displays the parameters of an existing fencing device.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@rh7-1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith config myapc\u003C/strong>\u003C/span>\n Resource: myapc (class=stonith type=fence_apc_snmp)\n  Attributes: ipaddr=zapc.example.com pcmk_host_map=z1.example.com:1;z2.example.com:2 login=apc passwd=apc\n  Operations: monitor interval=60s (myapc-monitor-interval-60s)\u003C/pre>\u003Cp>\n\t\t\t\tAfter configuring your fence device, you should test the device. For information about testing a fence device, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters#proc_testing-fence-devices-configuring-fencing\">Testing a fence device\u003C/a>.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tDo not test your fence device by disabling the network interface, as this will not properly test fencing.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tOnce fencing is configured and a cluster has been started, a network restart will trigger fencing for the node which restarts the network even when the timeout is not exceeded. For this reason, do not restart the network service while the cluster service is running because it will trigger unintentional fencing on the node.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/section>\u003Csection class=\"section\" id=\"proc_cluster-backup-creating-high-availability-cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">4.6. Backing up and restoring a cluster configuration\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe following commands back up a cluster configuration in a tar archive and restore the cluster configuration files on all nodes from the backup.\n\t\t\t\u003C/p>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\tUse the following command to back up the cluster configuration in a tar archive. If you do not specify a file name, the standard output will be used.\n\t\t\t\t\u003C/p>\u003C/div>\u003Cpre class=\"literallayout\">pcs config backup \u003Cspan class=\"emphasis\">\u003Cem>filename\u003C/em>\u003C/span>\u003C/pre>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tThe \u003Ccode class=\"literal command\">pcs config backup\u003C/code> command backs up only the cluster configuration itself as configured in the CIB; the configuration of resource daemons is out of the scope of this command. For example if you have configured an Apache resource in the cluster, the resource settings (which are in the CIB) will be backed up, while the Apache daemon settings (as set in`/etc/httpd`) and the files it serves will not be backed up. Similarly, if there is a database resource configured in the cluster, the database itself will not be backed up, while the database resource configuration (CIB) will be.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tUse the following command to restore the cluster configuration files on all cluster nodes from the backup. Specifying the \u003Ccode class=\"literal\">--local\u003C/code> option restores the cluster configuration files only on the node from which you run this command. If you do not specify a file name, the standard input will be used.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs config restore [--local] [\u003Cspan class=\"emphasis\">\u003Cem>filename\u003C/em>\u003C/span>]\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_enabling-ports-for-high-availability-creating-high-availability-cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">4.7. Enabling ports for the High Availability Add-On\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe ideal firewall configuration for cluster components depends on the local environment, where you may need to take into account such considerations as whether the nodes have multiple network interfaces or whether off-host firewalling is present.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf you are running the \u003Ccode class=\"literal command\">firewalld\u003C/code> daemon, execute the following commands to enable the ports that are required by the Red Hat High Availability Add-On.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --permanent --add-service=high-availability\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --add-service=high-availability\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tYou may need to modify which ports are open to suit local conditions.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tYou can determine whether the \u003Ccode class=\"literal command\">firewalld\u003C/code> daemon is installed on your system with the \u003Ccode class=\"literal command\">rpm -q firewalld\u003C/code> command. If the \u003Ccode class=\"literal command\">firewalld\u003C/code> daemon is installed, you can determine whether it is running with the \u003Ccode class=\"literal command\">firewall-cmd --state\u003C/code> command.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tThe following table shows the ports to enable for the Red Hat High Availability Add-On and provides an explanation for what the port is used for.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-portenable-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 4.1. Ports to Enable for High Availability Add-On\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 50%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 50%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686152097600\" scope=\"col\">Port\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686152096512\" scope=\"col\">When Required\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686152097600\"> \u003Cp>\n\t\t\t\t\t\t\t\tTCP 2224\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686152096512\"> \u003Cp>\n\t\t\t\t\t\t\t\tDefault \u003Ccode class=\"literal\">pcsd\u003C/code> port required on all nodes (needed by the pcsd Web UI and required for node-to-node communication). You can configure the \u003Ccode class=\"literal\">pcsd\u003C/code> port by means of the \u003Ccode class=\"literal\">PCSD_PORT\u003C/code> parameter in the \u003Ccode class=\"literal\">/etc/sysconfig/pcsd\u003C/code> file.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tIt is crucial to open port 2224 in such a way that \u003Ccode class=\"literal command\">pcs\u003C/code> from any node can talk to all nodes in the cluster, including itself. When using the Booth cluster ticket manager or a quorum device you must open port 2224 on all related hosts, such as Booth arbitrators or the quorum device host.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686152097600\"> \u003Cp>\n\t\t\t\t\t\t\t\tTCP 3121\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686152096512\"> \u003Cp>\n\t\t\t\t\t\t\t\tRequired on all nodes if the cluster has any Pacemaker Remote nodes\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tPacemaker’s \u003Ccode class=\"literal\">pacemaker-based\u003C/code> daemon on the full cluster nodes will contact the \u003Ccode class=\"literal\">pacemaker_remoted\u003C/code> daemon on Pacemaker Remote nodes at port 3121. If a separate interface is used for cluster communication, the port only needs to be open on that interface. At a minimum, the port should open on Pacemaker Remote nodes to full cluster nodes. Because users may convert a host between a full node and a remote node, or run a remote node inside a container using the host’s network, it can be useful to open the port to all nodes. It is not necessary to open the port to any hosts other than nodes.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686152097600\"> \u003Cp>\n\t\t\t\t\t\t\t\tTCP 5403\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686152096512\"> \u003Cp>\n\t\t\t\t\t\t\t\tRequired on the quorum device host when using a quorum device with \u003Ccode class=\"literal\">corosync-qnetd\u003C/code>. The default value can be changed with the \u003Ccode class=\"literal option\">-p\u003C/code> option of the \u003Ccode class=\"literal command\">corosync-qnetd\u003C/code> command.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686152097600\"> \u003Cp>\n\t\t\t\t\t\t\t\tUDP 5404-5412\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686152096512\"> \u003Cp>\n\t\t\t\t\t\t\t\tRequired on corosync nodes to facilitate communication between nodes. It is crucial to open ports 5404-5412 in such a way that \u003Ccode class=\"literal\">corosync\u003C/code> from any node can talk to all nodes in the cluster, including itself.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686152097600\"> \u003Cp>\n\t\t\t\t\t\t\t\tTCP 21064\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686152096512\"> \u003Cp>\n\t\t\t\t\t\t\t\tRequired on all nodes if the cluster contains any resources requiring DLM (such as \u003Ccode class=\"literal\">GFS2\u003C/code>).\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686152097600\"> \u003Cp>\n\t\t\t\t\t\t\t\tTCP 9929, UDP 9929\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686152096512\"> \u003Cp>\n\t\t\t\t\t\t\t\tRequired to be open on all cluster nodes and Booth arbitrator nodes to connections from any of those same nodes when the Booth ticket manager is used to establish a multi-site cluster.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 5. Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tConfigure an active/passive Apache HTTP server in a two-node Red Hat Enterprise Linux High Availability Add-On cluster with the following procedure. In this use case, clients access the Apache HTTP server through a floating IP address. The web server runs on one of two nodes in the cluster. If the node on which the web server is running becomes inoperative, the web server starts up again on the second node of the cluster with minimal service interruption.\n\t\t\u003C/p>\u003Cp>\n\t\t\tThe following illustration shows a high-level overview of the cluster in which the cluster is a two-node Red Hat High Availability cluster which is configured with a network power switch and with shared storage. The cluster nodes are connected to a public network, for client access to the Apache HTTP server through a virtual IP. The Apache server runs on either Node 1 or Node 2, each of which has access to the storage on which the Apache data is kept. In this illustration, the web server is running on Node 1 while Node 2 is available to run the server if Node 1 becomes inoperative.\n\t\t\u003C/p>\u003Cdiv class=\"figure\" id=\"idm140686155947552\">\u003Cp class=\"title\">\u003Cstrong>Figure 5.1. Apache in a Red Hat High Availability Two-Node Cluster\u003C/strong>\u003C/p>\u003Cdiv class=\"figure-contents\">\u003Cdiv class=\"mediaobject\">\u003Cimg src=\"https://access.redhat.com/webassets/avalon/d/Red_Hat_Enterprise_Linux-9-Configuring_and_managing_high_availability_clusters-en-US/images/1329e69b8957bc6e39af37981d99dbb3/291627-haserver_cluster4.png\" alt=\"Apache in a Red Hat High Availability Two-Node Cluster\"/>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\tThis use case requires that your system include the following components:\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\tA two-node Red Hat High Availability cluster with power fencing configured for each node. We recommend but do not require a private network. This procedure uses the cluster example provided in \u003Ca class=\"link\" href=\"#assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters\" title=\"Chapter 4. Creating a Red Hat High-Availability cluster with Pacemaker\">Creating a Red Hat High-Availability cluster with Pacemaker\u003C/a>.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tA public virtual IP address, required for Apache.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tShared storage for the nodes in the cluster, using iSCSI, Fibre Channel, or other shared network block device.\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\tThe cluster is configured with an Apache resource group, which contains the cluster components that the web server requires: an LVM resource, a file system resource, an IP address resource, and a web server resource. This resource group can fail over from one node of the cluster to the other, allowing either node to run the web server. Before creating the resource group for this cluster, you will be performing the following procedures:\n\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\tConfigure an XFS file system on the logical volume \u003Ccode class=\"literal\">my_lv\u003C/code>.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tConfigure a web server.\n\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\tAfter performing these steps, you create the resource group and the resources it contains.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">5.1. Configuring an LVM volume with an XFS file system in a Pacemaker cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tCreate an LVM logical volume on storage that is shared between the nodes of the cluster with the following procedure.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tLVM volumes and the corresponding partitions and devices used by cluster nodes must be connected to the cluster nodes only.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tThe following procedure creates an LVM logical volume and then creates an XFS file system on that volume for use in a Pacemaker cluster. In this example, the shared partition \u003Ccode class=\"literal\">/dev/sdb1\u003C/code> is used to store the LVM physical volume from which the LVM logical volume will be created.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes of the cluster, perform the following steps to set the value for the LVM system ID to the value of the \u003Ccode class=\"literal\">uname\u003C/code> identifier for the system. The LVM system ID will be used to ensure that only the cluster is capable of activating the volume group.\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tSet the \u003Ccode class=\"literal\">system_id_source\u003C/code> configuration option in the \u003Ccode class=\"literal\">/etc/lvm/lvm.conf\u003C/code> configuration file to \u003Ccode class=\"literal\">uname\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># Configuration option global/system_id_source.\nsystem_id_source = \"uname\"\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tVerify that the LVM system ID on the node matches the \u003Ccode class=\"literal\">uname\u003C/code> for the node.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>lvm systemid\u003C/strong>\u003C/span>\n  system ID: z1.example.com\n# \u003Cspan class=\"strong strong\">\u003Cstrong>uname -n\u003C/strong>\u003C/span>\n  z1.example.com\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate the LVM volume and create an XFS file system on that volume. Since the \u003Ccode class=\"literal\">/dev/sdb1\u003C/code> partition is storage that is shared, you perform this part of the procedure on one node only.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tIf your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order\">Configuring startup order for resource dependencies not managed by Pacemaker\u003C/a>.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate an LVM physical volume on partition \u003Ccode class=\"literal\">/dev/sdb1\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pvcreate /dev/sdb1\u003C/strong>\u003C/span>\n  Physical volume \"/dev/sdb1\" successfully created\u003C/pre>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\t\tIf your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order\">Configuring startup order for resource dependencies not managed by Pacemaker\u003C/a>.\n\t\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate the volume group \u003Ccode class=\"literal\">my_vg\u003C/code> that consists of the physical volume \u003Ccode class=\"literal\">/dev/sdb1\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tSpecify the \u003Ccode class=\"literal\">--setautoactivation n\u003C/code> flag to ensure that volume groups managed by Pacemaker in a cluster will not be automatically activated on startup. If you are using an existing volume group for the LVM volume you are creating, you can reset this flag with the \u003Ccode class=\"literal\">vgchange --setautoactivation n\u003C/code> command for the volume group.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgcreate --setautoactivation n my_vg /dev/sdb1\u003C/strong>\u003C/span>\n  Volume group \"my_vg\" successfully created\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tVerify that the new volume group has the system ID of the node on which you are running and from which you created the volume group.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgs -o+systemid\u003C/strong>\u003C/span>\n  VG    #PV #LV #SN Attr   VSize  VFree  System ID\n  my_vg   1   0   0 wz--n- &lt;1.82t &lt;1.82t z1.example.com\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate a logical volume using the volume group \u003Ccode class=\"literal\">my_vg\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvcreate -L450 -n my_lv my_vg\u003C/strong>\u003C/span>\n  Rounding up size to full physical extent 452.00 MiB\n  Logical volume \"my_lv\" created\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tYou can use the \u003Ccode class=\"literal command\">lvs\u003C/code> command to display the logical volume.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvs\u003C/strong>\u003C/span>\n  LV      VG      Attr      LSize   Pool Origin Data%  Move Log Copy%  Convert\n  my_lv   my_vg   -wi-a---- 452.00m\n  ...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate an XFS file system on the logical volume \u003Ccode class=\"literal\">my_lv\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkfs.xfs /dev/my_vg/my_lv\u003C/strong>\u003C/span>\nmeta-data=/dev/my_vg/my_lv       isize=512    agcount=4, agsize=28928 blks\n         =                       sectsz=512   attr=2, projid32bit=1\n...\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf the use of a devices file is enabled with the \u003Ccode class=\"literal\">use_devicesfile = 1\u003C/code> parameter in the \u003Ccode class=\"literal\">lvm.conf\u003C/code> file, add the shared device to the devices file on the second node in the cluster. This feature is enabled by default.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvmdevices --adddev /dev/sdb1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-apache-http-web-server-configuring-ha-http\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">5.2. Configuring an Apache HTTP Server\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tConfigure an Apache HTTP Server with the following procedure.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tEnsure that the Apache HTTP Server is installed on each node in the cluster. You also need the \u003Ccode class=\"literal\">wget\u003C/code> tool installed on the cluster to be able to check the status of the Apache HTTP Server.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn each node, execute the following command.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install -y httpd wget\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf you are running the \u003Ccode class=\"literal\">firewalld\u003C/code> daemon, on each node in the cluster enable the ports that are required by the Red Hat High Availability Add-On and enable the ports you will require for running \u003Ccode class=\"literal\">httpd\u003C/code>. This example enables the \u003Ccode class=\"literal\">httpd\u003C/code> ports for public access, but the specific ports to enable for \u003Ccode class=\"literal\">httpd\u003C/code> may vary for production use.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --permanent --add-service=http\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --permanent --zone=public --add-service=http\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --reload\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIn order for the Apache resource agent to get the status of Apache, on each node in the cluster create the following addition to the existing configuration to enable the status server URL.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>cat &lt;&lt;-END &gt; /etc/httpd/conf.d/status.conf\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;Location /server-status&gt;\u003C/strong>\u003C/span>\n    \u003Cspan class=\"strong strong\">\u003Cstrong>SetHandler server-status\u003C/strong>\u003C/span>\n    \u003Cspan class=\"strong strong\">\u003Cstrong>Require local\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;/Location&gt;\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>END\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate a web page for Apache to serve up.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn one node in the cluster, ensure that the logical volume you created in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http\">Configuring an LVM volume with an XFS file system\u003C/a> is activated, mount the file system that you created on that logical volume, create the file \u003Ccode class=\"literal\">index.html\u003C/code> on that file system, and then unmount the file system.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>lvchange -ay my_vg/my_lv\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>mount /dev/my_vg/my_lv /var/www/\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>mkdir /var/www/html\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>mkdir /var/www/cgi-bin\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>mkdir /var/www/error\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>restorecon -R /var/www\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>cat &lt;&lt;-END &gt;/var/www/html/index.html\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;html&gt;\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;body&gt;Hello&lt;/body&gt;\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>&lt;/html&gt;\u003C/strong>\u003C/span>\n\u003Cspan class=\"strong strong\">\u003Cstrong>END\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>umount /var/www\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-resources-for-http-server-in-a-cluster-configuring-ha-http\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">5.3. Creating the resources and resource groups\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tCreate the resources for your cluster with the following procedure. To ensure these resources all run on the same node, they are configured as part of the resource group \u003Ccode class=\"literal\">apachegroup\u003C/code>. The resources to create are as follows, listed in the order in which they will start.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tAn \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource named \u003Ccode class=\"literal\">my_lvm\u003C/code> that uses the LVM volume group you created in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs\">Configuring an LVM volume with an XFS file system\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tA \u003Ccode class=\"literal\">Filesystem\u003C/code> resource named \u003Ccode class=\"literal\">my_fs\u003C/code>, that uses the file system device \u003Ccode class=\"literal\">/dev/my_vg/my_lv\u003C/code> you created in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs\">Configuring an LVM volume with an XFS file system\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tAn \u003Ccode class=\"literal\">IPaddr2\u003C/code> resource, which is a floating IP address for the \u003Ccode class=\"literal\">apachegroup\u003C/code> resource group. The IP address must not be one already associated with a physical node. If the \u003Ccode class=\"literal\">IPaddr2\u003C/code> resource’s NIC device is not specified, the floating IP must reside on the same network as one of the node’s statically assigned IP addresses, otherwise the NIC device to assign the floating IP address cannot be properly detected.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tAn \u003Ccode class=\"literal\">apache\u003C/code> resource named \u003Ccode class=\"literal\">Website\u003C/code> that uses the \u003Ccode class=\"literal\">index.html\u003C/code> file and the Apache configuration you defined in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-apache-http-web-server-configuring-ha-http\">Configuring an Apache HTTP server\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\tThe following procedure creates the resource group \u003Ccode class=\"literal\">apachegroup\u003C/code> and the resources that the group contains. The resources will start in the order in which you add them to the group, and they will stop in the reverse order in which they are added to the group. Run this procedure from one node of the cluster only.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following command creates the \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource \u003Ccode class=\"literal\">my_lvm\u003C/code>. Because the resource group \u003Ccode class=\"literal\">apachegroup\u003C/code> does not yet exist, this command creates the resource group.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tDo not configure more than one \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource that uses the same LVM volume group in an active/passive HA configuration, as this could cause data corruption. Additionally, do not configure an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource as a clone resource in an active/passive HA configuration.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create my_lvm ocf:heartbeat:LVM-activate vgname=my_vg vg_access_mode=system_id --group apachegroup\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tWhen you create a resource, the resource is started automatically. You can use the following command to confirm that the resource was created and has started.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource status\u003C/strong>\u003C/span>\n Resource Group: apachegroup\n     my_lvm\t(ocf::heartbeat:LVM-activate):\tStarted\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tYou can manually stop and start an individual resource with the \u003Ccode class=\"literal command\">pcs resource disable\u003C/code> and \u003Ccode class=\"literal command\">pcs resource enable\u003C/code> commands.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following commands create the remaining resources for the configuration, adding them to the existing resource group \u003Ccode class=\"literal\">apachegroup\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create my_fs Filesystem device=\"/dev/my_vg/my_lv\" directory=\"/var/www\" fstype=\"xfs\" --group apachegroup\u003C/strong>\u003C/span>\n\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create VirtualIP IPaddr2 ip=198.51.100.3 cidr_netmask=24 --group apachegroup\u003C/strong>\u003C/span>\n\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create Website apache configfile=\"/etc/httpd/conf/httpd.conf\" statusurl=\"http://127.0.0.1/server-status\" --group apachegroup\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAfter creating the resources and the resource group that contains them, you can check the status of the cluster. Note that all four resources are running on the same node.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\nCluster name: my_cluster\nLast updated: Wed Jul 31 16:38:51 2013\nLast change: Wed Jul 31 16:42:14 2013 via crm_attribute on z1.example.com\nStack: corosync\nCurrent DC: z2.example.com (2) - partition with quorum\nVersion: 1.1.10-5.el7-9abe687\n2 Nodes configured\n6 Resources configured\n\nOnline: [ z1.example.com z2.example.com ]\n\nFull list of resources:\n myapc\t(stonith:fence_apc_snmp):\tStarted z1.example.com\n Resource Group: apachegroup\n     my_lvm\t(ocf::heartbeat:LVM-activate):\tStarted z1.example.com\n     my_fs\t(ocf::heartbeat:Filesystem):\tStarted z1.example.com\n     VirtualIP\t(ocf::heartbeat:IPaddr2):\tStarted z1.example.com\n     Website\t(ocf::heartbeat:apache):\tStarted z1.example.com\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tNote that if you have not configured a fencing device for your cluster, by default the resources do not start.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOnce the cluster is up and running, you can point a browser to the IP address you defined as the \u003Ccode class=\"literal\">IPaddr2\u003C/code> resource to view the sample display, consisting of the simple word \"Hello\".\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">Hello\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf you find that the resources you configured are not running, you can run the \u003Ccode class=\"literal command\">pcs resource debug-start \u003Cspan class=\"emphasis\">\u003Cem>resource\u003C/em>\u003C/span>\u003C/code> command to test the resource configuration.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tWhen you use the \u003Ccode class=\"literal\">apache\u003C/code> resource agent to manage Apache, it does not use \u003Ccode class=\"literal\">systemd\u003C/code>. Because of this, you must edit the \u003Ccode class=\"literal\">logrotate\u003C/code> script supplied with Apache so that it does not use \u003Ccode class=\"literal\">systemctl\u003C/code> to reload Apache.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tRemove the following line in the \u003Ccode class=\"literal\">/etc/logrotate.d/httpd\u003C/code> file on each node in the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">/bin/systemctl reload httpd.service &gt; /dev/null 2&gt;/dev/null || true\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tReplace the line you removed with the following three lines, specifying \u003Ccode class=\"literal\">/var/run/httpd-\u003Cspan class=\"emphasis\">\u003Cem>website\u003C/em>\u003C/span>.pid\u003C/code> as the PID file path where \u003Cspan class=\"emphasis\">\u003Cem>website\u003C/em>\u003C/span> is the name of the Apache resource. In this example, the Apache resource name is \u003Ccode class=\"literal\">Website\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">/usr/bin/test -f /var/run/httpd-Website.pid &gt;/dev/null 2&gt;/dev/null &amp;&amp;\n/usr/bin/ps -q $(/usr/bin/cat /var/run/httpd-Website.pid) &gt;/dev/null 2&gt;/dev/null &amp;&amp;\n/usr/sbin/httpd -f /etc/httpd/conf/httpd.conf -c \"PidFile /var/run/httpd-Website.pid\" -k graceful &gt; /dev/null 2&gt;/dev/null || true\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_testing-resource-configuration-in-a-cluster-configuring-ha-http\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">5.4. Testing the resource configuration\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tTest the resource configuration in a cluster with the following procedure.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIn the cluster status display shown in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-resources-for-http-server-in-a-cluster-configuring-ha-http\">Creating the resources and resource groups\u003C/a>, all of the resources are running on node \u003Ccode class=\"literal\">z1.example.com\u003C/code>. You can test whether the resource group fails over to node \u003Ccode class=\"literal\">z2.example.com\u003C/code> by using the following procedure to put the first node in \u003Ccode class=\"literal\">standby\u003C/code> mode, after which the node will no longer be able to host resources.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following command puts node \u003Ccode class=\"literal\">z1.example.com\u003C/code> in \u003Ccode class=\"literal\">standby\u003C/code> mode.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node standby z1.example.com\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAfter putting node \u003Ccode class=\"literal\">z1\u003C/code> in \u003Ccode class=\"literal\">standby\u003C/code> mode, check the cluster status. Note that the resources should now all be running on \u003Ccode class=\"literal\">z2\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\nCluster name: my_cluster\nLast updated: Wed Jul 31 17:16:17 2013\nLast change: Wed Jul 31 17:18:34 2013 via crm_attribute on z1.example.com\nStack: corosync\nCurrent DC: z2.example.com (2) - partition with quorum\nVersion: 1.1.10-5.el7-9abe687\n2 Nodes configured\n6 Resources configured\n\nNode z1.example.com (1): standby\nOnline: [ z2.example.com ]\n\nFull list of resources:\n\n myapc\t(stonith:fence_apc_snmp):\tStarted z1.example.com\n Resource Group: apachegroup\n     my_lvm\t(ocf::heartbeat:LVM-activate):\tStarted z2.example.com\n     my_fs\t(ocf::heartbeat:Filesystem):\tStarted z2.example.com\n     VirtualIP\t(ocf::heartbeat:IPaddr2):\tStarted z2.example.com\n     Website\t(ocf::heartbeat:apache):\tStarted z2.example.com\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe web site at the defined IP address should still display, without interruption.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tTo remove \u003Ccode class=\"literal\">z1\u003C/code> from \u003Ccode class=\"literal\">standby\u003C/code> mode, enter the following command.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node unstandby z1.example.com\u003C/strong>\u003C/span>\u003C/pre>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tRemoving a node from \u003Ccode class=\"literal\">standby\u003C/code> mode does not in itself cause the resources to fail back over to that node. This will depend on the \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value for the resources. For information about the \u003Ccode class=\"literal\">resource-stickiness\u003C/code> meta attribute, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters#proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on\">Configuring a resource to prefer its current node\u003C/a>.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 6. Configuring an active/passive NFS server in a Red Hat High Availability cluster\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tThe Red Hat High Availability Add-On provides support for running a highly available active/passive NFS server on a Red Hat Enterprise Linux High Availability Add-On cluster using shared storage. In the following example, you are configuring a two-node cluster in which clients access the NFS file system through a floating IP address. The NFS server runs on one of the two nodes in the cluster. If the node on which the NFS server is running becomes inoperative, the NFS server starts up again on the second node of the cluster with minimal service interruption.\n\t\t\u003C/p>\u003Cp>\n\t\t\tThis use case requires that your system include the following components:\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\tA two-node Red Hat High Availability cluster with power fencing configured for each node. We recommend but do not require a private network. This procedure uses the cluster example provided in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters\">Creating a Red Hat High-Availability cluster with Pacemaker\u003C/a>.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tA public virtual IP address, required for the NFS server.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tShared storage for the nodes in the cluster, using iSCSI, Fibre Channel, or other shared network block device.\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\tConfiguring a highly available active/passive NFS server on an existing two-node Red Hat Enterprise Linux High Availability cluster requires that you perform the following steps:\n\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\tConfigure a file system on an LVM logical volume on the shared storage for the nodes in the cluster.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tConfigure an NFS share on the shared storage on the LVM logical volume.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tCreate the cluster resources.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tTest the NFS server you have configured.\n\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003Csection class=\"section\" id=\"proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">6.1. Configuring an LVM volume with an XFS file system in a Pacemaker cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tCreate an LVM logical volume on storage that is shared between the nodes of the cluster with the following procedure.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tLVM volumes and the corresponding partitions and devices used by cluster nodes must be connected to the cluster nodes only.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tThe following procedure creates an LVM logical volume and then creates an XFS file system on that volume for use in a Pacemaker cluster. In this example, the shared partition \u003Ccode class=\"literal\">/dev/sdb1\u003C/code> is used to store the LVM physical volume from which the LVM logical volume will be created.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes of the cluster, perform the following steps to set the value for the LVM system ID to the value of the \u003Ccode class=\"literal\">uname\u003C/code> identifier for the system. The LVM system ID will be used to ensure that only the cluster is capable of activating the volume group.\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tSet the \u003Ccode class=\"literal\">system_id_source\u003C/code> configuration option in the \u003Ccode class=\"literal\">/etc/lvm/lvm.conf\u003C/code> configuration file to \u003Ccode class=\"literal\">uname\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># Configuration option global/system_id_source.\nsystem_id_source = \"uname\"\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tVerify that the LVM system ID on the node matches the \u003Ccode class=\"literal\">uname\u003C/code> for the node.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>lvm systemid\u003C/strong>\u003C/span>\n  system ID: z1.example.com\n# \u003Cspan class=\"strong strong\">\u003Cstrong>uname -n\u003C/strong>\u003C/span>\n  z1.example.com\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate the LVM volume and create an XFS file system on that volume. Since the \u003Ccode class=\"literal\">/dev/sdb1\u003C/code> partition is storage that is shared, you perform this part of the procedure on one node only.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tIf your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order\">Configuring startup order for resource dependencies not managed by Pacemaker\u003C/a>.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate an LVM physical volume on partition \u003Ccode class=\"literal\">/dev/sdb1\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pvcreate /dev/sdb1\u003C/strong>\u003C/span>\n  Physical volume \"/dev/sdb1\" successfully created\u003C/pre>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\t\tIf your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order\">Configuring startup order for resource dependencies not managed by Pacemaker\u003C/a>.\n\t\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate the volume group \u003Ccode class=\"literal\">my_vg\u003C/code> that consists of the physical volume \u003Ccode class=\"literal\">/dev/sdb1\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tSpecify the \u003Ccode class=\"literal\">--setautoactivation n\u003C/code> flag to ensure that volume groups managed by Pacemaker in a cluster will not be automatically activated on startup. If you are using an existing volume group for the LVM volume you are creating, you can reset this flag with the \u003Ccode class=\"literal\">vgchange --setautoactivation n\u003C/code> command for the volume group.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgcreate --setautoactivation n my_vg /dev/sdb1\u003C/strong>\u003C/span>\n  Volume group \"my_vg\" successfully created\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tVerify that the new volume group has the system ID of the node on which you are running and from which you created the volume group.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgs -o+systemid\u003C/strong>\u003C/span>\n  VG    #PV #LV #SN Attr   VSize  VFree  System ID\n  my_vg   1   0   0 wz--n- &lt;1.82t &lt;1.82t z1.example.com\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate a logical volume using the volume group \u003Ccode class=\"literal\">my_vg\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvcreate -L450 -n my_lv my_vg\u003C/strong>\u003C/span>\n  Rounding up size to full physical extent 452.00 MiB\n  Logical volume \"my_lv\" created\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tYou can use the \u003Ccode class=\"literal command\">lvs\u003C/code> command to display the logical volume.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvs\u003C/strong>\u003C/span>\n  LV      VG      Attr      LSize   Pool Origin Data%  Move Log Copy%  Convert\n  my_lv   my_vg   -wi-a---- 452.00m\n  ...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate an XFS file system on the logical volume \u003Ccode class=\"literal\">my_lv\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkfs.xfs /dev/my_vg/my_lv\u003C/strong>\u003C/span>\nmeta-data=/dev/my_vg/my_lv       isize=512    agcount=4, agsize=28928 blks\n         =                       sectsz=512   attr=2, projid32bit=1\n...\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf the use of a devices file is enabled with the \u003Ccode class=\"literal\">use_devicesfile = 1\u003C/code> parameter in the \u003Ccode class=\"literal\">lvm.conf\u003C/code> file, add the shared device to the devices file on the second node in the cluster. This feature is enabled by default.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvmdevices --adddev /dev/sdb1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-nfs-share-configuring-ha-nfs\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">6.2. Configuring an NFS share\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tConfigure an NFS share for an NFS service failover with the following procedure.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes in the cluster, create the \u003Ccode class=\"literal\">/nfsshare\u003C/code> directory.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>mkdir /nfsshare\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn one node in the cluster, perform the following procedure.\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tEnsure that the logical volume you you created in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs\">Configuring an LVM volume with an XFS file system\u003C/a> is activated, then mount the file system you created on the logical volume on the \u003Ccode class=\"literal\">/nfsshare\u003C/code> directory.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvchange -ay my_vg/my_lv\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mount /dev/my_vg/my_lv /nfsshare\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate an \u003Ccode class=\"literal\">exports\u003C/code> directory tree on the \u003Ccode class=\"literal\">/nfsshare\u003C/code> directory.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkdir -p /nfsshare/exports\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkdir -p /nfsshare/exports/export1\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkdir -p /nfsshare/exports/export2\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tPlace files in the \u003Ccode class=\"literal\">exports\u003C/code> directory for the NFS clients to access. For this example, we are creating test files named \u003Ccode class=\"literal\">clientdatafile1\u003C/code> and \u003Ccode class=\"literal\">clientdatafile2\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>touch /nfsshare/exports/export1/clientdatafile1\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>touch /nfsshare/exports/export2/clientdatafile2\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tUnmount the file system and deactivate the LVM volume group.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>umount /dev/my_vg/my_lv\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgchange -an my_vg\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring_resources_for_nfs_server_in_a_cluster-configuring-ha-nfs\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">6.3. Configuring the resources and resource group for an NFS server in a cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tConfigure the cluster resources for an NFS server in a cluster with the following procedure.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tIf you have not configured a fencing device for your cluster, by default the resources do not start.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tIf you find that the resources you configured are not running, you can run the \u003Ccode class=\"literal command\">pcs resource debug-start \u003Cspan class=\"emphasis\">\u003Cem>resource\u003C/em>\u003C/span>\u003C/code> command to test the resource configuration. This starts the service outside of the cluster’s control and knowledge. At the point the configured resources are running again, run \u003Ccode class=\"literal command\">pcs resource cleanup \u003Cspan class=\"emphasis\">\u003Cem>resource\u003C/em>\u003C/span>\u003C/code> to make the cluster aware of the updates.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\tThe following procedure configures the system resources. To ensure these resources all run on the same node, they are configured as part of the resource group \u003Ccode class=\"literal\">nfsgroup\u003C/code>. The resources will start in the order in which you add them to the group, and they will stop in the reverse order in which they are added to the group. Run this procedure from one node of the cluster only.\n\t\t\t\t\u003C/p>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate the LVM-activate resource named \u003Ccode class=\"literal\">my_lvm\u003C/code>. Because the resource group \u003Ccode class=\"literal\">nfsgroup\u003C/code> does not yet exist, this command creates the resource group.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tDo not configure more than one \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource that uses the same LVM volume group in an active/passive HA configuration, as this risks data corruption. Additionally, do not configure an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource as a clone resource in an active/passive HA configuration.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create my_lvm ocf:heartbeat:LVM-activate vgname=my_vg vg_access_mode=system_id --group nfsgroup\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCheck the status of the cluster to verify that the resource is running.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">root@z1 ~]#  \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\nCluster name: my_cluster\nLast updated: Thu Jan  8 11:13:17 2015\nLast change: Thu Jan  8 11:13:08 2015\nStack: corosync\nCurrent DC: z2.example.com (2) - partition with quorum\nVersion: 1.1.12-a14efad\n2 Nodes configured\n3 Resources configured\n\nOnline: [ z1.example.com z2.example.com ]\n\nFull list of resources:\n myapc  (stonith:fence_apc_snmp):       Started z1.example.com\n Resource Group: nfsgroup\n     my_lvm     (ocf::heartbeat:LVM-activate):   Started z1.example.com\n\nPCSD Status:\n  z1.example.com: Online\n  z2.example.com: Online\n\nDaemon Status:\n  corosync: active/enabled\n  pacemaker: active/enabled\n  pcsd: active/enabled\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tConfigure a \u003Ccode class=\"literal\">Filesystem\u003C/code> resource for the cluster.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following command configures an XFS \u003Ccode class=\"literal\">Filesystem\u003C/code> resource named \u003Ccode class=\"literal\">nfsshare\u003C/code> as part of the \u003Ccode class=\"literal\">nfsgroup\u003C/code> resource group. This file system uses the LVM volume group and XFS file system you created in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs\">Configuring an LVM volume with an XFS file system\u003C/a> and will be mounted on the \u003Ccode class=\"literal\">/nfsshare\u003C/code> directory you created in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-nfs-share-configuring-ha-nfs\">Configuring an NFS share\u003C/a>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create nfsshare Filesystem device=/dev/my_vg/my_lv directory=/nfsshare fstype=xfs --group nfsgroup\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tYou can specify mount options as part of the resource configuration for a \u003Ccode class=\"literal\">Filesystem\u003C/code> resource with the \u003Ccode class=\"literal\">options=\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>\u003C/code> parameter. Run the \u003Ccode class=\"literal command\">pcs resource describe Filesystem\u003C/code> command for full configuration options.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tVerify that the \u003Ccode class=\"literal\">my_lvm\u003C/code> and \u003Ccode class=\"literal\">nfsshare\u003C/code> resources are running.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\n...\nFull list of resources:\n myapc  (stonith:fence_apc_snmp):       Started z1.example.com\n Resource Group: nfsgroup\n     my_lvm     (ocf::heartbeat:LVM-activate):   Started z1.example.com\n     nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com\n...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate the \u003Ccode class=\"literal\">nfsserver\u003C/code> resource named \u003Ccode class=\"literal\">nfs-daemon\u003C/code> as part of the resource group \u003Ccode class=\"literal\">nfsgroup\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tThe \u003Ccode class=\"literal\">nfsserver\u003C/code> resource allows you to specify an \u003Ccode class=\"literal\">nfs_shared_infodir\u003C/code> parameter, which is a directory that NFS servers use to store NFS-related stateful information.\n\t\t\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\t\t\tIt is recommended that this attribute be set to a subdirectory of one of the \u003Ccode class=\"literal\">Filesystem\u003C/code> resources you created in this collection of exports. This ensures that the NFS servers are storing their stateful information on a device that will become available to another node if this resource group needs to relocate. In this example;\n\t\t\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">/nfsshare\u003C/code> is the shared-storage directory managed by the \u003Ccode class=\"literal\">Filesystem\u003C/code> resource\n\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">/nfsshare/exports/export1\u003C/code> and \u003Ccode class=\"literal\">/nfsshare/exports/export2\u003C/code> are the export directories\n\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">/nfsshare/nfsinfo\u003C/code> is the shared-information directory for the \u003Ccode class=\"literal\">nfsserver\u003C/code> resource\n\t\t\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create nfs-daemon nfsserver nfs_shared_infodir=/nfsshare/nfsinfo nfs_no_notify=true --group nfsgroup\u003C/strong>\u003C/span>\n\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\n...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAdd the \u003Ccode class=\"literal\">exportfs\u003C/code> resources to export the \u003Ccode class=\"literal\">/nfsshare/exports\u003C/code> directory. These resources are part of the resource group \u003Ccode class=\"literal\">nfsgroup\u003C/code>. This builds a virtual directory for NFSv4 clients. NFSv3 clients can access these exports as well.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tThe \u003Ccode class=\"literal\">fsid=0\u003C/code> option is required only if you want to create a virtual directory for NFSv4 clients. For more information, see the Red Hat Knowledgebase solution \u003Ca class=\"link\" href=\"https://access.redhat.com/solutions/548083\">How do I configure the fsid option in an NFS server’s /etc/exports file?\u003C/a>.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create nfs-root exportfs clientspec=192.168.122.0/255.255.255.0 options=rw,sync,no_root_squash directory=/nfsshare/exports fsid=0 --group nfsgroup\u003C/strong>\u003C/span>\n\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create nfs-export1 exportfs clientspec=192.168.122.0/255.255.255.0 options=rw,sync,no_root_squash directory=/nfsshare/exports/export1 fsid=1 --group nfsgroup\u003C/strong>\u003C/span>\n\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create nfs-export2 exportfs clientspec=192.168.122.0/255.255.255.0 options=rw,sync,no_root_squash directory=/nfsshare/exports/export2 fsid=2 --group nfsgroup\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAdd the floating IP address resource that NFS clients will use to access the NFS share. This resource is part of the resource group \u003Ccode class=\"literal\">nfsgroup\u003C/code>. For this example deployment, we are using 192.168.122.200 as the floating IP address.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create nfs_ip IPaddr2 ip=192.168.122.200 cidr_netmask=24 --group nfsgroup\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAdd an \u003Ccode class=\"literal\">nfsnotify\u003C/code> resource for sending NFSv3 reboot notifications once the entire NFS deployment has initialized. This resource is part of the resource group \u003Ccode class=\"literal\">nfsgroup\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tFor the NFS notification to be processed correctly, the floating IP address must have a host name associated with it that is consistent on both the NFS servers and the NFS client.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create nfs-notify nfsnotify source_host=192.168.122.200 --group nfsgroup\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAfter creating the resources and the resource constraints, you can check the status of the cluster. Note that all resources are running on the same node.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\n...\nFull list of resources:\n myapc  (stonith:fence_apc_snmp):       Started z1.example.com\n Resource Group: nfsgroup\n     my_lvm     (ocf::heartbeat:LVM-activate):   Started z1.example.com\n     nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com\n     nfs-daemon (ocf::heartbeat:nfsserver):     Started z1.example.com\n     nfs-root   (ocf::heartbeat:exportfs):      Started z1.example.com\n     nfs-export1        (ocf::heartbeat:exportfs):      Started z1.example.com\n     nfs-export2        (ocf::heartbeat:exportfs):      Started z1.example.com\n     nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z1.example.com\n     nfs-notify (ocf::heartbeat:nfsnotify):     Started z1.example.com\n...\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_testing-nfs-resource-configuration-configuring-ha-nfs\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">6.4. Testing the NFS resource configuration\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can validate your NFS resource configuration in a high availability cluster with the following procedures. You should be able to mount the exported file system with either NFSv3 or NFSv4.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"testing_the_nfs_export\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">6.4.1. Testing the NFS export\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tIf you are running the \u003Ccode class=\"literal\">firewalld\u003C/code> daemon on your cluster nodes, ensure that the ports that your system requires for NFS access are enabled on all nodes.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn a node outside of the cluster, residing in the same network as the deployment, verify that the NFS share can be seen by mounting the NFS share. For this example, we are using the 192.168.122.0/24 network.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>showmount -e 192.168.122.200\u003C/strong>\u003C/span>\nExport list for 192.168.122.200:\n/nfsshare/exports/export1 192.168.122.0/255.255.255.0\n/nfsshare/exports         192.168.122.0/255.255.255.0\n/nfsshare/exports/export2 192.168.122.0/255.255.255.0\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tTo verify that you can mount the NFS share with NFSv4, mount the NFS share to a directory on the client node. After mounting, verify that the contents of the export directories are visible. Unmount the share after testing.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>mkdir nfsshare\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>mount -o \"vers=4\" 192.168.122.200:export1 nfsshare\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>ls nfsshare\u003C/strong>\u003C/span>\nclientdatafile1\n# \u003Cspan class=\"strong strong\">\u003Cstrong>umount nfsshare\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tVerify that you can mount the NFS share with NFSv3. After mounting, verify that the test file \u003Ccode class=\"literal\">clientdatafile1\u003C/code> is visible. Unlike NFSv4, since NFSv3 does not use the virtual file system, you must mount a specific export. Unmount the share after testing.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>mkdir nfsshare\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>mount -o \"vers=3\" 192.168.122.200:/nfsshare/exports/export2 nfsshare\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>ls nfsshare\u003C/strong>\u003C/span>\nclientdatafile2\n# \u003Cspan class=\"strong strong\">\u003Cstrong>umount nfsshare\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"testing_for_failover\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">6.4.2. Testing for failover\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn a node outside of the cluster, mount the NFS share and verify access to the \u003Ccode class=\"literal\">clientdatafile1\u003C/code> file you created in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-nfs-share-configuring-ha-nfs\">Configuring an NFS share\u003C/a>.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>mkdir nfsshare\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>mount -o \"vers=4\" 192.168.122.200:export1 nfsshare\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>ls nfsshare\u003C/strong>\u003C/span>\nclientdatafile1\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tFrom a node within the cluster, determine which node in the cluster is running \u003Ccode class=\"literal\">nfsgroup\u003C/code>. In this example, \u003Ccode class=\"literal\">nfsgroup\u003C/code> is running on \u003Ccode class=\"literal\">z1.example.com\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\n...\nFull list of resources:\n myapc  (stonith:fence_apc_snmp):       Started z1.example.com\n Resource Group: nfsgroup\n     my_lvm     (ocf::heartbeat:LVM-activate):   Started z1.example.com\n     nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com\n     nfs-daemon (ocf::heartbeat:nfsserver):     Started z1.example.com\n     nfs-root   (ocf::heartbeat:exportfs):      Started z1.example.com\n     nfs-export1        (ocf::heartbeat:exportfs):      Started z1.example.com\n     nfs-export2        (ocf::heartbeat:exportfs):      Started z1.example.com\n     nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z1.example.com\n     nfs-notify (ocf::heartbeat:nfsnotify):     Started z1.example.com\n...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tFrom a node within the cluster, put the node that is running \u003Ccode class=\"literal\">nfsgroup\u003C/code> in standby mode.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node standby z1.example.com\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tVerify that \u003Ccode class=\"literal\">nfsgroup\u003C/code> successfully starts on the other cluster node.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\n...\nFull list of resources:\n Resource Group: nfsgroup\n     my_lvm     (ocf::heartbeat:LVM-activate):   Started z2.example.com\n     nfsshare   (ocf::heartbeat:Filesystem):    Started z2.example.com\n     nfs-daemon (ocf::heartbeat:nfsserver):     Started z2.example.com\n     nfs-root   (ocf::heartbeat:exportfs):      Started z2.example.com\n     nfs-export1        (ocf::heartbeat:exportfs):      Started z2.example.com\n     nfs-export2        (ocf::heartbeat:exportfs):      Started z2.example.com\n     nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z2.example.com\n     nfs-notify (ocf::heartbeat:nfsnotify):     Started z2.example.com\n...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tFrom the node outside the cluster on which you have mounted the NFS share, verify that this outside node still continues to have access to the test file within the NFS mount.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>ls nfsshare\u003C/strong>\u003C/span>\nclientdatafile1\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tService will be lost briefly for the client during the failover but the client should recover it with no user intervention. By default, clients using NFSv4 may take up to 90 seconds to recover the mount; this 90 seconds represents the NFSv4 file lease grace period observed by the server on startup. NFSv3 clients should recover access to the mount in a matter of a few seconds.\n\t\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tFrom a node within the cluster, remove the node that was initially running \u003Ccode class=\"literal\">nfsgroup\u003C/code> from standby mode.\n\t\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\tRemoving a node from \u003Ccode class=\"literal\">standby\u003C/code> mode does not in itself cause the resources to fail back over to that node. This will depend on the \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value for the resources. For information about the \u003Ccode class=\"literal\">resource-stickiness\u003C/code> meta attribute, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters#proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on\">Configuring a resource to prefer its current node\u003C/a>.\n\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node unstandby z1.example.com\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 7. GFS2 file systems in a cluster\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tUse the following administrative procedures to configure GFS2 file systems in a Red Hat high availability cluster.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">7.1. Configuring a GFS2 file system in a cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can set up a Pacemaker cluster that includes GFS2 file systems with the following procedure. In this example, you create three GFS2 file systems on three logical volumes in a two-node cluster.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Prerequisites\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tInstall and start the cluster software on both cluster nodes and create a basic two-node cluster.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tConfigure fencing for the cluster.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tFor information about creating a Pacemaker cluster and configuring fencing for the cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters\">Creating a Red Hat High-Availability cluster with Pacemaker\u003C/a>.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes in the cluster, enable the repository for Resilient Storage that corresponds to your system architecture. For example, to enable the Resilient Storage repository for an x86_64 system, you can enter the following \u003Ccode class=\"literal\">subscription-manager\u003C/code> command:\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>subscription-manager repos --enable=rhel-9-for-x86_64-resilientstorage-rpms\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tNote that the Resilient Storage repository is a superset of the High Availability repository. If you enable the Resilient Storage repository you do not also need to enable the High Availability repository.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes of the cluster, install the \u003Ccode class=\"literal\">lvm2-lockd\u003C/code>, \u003Ccode class=\"literal\">gfs2-utils\u003C/code>, and \u003Ccode class=\"literal\">dlm\u003C/code> packages. To support these packages, you must be subscribed to the AppStream channel and the Resilient Storage channel.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install lvm2-lockd gfs2-utils dlm\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes of the cluster, set the \u003Ccode class=\"literal\">use_lvmlockd\u003C/code> configuration option in the \u003Ccode class=\"literal\">/etc/lvm/lvm.conf\u003C/code> file to \u003Ccode class=\"literal\">use_lvmlockd=1\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">...\nuse_lvmlockd = 1\n...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSet the global Pacemaker parameter \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> to \u003Ccode class=\"literal\">freeze\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tBy default, the value of \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> is set to \u003Ccode class=\"literal\">stop\u003C/code>, indicating that once quorum is lost, all the resources on the remaining partition will immediately be stopped. Typically this default is the safest and most optimal option, but unlike most resources, GFS2 requires quorum to function. When quorum is lost both the applications using the GFS2 mounts and the GFS2 mount itself cannot be correctly stopped. Any attempts to stop these resources without quorum will fail which will ultimately result in the entire cluster being fenced every time quorum is lost.\n\t\t\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\t\t\tTo address this situation, set \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> to \u003Ccode class=\"literal\">freeze\u003C/code> when GFS2 is in use. This means that when quorum is lost, the remaining partition will do nothing until quorum is regained.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set no-quorum-policy=freeze\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSet up a \u003Ccode class=\"literal\">dlm\u003C/code> resource. This is a required dependency for configuring a GFS2 file system in a cluster. This example creates the \u003Ccode class=\"literal\">dlm\u003C/code> resource as part of a resource group named \u003Ccode class=\"literal\">locking\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tClone the \u003Ccode class=\"literal\">locking\u003C/code> resource group so that the resource group can be active on both nodes of the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource clone locking interleave=true\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSet up an \u003Ccode class=\"literal\">lvmlockd\u003C/code> resource as part of the \u003Ccode class=\"literal\">locking\u003C/code> resource group.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCheck the status of the cluster to ensure that the \u003Ccode class=\"literal\">locking\u003C/code> resource group has started on both nodes of the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status --full\u003C/strong>\u003C/span>\nCluster name: my_cluster\n[...]\n\nOnline: [ z1.example.com (1) z2.example.com (2) ]\n\nFull list of resources:\n\n smoke-apc      (stonith:fence_apc):    Started z1.example.com\n Clone Set: locking-clone [locking]\n     Resource Group: locking:0\n         dlm    (ocf::pacemaker:controld):      Started z1.example.com\n         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com\n     Resource Group: locking:1\n         dlm    (ocf::pacemaker:controld):      Started z2.example.com\n         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com\n     Started: [ z1.example.com z2.example.com ]\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn one node of the cluster, create two shared volume groups. One volume group will contain two GFS2 file systems, and the other volume group will contain one GFS2 file system.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tIf your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order\">Configuring startup order for resource dependencies not managed by Pacemaker\u003C/a>.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following command creates the shared volume group \u003Ccode class=\"literal\">shared_vg1\u003C/code> on \u003Ccode class=\"literal\">/dev/vdb\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgcreate --shared shared_vg1 /dev/vdb\u003C/strong>\u003C/span>\n  Physical volume \"/dev/vdb\" successfully created.\n  Volume group \"shared_vg1\" successfully created\n  VG shared_vg1 starting dlm lockspace\n  Starting locking.  Waiting until locks are ready...\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following command creates the shared volume group \u003Ccode class=\"literal\">shared_vg2\u003C/code> on \u003Ccode class=\"literal\">/dev/vdc\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgcreate --shared shared_vg2 /dev/vdc\u003C/strong>\u003C/span>\n  Physical volume \"/dev/vdc\" successfully created.\n  Volume group \"shared_vg2\" successfully created\n  VG shared_vg2 starting dlm lockspace\n  Starting locking.  Waiting until locks are ready...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn the second node in the cluster:\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tIf the use of a devices file is enabled with the \u003Ccode class=\"literal\">use_devicesfile = 1\u003C/code> parameter in the \u003Ccode class=\"literal\">lvm.conf\u003C/code> file, add the shared devices to the devices file This feature is enabled by default.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvmdevices --adddev /dev/vdb\u003C/strong>\u003C/span>\n[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvmdevices --adddev /dev/vdc\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tStart the lock manager for each of the shared volume groups.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgchange --lockstart shared_vg1\u003C/strong>\u003C/span>\n  VG shared_vg1 starting dlm lockspace\n  Starting locking.  Waiting until locks are ready...\n[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgchange --lockstart shared_vg2\u003C/strong>\u003C/span>\n  VG shared_vg2 starting dlm lockspace\n  Starting locking.  Waiting until locks are ready...\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn one node in the cluster, create the shared logical volumes and format the volumes with a GFS2 file system. One journal is required for each node that mounts the file system. Ensure that you create enough journals for each of the nodes in your cluster. The format of the lock table name is \u003Cspan class=\"emphasis\">\u003Cem>ClusterName:FSName\u003C/em>\u003C/span> where \u003Cspan class=\"emphasis\">\u003Cem>ClusterName\u003C/em>\u003C/span> is the name of the cluster for which the GFS2 file system is being created and \u003Cspan class=\"emphasis\">\u003Cem>FSName\u003C/em>\u003C/span> is the file system name, which must be unique for all \u003Ccode class=\"literal\">lock_dlm\u003C/code> file systems over the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvcreate --activate sy -L5G -n shared_lv1 shared_vg1\u003C/strong>\u003C/span>\n  Logical volume \"shared_lv1\" created.\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvcreate --activate sy -L5G -n shared_lv2 shared_vg1\u003C/strong>\u003C/span>\n  Logical volume \"shared_lv2\" created.\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvcreate --activate sy -L5G -n shared_lv1 shared_vg2\u003C/strong>\u003C/span>\n  Logical volume \"shared_lv1\" created.\n\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo1 /dev/shared_vg1/shared_lv1\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo2 /dev/shared_vg1/shared_lv2\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo3 /dev/shared_vg2/shared_lv1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource for each logical volume to automatically activate that logical volume on all nodes.\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource named \u003Ccode class=\"literal\">sharedlv1\u003C/code> for the logical volume \u003Ccode class=\"literal\">shared_lv1\u003C/code> in volume group \u003Ccode class=\"literal\">shared_vg1\u003C/code>. This command also creates the resource group \u003Ccode class=\"literal\">shared_vg1\u003C/code> that includes the resource. In this example, the resource group has the same name as the shared volume group that includes the logical volume.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedlv1 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource named \u003Ccode class=\"literal\">sharedlv2\u003C/code> for the logical volume \u003Ccode class=\"literal\">shared_lv2\u003C/code> in volume group \u003Ccode class=\"literal\">shared_vg1\u003C/code>. This resource will also be part of the resource group \u003Ccode class=\"literal\">shared_vg1\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedlv2 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv2 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource named \u003Ccode class=\"literal\">sharedlv3\u003C/code> for the logical volume \u003Ccode class=\"literal\">shared_lv1\u003C/code> in volume group \u003Ccode class=\"literal\">shared_vg2\u003C/code>. This command also creates the resource group \u003Ccode class=\"literal\">shared_vg2\u003C/code> that includes the resource.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedlv3 --group shared_vg2 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg2 activation_mode=shared vg_access_mode=lvmlockd\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tClone the two new resource groups.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource clone shared_vg1 interleave=true\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource clone shared_vg2 interleave=true\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tConfigure ordering constraints to ensure that the \u003Ccode class=\"literal\">locking\u003C/code> resource group that includes the \u003Ccode class=\"literal\">dlm\u003C/code> and \u003Ccode class=\"literal\">lvmlockd\u003C/code> resources starts first.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order start locking-clone then shared_vg1-clone\u003C/strong>\u003C/span>\nAdding locking-clone shared_vg1-clone (kind: Mandatory) (Options: first-action=start then-action=start)\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order start locking-clone then shared_vg2-clone\u003C/strong>\u003C/span>\nAdding locking-clone shared_vg2-clone (kind: Mandatory) (Options: first-action=start then-action=start)\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tConfigure colocation constraints to ensure that the \u003Ccode class=\"literal\">vg1\u003C/code> and \u003Ccode class=\"literal\">vg2\u003C/code> resource groups start on the same node as the \u003Ccode class=\"literal\">locking\u003C/code> resource group.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint colocation add shared_vg1-clone with locking-clone\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint colocation add shared_vg2-clone with locking-clone\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes in the cluster, verify that the logical volumes are active. There may be a delay of a few seconds.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvs\u003C/strong>\u003C/span>\n  LV         VG          Attr       LSize\n  shared_lv1 shared_vg1  -wi-a----- 5.00g\n  shared_lv2 shared_vg1  -wi-a----- 5.00g\n  shared_lv1 shared_vg2  -wi-a----- 5.00g\n\n[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvs\u003C/strong>\u003C/span>\n  LV         VG          Attr       LSize\n  shared_lv1 shared_vg1  -wi-a----- 5.00g\n  shared_lv2 shared_vg1  -wi-a----- 5.00g\n  shared_lv1 shared_vg2  -wi-a----- 5.00g\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate a file system resource to automatically mount each GFS2 file system on all nodes.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tYou should not add the file system to the \u003Ccode class=\"literal\">/etc/fstab\u003C/code> file because it will be managed as a Pacemaker cluster resource. Mount options can be specified as part of the resource configuration with \u003Ccode class=\"literal\">options=\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>\u003C/code>. Run the \u003Ccode class=\"literal command\">pcs resource describe Filesystem\u003C/code> command to display the full configuration options.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following commands create the file system resources. These commands add each resource to the resource group that includes the logical volume resource for that file system.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device=\"/dev/shared_vg1/shared_lv1\" directory=\"/mnt/gfs1\" fstype=\"gfs2\" options=noatime op monitor interval=10s on-fail=fence\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedfs2 --group shared_vg1 ocf:heartbeat:Filesystem device=\"/dev/shared_vg1/shared_lv2\" directory=\"/mnt/gfs2\" fstype=\"gfs2\" options=noatime op monitor interval=10s on-fail=fence\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedfs3 --group shared_vg2 ocf:heartbeat:Filesystem device=\"/dev/shared_vg2/shared_lv1\" directory=\"/mnt/gfs3\" fstype=\"gfs2\" options=noatime op monitor interval=10s on-fail=fence\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Verification\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tVerify that the GFS2 file systems are mounted on both nodes of the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mount | grep gfs2\u003C/strong>\u003C/span>\n/dev/mapper/shared_vg1-shared_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)\n/dev/mapper/shared_vg1-shared_lv2 on /mnt/gfs2 type gfs2 (rw,noatime,seclabel)\n/dev/mapper/shared_vg2-shared_lv1 on /mnt/gfs3 type gfs2 (rw,noatime,seclabel)\n\n[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mount | grep gfs2\u003C/strong>\u003C/span>\n/dev/mapper/shared_vg1-shared_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)\n/dev/mapper/shared_vg1-shared_lv2 on /mnt/gfs2 type gfs2 (rw,noatime,seclabel)\n/dev/mapper/shared_vg2-shared_lv1 on /mnt/gfs3 type gfs2 (rw,noatime,seclabel)\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCheck the status of the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status --full\u003C/strong>\u003C/span>\nCluster name: my_cluster\n[...]\n\nFull list of resources:\n\n smoke-apc      (stonith:fence_apc):    Started z1.example.com\n Clone Set: locking-clone [locking]\n     Resource Group: locking:0\n         dlm    (ocf::pacemaker:controld):      Started z2.example.com\n         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com\n     Resource Group: locking:1\n         dlm    (ocf::pacemaker:controld):      Started z1.example.com\n         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com\n     Started: [ z1.example.com z2.example.com ]\n Clone Set: shared_vg1-clone [shared_vg1]\n     Resource Group: shared_vg1:0\n         sharedlv1      (ocf::heartbeat:LVM-activate):  Started z2.example.com\n         sharedlv2      (ocf::heartbeat:LVM-activate):  Started z2.example.com\n         sharedfs1      (ocf::heartbeat:Filesystem):    Started z2.example.com\n         sharedfs2      (ocf::heartbeat:Filesystem):    Started z2.example.com\n     Resource Group: shared_vg1:1\n         sharedlv1      (ocf::heartbeat:LVM-activate):  Started z1.example.com\n         sharedlv2      (ocf::heartbeat:LVM-activate):  Started z1.example.com\n         sharedfs1      (ocf::heartbeat:Filesystem):    Started z1.example.com\n         sharedfs2      (ocf::heartbeat:Filesystem):    Started z1.example.com\n     Started: [ z1.example.com z2.example.com ]\n Clone Set: shared_vg2-clone [shared_vg2]\n     Resource Group: shared_vg2:0\n         sharedlv3      (ocf::heartbeat:LVM-activate):  Started z2.example.com\n         sharedfs3      (ocf::heartbeat:Filesystem):    Started z2.example.com\n     Resource Group: shared_vg2:1\n         sharedlv3      (ocf::heartbeat:LVM-activate):  Started z1.example.com\n         sharedfs3      (ocf::heartbeat:Filesystem):    Started z1.example.com\n     Started: [ z1.example.com z2.example.com ]\n\n...\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv class=\"itemizedlist _additional-resources\">\u003Cp class=\"title\">\u003Cstrong>Additional resources\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist _additional-resources\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/index\">Configuring GFS2 file systems\u003C/a>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ca class=\"link\" href=\"https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/deploying_rhel_9_on_microsoft_azure/index#configuring-rhel-high-availability-on-azure_cloud-content-azure\">Configuring a Red Hat High Availability cluster on Microsoft Azure\u003C/a>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ca class=\"link\" href=\"https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/deploying_rhel_9_on_amazon_web_services/index#configuring-a-red-hat-high-availability-cluster-on-aws_deploying-a-virtual-machine-on-aws\">Configuring a Red Hat High Availability cluster on AWS\u003C/a>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ca class=\"link\" href=\"https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/deploying_rhel_9_on_google_cloud_platform/index#configuring-rhel-ha-on-gcp_cloud-content-gcp\">Configuring a Red Hat High Availability Cluster on Google Cloud Platform\u003C/a>\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">7.2. Configuring an encrypted GFS2 file system in a cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can create a Pacemaker cluster that includes a LUKS encrypted GFS2 file system with the following procedure. In this example, you create one GFS2 file systems on a logical volume and encrypt the file system. Encrypted GFS2 file systems are supported using the \u003Ccode class=\"literal\">crypt\u003C/code> resource agent, which provides support for LUKS encryption.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThere are three parts to this procedure:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tConfiguring a shared logical volume in a Pacemaker cluster\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tEncrypting the logical volume and creating a \u003Ccode class=\"literal\">crypt\u003C/code> resource\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tFormatting the encrypted logical volume with a GFS2 file system and creating a file system resource for the cluster\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Csection class=\"section\" id=\"configure_a_shared_logical_volume_in_a_pacemaker_cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">7.2.1. Configure a shared logical volume in a Pacemaker cluster\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Prerequisites\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tInstall and start the cluster software on two cluster nodes and create a basic two-node cluster.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tConfigure fencing for the cluster.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\t\tFor information about creating a Pacemaker cluster and configuring fencing for the cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters\">Creating a Red Hat High-Availability cluster with Pacemaker\u003C/a>.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn both nodes in the cluster, enable the repository for Resilient Storage that corresponds to your system architecture. For example, to enable the Resilient Storage repository for an x86_64 system, you can enter the following \u003Ccode class=\"literal\">subscription-manager\u003C/code> command:\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>subscription-manager repos --enable=rhel-9-for-x86_64-resilientstorage-rpms\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tNote that the Resilient Storage repository is a superset of the High Availability repository. If you enable the Resilient Storage repository you do not also need to enable the High Availability repository.\n\t\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn both nodes of the cluster, install the \u003Ccode class=\"literal\">lvm2-lockd\u003C/code>, \u003Ccode class=\"literal\">gfs2-utils\u003C/code>, and \u003Ccode class=\"literal\">dlm\u003C/code> packages. To support these packages, you must be subscribed to the AppStream channel and the Resilient Storage channel.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install lvm2-lockd gfs2-utils dlm\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn both nodes of the cluster, set the \u003Ccode class=\"literal\">use_lvmlockd\u003C/code> configuration option in the \u003Ccode class=\"literal\">/etc/lvm/lvm.conf\u003C/code> file to \u003Ccode class=\"literal\">use_lvmlockd=1\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">...\nuse_lvmlockd = 1\n...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tSet the global Pacemaker parameter \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> to \u003Ccode class=\"literal\">freeze\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\tBy default, the value of \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> is set to \u003Ccode class=\"literal\">stop\u003C/code>, indicating that when quorum is lost, all the resources on the remaining partition will immediately be stopped. Typically this default is the safest and most optimal option, but unlike most resources, GFS2 requires quorum to function. When quorum is lost both the applications using the GFS2 mounts and the GFS2 mount itself cannot be correctly stopped. Any attempts to stop these resources without quorum will fail which will ultimately result in the entire cluster being fenced every time quorum is lost.\n\t\t\t\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\t\t\t\tTo address this situation, set \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> to \u003Ccode class=\"literal\">freeze\u003C/code> when GFS2 is in use. This means that when quorum is lost, the remaining partition will do nothing until quorum is regained.\n\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set no-quorum-policy=freeze\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tSet up a \u003Ccode class=\"literal\">dlm\u003C/code> resource. This is a required dependency for configuring a GFS2 file system in a cluster. This example creates the \u003Ccode class=\"literal\">dlm\u003C/code> resource as part of a resource group named \u003Ccode class=\"literal\">locking\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tClone the \u003Ccode class=\"literal\">locking\u003C/code> resource group so that the resource group can be active on both nodes of the cluster.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource clone locking interleave=true\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tSet up an \u003Ccode class=\"literal\">lvmlockd\u003C/code> resource as part of the group \u003Ccode class=\"literal\">locking\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCheck the status of the cluster to ensure that the \u003Ccode class=\"literal\">locking\u003C/code> resource group has started on both nodes of the cluster.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status --full\u003C/strong>\u003C/span>\nCluster name: my_cluster\n[...]\n\nOnline: [ z1.example.com (1) z2.example.com (2) ]\n\nFull list of resources:\n\n smoke-apc      (stonith:fence_apc):    Started z1.example.com\n Clone Set: locking-clone [locking]\n     Resource Group: locking:0\n         dlm    (ocf::pacemaker:controld):      Started z1.example.com\n         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com\n     Resource Group: locking:1\n         dlm    (ocf::pacemaker:controld):      Started z2.example.com\n         lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com\n     Started: [ z1.example.com z2.example.com ]\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn one node of the cluster, create a shared volume group.\n\t\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\tIf your LVM volume group contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, Red Hat recommends that you ensure that the service starts before Pacemaker starts. For information about configuring startup order for a remote physical volume used by a Pacemaker cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order\">Configuring startup order for resource dependencies not managed by Pacemaker\u003C/a>.\n\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tThe following command creates the shared volume group \u003Ccode class=\"literal\">shared_vg1\u003C/code> on \u003Ccode class=\"literal\">/dev/sda1\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgcreate --shared shared_vg1 /dev/sda1\u003C/strong>\u003C/span>\n  Physical volume \"/dev/sda1\" successfully created.\n  Volume group \"shared_vg1\" successfully created\n  VG shared_vg1 starting dlm lockspace\n  Starting locking.  Waiting until locks are ready...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn the second node in the cluster:\n\t\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\t\tIf the use of a devices file is enabled with the \u003Ccode class=\"literal\">use_devicesfile = 1\u003C/code> parameter in the \u003Ccode class=\"literal\">lvm.conf\u003C/code> file, add the shared device to the devices file on the second node in the cluster. This feature is enabled by default.\n\t\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvmdevices --adddev /dev/sda1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\t\tStart the lock manager for the shared volume group.\n\t\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgchange --lockstart shared_vg1\u003C/strong>\u003C/span>\n  VG shared_vg1 starting dlm lockspace\n  Starting locking.  Waiting until locks are ready...\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn one node in the cluster, create the shared logical volume.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvcreate --activate sy -L5G -n shared_lv1 shared_vg1\u003C/strong>\u003C/span>\n  Logical volume \"shared_lv1\" created.\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCreate an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource for the logical volume to automatically activate the logical volume on all nodes.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tThe following command creates an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource named \u003Ccode class=\"literal\">sharedlv1\u003C/code> for the logical volume \u003Ccode class=\"literal\">shared_lv1\u003C/code> in volume group \u003Ccode class=\"literal\">shared_vg1\u003C/code>. This command also creates the resource group \u003Ccode class=\"literal\">shared_vg1\u003C/code> that includes the resource. In this example, the resource group has the same name as the shared volume group that includes the logical volume.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedlv1 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tClone the new resource group.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource clone shared_vg1 interleave=true\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tConfigure an ordering constraints to ensure that the \u003Ccode class=\"literal\">locking\u003C/code> resource group that includes the \u003Ccode class=\"literal\">dlm\u003C/code> and \u003Ccode class=\"literal\">lvmlockd\u003C/code> resources starts first.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order start locking-clone then shared_vg1-clone\u003C/strong>\u003C/span>\nAdding locking-clone shared_vg1-clone (kind: Mandatory) (Options: first-action=start then-action=start)\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tConfigure a colocation constraints to ensure that the \u003Ccode class=\"literal\">vg1\u003C/code> and \u003Ccode class=\"literal\">vg2\u003C/code> resource groups start on the same node as the \u003Ccode class=\"literal\">locking\u003C/code> resource group.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint colocation add shared_vg1-clone with locking-clone\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Verification\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\t\tOn both nodes in the cluster, verify that the logical volume is active. There may be a delay of a few seconds.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvs\u003C/strong>\u003C/span>\n  LV         VG          Attr       LSize\n  shared_lv1 shared_vg1  -wi-a----- 5.00g\n\n[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvs\u003C/strong>\u003C/span>\n  LV         VG          Attr       LSize\n  shared_lv1 shared_vg1  -wi-a----- 5.00g\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"encrypt_the_logical_volume_and_create_a_crypt_resource\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">7.2.2. Encrypt the logical volume and create a crypt resource\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Prerequisites\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tYou have configured a shared logical volume in a Pacemaker cluster.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn one node in the cluster, create a new file that will contain the crypt key and set the permissions on the file so that it is readable only by root.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>touch /etc/crypt_keyfile\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>chmod 600 /etc/crypt_keyfile\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCreate the crypt key.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>dd if=/dev/urandom bs=4K count=1 of=/etc/crypt_keyfile\u003C/strong>\u003C/span>\n1+0 records in\n1+0 records out\n4096 bytes (4.1 kB, 4.0 KiB) copied, 0.000306202 s, 13.4 MB/s\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>scp /etc/crypt_keyfile root@z2.example.com:/etc/\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tDistribute the crypt keyfile to the other nodes in the cluster, using the \u003Ccode class=\"literal\">-p\u003C/code> parameter to preserve the permissions you set.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>scp -p /etc/crypt_keyfile root@z2.example.com:/etc/\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCreate the encrypted device on the LVM volume where you will configure the encrypted GFS2 file system.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>cryptsetup luksFormat /dev/shared_vg1/shared_lv1 --type luks2 --key-file=/etc/crypt_keyfile\u003C/strong>\u003C/span>\nWARNING!\n========\nThis will overwrite data on /dev/shared_vg1/shared_lv1 irrevocably.\n\nAre you sure? (Type 'yes' in capital letters): YES\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCreate the crypt resource as part of the \u003Ccode class=\"literal\">shared_vg1\u003C/code> volume group.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create crypt --group shared_vg1 ocf:heartbeat:crypt crypt_dev=\"luks_lv1\" crypt_type=luks2 key_file=/etc/crypt_keyfile encrypted_dev=\"/dev/shared_vg1/shared_lv1\"\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Verification\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\t\tEnsure that the crypt resource has created the crypt device, which in this example is \u003Ccode class=\"literal\">/dev/mapper/luks_lv1\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>ls -l /dev/mapper/\u003C/strong>\u003C/span>\n...\nlrwxrwxrwx 1 root root 7 Mar 4 09:52 luks_lv1 -&gt; ../dm-3\n...\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"format_the_encrypted_logical_volume_with_a_gfs2_file_system_and_create_a_file_system_resource_for_the_cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">7.2.3. Format the encrypted logical volume with a GFS2 file system and create a file system resource for the cluster\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Prerequisites\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tYou have encrypted the logical volume and created a crypt resource.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn one node in the cluster, format the volume with a GFS2 file system. One journal is required for each node that mounts the file system. Ensure that you create enough journals for each of the nodes in your cluster. The format of the lock table name is \u003Cspan class=\"emphasis\">\u003Cem>ClusterName:FSName\u003C/em>\u003C/span> where \u003Cspan class=\"emphasis\">\u003Cem>ClusterName\u003C/em>\u003C/span> is the name of the cluster for which the GFS2 file system is being created and \u003Cspan class=\"emphasis\">\u003Cem>FSName\u003C/em>\u003C/span> is the file system name, which must be unique for all \u003Ccode class=\"literal\">lock_dlm\u003C/code> file systems over the cluster.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkfs.gfs2 -j3 -p lock_dlm -t my_cluster:gfs2-demo1 /dev/mapper/luks_lv1\u003C/strong>\u003C/span>\n/dev/mapper/luks_lv1 is a symbolic link to /dev/dm-3\nThis will destroy any data on /dev/dm-3\nAre you sure you want to proceed? [y/n] y\nDiscarding device contents (may take a while on large devices): Done\nAdding journals: Done\nBuilding resource groups: Done\nCreating quota file: Done\nWriting superblock and syncing: Done\nDevice:                    /dev/mapper/luks_lv1\nBlock size:                4096\nDevice size:               4.98 GB (1306624 blocks)\nFilesystem size:           4.98 GB (1306622 blocks)\nJournals:                  3\nJournal size:              16MB\nResource groups:           23\nLocking protocol:          \"lock_dlm\"\nLock table:                \"my_cluster:gfs2-demo1\"\nUUID:                      de263f7b-0f12-4d02-bbb2-56642fade293\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCreate a file system resource to automatically mount the GFS2 file system on all nodes.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tDo not add the file system to the \u003Ccode class=\"literal\">/etc/fstab\u003C/code> file because it will be managed as a Pacemaker cluster resource. Mount options can be specified as part of the resource configuration with \u003Ccode class=\"literal\">options=\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>\u003C/code>. Run the \u003Ccode class=\"literal\">pcs resource describe Filesystem\u003C/code> command for full configuration options.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tThe following command creates the file system resource. This command adds the resource to the resource group that includes the logical volume resource for that file system.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device=\"/dev/mapper/luks_lv1\" directory=\"/mnt/gfs1\" fstype=\"gfs2\" options=noatime op monitor interval=10s on-fail=fence\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Verification\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tVerify that the GFS2 file system is mounted on both nodes of the cluster.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mount | grep gfs2\u003C/strong>\u003C/span>\n/dev/mapper/luks_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)\n\n[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mount | grep gfs2\u003C/strong>\u003C/span>\n/dev/mapper/luks_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCheck the status of the cluster.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status --full\u003C/strong>\u003C/span>\nCluster name: my_cluster\n[...]\n\nFull list of resources:\n\n  smoke-apc      (stonith:fence_apc):    Started z1.example.com\n  Clone Set: locking-clone [locking]\n      Resource Group: locking:0\n          dlm    (ocf::pacemaker:controld):      Started z2.example.com\n          lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com\n      Resource Group: locking:1\n          dlm    (ocf::pacemaker:controld):      Started z1.example.com\n          lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com\n     Started: [ z1.example.com z2.example.com ]\n  Clone Set: shared_vg1-clone [shared_vg1]\n     Resource Group: shared_vg1:0\n             sharedlv1      (ocf::heartbeat:LVM-activate):  Started z2.example.com\n             crypt       (ocf::heartbeat:crypt) Started z2.example.com\n             sharedfs1      (ocf::heartbeat:Filesystem):    Started z2.example.com\n    Resource Group: shared_vg1:1\n             sharedlv1      (ocf::heartbeat:LVM-activate):  Started z1.example.com\n             crypt      (ocf::heartbeat:crypt)  Started z1.example.com\n             sharedfs1      (ocf::heartbeat:Filesystem):    Started z1.example.com\n          Started:  [z1.example.com z2.example.com ]\n...\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv class=\"itemizedlist _additional-resources\">\u003Cp class=\"title\">\u003Cstrong>Additional resources\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist _additional-resources\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_gfs2_file_systems/index\">Configuring GFS2 file systems\u003C/a>\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-active-active-samba-in-a-cluster-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 8. Configuring an active/active Samba server in a Red Hat High Availability cluster\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\tThe Red Hat High Availability Add-On provides support for configuring Samba in an active/active cluster configuration. In the following example, you are configuring an active/active Samba server on a two-node RHEL cluster.\n\t\t\u003C/p>\u003Cp>\n\t\t\tFor information about support policies for Samba, see \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/3278591\">Support Policies for RHEL High Availability - ctdb General Policies\u003C/a> and \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/3252211\">Support Policies for RHEL Resilient Storage - Exporting gfs2 contents via other protocols\u003C/a> on the Red Hat Customer Portal.\n\t\t\u003C/p>\u003Cp>\n\t\t\tTo configure Samba in an active/active cluster:\n\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\tConfigure a GFS2 file system and its associated cluster resources.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tConfigure Samba on the cluster nodes.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tConfigure the Samba cluster resources.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tTest the Samba server you have configured.\n\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003Csection class=\"section\" id=\"proc_configuring-gfs2-for-clustered-samba_adoc-configuring-ha-samba\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">8.1. Configuring a GFS2 file system for a Samba service in a high availability cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tBefore configuring an active/active Samba service in a Pacemaker cluster, configure a GFS2 file system for the cluster.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Prerequisites\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tA two-node Red Hat High Availability cluster with fencing configured for each node\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tShared storage available for each cluster node\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tA subscription to the AppStream channel and the Resilient Storage channel for each cluster node\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tFor information about creating a Pacemaker cluster and configuring fencing for the cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters\">Creating a Red Hat High-Availability cluster with Pacemaker\u003C/a>.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes in the cluster, perform the following initial setup steps.\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tEnable the repository for Resilient Storage that corresponds to your system architecture. For example, to enable the Resilient Storage repository for an x86_64 system, enter the following \u003Ccode class=\"literal\">subscription-manager\u003C/code> command:\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>subscription-manager repos --enable=rhel-9-for-x86_64-resilientstorage-rpms\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tThe Resilient Storage repository is a superset of the High Availability repository. If you enable the Resilient Storage repository, you do not need to also enable the High Availability repository.\n\t\t\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tInstall the \u003Ccode class=\"literal\">lvm2-lockd\u003C/code>, \u003Ccode class=\"literal\">gfs2-utils\u003C/code>, and \u003Ccode class=\"literal\">dlm\u003C/code> packages.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>yum install lvm2-lockd gfs2-utils dlm\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tSet the \u003Ccode class=\"literal\">use_lvmlockd\u003C/code> configuration option in the \u003Ccode class=\"literal\">/etc/lvm/lvm.conf\u003C/code> file to \u003Ccode class=\"literal\">use_lvmlockd=1\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">...\n\nuse_lvmlockd = 1\n\n...\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn one node in the cluster, set the global Pacemaker parameter \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> to \u003Ccode class=\"literal\">freeze\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tBy default, the value of \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> is set to \u003Ccode class=\"literal\">stop\u003C/code>, indicating that once quorum is lost, all the resources on the remaining partition will immediately be stopped. Typically this default is the safest and most optimal option, but unlike most resources, GFS2 requires quorum to function. When quorum is lost both the applications using the GFS2 mounts and the GFS2 mount itself cannot be correctly stopped. Any attempts to stop these resources without quorum will fail which will ultimately result in the entire cluster being fenced every time quorum is lost.\n\t\t\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\t\t\tTo address this situation, set \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> to \u003Ccode class=\"literal\">freeze\u003C/code> when GFS2 is in use. This means that when quorum is lost, the remaining partition will do nothing until quorum is regained.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set no-quorum-policy=freeze\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSet up a \u003Ccode class=\"literal\">dlm\u003C/code> resource. This is a required dependency for configuring a GFS2 file system in a cluster. This example creates the \u003Ccode class=\"literal\">dlm\u003C/code> resource as part of a resource group named \u003Ccode class=\"literal\">locking\u003C/code>. If you have not previously configured fencing for the cluster, this step fails and the \u003Ccode class=\"literal\">pcs status\u003C/code> command displays a resource failure message.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tClone the \u003Ccode class=\"literal\">locking\u003C/code> resource group so that the resource group can be active on both nodes of the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource clone locking interleave=true\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSet up an \u003Ccode class=\"literal\">lvmlockd\u003C/code> resource as part of the \u003Ccode class=\"literal\">locking\u003C/code> resource group.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate a physical volume and a shared volume group on the shared device \u003Ccode class=\"literal\">/dev/vdb\u003C/code>. This example creates the shared volume group \u003Ccode class=\"literal\">csmb_vg\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pvcreate /dev/vdb\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgcreate -Ay --shared csmb_vg /dev/vdb\u003C/strong>\u003C/span>\nVolume group \"csmb_vg\" successfully created\nVG csmb_vg starting dlm lockspace\nStarting locking.  Waiting until locks are ready\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tOn the second node in the cluster:\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf the use of a devices file is enabled with the \u003Ccode class=\"literal\">use_devicesfile = 1\u003C/code> parameter in the \u003Ccode class=\"literal\">lvm.conf\u003C/code> file, add the shared device to the devices file on the second node in the cluster. This feature is enabled by default.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvmdevices --adddev /dev/vdb\u003C/strong>\u003C/span>\u003C/pre>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tStart the lock manager for the shared volume group.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>vgchange --lockstart csmb_vg\u003C/strong>\u003C/span>\n  VG csmb_vg starting dlm lockspace\n  Starting locking.  Waiting until locks are ready...\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn one node in the cluster, create a logical volume and format the volume with a GFS2 file system that will be used exclusively by CTDB for internal locking. Only one such file system is required in a cluster even if your deployment exports multiple shares.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tWhen specifying the lock table name with the \u003Ccode class=\"literal\">-t\u003C/code> option of the \u003Ccode class=\"literal\">mkfs.gfs2\u003C/code> command, ensure that the first component of the \u003Cspan class=\"emphasis\">\u003Cem>clustername:filesystemname\u003C/em>\u003C/span> you specify matches the name of your cluster. In this example, the cluster name is \u003Ccode class=\"literal\">my_cluster\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvcreate -L1G -n ctdb_lv csmb_vg\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkfs.gfs2 -j3 -p lock_dlm -t my_cluster:ctdb /dev/csmb_vg/ctdb_lv\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate a logical volume for each GFS2 file system that will be shared over Samba and format the volume with the GFS2 file system. This example creates a single GFS2 file system and Samba share, but you can create multiple file systems and shares.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>lvcreate -L50G -n csmb_lv1 csmb_vg\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkfs.gfs2 -j3 -p lock_dlm -t my_cluster:csmb1 /dev/csmb_vg/csmb_lv1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSet up \u003Ccode class=\"literal\">LVM_Activate\u003C/code> resources to ensure that the required shared volumes are activated. This example creates the \u003Ccode class=\"literal\">LVM_Activate\u003C/code> resources as part of a resource group \u003Ccode class=\"literal\">shared_vg\u003C/code>, and then clones that resource group so that it runs on all nodes in the cluster.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate the resources as disabled so they do not start automatically before you have configured the necessary order constraints.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create --disabled --group shared_vg ctdb_lv ocf:heartbeat:LVM-activate lvname=ctdb_lv vgname=csmb_vg activation_mode=shared vg_access_mode=lvmlockd\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create --disabled --group shared_vg csmb_lv1 ocf:heartbeat:LVM-activate lvname=csmb_lv1 vgname=csmb_vg activation_mode=shared vg_access_mode=lvmlockd\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource clone shared_vg interleave=true\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tConfigure an ordering constraint to start all members of the \u003Ccode class=\"literal\">locking\u003C/code> resource group before the members of the \u003Ccode class=\"literal\">shared_vg\u003C/code> resource group.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order start locking-clone then shared_vg-clone\u003C/strong>\u003C/span>\nAdding locking-clone shared_vg-clone (kind: Mandatory) (Options: first-action=start then-action=start)\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tEnable the \u003Ccode class=\"literal\">LVM-activate\u003C/code> resources.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource enable ctdb_lv csmb_lv1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn one node in the cluster, perform the following steps to create the \u003Ccode class=\"literal\">Filesystem\u003C/code> resources you require.\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate \u003Ccode class=\"literal\">Filesystem\u003C/code> resources as cloned resources, using the GFS2 file systems you previously configured on your LVM volumes. This configures Pacemaker to mount and manage file systems.\n\t\t\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\t\tYou should not add the file system to the \u003Ccode class=\"literal\">/etc/fstab\u003C/code> file because it will be managed as a Pacemaker cluster resource. You can specify mount options as part of the resource configuration with \u003Ccode class=\"literal\">options=\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>\u003C/code>. Run the \u003Ccode class=\"literal command\">pcs resource describe Filesystem\u003C/code> command to display the full configuration options.\n\t\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create ctdb_fs Filesystem device=\"/dev/csmb_vg/ctdb_lv\" directory=\"/mnt/ctdb\" fstype=\"gfs2\" op monitor interval=10s on-fail=fence clone interleave=true\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create csmb_fs1 Filesystem device=\"/dev/csmb_vg/csmb_lv1\" directory=\"/srv/samba/share1\" fstype=\"gfs2\" op monitor interval=10s on-fail=fence clone interleave=true\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tConfigure ordering constraints to ensure that Pacemaker mounts the file systems after the shared volume group \u003Ccode class=\"literal\">shared_vg\u003C/code> has started.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order start shared_vg-clone then ctdb_fs-clone\u003C/strong>\u003C/span>\nAdding shared_vg-clone ctdb_fs-clone (kind: Mandatory) (Options: first-action=start then-action=start)\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order start shared_vg-clone then csmb_fs1-clone\u003C/strong>\u003C/span>\nAdding shared_vg-clone csmb_fs1-clone (kind: Mandatory) (Options: first-action=start then-action=start)\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-samba-for-ha-cluster_adoc-configuring-ha-samba\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">8.2. Configuring Samba in a high availability cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tTo configure a Samba service in a Pacemaker cluster, configure the service on all nodes in the cluster.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Prerequisites\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tA two-node Red Hat High Availability cluster configured with a GFS2 file system, as described in \u003Ca class=\"link\" href=\"#proc_configuring-gfs2-for-clustered-samba_adoc-configuring-ha-samba\" title=\"8.1. Configuring a GFS2 file system for a Samba service in a high availability cluster\">Configuring a GFS2 file system for a Samba service in a high availability cluster\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tA public directory created on your GFS2 file system to use for the Samba share. In this example, the directory is \u003Ccode class=\"literal\">/srv/samba/share1\u003C/code>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tPublic virtual IP addresses that can be used to access the Samba share exported by this cluster.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes in the cluster, configure the Samba service and set up a share definition:\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tInstall the Samba and CTDB packages.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf -y install samba ctdb cifs-utils samba-winbind\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tEnsure that the \u003Ccode class=\"literal\">ctdb\u003C/code>, \u003Ccode class=\"literal\">smb\u003C/code>, \u003Ccode class=\"literal\">nmb\u003C/code>, and \u003Ccode class=\"literal\">winbind\u003C/code> services are not running and do not start at boot.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl disable --now ctdb smb nmb winbind\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tIn the \u003Ccode class=\"literal\">/etc/samba/smb.conf\u003C/code> file, configure the Samba service and set up the share definition, as in the following example for a standalone server with one share.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[global]\n    netbios name = linuxserver\n    workgroup = WORKGROUP\n    security = user\n    clustering = yes\n[share1]\n    path = /srv/samba/share1\n    read only = no\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tVerify the \u003Ccode class=\"literal\">/etc/samba/smb.conf\u003C/code> file.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>testparm\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes in the cluster, configure CTDB:\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate the \u003Ccode class=\"literal\">/etc/ctdb/nodes\u003C/code> file and add the IP addresses of the cluster nodes, as in this example nodes file.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">192.0.2.11\n192.0.2.12\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate the \u003Ccode class=\"literal\">/etc/ctdb/public_addresses\u003C/code> file and add the IP addresses and network device names of the cluster’s public interfaces to the file. When assigning IP addresses in the \u003Ccode class=\"literal\">public_addresses\u003C/code> file, ensure that these addresses are not in use and that those addresses are routable from the intended client. The second field in each entry of the \u003Ccode class=\"literal\">/etc/ctdb/public_addresses\u003C/code> file is the interface to use on the cluster machines for the corresponding public address. In this example \u003Ccode class=\"literal\">public_addresses\u003C/code> file, the interface \u003Ccode class=\"literal\">enp1s0\u003C/code> is used for all the public addresses.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">192.0.2.201/24 enp1s0\n192.0.2.202/24 enp1s0\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tThe public interfaces of the cluster are the ones that clients use to access Samba from their network. For load balancing purposes, add an A record for each public IP address of the cluster to your DNS zone. Each of these records must resolve to the same hostname. Clients use the hostname to access Samba and DNS distributes the clients to the different nodes of the cluster.\n\t\t\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tIf you are running the \u003Ccode class=\"literal\">firewalld\u003C/code> service, enable the ports that are required by the \u003Ccode class=\"literal\">ctdb\u003C/code> and \u003Ccode class=\"literal\">samba\u003C/code> services.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --add-service=ctdb --add-service=samba --permanent\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --reload\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn one node in the cluster, update the SELinux contexts:\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tUpdate the SELinux contexts on the GFS2 share.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>semanage fcontext -at ctdbd_var_run_t -s system_u \"/mnt/ctdb(/.\u003C/strong>\u003C/span>)?\"\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>restorecon -Rv /mnt/ctdb\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tUpdate the SELinux context on the directory shared in Samba.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>semanage fcontext -at samba_share_t -s system_u \"/srv/samba/share1(/.\u003C/strong>\u003C/span>)?\"\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>restorecon -Rv /srv/samba/share1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Additional resources\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tFor further information about configuring Samba as a standalone server, as in this example, see the \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/deploying_different_types_of_servers/assembly_using-samba-as-a-server_deploying-different-types-of-servers\">Using Samba as a server\u003C/a> chapter of \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/configuring_and_using_network_file_services/index\">Configuring and using network file services\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_networking_infrastructure_services/assembly_setting-up-and-configuring-a-bind-dns-server_networking-infrastructure-services#proc_setting-up-a-forward-zone-on-a-bind-primary-server_assembly_configuring-zones-on-a-bind-dns-server\">Setting up a forward zone on a BIND primary server\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-samba-cluster-resources_adoc-configuring-ha-samba\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">8.3. Configuring Samba cluster resources\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tAfter configuring a Samba service on both nodes of a two-node high availability cluster, configure the Samba cluster resources for the cluster.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Prerequisites\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tA two-node Red Hat High Availability cluster configured with a GFS2 file system, as described in \u003Ca class=\"link\" href=\"#proc_configuring-gfs2-for-clustered-samba_adoc-configuring-ha-samba\" title=\"8.1. Configuring a GFS2 file system for a Samba service in a high availability cluster\">Configuring a GFS2 file system for a Samba service in a high availability cluster\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tSamba service configured on both cluster nodes, as described in \u003Ca class=\"link\" href=\"#proc_configuring-samba-for-ha-cluster_adoc-configuring-ha-samba\" title=\"8.2. Configuring Samba in a high availability cluster\">Configuring Samba in a high availability cluster\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn one node in the cluster, configure the Samba cluster resources:\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate the CTDB resource, in group \u003Ccode class=\"literal\">samba-group\u003C/code>. The CTDB resource agent uses the \u003Ccode class=\"literal\">ctdb_*\u003C/code> options specified with the \u003Ccode class=\"literal\">pcs\u003C/code> command to create the CTDB configuration file. Create the resource as disabled so it does not start automatically before you have configured the necessary order constraints.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create --disabled ctdb --group samba-group ocf:heartbeat:CTDB ctdb_recovery_lock=/mnt/ctdb/ctdb.lock ctdb_dbdir=/var/lib/ctdb ctdb_logfile=/var/log/ctdb.log op monitor interval=10 timeout=30 op start timeout=90 op stop timeout=100\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tClone the \u003Ccode class=\"literal\">samba-group\u003C/code> resource group.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource clone samba-group\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate ordering constraints to ensure that all \u003Ccode class=\"literal\">Filesystem\u003C/code> resources are running before the resources in \u003Ccode class=\"literal\">samba-group\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order start ctdb_fs-clone then samba-group-clone\u003C/strong>\u003C/span>\n[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order start csmb_fs1-clone then samba-group-clone\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCreate the \u003Ccode class=\"literal\">samba\u003C/code> resource in the resource group \u003Ccode class=\"literal\">samba-group\u003C/code>. This creates an implicit ordering constraint between CTDB and Samba, based on the order they are added.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create samba --group samba-group systemd:smb\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tEnable the \u003Ccode class=\"literal\">ctdb\u003C/code> and \u003Ccode class=\"literal\">samba\u003C/code> resources.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource enable ctdb samba\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tCheck that all the services have started successfully.\n\t\t\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\t\tIt can take a couple of minutes for CTDB to start Samba, export the shares, and stabilize. If you check the cluster status before this process has completed, you may see that the \u003Ccode class=\"literal\">samba\u003C/code> services are not yet running.\n\t\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\n\n...\n\nFull List of Resources:\n  * fence-z1   (stonith:fence_xvm): Started z1.example.com\n  * fence-z2   (stonith:fence_xvm): Started z2.example.com\n  * Clone Set: locking-clone [locking]:\n\t* Started: [ z1.example.com z2.example.com ]\n  * Clone Set: shared_vg-clone [shared_vg]:\n\t* Started: [ z1.example.com z2.example.com ]\n  * Clone Set: ctdb_fs-clone [ctdb_fs]:\n\t* Started: [ z1.example.com z2.example.com ]\n  * Clone Set: csmb_fs1-clone [csmb_fs1]:\n\t* Started: [ z1.example.com z2.example.com ]\n   * Clone Set: samba-group-clone [samba-group]:\n\t* Started: [ z1.example.com z2.example.com ]\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn both nodes in the cluster, add a local user for the test share directory.\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tAdd the user.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>useradd -M -s /sbin/nologin example_user\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tSet a password for the user.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>passwd example_user\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tSet an SMB password for the user.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>smbpasswd -a example_user\u003C/strong>\u003C/span>\nNew SMB password:\nRetype new SMB password:\nAdded user example_user\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tActivate the user in the Samba database.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>smbpasswd -e example_user\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tUpdate the file ownership and permissions on the GFS2 share for the Samba user.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>chown example_user:users /srv/samba/share1/\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>chmod 755 /srv/samba/share1/\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_verifying-clustered-samba-configuration.adoc-configuring-ha-samba\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">8.4. Verifying clustered Samba configuration\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tIf your clustered Samba configuration was successful, you are able to mount the Samba share. After mounting the share, you can test for Samba recovery if the cluster node that is exporting the Samba share becomes unavailable.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn a system that has access to one or more of the public IP addresses configured in the \u003Ccode class=\"literal\">/etc/ctdb/public_addresses\u003C/code> file on the cluster nodes, mount the Samba share using one of these public IP addresses.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@testmount ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mkdir /mnt/sambashare\u003C/strong>\u003C/span>\n[root@testmount ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mount -t cifs -o user=example_user //192.0.2.201/share1 /mnt/sambashare\u003C/strong>\u003C/span>\nPassword for example_user@//192.0.2.201/public: XXXXXXX\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tVerify that the file system is mounted.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@testmount ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>mount | grep /mnt/sambashare\u003C/strong>\u003C/span>\n//192.0.2.201/public on /mnt/sambashare type cifs (rw,relatime,vers=1.0,cache=strict,username=example_user,domain=LINUXSERVER,uid=0,noforceuid,gid=0,noforcegid,addr=192.0.2.201,unix,posixpaths,serverino,mapposix,acl,rsize=1048576,wsize=65536,echo_interval=60,actimeo=1,user=example_user)\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tVerify that you can create a file on the mounted file system.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@testmount ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>touch /mnt/sambashare/testfile1\u003C/strong>\u003C/span>\n[root@testmount ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>ls /mnt/sambashare\u003C/strong>\u003C/span>\ntestfile1\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tDetermine which cluster node is exporting the Samba share:\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tOn each cluster node, display the IP addresses assigned to the interface specified in the \u003Ccode class=\"literal\">public_addresses\u003C/code> file. The following commands display the IPv4 addresses assigned to the \u003Ccode class=\"literal\">enp1s0\u003C/code> interface on each node.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>ip -4 addr show enp1s0 | grep inet\u003C/strong>\u003C/span>\n     inet 192.0.2.11/24 brd 192.0.2.255 scope global dynamic noprefixroute enp1s0\n     inet 192.0.2.201/24 brd 192.0.2.255 scope global secondary enp1s0\n\n[root@z2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>ip -4 addr show enp1s0 | grep inet\u003C/strong>\u003C/span>\n     inet 192.0.2.12/24 brd 192.0.2.255 scope global dynamic noprefixroute enp1s0\n     inet 192.0.2.202/24 brd 192.0.2.255 scope global secondary enp1s0\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tIn the output of the \u003Ccode class=\"literal\">ip\u003C/code> command, find the node with the IP address you specified with the \u003Ccode class=\"literal\">mount\u003C/code> command when you mounted the share.\n\t\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tIn this example, the IP address specified in the mount command is 192.0.2.201. The output of the \u003Ccode class=\"literal\">ip\u003C/code> command shows that the IP address 192.0.2.201 is assigned to \u003Ccode class=\"literal\">z1.example.com\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tPut the node exporting the Samba share in \u003Ccode class=\"literal\">standby\u003C/code> mode, which will cause the node to be unable to host any cluster resources.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node standby z1.example.com\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tFrom the system on which you mounted the file system, verify that you can still create a file on the file system.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@testmount ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>touch /mnt/sambashare/testfile2\u003C/strong>\u003C/span>\n[root@testmount ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>ls /mnt/sambashare\u003C/strong>\u003C/span>\ntestfile1  testfile2\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tDelete the files you have created to verify that the file system has successfully mounted. If you no longer require the file system to be mounted, unmount it at this point.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@testmount ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>rm /mnt/sambashare/testfile1 /mnt/sambashare/testfile2\u003C/strong>\u003C/span>\nrm: remove regular empty file '/mnt/sambashare/testfile1'? \u003Cspan class=\"strong strong\">\u003Cstrong>y\u003C/strong>\u003C/span>\nrm: remove regular empty file '/mnt/sambashare/testfile1'? \u003Cspan class=\"strong strong\">\u003Cstrong>y\u003C/strong>\u003C/span>\n[root@testmount ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>umount /mnt/sambashare\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tFrom one of the cluster nodes, restore cluster services to the node that you previously put into standby mode. This will not necessarily move the service back to that node.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node unstandby z1.example.com\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 9. Getting started with the pcsd Web UI\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tThe \u003Ccode class=\"literal\">pcsd\u003C/code> Web UI is a graphical user interface to create and configure Pacemaker/Corosync clusters.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_setting-up-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">9.1. Setting up the pcsd Web UI\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tSet up your system to use the \u003Ccode class=\"literal\">pcsd\u003C/code> Web UI to configure a cluster with the following procedure.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cp class=\"title\">\u003Cstrong>Prerequisites\u003C/strong>\u003C/p>\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe Pacemaker configuration tools are installed.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tYour system is set up for cluster configuration.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tFor instructions on installing cluster software and setting up your system for cluster configuration, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters#proc_installing-cluster-software-creating-high-availability-cluster\">Installing cluster software\u003C/a>.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn any system, open a browser to the following URL, specifying one of the nodes of the cluster (note that this uses the \u003Ccode class=\"literal\">https\u003C/code> protocol). This brings up the \u003Ccode class=\"literal command\">pcsd\u003C/code> Web UI login screen.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">https://\u003Cspan class=\"emphasis\">\u003Cem>nodename\u003C/em>\u003C/span>:2224\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tLog in as user \u003Ccode class=\"literal\">hacluster\u003C/code>. This brings up the \u003Ccode class=\"literal\">Clusters\u003C/code> page.\n\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-ha-pcsd-web-ui-getting-started-with-the-pcsd-web-ui\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">9.2. Configuring a high availability pcsd Web UI\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen you use the \u003Ccode class=\"literal\">pcsd\u003C/code> Web UI, you connect to one of the nodes of the cluster to display the cluster management pages. If the node to which you are connecting goes down or becomes unavailable, you can reconnect to the cluster by opening your browser to a URL that specifies a different node of the cluster. It is possible, however, to configure the \u003Ccode class=\"literal\">pcsd\u003C/code> Web UI itself for high availability, in which case you can continue to manage the cluster without entering a new URL.\n\t\t\t\u003C/p>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\tTo configure the \u003Ccode class=\"literal\">pcsd\u003C/code> Web UI for high availability, perform the following steps.\n\t\t\t\t\u003C/p>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tEnsure that the \u003Ccode class=\"literal\">pcsd\u003C/code> certificates are synced across the nodes of the cluster by setting \u003Ccode class=\"literal\">PCSD_SSL_CERT_SYNC_ENABLED\u003C/code> to \u003Ccode class=\"literal\">true\u003C/code> in the \u003Ccode class=\"literal\">/etc/sysconfig/pcsd\u003C/code> configuration file. Enabling certificate syncing causes \u003Ccode class=\"literal\">pcsd\u003C/code> to sync the certificates for the cluster setup and node add commands. \u003Ccode class=\"literal\">PCSD_SSL_CERT_SYNC_ENABLED\u003C/code> is set to \u003Ccode class=\"literal\">false\u003C/code> by default.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tCreate an \u003Ccode class=\"literal\">IPaddr2\u003C/code> cluster resource, which is a floating IP address that you will use to connect to the \u003Ccode class=\"literal\">pcsd\u003C/code> Web UI. The IP address must not be one already associated with a physical node. If the \u003Ccode class=\"literal\">IPaddr2\u003C/code> resource’s NIC device is not specified, the floating IP must reside on the same network as one of the node’s statically assigned IP addresses, otherwise the NIC device to assign the floating IP address cannot be properly detected.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate custom SSL certificates for use with \u003Ccode class=\"literal\">pcsd\u003C/code> and ensure that they are valid for the addresses of the nodes used to connect to the \u003Ccode class=\"literal\">pcsd\u003C/code> Web UI.\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\tTo create custom SSL certificates, you can use either wildcard certificates or you can use the Subject Alternative Name certificate extension. For information about the Red Hat Certificate System, see the \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_certificate_system/10/html/administration_guide/index\">Red Hat Certificate System Administration Guide\u003C/a>.\n\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\tInstall the custom certificates for \u003Ccode class=\"literal\">pcsd\u003C/code> with the \u003Ccode class=\"literal\">pcs pcsd certkey\u003C/code> command.\n\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\tSync the \u003Ccode class=\"literal\">pcsd\u003C/code> certificates to all nodes in the cluster with the \u003Ccode class=\"literal\">pcs pcsd sync-certificates\u003C/code> command.\n\t\t\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tConnect to the \u003Ccode class=\"literal\">pcsd\u003C/code> Web UI using the floating IP address you configured as a cluster resource.\n\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tEven when you configure the \u003Ccode class=\"literal\">pcsd\u003C/code> Web UI for high availability, you will be asked to log in again when the node to which you are connecting goes down.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-fencing-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 10. Configuring fencing in a Red Hat High Availability cluster\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\tA node that is unresponsive may still be accessing data. The only way to be certain that your data is safe is to fence the node using STONITH. STONITH is an acronym for \"Shoot The Other Node In The Head\" and it protects your data from being corrupted by rogue nodes or concurrent access. Using STONITH, you can be certain that a node is truly offline before allowing the data to be accessed from another node.\n\t\t\u003C/p>\u003Cp>\n\t\t\tSTONITH also has a role to play in the event that a clustered service cannot be stopped. In this case, the cluster uses STONITH to force the whole node offline, thereby making it safe to start the service elsewhere.\n\t\t\u003C/p>\u003Cp>\n\t\t\tFor more complete general information about fencing and its importance in a Red Hat High Availability cluster, see the Red Hat Knowledgebase solution \u003Ca class=\"link\" href=\"https://access.redhat.com/solutions/15575\">Fencing in a Red Hat High Availability Cluster\u003C/a>.\n\t\t\u003C/p>\u003Cp>\n\t\t\tYou implement STONITH in a Pacemaker cluster by configuring fence devices for the nodes of the cluster.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_displaying-fence-agents-configuring-fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.1. Displaying available fence agents and their options\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tThe following commands can be used to view available fencing agents and the available options for specific fencing agents.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tYour system’s hardware determines the type of fencing device to use for your cluster. For information about supported platforms and architectures and the different fencing devices, see the \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/2912891#platforms\">Cluster Platforms and Architectures\u003C/a> section of the article \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/2912891\">Support Policies for RHEL High Availability Clusters\u003C/a>.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tRun the following command to list all available fencing agents. When you specify a filter, this command displays only the fencing agents that match the filter.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs stonith list [\u003Cspan class=\"emphasis\">\u003Cem>filter\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\tRun the following command to display the options for the specified fencing agent.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs stonith describe [\u003Cspan class=\"emphasis\">\u003Cem>stonith_agent\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\tFor example, the following command displays the options for the fence agent for APC over telnet/SSH.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith describe fence_apc\u003C/strong>\u003C/span>\nStonith options for: fence_apc\n  ipaddr (required): IP Address or Hostname\n  login (required): Login Name\n  passwd: Login password or passphrase\n  passwd_script: Script to retrieve password\n  cmd_prompt: Force command prompt\n  secure: SSH connection\n  port (required): Physical plug number or name of virtual machine\n  identity_file: Identity file for ssh\n  switch: Physical switch number on device\n  inet4_only: Forces agent to use IPv4 addresses only\n  inet6_only: Forces agent to use IPv6 addresses only\n  ipport: TCP port to use for connection with device\n  action (required): Fencing Action\n  verbose: Verbose mode\n  debug: Write debug information to given file\n  version: Display version information and exit\n  help: Display help and exit\n  separator: Separator for CSV created by operation list\n  power_timeout: Test X seconds for status change after ON/OFF\n  shell_timeout: Wait X seconds for cmd prompt after issuing command\n  login_timeout: Wait X seconds for cmd prompt after login\n  power_wait: Wait X seconds after issuing ON/OFF\n  delay: Wait X seconds before fencing is started\n  retry_on: Count of attempts to retry power on\u003C/pre>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tFor fence agents that provide a \u003Ccode class=\"literal\">method\u003C/code> option, with the exception of the \u003Ccode class=\"literal\">fence_sbd\u003C/code> agent a value of \u003Ccode class=\"literal\">cycle\u003C/code> is unsupported and should not be specified, as it may cause data corruption. Even for \u003Ccode class=\"literal\">fence_sbd\u003C/code>, however. you should not specify a method and instead use the default value.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/section>\u003Csection class=\"section\" id=\"proc_creating-fence-devices-configuring-fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.2. Creating a fence device\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe format for the command to create a fence device is as follows. For a listing of the available fence device creation options, see the \u003Ccode class=\"literal\">pcs stonith -h\u003C/code> display.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs stonith create \u003Cspan class=\"emphasis\">\u003Cem>stonith_id\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>stonith_device_type\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>stonith_device_options\u003C/em>\u003C/span>] [op  \u003Cspan class=\"emphasis\">\u003Cem>operation_action\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>operation_options\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\tThe following command creates a single fencing device for a single node.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith create MyStonith fence_virt pcmk_host_list=f1 op monitor interval=30s\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tSome fence devices can fence only a single node, while other devices can fence multiple nodes. The parameters you specify when you create a fencing device depend on what your fencing device supports and requires.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tSome fence devices can automatically determine what nodes they can fence.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tYou can use the \u003Ccode class=\"literal\">pcmk_host_list\u003C/code> parameter when creating a fencing device to specify all of the machines that are controlled by that fencing device.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tSome fence devices require a mapping of host names to the specifications that the fence device understands. You can map host names with the \u003Ccode class=\"literal\">pcmk_host_map\u003C/code> parameter when creating a fencing device.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tFor information about the \u003Ccode class=\"literal\">pcmk_host_list\u003C/code> and \u003Ccode class=\"literal\">pcmk_host_map\u003C/code> parameters, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters#ref_general-fence-device-properties-configuring-fencing\">General properties of fencing devices\u003C/a>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tAfter configuring a fence device, it is imperative that you test the device to ensure that it is working correctly. For information about testing a fence device, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters#proc_testing-fence-devices-configuring-fencing\">Testing a fence device\u003C/a>.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"ref_general-fence-device-properties-configuring-fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.3. General properties of fencing devices\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThere are many general properties you can set for fencing devices, as well as various cluster properties that determine fencing behavior.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tAny cluster node can fence any other cluster node with any fence device, regardless of whether the fence resource is started or stopped. Whether the resource is started controls only the recurring monitor for the device, not whether it can be used, with the following exceptions:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tYou can disable a fencing device by running the \u003Ccode class=\"literal command\">pcs stonith disable \u003Cspan class=\"emphasis\">\u003Cem>stonith_id\u003C/em>\u003C/span>\u003C/code> command. This will prevent any node from using that device.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tTo prevent a specific node from using a fencing device, you can configure location constraints for the fencing resource with the \u003Ccode class=\"literal command\">pcs constraint location …​ avoids\u003C/code> command.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tConfiguring \u003Ccode class=\"literal\">stonith-enabled=false\u003C/code> will disable fencing altogether. Note, however, that Red Hat does not support clusters when fencing is disabled, as it is not suitable for a production environment.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tThe following table describes the general properties you can set for fencing devices.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-fencedevice-props-HAAR\">\u003Ctable class=\"gt-4-cols lt-7-rows\">\u003Ccaption>Table 10.1. General Properties of Fencing Devices\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 25%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 13%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 25%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 38%; \" class=\"col_4\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686147652064\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686147650976\" scope=\"col\">Type\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686147649888\" scope=\"col\">Default\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686147648800\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686147652064\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_host_map\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686147650976\"> \u003Cp>\n\t\t\t\t\t\t\t\tstring\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686147649888\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686147648800\"> \u003Cp>\n\t\t\t\t\t\t\t\tA mapping of host names to port numbers for devices that do not support host names. For example: \u003Ccode class=\"literal\">node1:1;node2:2,3\u003C/code> tells the cluster to use port 1 for node1 and ports 2 and 3 for node2. the \u003Ccode class=\"literal\">pcmk_host_map\u003C/code> property supports special characters inside \u003Ccode class=\"literal\">pcmk_host_map\u003C/code> values using a backslash in front of the value. For example, you can specify \u003Ccode class=\"literal\">pcmk_host_map=\"node3:plug\\ 1\"\u003C/code> to include a space in the host alias.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686147652064\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_host_list\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686147650976\"> \u003Cp>\n\t\t\t\t\t\t\t\tstring\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686147649888\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686147648800\"> \u003Cp>\n\t\t\t\t\t\t\t\tA list of machines controlled by this device (Optional unless \u003Ccode class=\"literal\">pcmk_host_check=static-list\u003C/code>).\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686147652064\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_host_check\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686147650976\"> \u003Cp>\n\t\t\t\t\t\t\t\tstring\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686147649888\"> \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">static-list\u003C/code> if either \u003Ccode class=\"literal\">pcmk_host_list\u003C/code> or \u003Ccode class=\"literal\">pcmk_host_map\u003C/code> is set\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* Otherwise, \u003Ccode class=\"literal\">dynamic-list\u003C/code> if the fence device supports the \u003Ccode class=\"literal\">list\u003C/code> action\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* Otherwise, \u003Ccode class=\"literal\">status\u003C/code> if the fence device supports the \u003Ccode class=\"literal\">status\u003C/code> action\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t*Otherwise, \u003Ccode class=\"literal\">none\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686147648800\"> \u003Cp>\n\t\t\t\t\t\t\t\tHow to determine which machines are controlled by the device. Allowed values: \u003Ccode class=\"literal\">dynamic-list\u003C/code> (query the device), \u003Ccode class=\"literal\">static-list\u003C/code> (check the \u003Ccode class=\"literal\">pcmk_host_list\u003C/code> attribute), none (assume every device can fence every machine)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tThe following table summarizes additional properties you can set for fencing devices. Note that these properties are for advanced use only.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-fencepropsadvanced-HAAR\">\u003Ctable class=\"gt-4-cols lt-7-rows\">\u003Ccaption>Table 10.2. Advanced Properties of Fencing Devices\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 29%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 14%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 14%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 43%; \" class=\"col_4\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686150721760\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686150720672\" scope=\"col\">Type\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686150719584\" scope=\"col\">Default\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686150718496\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_host_argument\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tstring\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\tport\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tAn alternate parameter to supply instead of port. Some devices do not support the standard port parameter or may provide additional ones. Use this to specify an alternate, device-specific parameter that should indicate the machine to be fenced. A value of \u003Ccode class=\"literal\">none\u003C/code> can be used to tell the cluster not to supply any additional parameters.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_reboot_action\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tstring\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\treboot\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tAn alternate command to run instead of \u003Ccode class=\"literal\">reboot\u003C/code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the reboot action.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_reboot_timeout\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\ttime\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t60s\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tSpecify an alternate timeout to use for reboot actions instead of \u003Ccode class=\"literal\">stonith-timeout\u003C/code>. Some devices need much more/less time to complete than normal. Use this to specify an alternate, device-specific, timeout for reboot actions.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_reboot_retries\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tinteger\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t2\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe maximum number of times to retry the \u003Ccode class=\"literal\">reboot\u003C/code> command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries reboot actions before giving up.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_off_action\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tstring\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\toff\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tAn alternate command to run instead of \u003Ccode class=\"literal\">off\u003C/code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the off action.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_off_timeout\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\ttime\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t60s\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tSpecify an alternate timeout to use for off actions instead of \u003Ccode class=\"literal\">stonith-timeout\u003C/code>. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for off actions.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_off_retries\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tinteger\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t2\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe maximum number of times to retry the off command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries off actions before giving up.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_list_action\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tstring\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\tlist\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tAn alternate command to run instead of \u003Ccode class=\"literal\">list\u003C/code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the list action.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_list_timeout\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\ttime\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t60s\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tSpecify an alternate timeout to use for list actions. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for list actions.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_list_retries\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tinteger\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t2\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe maximum number of times to retry the \u003Ccode class=\"literal\">list\u003C/code> command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries list actions before giving up.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_monitor_action\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tstring\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\tmonitor\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tAn alternate command to run instead of \u003Ccode class=\"literal\">monitor\u003C/code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the monitor action.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_monitor_timeout\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\ttime\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t60s\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tSpecify an alternate timeout to use for monitor actions instead of \u003Ccode class=\"literal\">stonith-timeout\u003C/code>. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for monitor actions.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_monitor_retries\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tinteger\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t2\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe maximum number of times to retry the \u003Ccode class=\"literal\">monitor\u003C/code> command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries monitor actions before giving up.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_status_action\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tstring\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\tstatus\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tAn alternate command to run instead of \u003Ccode class=\"literal\">status\u003C/code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the status action.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_status_timeout\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\ttime\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t60s\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tSpecify an alternate timeout to use for status actions instead of \u003Ccode class=\"literal\">stonith-timeout\u003C/code>. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for status actions.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_status_retries\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tinteger\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t2\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe maximum number of times to retry the status command within the timeout period. Some devices do not support multiple connections. Operations may fail if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries status actions before giving up.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_delay_base\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tstring\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t0s\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tEnables a base delay for fencing actions and specifies a base delay value. You can specify different values for different nodes with the \u003Ccode class=\"literal\">pcmk_delay_base\u003C/code> parameter. For general information about fencing delay parameters and their interactions, see \u003Ca class=\"link\" href=\"#ref_fence-delays-configuring-fencing\" title=\"10.4. Fencing delays\">Fencing delays\u003C/a>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_delay_max\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\ttime\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t0s\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tEnables a random delay for fencing actions and specifies the maximum delay, which is the maximum value of the combined base delay and random delay. For example, if the base delay is 3 and \u003Ccode class=\"literal\">pcmk_delay_max\u003C/code> is 10, the random delay will be between 3 and 10. For general information about fencing delay parameters and their interactions, see \u003Ca class=\"link\" href=\"#ref_fence-delays-configuring-fencing\" title=\"10.4. Fencing delays\">Fencing delays\u003C/a>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_action_limit\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tinteger\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t1\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe maximum number of actions that can be performed in parallel on this device. The cluster property \u003Ccode class=\"literal\">concurrent-fencing=true\u003C/code> needs to be configured first (this is the default value). A value of -1 is unlimited.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_on_action\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tstring\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\ton\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tFor advanced use only: An alternate command to run instead of \u003Ccode class=\"literal\">on\u003C/code>. Some devices do not support the standard commands or may provide additional ones. Use this to specify an alternate, device-specific, command that implements the \u003Ccode class=\"literal\">on\u003C/code> action.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_on_timeout\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\ttime\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t60s\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tFor advanced use only: Specify an alternate timeout to use for \u003Ccode class=\"literal\">on\u003C/code> actions instead of \u003Ccode class=\"literal\">stonith-timeout\u003C/code>. Some devices need much more or much less time to complete than normal. Use this to specify an alternate, device-specific, timeout for \u003Ccode class=\"literal\">on\u003C/code> actions.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150721760\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pcmk_on_retries\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150720672\"> \u003Cp>\n\t\t\t\t\t\t\t\tinteger\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150719584\"> \u003Cp>\n\t\t\t\t\t\t\t\t2\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686150718496\"> \u003Cp>\n\t\t\t\t\t\t\t\tFor advanced use only: The maximum number of times to retry the \u003Ccode class=\"literal\">on\u003C/code> command within the timeout period. Some devices do not support multiple connections. Operations may \u003Ccode class=\"literal\">fail\u003C/code> if the device is busy with another task so Pacemaker will automatically retry the operation, if there is time remaining. Use this option to alter the number of times Pacemaker retries \u003Ccode class=\"literal\">on\u003C/code> actions before giving up.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tIn addition to the properties you can set for individual fence devices, there are also cluster properties you can set that determine fencing behavior, as described in the following table.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-clusterfenceprops-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 10.3. Cluster Properties that Determine Fencing Behavior\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 29%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 29%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 43%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686145198016\" scope=\"col\">Option\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686145196928\" scope=\"col\">Default\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686145195840\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145198016\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">stonith-enabled\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145196928\"> \u003Cp>\n\t\t\t\t\t\t\t\ttrue\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145195840\"> \u003Cp>\n\t\t\t\t\t\t\t\tIndicates that failed nodes and nodes with resources that cannot be stopped should be fenced. Protecting your data requires that you set this \u003Ccode class=\"literal\">true\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tIf \u003Ccode class=\"literal\">true\u003C/code>, or unset, the cluster will refuse to start resources unless one or more STONITH resources have been configured also.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tRed Hat only supports clusters with this value set to \u003Ccode class=\"literal\">true\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145198016\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">stonith-action\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145196928\"> \u003Cp>\n\t\t\t\t\t\t\t\treboot\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145195840\"> \u003Cp>\n\t\t\t\t\t\t\t\tAction to send to fencing device. Allowed values: \u003Ccode class=\"literal\">reboot\u003C/code>, \u003Ccode class=\"literal\">off\u003C/code>. The value \u003Ccode class=\"literal\">poweroff\u003C/code> is also allowed, but is only used for legacy devices.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145198016\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">stonith-timeout\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145196928\"> \u003Cp>\n\t\t\t\t\t\t\t\t60s\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145195840\"> \u003Cp>\n\t\t\t\t\t\t\t\tHow long to wait for a STONITH action to complete.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145198016\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">stonith-max-attempts\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145196928\"> \u003Cp>\n\t\t\t\t\t\t\t\t10\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145195840\"> \u003Cp>\n\t\t\t\t\t\t\t\tHow many times fencing can fail for a target before the cluster will no longer immediately re-attempt it.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145198016\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">stonith-watchdog-timeout\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145196928\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145195840\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe maximum time to wait until a node can be assumed to have been killed by the hardware watchdog. It is recommended that this value be set to twice the value of the hardware watchdog timeout. This option is needed only if watchdog-only SBD configuration is used for fencing.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145198016\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">concurrent-fencing\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145196928\"> \u003Cp>\n\t\t\t\t\t\t\t\ttrue\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145195840\"> \u003Cp>\n\t\t\t\t\t\t\t\tAllow fencing operations to be performed in parallel.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145198016\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">fence-reaction\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145196928\"> \u003Cp>\n\t\t\t\t\t\t\t\tstop\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145195840\"> \u003Cp>\n\t\t\t\t\t\t\t\tDetermines how a cluster node should react if notified of its own fencing. A cluster node may receive notification of its own fencing if fencing is misconfigured, or if fabric fencing is in use that does not cut cluster communication. Allowed values are \u003Ccode class=\"literal\">stop\u003C/code> to attempt to immediately stop Pacemaker and stay stopped, or \u003Ccode class=\"literal\">panic\u003C/code> to attempt to immediately reboot the local node, falling back to stop on failure.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tAlthough the default value for this property is \u003Ccode class=\"literal\">stop\u003C/code>, the safest choice for this value is \u003Ccode class=\"literal\">panic\u003C/code>, which attempts to immediately reboot the local node. If you prefer the stop behavior, as is most likely to be the case in conjunction with fabric fencing, it is recommended that you set this explicitly.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145198016\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">priority-fencing-delay\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145196928\"> \u003Cp>\n\t\t\t\t\t\t\t\t0 (disabled)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686145195840\"> \u003Cp>\n\t\t\t\t\t\t\t\tSets a fencing delay that allows you to configure a two-node cluster so that in a split-brain situation the node with the fewest or least important resources running is the node that gets fenced. For general information about fencing delay parameters and their interactions, see \u003Ca class=\"link\" href=\"#ref_fence-delays-configuring-fencing\" title=\"10.4. Fencing delays\">Fencing delays\u003C/a>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tFor information about setting cluster properties, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_controlling-cluster-behavior-configuring-and-managing-high-availability-clusters#setting-cluster-properties-controlling-cluster-behavior\">Setting and removing cluster properties\u003C/a>.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"ref_fence-delays-configuring-fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.4. Fencing delays\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tWhen cluster communication is lost in a two-node cluster, one node may detect this first and fence the other node. If both nodes detect this at the same time, however, each node may be able to initiate fencing of the other, leaving both nodes powered down or reset. By setting a fencing delay, you can decrease the likelihood of both cluster nodes fencing each other. You can set delays in a cluster with more than two nodes, but this is generally not of any benefit because only a partition with quorum will initiate fencing.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can set different types of fencing delays, depending on your system requirements.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\u003Cspan class=\"strong strong\">\u003Cstrong>static fencing delays\u003C/strong>\u003C/span>\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tA static fencing delay is a fixed, predetermined delay. Setting a static delay on one node makes that node more likely to be fenced because it increases the chances that the other node will initiate fencing first after detecting lost communication. In an active/passive cluster, setting a delay on a passive node makes it more likely that the passive node will be fenced when communication breaks down. You configure a static delay by using the \u003Ccode class=\"literal\">pcs_delay_base\u003C/code> cluster property. You can set this property when a separate fence device is used for each node or when a single fence device is used for all nodes.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\u003Cspan class=\"strong strong\">\u003Cstrong>dynamic fencing delays\u003C/strong>\u003C/span>\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tA dynamic fencing delay is random. It can vary and is determined at the time fencing is needed. You configure a random delay and specify a maximum value for the combined base delay and random delay with the \u003Ccode class=\"literal\">pcs_delay_max\u003C/code> cluster property. When the fencing delay for each node is random, which node is fenced is also random. You may find this feature useful if your cluster is configured with a single fence device for all nodes in an active/active design.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\u003Cspan class=\"strong strong\">\u003Cstrong>priority fencing delays\u003C/strong>\u003C/span>\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tA priority fencing delay is based on active resource priorities. If all resources have the same priority, the node with the fewest resources running is the node that gets fenced. In most cases, you use only one delay-related parameter, but it is possible to combine them. Combining delay-related parameters adds the priority values for the resources together to create a total delay. You configure a priority fencing delay with the \u003Ccode class=\"literal\">priority-fencing-delay\u003C/code> cluster property. You may find this feature useful in an active/active cluster design because it can make the node running the fewest resources more likely to be fenced when communication between the nodes is lost.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003C/ul>\u003C/div>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>The \u003Ccode class=\"literal\">pcmk_delay_base\u003C/code> cluster property\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\tSetting the \u003Ccode class=\"literal\">pcmk_delay_base\u003C/code> cluster property enables a base delay for fencing and specifies a base delay value.\n\t\t\t\t\u003C/p>\u003C/div>\u003Cp>\n\t\t\t\tWhen you set the \u003Ccode class=\"literal\">pcmk_delay_max\u003C/code> cluster property in addition to the \u003Ccode class=\"literal\">pcmk_delay_base\u003C/code> property, the overall delay is derived from a random delay value added to this static delay so that the sum is kept below the maximum delay. When you set \u003Ccode class=\"literal\">pcmk_delay_base\u003C/code> but do not set \u003Ccode class=\"literal\">pcmk_delay_max\u003C/code>, there is no random component to the delay and it will be the value of \u003Ccode class=\"literal\">pcmk_delay_base\u003C/code>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can specify different values for different nodes with the \u003Ccode class=\"literal\">pcmk_delay_base\u003C/code> parameter. This allows a single fence device to be used in a two-node cluster, with a different delay for each node. You do not need to configure two separate devices to use separate delays. To specify different values for different nodes, you map the host names to the delay value for that node using a similar syntax to \u003Ccode class=\"literal\">pcmk_host_map\u003C/code>. For example, \u003Ccode class=\"literal\">node1:0;node2:10s\u003C/code> would use no delay when fencing \u003Ccode class=\"literal\">node1\u003C/code> and a 10-second delay when fencing \u003Ccode class=\"literal\">node2\u003C/code>.\n\t\t\t\u003C/p>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>The \u003Ccode class=\"literal\">pcmk_delay_max\u003C/code> cluster property\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\tSetting the \u003Ccode class=\"literal\">pcmk_delay_max\u003C/code> cluster property enables a random delay for fencing actions and specifies the maximum delay, which is the maximum value of the combined base delay and random delay. For example, if the base delay is 3 and \u003Ccode class=\"literal\">pcmk_delay_max\u003C/code> is 10, the random delay will be between 3 and 10.\n\t\t\t\t\u003C/p>\u003C/div>\u003Cp>\n\t\t\t\tWhen you set the \u003Ccode class=\"literal\">pcmk_delay_base\u003C/code> cluster property in addition to the \u003Ccode class=\"literal\">pcmk_delay_max\u003C/code> property, the overall delay is derived from a random delay value added to this static delay so that the sum is kept below the maximum delay. When you set \u003Ccode class=\"literal\">pcmk_delay_max\u003C/code> but do not set \u003Ccode class=\"literal\">pcmk_delay_base\u003C/code> there is no static component to the delay.\n\t\t\t\u003C/p>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>The \u003Ccode class=\"literal\">priority-fencing-delay\u003C/code> cluster property\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\tSetting the \u003Ccode class=\"literal\">priority-fencing-delay\u003C/code> cluster property allows you to configure a two-node cluster so that in a split-brain situation the node with the fewest or least important resources running is the node that gets fenced.\n\t\t\t\t\u003C/p>\u003C/div>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal\">priority-fencing-delay\u003C/code> property can be set to a time duration. The default value for this property is 0 (disabled). If this property is set to a non-zero value, and the priority meta-attribute is configured for at least one resource, then in a split-brain situation the node with the highest combined priority of all resources running on it will be more likely to remain operational. For example, if you set \u003Ccode class=\"literal\">pcs resource defaults update priority=1\u003C/code> and \u003Ccode class=\"literal\">pcs property set priority-fencing-delay=15s\u003C/code> and no other priorities are set, then the node running the most resources will be more likely to remain operational because the other node will wait 15 seconds before initiating fencing. If a particular resource is more important than the rest, you can give it a higher priority.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe node running the promoted role of a promotable clone gets an extra 1 point if a priority has been configured for that clone.\n\t\t\t\u003C/p>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Interaction of fencing delays\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\tSetting more than one type of fencing delay yields the following results:\n\t\t\t\t\u003C/p>\u003C/div>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tAny delay set with the \u003Ccode class=\"literal\">priority-fencing-delay\u003C/code> property is added to any delay from the \u003Ccode class=\"literal\">pcmk_delay_base\u003C/code> and \u003Ccode class=\"literal\">pcmk_delay_max\u003C/code> fence device properties. This behavior allows some delay when both nodes have equal priority, or both nodes need to be fenced for some reason other than node loss, as when \u003Ccode class=\"literal\">on-fail=fencing\u003C/code> is set for a resource monitor operation. When setting these delays in combination, set the \u003Ccode class=\"literal\">priority-fencing-delay\u003C/code> property to a value that is significantly greater than the maximum delay from \u003Ccode class=\"literal\">pcmk_delay_base\u003C/code> and \u003Ccode class=\"literal\">pcmk_delay_max\u003C/code> to be sure the prioritized node is preferred. Setting this property to twice this value is always safe.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tOnly fencing scheduled by Pacemaker itself observes fencing delays. Fencing scheduled by external code such as \u003Ccode class=\"literal\">dlm_controld\u003C/code> and fencing implemented by the \u003Ccode class=\"literal\">pcs stonith fence\u003C/code> command do not provide the necessary information to the fence device.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tSome individual fence agents implement a delay parameter, with a name determined by the agent and which is independent of delays configured with a \u003Ccode class=\"literal\">pcmk_delay_\u003C/code>* property. If both of these delays are configured, they are added together and would generally not be used in conjunction.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_testing-fence-devices-configuring-fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.5. Testing a fence device\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tFencing is a fundamental part of the Red Hat Cluster infrastructure and it is important to validate or test that fencing is working properly.\n\t\t\t\u003C/p>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\tUse the following procedure to test a fence device.\n\t\t\t\t\u003C/p>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tUse ssh, telnet, HTTP, or whatever remote protocol is used to connect to the device to manually log in and test the fence device or see what output is given. For example, if you will be configuring fencing for an IPMI-enabled device,then try to log in remotely with \u003Ccode class=\"literal command\">ipmitool\u003C/code>. Take note of the options used when logging in manually because those options might be needed when using the fencing agent.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf you are unable to log in to the fence device, verify that the device is pingable, there is nothing such as a firewall configuration that is preventing access to the fence device, remote access is enabled on the fencing device, and the credentials are correct.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tRun the fence agent manually, using the fence agent script. This does not require that the cluster services are running, so you can perform this step before the device is configured in the cluster. This can ensure that the fence device is responding properly before proceeding.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tThese examples use the \u003Ccode class=\"literal command\">fence_ipmilan\u003C/code> fence agent script for an iLO device. The actual fence agent you will use and the command that calls that agent will depend on your server hardware. You should consult the man page for the fence agent you are using to determine which options to specify. You will usually need to know the login and password for the fence device and other information related to the fence device.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following example shows the format you would use to run the \u003Ccode class=\"literal command\">fence_ipmilan\u003C/code> fence agent script with \u003Ccode class=\"literal\">-o status\u003C/code> parameter to check the status of the fence device interface on another node without actually fencing it. This allows you to test the device and get it working before attempting to reboot the node. When running this command, you specify the name and password of an iLO user that has power on and off permissions for the iLO device.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>fence_ipmilan -a ipaddress -l username -p password -o status\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following example shows the format you would use to run the \u003Ccode class=\"literal command\">fence_ipmilan\u003C/code> fence agent script with the \u003Ccode class=\"literal\">-o reboot\u003C/code> parameter. Running this command on one node reboots the node managed by this iLO device.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>fence_ipmilan -a ipaddress -l username -p password -o reboot\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf the fence agent failed to properly do a status, off, on, or reboot action, you should check the hardware, the configuration of the fence device, and the syntax of your commands. In addition, you can run the fence agent script with the debug output enabled. The debug output is useful for some fencing agents to see where in the sequence of events the fencing agent script is failing when logging into the fence device.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>fence_ipmilan -a ipaddress -l username -p password -o status -D /tmp/$(hostname)-fence_agent.debug\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tWhen diagnosing a failure that has occurred, you should ensure that the options you specified when manually logging in to the fence device are identical to what you passed on to the fence agent with the fence agent script.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tFor fence agents that support an encrypted connection, you may see an error due to certificate validation failing, requiring that you trust the host or that you use the fence agent’s \u003Ccode class=\"literal\">ssl-insecure\u003C/code> parameter. Similarly, if SSL/TLS is disabled on the target device, you may need to account for this when setting the SSL parameters for the fence agent.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tIf the fence agent that is being tested is a \u003Ccode class=\"literal command\">fence_drac\u003C/code>, \u003Ccode class=\"literal command\">fence_ilo\u003C/code>, or some other fencing agent for a systems management device that continues to fail, then fall back to trying \u003Ccode class=\"literal command\">fence_ipmilan\u003C/code>. Most systems management cards support IPMI remote login and the only supported fencing agent is \u003Ccode class=\"literal command\">fence_ipmilan\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOnce the fence device has been configured in the cluster with the same options that worked manually and the cluster has been started, test fencing with the \u003Ccode class=\"literal command\">pcs stonith fence\u003C/code> command from any node (or even multiple times from different nodes), as in the following example. The \u003Ccode class=\"literal command\">pcs stonith fence\u003C/code> command reads the cluster configuration from the CIB and calls the fence agent as configured to execute the fence action. This verifies that the cluster configuration is correct.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith fence node_name\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf the \u003Ccode class=\"literal command\">pcs stonith fence\u003C/code> command works properly, that means the fencing configuration for the cluster should work when a fence event occurs. If the command fails, it means that cluster management cannot invoke the fence device through the configuration it has retrieved. Check for the following issues and update your cluster configuration as needed.\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\tCheck your fence configuration. For example, if you have used a host map you should ensure that the system can find the node using the host name you have provided.\n\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\tCheck whether the password and user name for the device include any special characters that could be misinterpreted by the bash shell. Making sure that you enter passwords and user names surrounded by quotation marks could address this issue.\n\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\tCheck whether you can connect to the device using the exact IP address or host name you specified in the \u003Ccode class=\"literal command\">pcs stonith\u003C/code> command. For example, if you give the host name in the stonith command but test by using the IP address, that is not a valid test.\n\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tIf the protocol that your fence device uses is accessible to you, use that protocol to try to connect to the device. For example many agents use ssh or telnet. You should try to connect to the device with the credentials you provided when configuring the device, to see if you get a valid prompt and can log in to the device.\n\t\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tIf you determine that all your parameters are appropriate but you still have trouble connecting to your fence device, you can check the logging on the fence device itself, if the device provides that, which will show if the user has connected and what command the user issued. You can also search through the \u003Ccode class=\"literal\">/var/log/messages\u003C/code> file for instances of stonith and error, which could give some idea of what is transpiring, but some agents can provide additional information.\n\t\t\t\t\t\t\t\u003C/p>\u003C/li>\u003C/ul>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOnce the fence device tests are working and the cluster is up and running, test an actual failure. To do this, take an action in the cluster that should initiate a token loss.\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tTake down a network. How you take a network depends on your specific configuration. In many cases, you can physically pull the network or power cables out of the host. For information about simulating a network failure, see the Red Hat Knowledgebase solution \u003Ca class=\"link\" href=\"https://access.redhat.com/solutions/79523/\">What is the proper way to simulate a network failure on a RHEL Cluster?\u003C/a>.\n\t\t\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\t\tDisabling the network interface on the local host rather than physically disconnecting the network or power cables is not recommended as a test of fencing because it does not accurately simulate a typical real-world failure.\n\t\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tBlock corosync traffic both inbound and outbound using the local firewall.\n\t\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tThe following example blocks corosync, assuming the default corosync port is used, \u003Ccode class=\"literal\">firewalld\u003C/code> is used as the local firewall, and the network interface used by corosync is in the default firewall zone:\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --direct --add-rule ipv4 filter OUTPUT 2 -p udp --dport=5405 -j DROP\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --add-rich-rule='rule family=\"ipv4\" port port=\"5405\" protocol=\"udp\" drop\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tSimulate a crash and panic your machine with \u003Ccode class=\"literal\">sysrq-trigger\u003C/code>. Note, however, that triggering a kernel panic can cause data loss; it is recommended that you disable your cluster resources first.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>echo c &gt; /proc/sysrq-trigger\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ul>\u003C/div>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-fencing-levels-configuring-fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.6. Configuring fencing levels\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tPacemaker supports fencing nodes with multiple devices through a feature called fencing topologies. To implement topologies, create the individual devices as you normally would and then define one or more fencing levels in the fencing topology section in the configuration.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tPacemaker processes fencing levels as follows:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tEach level is attempted in ascending numeric order, starting at 1.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tIf a device fails, processing terminates for the current level. No further devices in that level are exercised and the next level is attempted instead.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tIf all devices are successfully fenced, then that level has succeeded and no other levels are tried.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe operation is finished when a level has passed (success), or all levels have been attempted (failed).\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tUse the following command to add a fencing level to a node. The devices are given as a comma-separated list of \u003Ccode class=\"literal\">stonith\u003C/code> ids, which are attempted for the node at that level.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs stonith level add \u003Cspan class=\"emphasis\">\u003Cem>level\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>devices\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe following command lists all of the fencing levels that are currently configured.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs stonith level\u003C/pre>\u003Cp>\n\t\t\t\tIn the following example, there are two fence devices configured for node \u003Ccode class=\"literal\">rh7-2\u003C/code>: an ilo fence device called \u003Ccode class=\"literal\">my_ilo\u003C/code> and an apc fence device called \u003Ccode class=\"literal\">my_apc\u003C/code>. These commands set up fence levels so that if the device \u003Ccode class=\"literal\">my_ilo\u003C/code> fails and is unable to fence the node, then Pacemaker will attempt to use the device \u003Ccode class=\"literal\">my_apc\u003C/code>. This example also shows the output of the \u003Ccode class=\"literal\">pcs stonith level\u003C/code> command after the levels are configured.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith level add 1 rh7-2 my_ilo\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith level add 2 rh7-2 my_apc\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith level\u003C/strong>\u003C/span>\n Node: rh7-2\n  Level 1 - my_ilo\n  Level 2 - my_apc\u003C/pre>\u003Cp>\n\t\t\t\tThe following command removes the fence level for the specified node and devices. If no nodes or devices are specified then the fence level you specify is removed from all nodes.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs stonith level remove \u003Cspan class=\"emphasis\">\u003Cem>level\u003C/em>\u003C/span>  [\u003Cspan class=\"emphasis\">\u003Cem>node_id\u003C/em>\u003C/span>] [\u003Cspan class=\"emphasis\">\u003Cem>stonith_id\u003C/em>\u003C/span>] ... [\u003Cspan class=\"emphasis\">\u003Cem>stonith_id\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\tThe following command clears the fence levels on the specified node or stonith id. If you do not specify a node or stonith id, all fence levels are cleared.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs stonith level clear [\u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span>]|\u003Cspan class=\"emphasis\">\u003Cem>stonith_id\u003C/em>\u003C/span>(s)]\u003C/pre>\u003Cp>\n\t\t\t\tIf you specify more than one stonith id, they must be separated by a comma and no spaces, as in the following example.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith level clear dev_a,dev_b\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe following command verifies that all fence devices and nodes specified in fence levels exist.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs stonith level verify\u003C/pre>\u003Cp>\n\t\t\t\tYou can specify nodes in fencing topology by a regular expression applied on a node name and by a node attribute and its value. For example, the following commands configure nodes \u003Ccode class=\"literal\">node1\u003C/code>, \u003Ccode class=\"literal\">node2\u003C/code>, and \u003Ccode class=\"literal\">node3\u003C/code> to use fence devices \u003Ccode class=\"literal\">apc1\u003C/code> and \u003Ccode class=\"literal\">apc2\u003C/code>, and nodes \u003Ccode class=\"literal\">node4\u003C/code>, \u003Ccode class=\"literal\">node5\u003C/code>, and \u003Ccode class=\"literal\">node6\u003C/code> to use fence devices \u003Ccode class=\"literal\">apc3\u003C/code> and \u003Ccode class=\"literal\">apc4\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith level add 1 \"regexp%node[1-3]\" apc1,apc2\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith level add 1 \"regexp%node[4-6]\" apc3,apc4\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe following commands yield the same results by using node attribute matching.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node attribute node1 rack=1\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node attribute node2 rack=1\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node attribute node3 rack=1\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node attribute node4 rack=2\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node attribute node5 rack=2\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node attribute node6 rack=2\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith level add 1 attrib%rack=1 apc1,apc2\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith level add 1 attrib%rack=2 apc3,apc4\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-fencing-for-redundant-power-configuring-fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.7. Configuring fencing for redundant power supplies\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen configuring fencing for redundant power supplies, the cluster must ensure that when attempting to reboot a host, both power supplies are turned off before either power supply is turned back on.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf the node never completely loses power, the node may not release its resources. This opens up the possibility of nodes accessing these resources simultaneously and corrupting them.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou need to define each device only once and to specify that both are required to fence the node, as in the following example.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith create apc1 fence_apc_snmp ipaddr=apc1.example.com login=user passwd='7a4D#1j!pz864' pcmk_host_map=\"node1.example.com:1;node2.example.com:2\"\u003C/strong>\u003C/span>\n\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith create apc2 fence_apc_snmp ipaddr=apc2.example.com login=user passwd='7a4D#1j!pz864' pcmk_host_map=\"node1.example.com:1;node2.example.com:2\"\u003C/strong>\u003C/span>\n\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith level add 1 node1.example.com apc1,apc2\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith level add 1 node2.example.com apc1,apc2\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_displaying-configuring-fence-devices-configuring-fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.8. Displaying configured fence devices\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe following command shows all currently configured fence devices. If a \u003Cspan class=\"emphasis\">\u003Cem>stonith_id\u003C/em>\u003C/span> is specified, the command shows the options for that configured fencing device only. If the \u003Ccode class=\"literal\">--full\u003C/code> option is specified, all configured fencing options are displayed.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs stonith config [\u003Cspan class=\"emphasis\">\u003Cem>stonith_id\u003C/em>\u003C/span>] [--full]\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_exporting-fence-devices-configuring-fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.9. Exporting fence devices as \u003Ccode class=\"literal\">pcs\u003C/code> commands\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tAs of Red Hat Enterprise Linux 9.1, you can display the \u003Ccode class=\"literal\">pcs\u003C/code> commands that can be used to re-create configured fence devices on a different system using the \u003Ccode class=\"literal\">--output-format=cmd\u003C/code> option of the \u003Ccode class=\"literal\">pcs stonith config\u003C/code> command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following commands create a \u003Ccode class=\"literal\">fence_apc_snmp\u003C/code> fence device and display the \u003Ccode class=\"literal\">pcs\u003C/code> command you can use to re-create the device.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith create myapc fence_apc_snmp ip=\"zapc.example.com\" pcmk_host_map=\"z1.example.com:1;z2.example.com:2\" username=\"apc\" password=\"apc\"\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith config --output-format=cmd\u003C/strong>\u003C/span>\nWarning: Only 'text' output format is supported for stonith levels\npcs stonith create --no-default-ops --force -- myapc fence_apc_snmp \\\n  ip=zapc.example.com password=apc 'pcmk_host_map=z1.example.com:1;z2.example.com:2' username=apc \\\n  op \\\n    monitor interval=60s id=myapc-monitor-interval-60s\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_modifying-fence-devices-configuring-fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.10. Modifying and deleting fence devices\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tModify or add options to a currently configured fencing device with the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs stonith update \u003Cspan class=\"emphasis\">\u003Cem>stonith_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>stonith_device_options\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\tUpdating a SCSI fencing device with the \u003Ccode class=\"literal\">pcs stonith update\u003C/code> command causes a restart of all resources running on the same node where the fencing resource was running. You can use either version of the following command to update SCSI devices without causing a restart of other cluster resources. As of RHEL 9.1, SCSI fencing devices can be configured as multipath devices.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs stonith update-scsi-devices \u003Cspan class=\"emphasis\">\u003Cem>stonith_id\u003C/em>\u003C/span> set \u003Cspan class=\"emphasis\">\u003Cem>device-path1\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>device-path2\u003C/em>\u003C/span>\npcs stonith update-scsi-devices \u003Cspan class=\"emphasis\">\u003Cem>stonith_id\u003C/em>\u003C/span> add \u003Cspan class=\"emphasis\">\u003Cem>device-path1\u003C/em>\u003C/span> remove \u003Cspan class=\"emphasis\">\u003Cem>device-path2\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tUse the following command to remove a fencing device from the current configuration.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs stonith delete \u003Cspan class=\"emphasis\">\u003Cem>stonith_id\u003C/em>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_manually-fencing-a-node-configuring-fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.11. Manually fencing a cluster node\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can fence a node manually with the following command. If you specify \u003Ccode class=\"literal option\">--off\u003C/code> this will use the \u003Ccode class=\"literal\">off\u003C/code> API call to stonith which will turn the node off instead of rebooting it.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs stonith fence \u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span> [--off]\u003C/pre>\u003Cp>\n\t\t\t\tIn a situation where no fence device is able to fence a node even if it is no longer active, the cluster may not be able to recover the resources on the node. If this occurs, after manually ensuring that the node is powered down you can enter the following command to confirm to the cluster that the node is powered down and free its resources for recovery.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tIf the node you specify is not actually off, but running the cluster software or services normally controlled by the cluster, data corruption/cluster failure will occur.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">pcs stonith confirm \u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_disabling-a-fence-device-configuring-fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.12. Disabling a fence device\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tTo disable a fencing device/resource, run the \u003Ccode class=\"literal\">pcs stonith disable\u003C/code> command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command disables the fence device \u003Ccode class=\"literal\">myapc\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs stonith disable myapc\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_preventing-a-node-from-using-a-fence-device-configuring-fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.13. Preventing a node from using a fencing device\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tTo prevent a specific node from using a fencing device, you can configure location constraints for the fencing resource.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following example prevents fence device \u003Ccode class=\"literal\">node1-ipmi\u003C/code> from running on \u003Ccode class=\"literal\">node1\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location node1-ipmi avoids node1\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-acpi-for-fence-devices-configuring-fencing\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">10.14. Configuring ACPI for use with integrated fence devices\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf your cluster uses integrated fence devices, you must configure ACPI (Advanced Configuration and Power Interface) to ensure immediate and complete fencing.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf a cluster node is configured to be fenced by an integrated fence device, disable ACPI Soft-Off for that node. Disabling ACPI Soft-Off allows an integrated fence device to turn off a node immediately and completely rather than attempting a clean shutdown (for example, \u003Ccode class=\"literal command\">shutdown -h now\u003C/code>). Otherwise, if ACPI Soft-Off is enabled, an integrated fence device can take four or more seconds to turn off a node (see the note that follows). In addition, if ACPI Soft-Off is enabled and a node panics or freezes during shutdown, an integrated fence device may not be able to turn off the node. Under those circumstances, fencing is delayed or unsuccessful. Consequently, when a node is fenced with an integrated fence device and ACPI Soft-Off is enabled, a cluster recovers slowly or requires administrative intervention to recover.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tThe amount of time required to fence a node depends on the integrated fence device used. Some integrated fence devices perform the equivalent of pressing and holding the power button; therefore, the fence device turns off the node in four to five seconds. Other integrated fence devices perform the equivalent of pressing the power button momentarily, relying on the operating system to turn off the node; therefore, the fence device turns off the node in a time span much longer than four to five seconds.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe preferred way to disable ACPI Soft-Off is to change the BIOS setting to \"instant-off\" or an equivalent setting that turns off the node without delay, as described in \"Disabling ACPI Soft-Off with the Bios\" below.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tDisabling ACPI Soft-Off with the BIOS may not be possible with some systems. If disabling ACPI Soft-Off with the BIOS is not satisfactory for your cluster, you can disable ACPI Soft-Off with one of the following alternate methods:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tSetting \u003Ccode class=\"literal\">HandlePowerKey=ignore\u003C/code> in the \u003Ccode class=\"literal\">/etc/systemd/logind.conf\u003C/code> file and verifying that the node node turns off immediately when fenced, as described in \"Disabling ACPI Soft-Off in the logind.conf file\", below. This is the first alternate method of disabling ACPI Soft-Off.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAppending \u003Ccode class=\"literal\">acpi=off\u003C/code> to the kernel boot command line, as described in \"Disabling ACPI completely in the GRUB 2 file\", below. This is the second alternate method of disabling ACPI Soft-Off, if the preferred or the first alternate method is not available.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition important\" state=\"warning\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Important\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tThis method completely disables ACPI; some computers do not boot correctly if ACPI is completely disabled. Use this method \u003Cspan class=\"emphasis\">\u003Cem>only\u003C/em>\u003C/span> if the other methods are not effective for your cluster.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/li>\u003C/ul>\u003C/div>\u003Csection class=\"section\" id=\"s2-bios-setting-CA\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">10.14.1. Disabling ACPI Soft-Off with the BIOS\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tYou can disable ACPI Soft-Off by configuring the BIOS of each cluster node with the following procedure.\n\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\tThe procedure for disabling ACPI Soft-Off with the BIOS may differ among server systems. You should verify this procedure with your hardware documentation.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tReboot the node and start the \u003Ccode class=\"literal command\">BIOS CMOS Setup Utility\u003C/code> program.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tNavigate to the Power menu (or equivalent power management menu).\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tAt the Power menu, set the \u003Ccode class=\"literal\">Soft-Off by PWR-BTTN\u003C/code> function (or equivalent) to \u003Ccode class=\"literal\">Instant-Off\u003C/code> (or the equivalent setting that turns off the node by means of the power button without delay). The \u003Ccode class=\"literal\">BIOS CMOS Setup Utiliy\u003C/code> example below shows a Power menu with \u003Ccode class=\"literal\">ACPI Function\u003C/code> set to \u003Ccode class=\"literal\">Enabled\u003C/code> and \u003Ccode class=\"literal\">Soft-Off by PWR-BTTN\u003C/code> set to \u003Ccode class=\"literal\">Instant-Off\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\tThe equivalents to \u003Ccode class=\"literal\">ACPI Function\u003C/code>, \u003Ccode class=\"literal\">Soft-Off by PWR-BTTN\u003C/code>, and \u003Ccode class=\"literal\">Instant-Off\u003C/code> may vary among computers. However, the objective of this procedure is to configure the BIOS so that the computer is turned off by means of the power button without delay.\n\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tExit the \u003Ccode class=\"literal command\">BIOS CMOS Setup Utility\u003C/code> program, saving the BIOS configuration.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tVerify that the node turns off immediately when fenced. For information about testing a fence device, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters#proc_testing-fence-devices-configuring-fencing\">Testing a fence device\u003C/a>.\n\t\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv id=\"ex-bios-acpi-off-CA\" class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>\u003Ccode class=\"literal command\">BIOS CMOS Setup Utility\u003C/code>:\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\t\t\n\u003Cpre class=\"literallayout\">`Soft-Off by PWR-BTTN` set to\n`Instant-Off`\u003C/pre>\n\n\t\t\t\t\t\u003C/p>\u003C/div>\u003Cdiv class=\"informalexample\">\u003Cpre class=\"literallayout\">+---------------------------------------------|-------------------+\n|    ACPI Function             [Enabled]      |    Item Help      |\n|    ACPI Suspend Type         [S1(POS)]      |-------------------|\n|  x Run VGABIOS if S3 Resume   Auto          |   Menu Level   *  |\n|    Suspend Mode              [Disabled]     |                   |\n|    HDD Power Down            [Disabled]     |                   |\n|    Soft-Off by PWR-BTTN      [Instant-Off   |                   |\n|    CPU THRM-Throttling       [50.0%]        |                   |\n|    Wake-Up by PCI card       [Enabled]      |                   |\n|    Power On by Ring          [Enabled]      |                   |\n|    Wake Up On LAN            [Enabled]      |                   |\n|  x USB KB Wake-Up From S3     Disabled      |                   |\n|    Resume by Alarm           [Disabled]     |                   |\n|  x  Date(of Month) Alarm       0            |                   |\n|  x  Time(hh:mm:ss) Alarm       0 :  0 :     |                   |\n|    POWER ON Function         [BUTTON ONLY   |                   |\n|  x KB Power ON Password       Enter         |                   |\n|  x Hot Key Power ON           Ctrl-F1       |                   |\n|                                             |                   |\n|                                             |                   |\n+---------------------------------------------|-------------------+\u003C/pre>\u003Cp>\n\t\t\t\t\tThis example shows \u003Ccode class=\"literal\">ACPI Function\u003C/code> set to \u003Ccode class=\"literal\">Enabled\u003C/code>, and \u003Ccode class=\"literal\">Soft-Off by PWR-BTTN\u003C/code> set to \u003Ccode class=\"literal\">Instant-Off\u003C/code>.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"s2-acpi-disable-logind-CA\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">10.14.2. Disabling ACPI Soft-Off in the logind.conf file\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tTo disable power-key handing in the \u003Ccode class=\"literal\">/etc/systemd/logind.conf\u003C/code> file, use the following procedure.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tDefine the following configuration in the \u003Ccode class=\"literal\">/etc/systemd/logind.conf\u003C/code> file:\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">HandlePowerKey=ignore\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tRestart the \u003Ccode class=\"literal\">systemd-logind\u003C/code> service:\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl restart systemd-logind.service\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tVerify that the node turns off immediately when fenced. For information about testing a fence device, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters#proc_testing-fence-devices-configuring-fencing\">Testing a fence device\u003C/a>.\n\t\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"s2-acpi-disable-boot-CA\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">10.14.3. Disabling ACPI completely in the GRUB 2 file\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tYou can disable ACPI Soft-Off by appending \u003Ccode class=\"literal\">acpi=off\u003C/code> to the GRUB menu entry for a kernel.\n\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition important\" state=\"warning\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Important\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\tThis method completely disables ACPI; some computers do not boot correctly if ACPI is completely disabled. Use this method \u003Cspan class=\"emphasis\">\u003Cem>only\u003C/em>\u003C/span> if the other methods are not effective for your cluster.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\t\tUse the following procedure to disable ACPI in the GRUB 2 file:\n\t\t\t\t\t\u003C/p>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tUse the \u003Ccode class=\"literal option\">--args\u003C/code> option in combination with the \u003Ccode class=\"literal option\">--update-kernel\u003C/code> option of the \u003Ccode class=\"literal command\">grubby\u003C/code> tool to change the \u003Ccode class=\"literal\">grub.cfg\u003C/code> file of each cluster node as follows:\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>grubby --args=acpi=off --update-kernel=ALL\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tReboot the node.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tVerify that the node turns off immediately when fenced. For information about testing a fence device, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters#proc_testing-fence-devices-configuring-fencing\">Testing a fence device\u003C/a>.\n\t\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 11. Configuring cluster resources\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tCreate and delete cluster resources with the following commands.\n\t\t\u003C/p>\u003Cp>\n\t\t\tThe format for the command to create a cluster resource is as follows:\n\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource create \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>standard\u003C/em>\u003C/span>:[\u003Cspan class=\"emphasis\">\u003Cem>provider\u003C/em>\u003C/span>:]]\u003Cspan class=\"emphasis\">\u003Cem>type\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>resource_options\u003C/em>\u003C/span>] [op \u003Cspan class=\"emphasis\">\u003Cem>operation_action operation_options\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>operation_action\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>operation options\u003C/em>\u003C/span>]...] [meta \u003Cspan class=\"emphasis\">\u003Cem>meta_options\u003C/em>\u003C/span>...] [clone [\u003Cspan class=\"emphasis\">\u003Cem>clone_id\u003C/em>\u003C/span>] [\u003Cspan class=\"emphasis\">\u003Cem>clone_options\u003C/em>\u003C/span>] | promotable [\u003Cspan class=\"emphasis\">\u003Cem>clone_id\u003C/em>\u003C/span>] [\u003Cspan class=\"emphasis\">\u003Cem>clone_options\u003C/em>\u003C/span>] [--wait[=\u003Cspan class=\"emphasis\">\u003Cem>n\u003C/em>\u003C/span>]]\u003C/pre>\u003Cp>\n\t\t\tKey cluster resource creation options include the following:\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\tThe \u003Ccode class=\"literal option\">--before\u003C/code> and \u003Ccode class=\"literal option\">--after\u003C/code> options specify the position of the added resource relative to a resource that already exists in a resource group.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tSpecifying the \u003Ccode class=\"literal option\">--disabled\u003C/code> option indicates that the resource is not started automatically.\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\tThere is no limit to the number of resources you can create in a cluster.\n\t\t\u003C/p>\u003Cp>\n\t\t\tYou can determine the behavior of a resource in a cluster by configuring constraints for that resource.\n\t\t\u003C/p>\u003Ch4 id=\"resource_creation_examples\">Resource creation examples\u003C/h4>\u003Cp>\n\t\t\tThe following command creates a resource with the name \u003Ccode class=\"literal\">VirtualIP\u003C/code> of standard \u003Ccode class=\"literal\">ocf\u003C/code>, provider \u003Ccode class=\"literal\">heartbeat\u003C/code>, and type \u003Ccode class=\"literal\">IPaddr2\u003C/code>. The floating address of this resource is 192.168.0.120, and the system will check whether the resource is running every 30 seconds.\n\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.120 cidr_netmask=24 op monitor interval=30s\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\tAlternately, you can omit the \u003Cspan class=\"emphasis\">\u003Cem>standard\u003C/em>\u003C/span> and \u003Cspan class=\"emphasis\">\u003Cem>provider\u003C/em>\u003C/span> fields and use the following command. This will default to a standard of \u003Ccode class=\"literal\">ocf\u003C/code> and a provider of \u003Ccode class=\"literal\">heartbeat\u003C/code>.\n\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create VirtualIP IPaddr2 ip=192.168.0.120 cidr_netmask=24 op monitor interval=30s\u003C/strong>\u003C/span>\u003C/pre>\u003Ch4 id=\"deleting_a_configured_resource\">Deleting a configured resource\u003C/h4>\u003Cp>\n\t\t\tDelete a configured resource with the following command.\n\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource delete \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\tFor example, the following command deletes an existing resource with a resource ID of \u003Ccode class=\"literal\">VirtualIP\u003C/code>.\n\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource delete VirtualIP\u003C/strong>\u003C/span>\u003C/pre>\u003Csection class=\"section\" id=\"ref_resource-properties.adoc-configuring-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">11.1. Resource agent identifiers\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe identifiers that you define for a resource tell the cluster which agent to use for the resource, where to find that agent and what standards it conforms to.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following table describes these properties of a resource agent.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-resource-props-summary-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 11.1. Resource Agent Identifiers\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 33%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 67%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686141112880\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686141111792\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141112880\"> \u003Cp>\n\t\t\t\t\t\t\t\tstandard\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141111792\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe standard the agent conforms to. Allowed values and their meaning:\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">ocf\u003C/code> - The specified \u003Cspan class=\"emphasis\">\u003Cem>type\u003C/em>\u003C/span> is the name of an executable file conforming to the Open Cluster Framework Resource Agent API and located beneath \u003Ccode class=\"literal\">/usr/lib/ocf/resource.d/\u003Cspan class=\"emphasis\">\u003Cem>provider\u003C/em>\u003C/span>\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">lsb\u003C/code> - The specified \u003Cspan class=\"emphasis\">\u003Cem>type\u003C/em>\u003C/span> is the name of an executable file conforming to Linux Standard Base Init Script Actions. If the type does not specify a full path, the system will look for it in the \u003Ccode class=\"literal\">/etc/init.d\u003C/code> directory.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">systemd\u003C/code> - The specified \u003Cspan class=\"emphasis\">\u003Cem>type\u003C/em>\u003C/span> is the name of an installed \u003Ccode class=\"literal\">systemd\u003C/code> unit\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">service\u003C/code> - Pacemaker will search for the specified \u003Cspan class=\"emphasis\">\u003Cem>type\u003C/em>\u003C/span>, first as an \u003Ccode class=\"literal\">lsb\u003C/code> agent, then as a \u003Ccode class=\"literal\">systemd\u003C/code> agent\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">nagios\u003C/code> - The specified \u003Cspan class=\"emphasis\">\u003Cem>type\u003C/em>\u003C/span> is the name of an executable file conforming to the Nagios Plugin API and located in the \u003Ccode class=\"literal\">/usr/libexec/nagios/plugins\u003C/code> directory, with OCF-style metadata stored separately in the \u003Ccode class=\"literal\">/usr/share/nagios/plugins-metadata\u003C/code> directory (available in the \u003Ccode class=\"literal\">nagios-agents-metadata\u003C/code> package for certain common plugins).\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141112880\"> \u003Cp>\n\t\t\t\t\t\t\t\ttype\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141111792\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe name of the resource agent you wish to use, for example \u003Ccode class=\"literal\">IPaddr\u003C/code> or \u003Ccode class=\"literal\">Filesystem\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141112880\"> \u003Cp>\n\t\t\t\t\t\t\t\tprovider\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141111792\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe OCF spec allows multiple vendors to supply the same resource agent. Most of the agents shipped by Red Hat use \u003Ccode class=\"literal\">heartbeat\u003C/code> as the provider.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tThe following table summarizes the commands that display the available resource properties.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-resource-displayopts-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 11.2. Commands to Display Resource Properties\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 50%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 50%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686136901488\" scope=\"col\">pcs Display Command\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686136900400\" scope=\"col\">Output\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136901488\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal command\">pcs resource list\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136900400\"> \u003Cp>\n\t\t\t\t\t\t\t\tDisplays a list of all available resources.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136901488\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal command\">pcs resource standards\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136900400\"> \u003Cp>\n\t\t\t\t\t\t\t\tDisplays a list of available resource agent standards.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136901488\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal command\">pcs resource providers\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136900400\"> \u003Cp>\n\t\t\t\t\t\t\t\tDisplays a list of available resource agent providers.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136901488\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal command\">pcs resource list \u003Cspan class=\"emphasis\">\u003Cem>string\u003C/em>\u003C/span>\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136900400\"> \u003Cp>\n\t\t\t\t\t\t\t\tDisplays a list of available resources filtered by the specified string. You can use this command to display resources filtered by the name of a standard, a provider, or a type.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003C/section>\u003Csection class=\"section\" id=\"proc_displaying-resource-specific-parameters-configuring-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">11.2. Displaying resource-specific parameters\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tFor any individual resource, you can use the following command to display a description of the resource, the parameters you can set for that resource, and the default values that are set for the resource.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource describe [\u003Cspan class=\"emphasis\">\u003Cem>standard\u003C/em>\u003C/span>:[\u003Cspan class=\"emphasis\">\u003Cem>provider\u003C/em>\u003C/span>:]]\u003Cspan class=\"emphasis\">\u003Cem>type\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tFor example, the following command displays information for a resource of type \u003Ccode class=\"literal\">apache\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource describe ocf:heartbeat:apache\u003C/strong>\u003C/span>\nThis is the resource agent for the Apache Web server.\nThis resource agent operates both version 1.x and version 2.x Apache\nservers.\n\n...\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-resource-meta-options-configuring-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">11.3. Configuring resource meta options\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIn addition to the resource-specific parameters, you can configure additional resource options for any resource. These options are used by the cluster to decide how your resource should behave.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following table describes the resource meta options.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-resource-options-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 11.3. Resource Meta Options\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 29%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 29%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 43%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686141499600\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686141498512\" scope=\"col\">Default\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686141497424\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141499600\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">priority\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141498512\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">0\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141497424\"> \u003Cp>\n\t\t\t\t\t\t\t\tIf not all resources can be active, the cluster will stop lower priority resources in order to keep higher priority ones active.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141499600\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">target-role\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141498512\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">Started\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141497424\"> \u003Cp>\n\t\t\t\t\t\t\t\tIndicates what state the cluster should attempt to keep this resource in. Allowed values:\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">Stopped\u003C/code> - Force the resource to be stopped\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">Started\u003C/code> - Allow the resource to be started (and in the case of promotable clones, promoted if appropriate)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">Promoted\u003C/code> - Allow the resource to be started and, if appropriate, promoted\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">Unpromoted\u003C/code> - Allow the resource to be started, but only in unpromoted mode if the resource is promotable\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141499600\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">is-managed\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141498512\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">true\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141497424\"> \u003Cp>\n\t\t\t\t\t\t\t\tIndicates whether the cluster is allowed to start and stop the resource. Allowed values: \u003Ccode class=\"literal\">true\u003C/code>, \u003Ccode class=\"literal\">false\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141499600\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">resource-stickiness\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141498512\"> \u003Cp>\n\t\t\t\t\t\t\t\t1\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141497424\"> \u003Cp>\n\t\t\t\t\t\t\t\tValue to indicate how much the resource prefers to stay where it is.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141499600\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">requires\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141498512\"> \u003Cp>\n\t\t\t\t\t\t\t\tCalculated\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141497424\"> \u003Cp>\n\t\t\t\t\t\t\t\tIndicates under what conditions the resource can be started.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tDefaults to \u003Ccode class=\"literal\">fencing\u003C/code> except under the conditions noted below. Possible values:\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">nothing\u003C/code> - The cluster can always start the resource.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">quorum\u003C/code> - The cluster can only start this resource if a majority of the configured nodes are active. This is the default value if \u003Ccode class=\"literal\">stonith-enabled\u003C/code> is \u003Ccode class=\"literal\">false\u003C/code> or the resource’s \u003Ccode class=\"literal\">standard\u003C/code> is \u003Ccode class=\"literal\">stonith\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">fencing\u003C/code> - The cluster can only start this resource if a majority of the configured nodes are active \u003Cspan class=\"emphasis\">\u003Cem>and\u003C/em>\u003C/span> any failed or unknown nodes have been fenced.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">unfencing\u003C/code> - The cluster can only start this resource if a majority of the configured nodes are active \u003Cspan class=\"emphasis\">\u003Cem>and\u003C/em>\u003C/span> any failed or unknown nodes have been fenced \u003Cspan class=\"emphasis\">\u003Cem>and\u003C/em>\u003C/span> only on nodes that have been \u003Cspan class=\"emphasis\">\u003Cem>unfenced\u003C/em>\u003C/span>. This is the default value if the \u003Ccode class=\"literal\">provides=unfencing\u003C/code> \u003Ccode class=\"literal\">stonith\u003C/code> meta option has been set for a fencing device.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141499600\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">migration-threshold\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141498512\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">INFINITY\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141497424\"> \u003Cp>\n\t\t\t\t\t\t\t\tHow many failures may occur for this resource on a node before this node is marked ineligible to host this resource. A value of 0 indicates that this feature is disabled (the node will never be marked ineligible); by contrast, the cluster treats \u003Ccode class=\"literal\">INFINITY\u003C/code> (the default) as a very large but finite number. This option has an effect only if the failed operation has \u003Ccode class=\"literal\">on-fail=restart\u003C/code> (the default), and additionally for failed start operations if the cluster property \u003Ccode class=\"literal\">start-failure-is-fatal\u003C/code> is \u003Ccode class=\"literal\">false\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141499600\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">failure-timeout\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141498512\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">0\u003C/code> (disabled)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141497424\"> \u003Cp>\n\t\t\t\t\t\t\t\tIgnore previously failed resource actions after this much time has passed without new failures. This potentially allows the resource to move back to the node on which it failed, if it previously reached its migration threshold there. A value of 0 indicates that failures do not expire.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\u003Cspan class=\"emphasis\">\u003Cem>WARNING\u003C/em>\u003C/span>: If this value is low, and pending cluster activity prevents the cluster from responding to a failure within that time, the failure is ignored completely and does not cause recovery of the resource, even if a recurring action continues to report failure. The value of this option should be at least greater than the longest action timeout for all resources in the cluster. A value in hours or days is reasonable.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141499600\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">multiple-active\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141498512\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">stop_start\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141497424\"> \u003Cp>\n\t\t\t\t\t\t\t\tIndicates what the cluster should do if it ever finds the resource active on more than one node. Allowed values:\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">block\u003C/code> - mark the resource as unmanaged\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">stop_only\u003C/code> - stop all active instances and leave them that way\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">stop_start\u003C/code> - stop all active instances and start the resource in one location only\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">stop_unexpected\u003C/code> - (RHEL 9.1 and later) stop only unexpected instances of the resource, without requiring a full restart. It is the user’s responsibility to verify that the service and its resource agent can function with extra active instances without requiring a full restart.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141499600\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">critical\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141498512\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">true\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141497424\"> \u003Cp>\n\t\t\t\t\t\t\t\tSets the default value for the \u003Ccode class=\"literal\">influence\u003C/code> option for all colocation constraints involving the resource as a dependent resource (\u003Cspan class=\"emphasis\">\u003Cem>target_resource\u003C/em>\u003C/span>), including implicit colocation constraints created when the resource is part of a resource group. The \u003Ccode class=\"literal\">influence\u003C/code> colocation constraint option determines whether the cluster will move both the primary and dependent resources to another node when the dependent resource reaches its migration threshold for failure, or whether the cluster will leave the dependent resource offline without causing a service switch. The \u003Ccode class=\"literal\">critical\u003C/code> resource meta option can have a value of \u003Ccode class=\"literal\">true\u003C/code> or \u003Ccode class=\"literal\">false\u003C/code>, with a default value of \u003Ccode class=\"literal\">true\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141499600\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">allow-unhealthy-nodes\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141498512\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">false\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141497424\"> \u003Cp>\n\t\t\t\t\t\t\t\t(RHEL 9.1 and later) When set to \u003Ccode class=\"literal\">true\u003C/code>, the resource is not forced off a node due to degraded node health. When health resources have this attribute set, the cluster can automatically detect if the node’s health recovers and move resources back to it. A node’s health is determined by a combination of the health attributes set by health resource agents based on local conditions, and the strategy-related options that determine how the cluster reacts to those conditions.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Csection class=\"section\" id=\"changing_the_default_value_of_a_resource_option\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">11.3.1. Changing the default value of a resource option\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tYou can change the default value of a resource option for all resources with the \u003Ccode class=\"literal\">pcs resource defaults update\u003C/code> command. The following command resets the default value of \u003Ccode class=\"literal\">resource-stickiness\u003C/code> to 100.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource defaults update resource-stickiness=100\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tThe original \u003Ccode class=\"literal\">pcs resource defaults \u003Cspan class=\"emphasis\">\u003Cem>name\u003C/em>\u003C/span>=\u003Cspan class=\"emphasis\">\u003Cem>value\u003C/em>\u003C/span>\u003C/code> command, which set defaults for all resources in previous releases, remains supported unless there is more than one set of defaults configured. However, \u003Ccode class=\"literal\">pcs resource defaults update\u003C/code> is now the preferred version of the command.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"changing_the_default_value_of_a_resource_option_for_sets_of_resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">11.3.2. Changing the default value of a resource option for sets of resources\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tYou can create multiple sets of resource defaults with the \u003Ccode class=\"literal\">pcs resource defaults set create\u003C/code> command, which allows you to specify a rule that contains \u003Ccode class=\"literal\">resource\u003C/code> expressions. Only \u003Ccode class=\"literal\">resource\u003C/code> and \u003Ccode class=\"literal\">date\u003C/code> expressions, including \u003Ccode class=\"literal\">and\u003C/code>, \u003Ccode class=\"literal\">or\u003C/code> and parentheses, are allowed in rules that you specify with this command.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tWith the \u003Ccode class=\"literal\">pcs resource defaults set create\u003C/code> command, you can configure a default resource value for all resources of a particular type. If, for example, you are running databases which take a long time to stop, you can increase the \u003Ccode class=\"literal\">resource-stickiness\u003C/code> default value for all resources of the database type to prevent those resources from moving to other nodes more often than you desire.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe following command sets the default value of \u003Ccode class=\"literal\">resource-stickiness\u003C/code> to 100 for all resources of type \u003Ccode class=\"literal\">pqsql\u003C/code>.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tThe \u003Ccode class=\"literal\">id\u003C/code> option, which names the set of resource defaults, is not mandatory. If you do not set this option \u003Ccode class=\"literal\">pcs\u003C/code> will generate an ID automatically. Setting this value allows you to provide a more descriptive name.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tIn this example, \u003Ccode class=\"literal\">::pgsql\u003C/code> means a resource of any class, any provider, of type \u003Ccode class=\"literal\">pgsql\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"circle\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\tSpecifying \u003Ccode class=\"literal\">ocf:heartbeat:pgsql\u003C/code> would indicate class \u003Ccode class=\"literal\">ocf\u003C/code>, provider \u003Ccode class=\"literal\">heartbeat\u003C/code>, type \u003Ccode class=\"literal\">pgsql\u003C/code>,\n\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\tSpecifying \u003Ccode class=\"literal\">ocf:pacemaker:\u003C/code> would indicate all resources of class \u003Ccode class=\"literal\">ocf\u003C/code>, provider \u003Ccode class=\"literal\">pacemaker\u003C/code>, of any type.\n\t\t\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/li>\u003C/ul>\u003C/div>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource defaults set create id=pgsql-stickiness meta resource-stickiness=100 rule resource ::pgsql\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tTo change the default values in an existing set, use the \u003Ccode class=\"literal\">pcs resource defaults set update\u003C/code> command.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"displaying_currently_configured_resource_defaults\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">11.3.3. Displaying currently configured resource defaults\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe \u003Ccode class=\"literal\">pcs resource defaults [config]\u003C/code> command displays a list of currently configured default values for resource options, including any rules that you specified. As of Red Hat Enterprise Linux 9.5, you can display the output of this command in text, JSON, and command formats.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tSpecifying \u003Ccode class=\"literal\">--output-format=text\u003C/code> displays the configured resource defaults in plain text format, which is the default value for this option.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tSpecifying \u003Ccode class=\"literal\">--output-format=cmd\u003C/code> displays the \u003Ccode class=\"literal\">pcs resource defaults\u003C/code> commands created from the current cluster defaults configuration. You can use these commands to re-create configured resource defaults on a different system.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tSpecifying \u003Ccode class=\"literal\">--output-format=json\u003C/code> displays the configured resource defaults in JSON format, which is suitable for machine parsing.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following examples show the three different output formats of the \u003Ccode class=\"literal\">pcs resource defaults config\u003C/code> command after the default values for any \u003Ccode class=\"literal\">ocf:pacemaker:pgsql\u003C/code> resource were reset with the following example command:\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource defaults set create id=set-1 score=100 meta resource-stickiness=10 rule resource ocf:pacemaker:pgsql\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tThis example displays the configured resource default values in plain text.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource defaults config\u003C/strong>\u003C/span>\nMeta Attrs: build-resource-defaults\n  resource-stickiness=1\nMeta Attrs: set-1 score=100\n  resource-stickiness=10\n  Rule: boolean-op=and score=INFINITY\n    Expression: resource ocf:pacemaker:pgsql\u003C/pre>\u003Cp>\n\t\t\t\t\tThis example displays the \u003Ccode class=\"literal\">pcs resource defaults\u003C/code> commands created from the current cluster defaults configuration.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource defaults config --output-format=cmd\u003C/strong>\u003C/span>\npcs -- resource defaults set create id=build-resource-defaults \\\n  meta resource-stickiness=1;\npcs -- resource defaults set create id=set-1 score=100 \\\n  meta resource-stickiness=10 \\\n  rule 'resource ocf:pacemaker:pgsql'\u003C/pre>\u003Cp>\n\t\t\t\t\tThis example displays the configured resource default values in JSON format.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource defaults config --output-format=json\u003C/strong>\u003C/span>\n{\"instance_attributes\": [], \"meta_attributes\": [{\"id\": \"build-resource-defaults\", \"options\": {}, \"rule\": null, \"nvpairs\": [{\"id\": \"build-resource-stickiness\", \"name\": \"resource-stickiness\", \"value\": \"1\"}]}, {\"id\": \"set-1\", \"options\": {\"score\": \"100\"}, \"rule\": {\"id\": \"set-1-rule\", \"type\": \"RULE\", \"in_effect\": \"UNKNOWN\", \"options\": {\"boolean-op\": \"and\", \"score\": \"INFINITY\"}, \"date_spec\": null, \"duration\": null, \"expressions\": [{\"id\": \"set-1-rule-rsc-ocf-pacemaker-pgsql\", \"type\": \"RSC_EXPRESSION\", \"in_effect\": \"UNKNOWN\", \"options\": {\"class\": \"ocf\", \"provider\": \"pacemaker\", \"type\": \"pgsql\"}, \"date_spec\": null, \"duration\": null, \"expressions\": [], \"as_string\": \"resource ocf:pacemaker:pgsql\"}], \"as_string\": \"resource ocf:pacemaker:pgsql\"}, \"nvpairs\": [{\"id\": \"set-1-resource-stickiness\", \"name\": \"resource-stickiness\", \"value\": \"10\"}]}]}\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"setting_meta_options_on_resource_creation\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">11.3.4. Setting meta options on resource creation\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tWhether you have reset the default value of a resource meta option or not, you can set a resource option for a particular resource to a value other than the default when you create the resource. The following shows the format of the \u003Ccode class=\"literal\">pcs resource create\u003C/code> command you use when specifying a value for a resource meta option.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource create \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>standard\u003C/em>\u003C/span>:[\u003Cspan class=\"emphasis\">\u003Cem>provider\u003C/em>\u003C/span>:]]\u003Cspan class=\"emphasis\">\u003Cem>type\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>resource options\u003C/em>\u003C/span>] [meta \u003Cspan class=\"emphasis\">\u003Cem>meta_options\u003C/em>\u003C/span>...]\u003C/pre>\u003Cp>\n\t\t\t\t\tFor example, the following command creates a resource with a \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value of 50.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.120 meta resource-stickiness=50\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tYou can also set the value of a resource meta option for an existing resource, group, or cloned resource with the following command.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource meta \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> | \u003Cspan class=\"emphasis\">\u003Cem>group_id\u003C/em>\u003C/span> | \u003Cspan class=\"emphasis\">\u003Cem>clone_id\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>meta_options\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tIn the following example, there is an existing resource named \u003Ccode class=\"literal\">dummy_resource\u003C/code>. This command sets the \u003Ccode class=\"literal\">failure-timeout\u003C/code> meta option to 20 seconds, so that the resource can attempt to restart on the same node in 20 seconds.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource meta dummy_resource failure-timeout=20s\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tAfter executing this command, you can display the values for the resource to verify that \u003Ccode class=\"literal\">failure-timeout=20s\u003C/code> is set.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource config dummy_resource\u003C/strong>\u003C/span>\n Resource: dummy_resource (class=ocf provider=heartbeat type=Dummy)\n  Meta Attrs: failure-timeout=20s\n  ...\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"proc_creating-resource-groups-configuring-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">11.4. Configuring resource groups\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tOne of the most common elements of a cluster is a set of resources that need to be located together, start sequentially, and stop in the reverse order. To simplify this configuration, Pacemaker supports the concept of resource groups.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"creating_a_resource_group\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">11.4.1. Creating a resource group\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tYou create a resource group with the following command, specifying the resources to include in the group. If the group does not exist, this command creates the group. If the group exists, this command adds additional resources to the group. The resources will start in the order you specify them with this command, and will stop in the reverse order of their starting order.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource group add \u003Cspan class=\"emphasis\">\u003Cem>group_name\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>] ... [\u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>] [--before \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> | --after \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\t\tYou can use the \u003Ccode class=\"literal option\">--before\u003C/code> and \u003Ccode class=\"literal option\">--after\u003C/code> options of this command to specify the position of the added resources relative to a resource that already exists in the group.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tYou can also add a new resource to an existing group when you create the resource, using the following command. The resource you create is added to the group named \u003Cspan class=\"emphasis\">\u003Cem>group_name\u003C/em>\u003C/span>. If the group \u003Cspan class=\"emphasis\">\u003Cem>group_name\u003C/em>\u003C/span> does not exist, it will be created.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource create \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>standard\u003C/em>\u003C/span>:[\u003Cspan class=\"emphasis\">\u003Cem>provider\u003C/em>\u003C/span>:]]\u003Cspan class=\"emphasis\">\u003Cem>type\u003C/em>\u003C/span> [resource_options] [op \u003Cspan class=\"emphasis\">\u003Cem>operation_action\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>operation_options\u003C/em>\u003C/span>] --group \u003Cspan class=\"emphasis\">\u003Cem>group_name\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tThere is no limit to the number of resources a group can contain. The fundamental properties of a group are as follows.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tResources are colocated within a group.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tResources are started in the order in which you specify them. If a resource in the group cannot run anywhere, then no resource specified after that resource is allowed to run.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tResources are stopped in the reverse order in which you specify them.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following example creates a resource group named \u003Ccode class=\"literal\">shortcut\u003C/code> that contains the existing resources \u003Ccode class=\"literal\">IPaddr\u003C/code> and \u003Ccode class=\"literal\">Email\u003C/code>.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource group add shortcut IPaddr Email\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tIn this example:\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tThe \u003Ccode class=\"literal\">IPaddr\u003C/code> is started first, then \u003Ccode class=\"literal\">Email\u003C/code>.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tThe \u003Ccode class=\"literal\">Email\u003C/code> resource is stopped first, then \u003Ccode class=\"literal\">IPAddr\u003C/code>.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tIf \u003Ccode class=\"literal\">IPaddr\u003C/code> cannot run anywhere, neither can \u003Ccode class=\"literal\">Email\u003C/code>.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tIf \u003Ccode class=\"literal\">Email\u003C/code> cannot run anywhere, however, this does not affect \u003Ccode class=\"literal\">IPaddr\u003C/code> in any way.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"removing_a_resource_group\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">11.4.2. Removing a resource group\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tYou remove a resource from a group with the following command. If there are no remaining resources in the group, this command removes the group itself.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource group remove \u003Cspan class=\"emphasis\">\u003Cem>group_name\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>...\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"displaying_resource_groups\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">11.4.3. Displaying resource groups\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following command lists all currently configured resource groups.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource group list\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"s2-group_options-HAAR\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">11.4.4. Group options\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tYou can set the following options for a resource group, and they maintain the same meaning as when they are set for a single resource: \u003Ccode class=\"literal\">priority\u003C/code>, \u003Ccode class=\"literal\">target-role\u003C/code>, \u003Ccode class=\"literal\">is-managed\u003C/code>. For information about resource meta options, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters#proc_configuring-resource-meta-options-configuring-cluster-resources\">Configuring resource meta options\u003C/a>.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"s2-group_stickiness-HAAR\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">11.4.5. Group stickiness\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tStickiness, the measure of how much a resource wants to stay where it is, is additive in groups. Every active resource of the group will contribute its stickiness value to the group’s total. So if the default \u003Ccode class=\"literal\">resource-stickiness\u003C/code> is 100, and a group has seven members, five of which are active, then the group as a whole will prefer its current location with a score of 500.\n\t\t\t\t\u003C/p>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"con_determining-resource-behavior-configuring-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">11.5. Determining resource behavior\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can determine the behavior of a resource in a cluster by configuring constraints for that resource. You can configure the following categories of constraints:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">location\u003C/code> constraints — A location constraint determines which nodes a resource can run on. For information about configuring location constraints, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters\">Determining which nodes a resource can run on\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">order\u003C/code> constraints — An ordering constraint determines the order in which the resources run. For information about configuring ordering constraints, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters\">Determining the order in which cluster resources are run\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">colocation\u003C/code> constraints — A colocation constraint determines where resources will be placed relative to other resources. For information about colocation constraints, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters\">Colocating cluster resources\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tAs a shorthand for configuring a set of constraints that will locate a set of resources together and ensure that the resources start sequentially and stop in reverse order, Pacemaker supports the concept of resource groups. After you have created a resource group, you can configure constraints on the group itself just as you configure constraints for individual resources.\n\t\t\t\u003C/p>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 12. Determining which nodes a resource can run on\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tLocation constraints determine which nodes a resource can run on. You can configure location constraints to determine whether a resource will prefer or avoid a specified node.\n\t\t\u003C/p>\u003Cp>\n\t\t\tIn addition to location constraints, the node on which a resource runs is influenced by the \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value for that resource, which determines to what degree a resource prefers to remain on the node where it is currently running. For information about setting the \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/configuring_and_managing_high_availability_clusters/index#proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on\">Configuring a resource to prefer its current node\u003C/a>.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_configuring-location-constraints-determining-which-node-a-resource-runs-on\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">12.1. Configuring location constraints\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can configure a basic location constraint to specify whether a resource prefers or avoids a node, with an optional \u003Ccode class=\"literal\">score\u003C/code> value to indicate the relative degree of preference for the constraint.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command creates a location constraint for a resource to prefer the specified node or nodes. Note that it is possible to create constraints on a particular resource for more than one node with a single command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs constraint location \u003Cspan class=\"emphasis\">\u003Cem>rsc\u003C/em>\u003C/span> prefers \u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span>[=\u003Cspan class=\"emphasis\">\u003Cem>score\u003C/em>\u003C/span>] [\u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span>[=\u003Cspan class=\"emphasis\">\u003Cem>score\u003C/em>\u003C/span>]] ...\u003C/pre>\u003Cp>\n\t\t\t\tThe following command creates a location constraint for a resource to avoid the specified node or nodes.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs constraint location \u003Cspan class=\"emphasis\">\u003Cem>rsc\u003C/em>\u003C/span> avoids \u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span>[=\u003Cspan class=\"emphasis\">\u003Cem>score\u003C/em>\u003C/span>] [\u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span>[=\u003Cspan class=\"emphasis\">\u003Cem>score\u003C/em>\u003C/span>]] ...\u003C/pre>\u003Cp>\n\t\t\t\tThe following table summarizes the meanings of the basic options for configuring location constraints.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-locationconstraint-options-HAAR-determining-which-node-a-resource-runs-on\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 12.1. Location Constraint Options\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 33%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 67%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686136405808\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686136404720\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136405808\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">rsc\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136404720\"> \u003Cp>\n\t\t\t\t\t\t\t\tA resource name\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136405808\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">node\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136404720\"> \u003Cp>\n\t\t\t\t\t\t\t\tA node’s name\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136405808\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">score\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136404720\"> \u003Cp>\n\t\t\t\t\t\t\t\tPositive integer value to indicate the degree of preference for whether the given resource should prefer or avoid the given node. \u003Ccode class=\"literal\">INFINITY\u003C/code> is the default \u003Ccode class=\"literal\">score\u003C/code> value for a resource location constraint.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tA value of \u003Ccode class=\"literal\">INFINITY\u003C/code> for \u003Ccode class=\"literal\">score\u003C/code> in a \u003Ccode class=\"literal\">pcs constraint location \u003Cspan class=\"emphasis\">\u003Cem>rsc\u003C/em>\u003C/span> prefers\u003C/code> command indicates that the resource will prefer that node if the node is available, but does not prevent the resource from running on another node if the specified node is unavailable.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tA value of \u003Ccode class=\"literal\">INFINITY\u003C/code> for \u003Ccode class=\"literal\">score\u003C/code> in a \u003Ccode class=\"literal\">pcs constraint location \u003Cspan class=\"emphasis\">\u003Cem>rsc\u003C/em>\u003C/span> avoids\u003C/code> command indicates that the resource will never run on that node, even if no other node is available. This is the equivalent of setting a \u003Ccode class=\"literal\">pcs constraint location add\u003C/code> command with a score of \u003Ccode class=\"literal\">-INFINITY\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tA numeric score (that is, not \u003Ccode class=\"literal\">INFINITY\u003C/code>) means the constraint is optional, and will be honored unless some other factor outweighs it. For example, if the resource is already placed on a different node, and its \u003Ccode class=\"literal\">resource-stickiness\u003C/code> score is higher than a \u003Ccode class=\"literal\">prefers\u003C/code> location constraint’s score, then the resource will be left where it is.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tThe following command creates a location constraint to specify that the resource \u003Ccode class=\"literal\">Webserver\u003C/code> prefers node \u003Ccode class=\"literal\">node1\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location Webserver prefers node1\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\u003Ccode class=\"literal command\">pcs\u003C/code> supports regular expressions in location constraints on the command line. These constraints apply to multiple resources based on the regular expression matching resource name. This allows you to configure multiple location constraints with a single command line.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command creates a location constraint to specify that resources \u003Ccode class=\"literal\">dummy0\u003C/code> to \u003Ccode class=\"literal\">dummy9\u003C/code> prefer \u003Ccode class=\"literal\">node1\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location 'regexp%dummy[0-9]' prefers node1\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tSince Pacemaker uses POSIX extended regular expressions as documented at \u003Ca class=\"link\" href=\"http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_04\">http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_04\u003C/a>, you can specify the same constraint with the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location 'regexp%dummy[[:digit:]]' prefers node1\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_limiting-resource-discovery-to-a-subset-of-nodes-determining-which-node-a-resource-runs-on\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">12.2. Limiting resource discovery to a subset of nodes\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tBefore Pacemaker starts a resource anywhere, it first runs a one-time monitor operation (often referred to as a \"probe\") on every node, to learn whether the resource is already running. This process of resource discovery can result in errors on nodes that are unable to execute the monitor.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tWhen configuring a location constraint on a node, you can use the \u003Ccode class=\"literal option\">resource-discovery\u003C/code> option of the \u003Ccode class=\"literal command\">pcs constraint location\u003C/code> command to indicate a preference for whether Pacemaker should perform resource discovery on this node for the specified resource. Limiting resource discovery to a subset of nodes the resource is physically capable of running on can significantly boost performance when a large set of nodes is present. When \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> is in use to expand the node count into the hundreds of nodes range, this option should be considered.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command shows the format for specifying the \u003Ccode class=\"literal option\">resource-discovery\u003C/code> option of the \u003Ccode class=\"literal command\">pcs constraint location\u003C/code> command. In this command, a positive value for \u003Cspan class=\"emphasis\">\u003Cem>score\u003C/em>\u003C/span> corresponds to a basic location constraint that configures a resource to prefer a node, while a negative value for \u003Cspan class=\"emphasis\">\u003Cem>score\u003C/em>\u003C/span> corresponds to a basic location`constraint that configures a resource to avoid a node. As with basic location constraints, you can use regular expressions for resources with these constraints as well.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs constraint location add \u003Cspan class=\"emphasis\">\u003Cem>id\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>rsc\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>score\u003C/em>\u003C/span> [resource-discovery=\u003Cspan class=\"emphasis\">\u003Cem>option\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\tThe following table summarizes the meanings of the basic parameters for configuring constraints for resource discovery.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-resourcediscoveryconstraint-options-HAAR-determining-which-node-a-resource-runs-on\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 12.2. Resource Discovery Constraint Parameters\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 40%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 60%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\"> \u003Cp>\n\t\t\t\t\t\t\t\tField\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\"> \u003Cp>\n\t\t\t\t\t\t\t\tDescription\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">id\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\"> \u003Cp>\n\t\t\t\t\t\t\t\tA user-chosen name for the constraint itself.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">rsc\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\"> \u003Cp>\n\t\t\t\t\t\t\t\tA resource name\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">node\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\"> \u003Cp>\n\t\t\t\t\t\t\t\tA node’s name\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">score\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\"> \u003Cp>\n\t\t\t\t\t\t\t\tInteger value to indicate the degree of preference for whether the given resource should prefer or avoid the given node. A positive value for score corresponds to a basic location constraint that configures a resource to prefer a node, while a negative value for score corresponds to a basic location constraint that configures a resource to avoid a node.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tA value of \u003Ccode class=\"literal\">INFINITY\u003C/code> for \u003Ccode class=\"literal\">score\u003C/code> indicates that the resource will prefer that node if the node is available, but does not prevent the resource from running on another node if the specified node is unavailable. A value of \u003Ccode class=\"literal\">-INFINITY\u003C/code> for \u003Ccode class=\"literal\">score\u003C/code> indicates that the resource will never run on that node, even if no other node is available.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tA numeric score (that is, not \u003Ccode class=\"literal\">INFINITY\u003C/code> or \u003Ccode class=\"literal\">-INFINITY\u003C/code>) means the constraint is optional, and will be honored unless some other factor outweighs it. For example, if the resource is already placed on a different node, and its \u003Ccode class=\"literal\">resource-stickiness\u003C/code> score is higher than a \u003Ccode class=\"literal\">prefers\u003C/code> location constraint’s score, then the resource will be left where it is.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">resource-discovery\u003C/code> options\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\"> \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">always\u003C/code> - Always perform resource discovery for the specified resource on this node. This is the default \u003Ccode class=\"literal\">resource-discovery\u003C/code> value for a resource location constraint.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">never\u003C/code> - Never perform resource discovery for the specified resource on this node.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">exclusive\u003C/code> - Perform resource discovery for the specified resource only on this node (and other nodes similarly marked as \u003Ccode class=\"literal\">exclusive\u003C/code>). Multiple location constraints using \u003Ccode class=\"literal\">exclusive\u003C/code> discovery for the same resource across different nodes creates a subset of nodes \u003Ccode class=\"literal\">resource-discovery\u003C/code> is exclusive to. If a resource is marked for \u003Ccode class=\"literal\">exclusive\u003C/code> discovery on one or more nodes, that resource is only allowed to be placed within that subset of nodes.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tSetting \u003Ccode class=\"literal\">resource-discovery\u003C/code> to \u003Ccode class=\"literal\">never\u003C/code> or \u003Ccode class=\"literal\">exclusive\u003C/code> removes Pacemaker’s ability to detect and stop unwanted instances of a service running where it is not supposed to be. It is up to the system administrator to make sure that the service can never be active on nodes without resource discovery (such as by leaving the relevant software uninstalled).\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-location-constraint-strategy-determining-which-node-a-resource-runs-on\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">12.3. Configuring a location constraint strategy\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen using location constraints, you can configure a general strategy for specifying which nodes a resource can run on:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tOpt-in clusters — Configure a cluster in which, by default, no resource can run anywhere and then selectively enable allowed nodes for specific resources.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tOpt-out clusters — Configure a cluster in which, by default, all resources can run anywhere and then create location constraints for resources that are not allowed to run on specific nodes.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tWhether you should choose to configure your cluster as an opt-in or opt-out cluster depends on both your personal preference and the make-up of your cluster. If most of your resources can run on most of the nodes, then an opt-out arrangement is likely to result in a simpler configuration. On the other hand, if most resources can only run on a small subset of nodes an opt-in configuration might be simpler.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"s3-optin-clusters-HAAR\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">12.3.1. Configuring an \"Opt-In\" cluster\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tTo create an opt-in cluster, set the \u003Ccode class=\"literal\">symmetric-cluster\u003C/code> cluster property to \u003Ccode class=\"literal\">false\u003C/code> to prevent resources from running anywhere by default.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set symmetric-cluster=false\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tEnable nodes for individual resources. The following commands configure location constraints so that the resource \u003Ccode class=\"literal\">Webserver\u003C/code> prefers node \u003Ccode class=\"literal\">example-1\u003C/code>, the resource \u003Ccode class=\"literal\">Database\u003C/code> prefers node \u003Ccode class=\"literal\">example-2\u003C/code>, and both resources can fail over to node \u003Ccode class=\"literal\">example-3\u003C/code> if their preferred node fails. When configuring location constraints for an opt-in cluster, setting a score of zero allows a resource to run on a node without indicating any preference to prefer or avoid the node.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location Webserver prefers example-1=200\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location Webserver prefers example-3=0\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location Database prefers example-2=200\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location Database prefers example-3=0\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"s3-optout-clusters-HAAR\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">12.3.2. Configuring an \"Opt-Out\" cluster\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tTo create an opt-out cluster, set the \u003Ccode class=\"literal\">symmetric-cluster\u003C/code> cluster property to \u003Ccode class=\"literal\">true\u003C/code> to allow resources to run everywhere by default. This is the default configuration if \u003Ccode class=\"literal\">symmetric-cluster\u003C/code> is not set explicitly.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set symmetric-cluster=true\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tThe following commands will then yield a configuration that is equivalent to the example in \"Configuring an \"Opt-In\" cluster\". Both resources can fail over to node \u003Ccode class=\"literal\">example-3\u003C/code> if their preferred node fails, since every node has an implicit score of 0.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location Webserver prefers example-1=200\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location Webserver avoids example-2=INFINITY\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location Database avoids example-1=INFINITY\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location Database prefers example-2=200\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tNote that it is not necessary to specify a score of INFINITY in these commands, since that is the default value for the score.\n\t\t\t\t\u003C/p>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">12.4. Configuring a resource to prefer its current node\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tResources have a \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value that you can set as a meta attribute when you create the resource, as described in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters#proc_configuring-resource-meta-options-configuring-cluster-resources\">Configuring resource meta options\u003C/a>. The \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value determines how much a resource wants to remain on the node where it is currently running. Pacemaker considers the \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value in conjunction with other settings (for example, the score values of location constraints) to determine whether to move a resource to another node or to leave it in place.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tWith a \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value of 0, a cluster may move resources as needed to balance resources across nodes. This may result in resources moving when unrelated resources start or stop. With a positive stickiness, resources have a preference to stay where they are, and move only if other circumstances outweigh the stickiness. This may result in newly-added nodes not getting any resources assigned to them without administrator intervention.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tNewly-created clusters in RHEL 9 set the default value for \u003Ccode class=\"literal\">resource-stickiness\u003C/code> to 1. This small value can easily be overridden by other constraints that you create, but it is enough to prevent Pacemaker from needlessly moving healthy resources around the cluster. If you prefer cluster behavior that results from a \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value of 0, you can change the \u003Ccode class=\"literal\">resource-stickiness\u003C/code> default value to 0 with the following command:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource defaults update resource-stickiness=0\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tIf you upgrade an existing cluster to RHEL 9 and you have not explicitly set a default value for \u003Ccode class=\"literal\">resource-stickiness\u003C/code>, the \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value remains 0 and the \u003Ccode class=\"literal\">pcs resource defaults\u003C/code> command will not show anything for stickiness.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tWith a positive \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value, no resources will move to a newly-added node. If resource balancing is desired at that point, you can temporarily set the \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value to 0.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tNote that if a location constraint score is higher than the \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value, the cluster may still move a healthy resource to the node where the location constraint points.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFor further information about how Pacemaker determines where to place a resource, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters\">Configuring a node placement strategy\u003C/a>.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_exporting-constraints-determining-which-node-a-resource-runs-on\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">12.5. Exporting resource constraints as \u003Ccode class=\"literal\">pcs\u003C/code> commands\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tAs of Red Hat Enterprise Linux 9.3, you can display the \u003Ccode class=\"literal\">pcs\u003C/code> commands that can be used to re-create configured resource constraints on a different system using the \u003Ccode class=\"literal\">--output-format=cmd\u003C/code> option of the \u003Ccode class=\"literal\">pcs constraint\u003C/code> command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following commands create an \u003Ccode class=\"literal\">IPaddr2\u003C/code> resource and an \u003Ccode class=\"literal\">apache\u003C/code> resource.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create VirtualIP IPaddr2 ip=198.51.100.3 cidr_netmask=24\u003C/strong>\u003C/span>\nAssumed agent name 'ocf:heartbeat:IPaddr2' (deduced from 'IPaddr2')\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create Website apache configfile=\"/etc/httpd/conf/httpd.conf\" statusurl=\"http://127.0.0.1/server-status\"\u003C/strong>\u003C/span>\nAssumed agent name 'ocf:heartbeat:apache' (deduced from 'apache')\u003C/pre>\u003Cp>\n\t\t\t\tThe following commands configure a location constraint, a colocation constraint, and an order constraint for the two resources.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location Website avoids node1\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint colocation add Website with VirtualIP\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order VirtualIP then Website\u003C/strong>\u003C/span>\nAdding VirtualIP Website (kind: Mandatory) (Options: first-action=start then-action=start)\u003C/pre>\u003Cp>\n\t\t\t\tAfter you create the resources and the constraints, the following command displays the \u003Ccode class=\"literal\">pcs\u003C/code> commands you can use to re-create the constraints on a different system.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint --output-format=cmd\u003C/strong>\u003C/span>\npcs -- constraint location add location-Website-node1--INFINITY resource%Website node1 -INFINITY;\npcs -- constraint colocation add Website with VirtualIP INFINITY \\\n  id=colocation-Website-VirtualIP-INFINITY;\npcs -- constraint order start VirtualIP then start Website \\\n  id=order-VirtualIP-Website-mandatory\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 13. Determining the order in which cluster resources are run\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tTo determine the order in which the resources run, you configure an ordering constraint.\n\t\t\u003C/p>\u003Cp>\n\t\t\tThe following shows the format for the command to configure an ordering constraint.\n\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs constraint order [\u003Cspan class=\"emphasis\">\u003Cem>action\u003C/em>\u003C/span>] \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> then [\u003Cspan class=\"emphasis\">\u003Cem>action\u003C/em>\u003C/span>] \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\tThe following table summarizes the properties and options for configuring ordering constraints.\n\t\t\u003C/p>\u003Crh-table id=\"idm140686137783136\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 13.1. Properties of an Order Constraint\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 40%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 60%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686137778288\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686137777200\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137778288\"> \u003Cp>\n\t\t\t\t\t\t\tresource_id\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137777200\"> \u003Cp>\n\t\t\t\t\t\t\tThe name of a resource on which an action is performed.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137778288\"> \u003Cp>\n\t\t\t\t\t\t\taction\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137777200\"> \u003Cp>\n\t\t\t\t\t\t\tThe action to be ordered on the resource. Possible values of the \u003Cspan class=\"emphasis\">\u003Cem>action\u003C/em>\u003C/span> property are as follows:\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">start\u003C/code> - Order start actions of the resource.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">stop\u003C/code> - Order stop actions of the resource.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">promote\u003C/code> - Promote the resource from an unpromoted resource to a promoted resource.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">demote\u003C/code> - Demote the resource from a promoted resource to an unpromoted resource.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\tIf no action is specified, the default action is \u003Ccode class=\"literal\">start\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137778288\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">kind\u003C/code> option\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137777200\"> \u003Cp>\n\t\t\t\t\t\t\tHow to enforce the constraint. The possible values of the \u003Ccode class=\"literal\">kind\u003C/code> option are as follows:\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">Optional\u003C/code> - Only applies if both resources are executing the specified action. For information about optional ordering, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/configuring_and_managing_high_availability_clusters/index#proc_configuring-advisory-ordering-determining-resource-order\">Configuring advisory ordering\u003C/a>.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">Mandatory\u003C/code> - Always enforce the constraint (default value). If the first resource you specified is stopping or cannot be started, the second resource you specified must be stopped. For information about mandatory ordering, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/configuring_and_managing_high_availability_clusters/index#con_configuring-mandatory-ordering-determining-resource-order\">Configuring mandatory ordering\u003C/a>.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">Serialize\u003C/code> - Ensure that no two stop/start actions occur concurrently for the resources you specify. The first and second resource you specify can start in either order, but one must complete starting before the other can be started. A typical use case is when resource startup puts a high load on the host.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137778288\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">symmetrical\u003C/code> option\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137777200\"> \u003Cp>\n\t\t\t\t\t\t\tIf true, the reverse of the constraint applies for the opposite action (for example, if B starts after A starts, then B stops before A stops). Ordering constraints for which \u003Ccode class=\"literal\">kind\u003C/code> is \u003Ccode class=\"literal\">Serialize\u003C/code> cannot be symmetrical. The default value is \u003Ccode class=\"literal\">true\u003C/code> for \u003Ccode class=\"literal\">Mandatory\u003C/code> and \u003Ccode class=\"literal\">Optional\u003C/code> kinds, \u003Ccode class=\"literal\">false\u003C/code> for \u003Ccode class=\"literal\">Serialize\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\tUse the following command to remove resources from any ordering constraint.\n\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs constraint order remove \u003Cspan class=\"emphasis\">\u003Cem>resource1\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>resourceN\u003C/em>\u003C/span>]...\u003C/pre>\u003Csection class=\"section\" id=\"con_configuring-mandatory-ordering-determining-resource-order\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">13.1. Configuring mandatory ordering\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tA mandatory ordering constraint indicates that the second action should not be initiated for the second resource unless and until the first action successfully completes for the first resource. Actions that may be ordered are \u003Ccode class=\"literal\">stop\u003C/code>, \u003Ccode class=\"literal\">start\u003C/code>, and additionally for promotable clones, \u003Ccode class=\"literal\">demote\u003C/code> and \u003Ccode class=\"literal\">promote\u003C/code>. For example, \"A then B\" (which is equivalent to \"start A then start B\") means that B will not be started unless and until A successfully starts. An ordering constraint is mandatory if the \u003Ccode class=\"literal\">kind\u003C/code> option for the constraint is set to \u003Ccode class=\"literal\">Mandatory\u003C/code> or left as default.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf the \u003Ccode class=\"literal\">symmetrical\u003C/code> option is set to \u003Ccode class=\"literal\">true\u003C/code> or left to default, the opposite actions will be ordered in reverse. The \u003Ccode class=\"literal\">start\u003C/code> and \u003Ccode class=\"literal\">stop\u003C/code> actions are opposites, and \u003Ccode class=\"literal\">demote\u003C/code> and \u003Ccode class=\"literal\">promote\u003C/code> are opposites. For example, a symmetrical \"promote A then start B\" ordering implies \"stop B then demote A\", which means that A cannot be demoted until and unless B successfully stops. A symmetrical ordering means that changes in A’s state can cause actions to be scheduled for B. For example, given \"A then B\", if A restarts due to failure, B will be stopped first, then A will be stopped, then A will be started, then B will be started.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tNote that the cluster reacts to each state change. If the first resource is restarted and is in a started state again before the second resource initiated a stop operation, the second resource will not need to be restarted.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-advisory-ordering-determining-resource-order\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">13.2. Configuring advisory ordering\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen the \u003Ccode class=\"literal\">kind=Optional\u003C/code> option is specified for an ordering constraint, the constraint is considered optional and only applies if both resources are executing the specified actions. Any change in state by the first resource you specify will have no effect on the second resource you specify.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command configures an advisory ordering constraint for the resources named \u003Ccode class=\"literal\">VirtualIP\u003C/code> and \u003Ccode class=\"literal\">dummy_resource\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order VirtualIP then dummy_resource kind=Optional\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-ordered-resource-sets.adocdetermining-resource-order\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">13.3. Configuring ordered resource sets\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tA common situation is for an administrator to create a chain of ordered resources, where, for example, resource A starts before resource B which starts before resource C. If your configuration requires that you create a set of resources that is colocated and started in order, you can configure a resource group that contains those resources.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThere are some situations, however, where configuring the resources that need to start in a specified order as a resource group is not appropriate:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tYou may need to configure resources to start in order and the resources are not necessarily colocated.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tYou may have a resource C that must start after either resource A or B has started but there is no relationship between A and B.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tYou may have resources C and D that must start after both resources A and B have started, but there is no relationship between A and B or between C and D.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tIn these situations, you can create an ordering constraint on a set or sets of resources with the \u003Ccode class=\"literal command\">pcs constraint order set\u003C/code> command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can set the following options for a set of resources with the \u003Ccode class=\"literal command\">pcs constraint order set\u003C/code> command.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">sequential\u003C/code>, which can be set to \u003Ccode class=\"literal\">true\u003C/code> or \u003Ccode class=\"literal\">false\u003C/code> to indicate whether the set of resources must be ordered relative to each other. The default value is \u003Ccode class=\"literal\">true\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSetting \u003Ccode class=\"literal\">sequential\u003C/code> to \u003Ccode class=\"literal\">false\u003C/code> allows a set to be ordered relative to other sets in the ordering constraint, without its members being ordered relative to each other. Therefore, this option makes sense only if multiple sets are listed in the constraint; otherwise, the constraint has no effect.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">require-all\u003C/code>, which can be set to \u003Ccode class=\"literal\">true\u003C/code> or \u003Ccode class=\"literal\">false\u003C/code> to indicate whether all of the resources in the set must be active before continuing. Setting \u003Ccode class=\"literal\">require-all\u003C/code> to \u003Ccode class=\"literal\">false\u003C/code> means that only one resource in the set needs to be started before continuing on to the next set. Setting \u003Ccode class=\"literal\">require-all\u003C/code> to \u003Ccode class=\"literal\">false\u003C/code> has no effect unless used in conjunction with unordered sets, which are sets for which \u003Ccode class=\"literal\">sequential\u003C/code> is set to \u003Ccode class=\"literal\">false\u003C/code>. The default value is \u003Ccode class=\"literal\">true\u003C/code>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">action\u003C/code>, which can be set to \u003Ccode class=\"literal\">start\u003C/code>, \u003Ccode class=\"literal\">promote\u003C/code>, \u003Ccode class=\"literal\">demote\u003C/code> or \u003Ccode class=\"literal\">stop\u003C/code>, as described in the \"Properties of an Order Constraint\" table in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters\">Determining the order in which cluster resources are run\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">role\u003C/code>, which can be set to \u003Ccode class=\"literal\">Stopped\u003C/code>, \u003Ccode class=\"literal\">Started\u003C/code>, \u003Ccode class=\"literal\">Promoted\u003C/code>, or \u003Ccode class=\"literal\">Unpromoted\u003C/code>.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tYou can set the following constraint options for a set of resources following the \u003Ccode class=\"literal\">setoptions\u003C/code> parameter of the \u003Ccode class=\"literal command\">pcs constraint order set\u003C/code> command.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">id\u003C/code>, to provide a name for the constraint you are defining.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">kind\u003C/code>, which indicates how to enforce the constraint, as described in the \"Properties of an Order Constraint\" table in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters\">Determining the order in which cluster resources are run\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">symmetrical\u003C/code>, to set whether the reverse of the constraint applies for the opposite action, as described in in the \"Properties of an Order Constraint\" table in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters\">Determining the order in which cluster resources are run\u003C/a>.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cpre class=\"literallayout\">pcs constraint order set \u003Cspan class=\"emphasis\">\u003Cem>resource1\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>resource2\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>resourceN\u003C/em>\u003C/span>]... [\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>] [set \u003Cspan class=\"emphasis\">\u003Cem>resourceX\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>resourceY\u003C/em>\u003C/span> ... [\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>]] [setoptions [\u003Cspan class=\"emphasis\">\u003Cem>constraint_options\u003C/em>\u003C/span>]]\u003C/pre>\u003Cp>\n\t\t\t\tIf you have three resources named \u003Ccode class=\"literal\">D1\u003C/code>, \u003Ccode class=\"literal\">D2\u003C/code>, and \u003Ccode class=\"literal\">D3\u003C/code>, the following command configures them as an ordered resource set.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order set D1 D2 D3\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tIf you have six resources named \u003Ccode class=\"literal\">A\u003C/code>, \u003Ccode class=\"literal\">B\u003C/code>, \u003Ccode class=\"literal\">C\u003C/code>, \u003Ccode class=\"literal\">D\u003C/code>, \u003Ccode class=\"literal\">E\u003C/code>, and \u003Ccode class=\"literal\">F\u003C/code>, this example configures an ordering constraint for the set of resources that will start as follows:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">A\u003C/code> and \u003Ccode class=\"literal\">B\u003C/code> start independently of each other\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">C\u003C/code> starts once either \u003Ccode class=\"literal\">A\u003C/code> or \u003Ccode class=\"literal\">B\u003C/code> has started\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">D\u003C/code> starts once \u003Ccode class=\"literal\">C\u003C/code> has started\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">E\u003C/code> and \u003Ccode class=\"literal\">F\u003C/code> start independently of each other once \u003Ccode class=\"literal\">D\u003C/code> has started\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tStopping the resources is not influenced by this constraint since \u003Ccode class=\"literal\">symmetrical=false\u003C/code> is set.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order set A B sequential=false require-all=false set C D set E F sequential=false setoptions symmetrical=false\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">13.4. Configuring startup order for resource dependencies not managed by Pacemaker\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIt is possible for a cluster to include resources with dependencies that are not themselves managed by the cluster. In this case, you must ensure that those dependencies are started before Pacemaker is started and stopped after Pacemaker is stopped.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can configure your startup order to account for this situation by means of the \u003Ccode class=\"literal\">systemd\u003C/code> \u003Ccode class=\"literal\">resource-agents-deps\u003C/code> target. You can create a \u003Ccode class=\"literal\">systemd\u003C/code> drop-in unit for this target and Pacemaker will order itself appropriately relative to this target.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFor example, if a cluster includes a resource that depends on the external service \u003Ccode class=\"literal\">foo\u003C/code> that is not managed by the cluster, perform the following procedure.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate the drop-in unit \u003Ccode class=\"literal\">/etc/systemd/system/resource-agents-deps.target.d/foo.conf\u003C/code> that contains the following:\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[Unit]\nRequires=foo.service\nAfter=foo.service\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tRun the \u003Ccode class=\"literal command\">systemctl daemon-reload\u003C/code> command.\n\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\tA cluster dependency specified in this way can be something other than a service. For example, you may have a dependency on mounting a file system at \u003Ccode class=\"literal\">/srv\u003C/code>, in which case you would perform the following procedure:\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tEnsure that \u003Ccode class=\"literal\">/srv\u003C/code> is listed in the \u003Ccode class=\"literal\">/etc/fstab\u003C/code> file. This will be converted automatically to the \u003Ccode class=\"literal\">systemd\u003C/code> file \u003Ccode class=\"literal\">srv.mount\u003C/code> at boot when the configuration of the system manager is reloaded. For more information, see the \u003Ccode class=\"literal\">systemd.mount\u003C/code>(5) and the \u003Ccode class=\"literal\">systemd-fstab-generator\u003C/code>(8) man pages.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tTo make sure that Pacemaker starts after the disk is mounted, create the drop-in unit \u003Ccode class=\"literal\">/etc/systemd/system/resource-agents-deps.target.d/srv.conf\u003C/code> that contains the following.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[Unit]\nRequires=srv.mount\nAfter=srv.mount\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tRun the \u003Ccode class=\"literal command\">systemctl daemon-reload\u003C/code> command.\n\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\tIf an LVM volume group used by a Pacemaker cluster contains one or more physical volumes that reside on remote block storage, such as an iSCSI target, you can configure a \u003Ccode class=\"literal\">systemd resource-agents-deps\u003C/code> target and a \u003Ccode class=\"literal\">systemd\u003C/code> drop-in unit for the target to ensure that the service starts before Pacemaker starts.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following procedure configures \u003Ccode class=\"literal\">blk-availability.service\u003C/code> as a dependency. The \u003Ccode class=\"literal\">blk-availability.service\u003C/code> service is a wrapper that includes \u003Ccode class=\"literal\">iscsi.service\u003C/code>, among other services. If your deployment requires it, you could configure \u003Ccode class=\"literal\">iscsi.service\u003C/code> (for iSCSI only) or \u003Ccode class=\"literal\">remote-fs.target\u003C/code> as the dependency instead of \u003Ccode class=\"literal\">blk-availability\u003C/code>.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate the drop-in unit \u003Ccode class=\"literal\">/etc/systemd/system/resource-agents-deps.target.d/blk-availability.conf\u003C/code> that contains the following:\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[Unit]\nRequires=blk-availability.service\nAfter=blk-availability.service\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tRun the \u003Ccode class=\"literal command\">systemctl daemon-reload\u003C/code> command.\n\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 14. Colocating cluster resources\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tTo specify that the location of one resource depends on the location of another resource, you configure a colocation constraint.\n\t\t\u003C/p>\u003Cp>\n\t\t\tThere is an important side effect of creating a colocation constraint between two resources: it affects the order in which resources are assigned to a node. This is because you cannot place resource A relative to resource B unless you know where resource B is. So when you are creating colocation constraints, it is important to consider whether you should colocate resource A with resource B or resource B with resource A.\n\t\t\u003C/p>\u003Cp>\n\t\t\tAnother thing to keep in mind when creating colocation constraints is that, assuming resource A is colocated with resource B, the cluster will also take into account resource A’s preferences when deciding which node to choose for resource B.\n\t\t\u003C/p>\u003Cp>\n\t\t\tThe following command creates a colocation constraint.\n\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs constraint colocation add [promoted|unpromoted] \u003Cspan class=\"emphasis\">\u003Cem>source_resource\u003C/em>\u003C/span> with [promoted|unpromoted] \u003Cspan class=\"emphasis\">\u003Cem>target_resource\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>score\u003C/em>\u003C/span>] [\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\tThe following table summarizes the properties and options for configuring colocation constraints.\n\t\t\u003C/p>\u003Crh-table id=\"idm140686139802688\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 14.1. Parameters of a Colocation Constraint\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 40%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 60%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686139797840\" scope=\"col\">Parameter\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686139796752\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139797840\"> \u003Cp>\n\t\t\t\t\t\t\tsource_resource\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139796752\"> \u003Cp>\n\t\t\t\t\t\t\tThe colocation source. If the constraint cannot be satisfied, the cluster may decide not to allow the resource to run at all.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139797840\"> \u003Cp>\n\t\t\t\t\t\t\ttarget_resource\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139796752\"> \u003Cp>\n\t\t\t\t\t\t\tThe colocation target. The cluster will decide where to put this resource first and then decide where to put the source resource.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139797840\"> \u003Cp>\n\t\t\t\t\t\t\tscore\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139796752\"> \u003Cp>\n\t\t\t\t\t\t\tPositive values indicate the resource should run on the same node. Negative values indicate the resources should not run on the same node. A value of +\u003Ccode class=\"literal\">INFINITY\u003C/code>, the default value, indicates that the \u003Cspan class=\"emphasis\">\u003Cem>source_resource\u003C/em>\u003C/span> must run on the same node as the \u003Cspan class=\"emphasis\">\u003Cem>target_resource\u003C/em>\u003C/span>. A value of -\u003Ccode class=\"literal\">INFINITY\u003C/code> indicates that the \u003Cspan class=\"emphasis\">\u003Cem>source_resource\u003C/em>\u003C/span> must not run on the same node as the \u003Cspan class=\"emphasis\">\u003Cem>target_resource\u003C/em>\u003C/span>.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139797840\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">influence\u003C/code> option\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139796752\"> \u003Cp>\n\t\t\t\t\t\t\tDetermines whether the cluster will move both the primary resource (\u003Cspan class=\"emphasis\">\u003Cem>source_resource\u003C/em>\u003C/span>) and dependent resources (\u003Cspan class=\"emphasis\">\u003Cem>target_resource)\u003C/em>\u003C/span> to another node when the dependent resource reaches its migration threshold for failure, or whether the cluster will leave the dependent resource offline without causing a service switch.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\tThe \u003Ccode class=\"literal\">influence\u003C/code> colocation constraint option can have a value of \u003Ccode class=\"literal\">true\u003C/code> or \u003Ccode class=\"literal\">false\u003C/code>. The default value for this option is determined by the value of the dependent resource’s \u003Ccode class=\"literal\">critical\u003C/code> resource meta option, which has a default value of \u003Ccode class=\"literal\">true\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\tWhen this option has a value of \u003Ccode class=\"literal\">true\u003C/code>, Pacemaker will attempt to keep both the primary and dependent resource active. If the dependent resource reaches its migration threshold for failures, both resources will move to another node if possible.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\tWhen this option has a value of \u003Ccode class=\"literal\">false\u003C/code>, Pacemaker will avoid moving the primary resource as a result of the status of the dependent resource. In this case, if the dependent resource reaches its migration threshold for failures, it will stop if the primary resource is active and can remain on its current node.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Csection class=\"section\" id=\"proc_specifying-mandatory-placement.adoc-colocating-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">14.1. Specifying mandatory placement of resources\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tMandatory placement occurs any time the constraint’s score is \u003Ccode class=\"literal\">+INFINITY\u003C/code> or \u003Ccode class=\"literal\">-INFINITY\u003C/code>. In such cases, if the constraint cannot be satisfied, then the \u003Cspan class=\"emphasis\">\u003Cem>source_resource\u003C/em>\u003C/span> is not permitted to run. For \u003Ccode class=\"literal\">score=INFINITY\u003C/code>, this includes cases where the \u003Cspan class=\"emphasis\">\u003Cem>target_resource\u003C/em>\u003C/span> is not active.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf you need \u003Ccode class=\"literal\">myresource1\u003C/code> to always run on the same machine as \u003Ccode class=\"literal\">myresource2\u003C/code>, you would add the following constraint:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint colocation add myresource1 with myresource2 score=INFINITY\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tBecause \u003Ccode class=\"literal\">INFINITY\u003C/code> was used, if \u003Ccode class=\"literal\">myresource2\u003C/code> cannot run on any of the cluster nodes (for whatever reason) then \u003Ccode class=\"literal\">myresource1\u003C/code> will not be allowed to run.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tAlternatively, you may want to configure the opposite, a cluster in which \u003Ccode class=\"literal\">myresource1\u003C/code> cannot run on the same machine as \u003Ccode class=\"literal\">myresource2\u003C/code>. In this case use \u003Ccode class=\"literal\">score=-INFINITY\u003C/code>\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint colocation add myresource1 with myresource2 score=-INFINITY\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tAgain, by specifying \u003Ccode class=\"literal\">-INFINITY\u003C/code>, the constraint is binding. So if the only place left to run is where \u003Ccode class=\"literal\">myresource2\u003C/code> already is, then \u003Ccode class=\"literal\">myresource1\u003C/code> may not run anywhere.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"con_specifying-advisory-placement-colocating-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">14.2. Specifying advisory placement of resources\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tAdvisory placement of resources indicates the placement of resources is a preference, but is not mandatory. For constraints with scores greater than \u003Ccode class=\"literal\">-INFINITY\u003C/code> and less than \u003Ccode class=\"literal\">INFINITY\u003C/code>, the cluster will try to accommodate your wishes but may ignore them if the alternative is to stop some of the cluster resources.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_colocating-resource-sets.adoc-colocating-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">14.3. Colocating sets of resources\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf your configuration requires that you create a set of resources that are colocated and started in order, you can configure a resource group that contains those resources. There are some situations, however, where configuring the resources that need to be colocated as a resource group is not appropriate:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tYou may need to colocate a set of resources but the resources do not necessarily need to start in order.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tYou may have a resource C that must be colocated with either resource A or B, but there is no relationship between A and B.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tYou may have resources C and D that must be colocated with both resources A and B, but there is no relationship between A and B or between C and D.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tIn these situations, you can create a colocation constraint on a set or sets of resources with the \u003Ccode class=\"literal command\">pcs constraint colocation set\u003C/code> command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can set the following options for a set of resources with the \u003Ccode class=\"literal command\">pcs constraint colocation set\u003C/code> command.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">sequential\u003C/code>, which can be set to \u003Ccode class=\"literal\">true\u003C/code> or \u003Ccode class=\"literal\">false\u003C/code> to indicate whether the members of the set must be colocated with each other.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSetting \u003Ccode class=\"literal\">sequential\u003C/code> to \u003Ccode class=\"literal\">false\u003C/code> allows the members of this set to be colocated with another set listed later in the constraint, regardless of which members of this set are active. Therefore, this option makes sense only if another set is listed after this one in the constraint; otherwise, the constraint has no effect.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">role\u003C/code>, which can be set to \u003Ccode class=\"literal\">Stopped\u003C/code>, \u003Ccode class=\"literal\">Started\u003C/code>, \u003Ccode class=\"literal\">Promoted\u003C/code>, or \u003Ccode class=\"literal\">Unpromoted\u003C/code>.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tYou can set the following constraint option for a set of resources following the \u003Ccode class=\"literal\">setoptions\u003C/code> parameter of the \u003Ccode class=\"literal command\">pcs constraint colocation set\u003C/code> command.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">id\u003C/code>, to provide a name for the constraint you are defining.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">score\u003C/code>, to indicate the degree of preference for this constraint. For information about this option, see the \"Location Constraint Options\" table in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters#proc_configuring-location-constraints-determining-which-node-a-resource-runs-on\">Configuring Location Constraints\u003C/a>\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tWhen listing members of a set, each member is colocated with the one before it. For example, \"set A B\" means \"B is colocated with A\". However, when listing multiple sets, each set is colocated with the one after it. For example, \"set C D sequential=false set A B\" means \"set C D (where C and D have no relation between each other) is colocated with set A B (where B is colocated with A)\".\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command creates a colocation constraint on a set or sets of resources.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs constraint colocation set \u003Cspan class=\"emphasis\">\u003Cem>resource1\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>resource2\u003C/em>\u003C/span>] [\u003Cspan class=\"emphasis\">\u003Cem>resourceN\u003C/em>\u003C/span>]... [\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>] [set \u003Cspan class=\"emphasis\">\u003Cem>resourceX\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>resourceY\u003C/em>\u003C/span>] ... [\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>]] [setoptions [\u003Cspan class=\"emphasis\">\u003Cem>constraint_options\u003C/em>\u003C/span>]]\u003C/pre>\u003Cp>\n\t\t\t\tUse the following command to remove colocation constraints with \u003Cspan class=\"emphasis\">\u003Cem>source_resource\u003C/em>\u003C/span>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs constraint colocation remove \u003Cspan class=\"emphasis\">\u003Cem>source_resource\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>target_resource\u003C/em>\u003C/span>\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"proc_displaying-resource-constraints.adoc-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 15. Displaying resource constraints and resource dependencies\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tThere are a several commands you can use to display constraints that have been configured. You can display all configured resource constraints, or you can limit the display of esource constraints to specific types of resource constraints. Additionally, you can display configured resource dependencies.\n\t\t\u003C/p>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Displaying all configured constraints\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\tThe following command lists all current location, order, and colocation constraints. If the \u003Ccode class=\"literal\">--full\u003C/code> option is specified, show the internal constraint IDs.\n\t\t\t\u003C/p>\u003C/div>\u003Cpre class=\"literallayout\">pcs constraint [list|show] [--full]\u003C/pre>\u003Cp>\n\t\t\tBy default, listing resource constraints does not display expired constraints. To include expired constaints in the listing, use the \u003Ccode class=\"literal\">--all\u003C/code> option of the \u003Ccode class=\"literal\">pcs constraint\u003C/code> command. This will list expired constraints, noting the constraints and their associated rules as \u003Ccode class=\"literal\">(expired)\u003C/code> in the display.\n\t\t\u003C/p>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Displaying location constraints\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\tThe following command lists all current location constraints.\n\t\t\t\u003C/p>\u003C/div>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\tIf \u003Ccode class=\"literal\">resources\u003C/code> is specified, location constraints are displayed per resource. This is the default behavior.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tIf \u003Ccode class=\"literal\">nodes\u003C/code> is specified, location constraints are displayed per node.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tIf specific resources or nodes are specified, then only information about those resources or nodes is displayed.\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cpre class=\"literallayout\">pcs constraint location [show [resources [\u003Cspan class=\"emphasis\">\u003Cem>resource\u003C/em>\u003C/span>...]] | [nodes [\u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span>...]]] [--full]\u003C/pre>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Displaying ordering constraints\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\tThe following command lists all current ordering constraints.\n\t\t\t\u003C/p>\u003C/div>\u003Cpre class=\"literallayout\">pcs constraint order [show]\u003C/pre>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Displaying colocation constraints\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\tThe following command lists all current colocation constraints.\n\t\t\t\u003C/p>\u003C/div>\u003Cpre class=\"literallayout\">pcs constraint colocation [show]\u003C/pre>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Displaying resource-specific constraints\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\tThe following command lists the constraints that reference specific resources.\n\t\t\t\u003C/p>\u003C/div>\u003Cpre class=\"literallayout\">pcs constraint ref \u003Cspan class=\"emphasis\">\u003Cem>resource\u003C/em>\u003C/span> ...\u003C/pre>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Displaying resource dependencies\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\tThe following command displays the relations between cluster resources in a tree structure.\n\t\t\t\u003C/p>\u003C/div>\u003Cpre class=\"literallayout\">pcs resource relations \u003Cspan class=\"emphasis\">\u003Cem>resource\u003C/em>\u003C/span> [--full]\u003C/pre>\u003Cp>\n\t\t\tIf the \u003Ccode class=\"literal\">--full\u003C/code> option is used, the command displays additional information, including the constraint IDs and the resource types.\n\t\t\u003C/p>\u003Cp>\n\t\t\tIn the following example, there are 3 configured resources: C, D, and E.\n\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order start C then start D\u003C/strong>\u003C/span>\nAdding C D (kind: Mandatory) (Options: first-action=start then-action=start)\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order start D then start E\u003C/strong>\u003C/span>\nAdding D E (kind: Mandatory) (Options: first-action=start then-action=start)\n\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource relations C\u003C/strong>\u003C/span>\nC\n`- order\n   |  start C then start D\n   `- D\n      `- order\n         |  start D then start E\n         `- E\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource relations D\u003C/strong>\u003C/span>\nD\n|- order\n|  |  start C then start D\n|  `- C\n`- order\n   |  start D then start E\n   `- E\n# pcs \u003Cspan class=\"strong strong\">\u003Cstrong>resource relations E\u003C/strong>\u003C/span>\nE\n`- order\n   |  start D then start E\n   `- D\n      `- order\n         |  start C then start D\n         `- C\u003C/pre>\u003Cp>\n\t\t\tIn the following example, there are 2 configured resources: A and B. Resources A and B are part of resource group G.\n\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource relations A\u003C/strong>\u003C/span>\nA\n`- outer resource\n   `- G\n      `- inner resource(s)\n         |  members: A B\n         `- B\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource relations B\u003C/strong>\u003C/span>\nB\n`- outer resource\n   `- G\n      `- inner resource(s)\n         |  members: A B\n         `- A\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource relations G\u003C/strong>\u003C/span>\nG\n`- inner resource(s)\n   |  members: A B\n   |- A\n   `- B\u003C/pre>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_determining-resource-location-with-rules-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 16. Determining resource location with rules\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tFor more complicated location constraints, you can use Pacemaker rules to determine a resource’s location.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"ref_pacemaker-rules.adoc-determining-resource-location-with-rules\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">16.1. Pacemaker rules\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tPacemaker rules can be used to make your configuration more dynamic. One use of rules might be to assign machines to different processing groups (using a node attribute) based on time and to then use that attribute when creating location constraints.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tEach rule can contain a number of expressions, date-expressions and even other rules. The results of the expressions are combined based on the rule’s \u003Ccode class=\"literal\">boolean-op\u003C/code> field to determine if the rule ultimately evaluates to \u003Ccode class=\"literal\">true\u003C/code> or \u003Ccode class=\"literal\">false\u003C/code>. What happens next depends on the context in which the rule is being used.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-rule-props-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 16.1. Properties of a Rule\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 50%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 50%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686137985360\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686137984272\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137985360\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">role\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137984272\"> \u003Cp>\n\t\t\t\t\t\t\t\tLimits the rule to apply only when the resource is in that role. Allowed values: \u003Ccode class=\"literal\">Started\u003C/code>, \u003Ccode class=\"literal\">Unpromoted,\u003C/code> and \u003Ccode class=\"literal\">Promoted\u003C/code>. NOTE: A rule with \u003Ccode class=\"literal\">role=\"Promoted\"\u003C/code> cannot determine the initial location of a clone instance. It will only affect which of the active instances will be promoted.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137985360\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">score\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137984272\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe score to apply if the rule evaluates to \u003Ccode class=\"literal\">true\u003C/code>. Limited to use in rules that are part of location constraints.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137985360\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">score-attribute\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137984272\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe node attribute to look up and use as a score if the rule evaluates to \u003Ccode class=\"literal\">true\u003C/code>. Limited to use in rules that are part of location constraints.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137985360\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">boolean-op\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137984272\"> \u003Cp>\n\t\t\t\t\t\t\t\tHow to combine the result of multiple expression objects. Allowed values: \u003Ccode class=\"literal\">and\u003C/code> and \u003Ccode class=\"literal\">or\u003C/code>. The default value is \u003Ccode class=\"literal\">and\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Csection class=\"section\" id=\"node_attribute_expressions\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">16.1.1. Node attribute expressions\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tNode attribute expressions are used to control a resource based on the attributes defined by a node or nodes.\n\t\t\t\t\u003C/p>\u003Crh-table id=\"tb-expressions-props-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 16.2. Properties of an Expression\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 50%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 50%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686137044976\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686137043888\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137044976\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">attribute\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137043888\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tThe node attribute to test\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137044976\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">type\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137043888\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tDetermines how the value(s) should be tested. Allowed values: \u003Ccode class=\"literal\">string\u003C/code>, \u003Ccode class=\"literal\">integer\u003C/code>, \u003Ccode class=\"literal\">number\u003C/code>, \u003Ccode class=\"literal\">version\u003C/code>. The default value is \u003Ccode class=\"literal\">string\u003C/code>.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137044976\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">operation\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137043888\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tThe comparison to perform. Allowed values:\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">lt\u003C/code> - True if the node attribute’s value is less than \u003Ccode class=\"literal\">value\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">gt\u003C/code> - True if the node attribute’s value is greater than \u003Ccode class=\"literal\">value\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">lte\u003C/code> - True if the node attribute’s value is less than or equal to \u003Ccode class=\"literal\">value\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">gte\u003C/code> - True if the node attribute’s value is greater than or equal to \u003Ccode class=\"literal\">value\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">eq\u003C/code> - True if the node attribute’s value is equal to \u003Ccode class=\"literal\">value\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">ne\u003C/code> - True if the node attribute’s value is not equal to \u003Ccode class=\"literal\">value\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">defined\u003C/code> - True if the node has the named attribute\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">not_defined\u003C/code> - True if the node does not have the named attribute\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137044976\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">value\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137043888\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tUser supplied value for comparison (required unless \u003Ccode class=\"literal\">operation\u003C/code> is \u003Ccode class=\"literal\">defined\u003C/code> or \u003Ccode class=\"literal\">not_defined\u003C/code>)\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\t\tIn addition to any attributes added by the administrator, the cluster defines special, built-in node attributes for each node that can also be used, as described in the following table.\n\t\t\t\t\u003C/p>\u003Crh-table id=\"tb-nodeattributes-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 16.3. Built-in Node Attributes\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 50%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 50%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686140515856\" scope=\"col\">Name\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686140514768\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140515856\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">#uname\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140514768\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tNode name\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140515856\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">#id\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140514768\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tNode ID\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140515856\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">#kind\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140514768\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tNode type. Possible values are \u003Ccode class=\"literal\">cluster\u003C/code>, \u003Ccode class=\"literal\">remote\u003C/code>, and \u003Ccode class=\"literal\">container\u003C/code>. The value of \u003Ccode class=\"literal\">kind\u003C/code> is \u003Ccode class=\"literal\">remote\u003C/code> for Pacemaker Remote nodes created with the \u003Ccode class=\"literal\">ocf:pacemaker:remote\u003C/code> resource, and \u003Ccode class=\"literal\">container\u003C/code> for Pacemaker Remote guest nodes and bundle nodes.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140515856\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">#is_dc\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140514768\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">true\u003C/code> if this node is a Designated Controller (DC), \u003Ccode class=\"literal\">false\u003C/code> otherwise\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140515856\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">#cluster_name\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140514768\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tThe value of the \u003Ccode class=\"literal\">cluster-name\u003C/code> cluster property, if set\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140515856\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">#site_name\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140514768\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tThe value of the \u003Ccode class=\"literal\">site-name\u003C/code> node attribute, if set, otherwise identical to \u003Ccode class=\"literal\">#cluster-name\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140515856\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">#role\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140514768\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tThe role the relevant promotable clone has on this node. Valid only within a rule for a location constraint for a promotable clone.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003C/section>\u003Csection class=\"section\" id=\"time_date_based_expressions\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">16.1.2. Time/date based expressions\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tDate expressions are used to control a resource or cluster option based on the current date/time. They can contain an optional date specification.\n\t\t\t\t\u003C/p>\u003Crh-table id=\"tb-dateexpress-props-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 16.4. Properties of a Date Expression\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 50%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 50%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686139006992\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686139005904\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139006992\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">start\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139005904\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tA date/time conforming to the ISO8601 specification.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139006992\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">end\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139005904\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tA date/time conforming to the ISO8601 specification.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139006992\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">operation\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139005904\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tCompares the current date/time with the start or the end date or both the start and end date, depending on the context. Allowed values:\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">gt\u003C/code> - True if the current date/time is after \u003Ccode class=\"literal\">start\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">lt\u003C/code> - True if the current date/time is before \u003Ccode class=\"literal\">end\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">in_range\u003C/code> - True if the current date/time is after \u003Ccode class=\"literal\">start\u003C/code> and before \u003Ccode class=\"literal\">end\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">date-spec\u003C/code> - performs a cron-like comparison to the current date/time\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003C/section>\u003Csection class=\"section\" id=\"date_specifications\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">16.1.3. Date specifications\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tDate specifications are used to create cron-like expressions relating to time. Each field can contain a single number or a single range. Instead of defaulting to zero, any field not supplied is ignored.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tFor example, \u003Ccode class=\"literal\">monthdays=\"1\"\u003C/code> matches the first day of every month and \u003Ccode class=\"literal\">hours=\"09-17\"\u003C/code> matches the hours between 9 am and 5 pm (inclusive). However, you cannot specify \u003Ccode class=\"literal\">weekdays=\"1,2\"\u003C/code> or \u003Ccode class=\"literal\">weekdays=\"1-2,5-6\"\u003C/code> since they contain multiple ranges.\n\t\t\t\t\u003C/p>\u003Crh-table id=\"tb-datespecs-props-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 16.5. Properties of a Date Specification\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 50%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 50%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686141555536\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686141554448\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141555536\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">id\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141554448\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tA unique name for the date\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141555536\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">hours\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141554448\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tAllowed values: 0-23\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141555536\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">monthdays\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141554448\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tAllowed values: 0-31 (depending on month and year)\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141555536\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">weekdays\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141554448\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tAllowed values: 1-7 (1=Monday, 7=Sunday)\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141555536\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">yeardays\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141554448\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tAllowed values: 1-366 (depending on the year)\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141555536\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">months\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141554448\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tAllowed values: 1-12\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141555536\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">weeks\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141554448\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tAllowed values: 1-53 (depending on \u003Ccode class=\"literal\">weekyear\u003C/code>)\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141555536\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">years\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141554448\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tYear according the Gregorian calendar\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141555536\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">weekyears\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141554448\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tMay differ from Gregorian years; for example, \u003Ccode class=\"literal\">2005-001 Ordinal\u003C/code> is also \u003Ccode class=\"literal\">2005-01-01 Gregorian\u003C/code> is also \u003Ccode class=\"literal\">2004-W53-6 Weekly\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141555536\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">moon\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686141554448\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tAllowed values: 0-7 (0 is new, 4 is full moon).\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"ref_configuring-constraint-using-rules.adoc-determining-resource-location-with-rules\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">16.2. Configuring a Pacemaker location constraint using rules\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tUse the following command to configure a Pacemaker constraint that uses rules. If \u003Ccode class=\"literal\">score\u003C/code> is omitted, it defaults to INFINITY. If \u003Ccode class=\"literal\">resource-discovery\u003C/code> is omitted, it defaults to \u003Ccode class=\"literal\">always\u003C/code>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFor information about the \u003Ccode class=\"literal\">resource-discovery\u003C/code> option, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters#proc_limiting-resource-discovery-to-a-subset-of-nodes-determining-which-node-a-resource-runs-on\">Limiting resource discovery to a subset of nodes\u003C/a>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tAs with basic location constraints, you can use regular expressions for resources with these constraints as well.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tWhen using rules to configure location constraints, the value of \u003Ccode class=\"literal\">score\u003C/code> can be positive or negative, with a positive value indicating \"prefers\" and a negative value indicating \"avoids\".\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs constraint location \u003Cspan class=\"emphasis\">\u003Cem>rsc\u003C/em>\u003C/span> rule [resource-discovery=\u003Cspan class=\"emphasis\">\u003Cem>option\u003C/em>\u003C/span>] [role=promoted|unpromoted] [score=\u003Cspan class=\"emphasis\">\u003Cem>score\u003C/em>\u003C/span> | score-attribute=\u003Cspan class=\"emphasis\">\u003Cem>attribute\u003C/em>\u003C/span>] \u003Cspan class=\"emphasis\">\u003Cem>expression\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe \u003Cspan class=\"emphasis\">\u003Cem>expression\u003C/em>\u003C/span> option can be one of the following where \u003Cspan class=\"emphasis\">\u003Cem>duration_options\u003C/em>\u003C/span> and \u003Cspan class=\"emphasis\">\u003Cem>date_spec_options\u003C/em>\u003C/span> are: hours, monthdays, weekdays, yeardays, months, weeks, years, weekyears, and moon as described in the \"Properties of a Date Specification\" table in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-location-with-rules-configuring-and-managing-high-availability-clusters#date_specifications\">Date specifications\u003C/a>.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">defined|not_defined \u003Cspan class=\"emphasis\">\u003Cem>attribute\u003C/em>\u003C/span>\u003C/code>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">\u003Cspan class=\"emphasis\">\u003Cem>attribute\u003C/em>\u003C/span> lt|gt|lte|gte|eq|ne [string|integer|number|version] \u003Cspan class=\"emphasis\">\u003Cem>value\u003C/em>\u003C/span>\u003C/code>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">date gt|lt \u003Cspan class=\"emphasis\">\u003Cem>date\u003C/em>\u003C/span>\u003C/code>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">date in_range \u003Cspan class=\"emphasis\">\u003Cem>date\u003C/em>\u003C/span> to \u003Cspan class=\"emphasis\">\u003Cem>date\u003C/em>\u003C/span>\u003C/code>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">date in_range \u003Cspan class=\"emphasis\">\u003Cem>date\u003C/em>\u003C/span> to duration \u003Cspan class=\"emphasis\">\u003Cem>duration_options\u003C/em>\u003C/span> …​\u003C/code>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">date-spec \u003Cspan class=\"emphasis\">\u003Cem>date_spec_options\u003C/em>\u003C/span>\u003C/code>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">\u003Cspan class=\"emphasis\">\u003Cem>expression\u003C/em>\u003C/span> and|or \u003Cspan class=\"emphasis\">\u003Cem>expression\u003C/em>\u003C/span>\u003C/code>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">(\u003Cspan class=\"emphasis\">\u003Cem>expression\u003C/em>\u003C/span>)\u003C/code>\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tNote that durations are an alternative way to specify an end for \u003Ccode class=\"literal\">in_range\u003C/code> operations by means of calculations. For example, you can specify a duration of 19 months.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following location constraint configures an expression that is true if now is any time in the year 2018.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location Webserver rule score=INFINITY date-spec years=2018\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe following command configures an expression that is true from 9 am to 5 pm, Monday through Friday. Note that the hours value of 16 matches up to 16:59:59, as the numeric value (hour) still matches.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location Webserver rule score=INFINITY date-spec hours=\"9-16\" weekdays=\"1-5\"\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe following command configures an expression that is true when there is a full moon on Friday the thirteenth.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location Webserver rule date-spec weekdays=5 monthdays=13 moon=4\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tTo remove a rule, use the following command. If the rule that you are removing is the last rule in its constraint, the constraint will be removed.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs constraint rule remove \u003Cspan class=\"emphasis\">\u003Cem>rule_id\u003C/em>\u003C/span>\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_managing-cluster-resources-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 17. Managing cluster resources\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tThere are a variety of commands you can use to display, modify, and administer cluster resources.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_display-configured-resources-managing-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">17.1. Displaying configured resources\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tTo display a list of all configured resources, use the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource status\u003C/pre>\u003Cp>\n\t\t\t\tFor example, if your system is configured with a resource named \u003Ccode class=\"literal\">VirtualIP\u003C/code> and a resource named \u003Ccode class=\"literal\">WebSite\u003C/code>, the \u003Ccode class=\"literal command\">pcs resource status\u003C/code> command yields the following output.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource status\u003C/strong>\u003C/span>\n VirtualIP\t(ocf::heartbeat:IPaddr2):\tStarted\n WebSite\t(ocf::heartbeat:apache):\tStarted\u003C/pre>\u003Cp>\n\t\t\t\tTo display the configured parameters for a resource, use the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource config \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tFor example, the following command displays the currently configured parameters for resource \u003Ccode class=\"literal\">VirtualIP\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource config VirtualIP\u003C/strong>\u003C/span>\n Resource: VirtualIP (type=IPaddr2 class=ocf provider=heartbeat)\n  Attributes: ip=192.168.0.120 cidr_netmask=24\n  Operations: monitor interval=30s\u003C/pre>\u003Cp>\n\t\t\t\tTo display the status of an individual resource, use the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource status \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tFor example, if your system is configured with a resource named \u003Ccode class=\"literal\">VirtualIP\u003C/code> the \u003Ccode class=\"literal\">pcs resource status VirtualIP\u003C/code> command yields the following output.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource status VirtualIP\u003C/strong>\u003C/span>\n VirtualIP      (ocf::heartbeat:IPaddr2):       Started\u003C/pre>\u003Cp>\n\t\t\t\tTo display the status of the resources running on a specific node, use the following command. You can use this command to display the status of resources on both cluster and remote nodes.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource status node=\u003Cspan class=\"emphasis\">\u003Cem>node_id\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tFor example, if \u003Ccode class=\"literal\">node-01\u003C/code> is running resources named \u003Ccode class=\"literal\">VirtualIP\u003C/code> and \u003Ccode class=\"literal\">WebSite\u003C/code> the \u003Ccode class=\"literal\">pcs resource status node=node-01\u003C/code> command might yield the following output.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource status node=node-01\u003C/strong>\u003C/span>\n VirtualIP      (ocf::heartbeat:IPaddr2):       Started\n WebSite        (ocf::heartbeat:apache):        Started\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_exporting-resources-managing-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">17.2. Exporting cluster resources as \u003Ccode class=\"literal\">pcs\u003C/code> commands\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tAs of Red Hat Enterprise Linux 9.1, you can display the \u003Ccode class=\"literal\">pcs\u003C/code> commands that can be used to re-create configured cluster resources on a different system using the \u003Ccode class=\"literal\">--output-format=cmd\u003C/code> option of the \u003Ccode class=\"literal\">pcs resource config\u003C/code> command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following commands create four resources created for an active/passive Apache HTTP server in a Red Hat high availability cluster: an \u003Ccode class=\"literal\">LVM-activate\u003C/code> resource, a \u003Ccode class=\"literal\">Filesystem\u003C/code> resource, an \u003Ccode class=\"literal\">IPaddr2\u003C/code> resource, and an \u003Ccode class=\"literal\">Apache\u003C/code> resource.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create my_lvm ocf:heartbeat:LVM-activate vgname=my_vg vg_access_mode=system_id --group apachegroup\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create my_fs Filesystem device=\"/dev/my_vg/my_lv\" directory=\"/var/www\" fstype=\"xfs\" --group apachegroup\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create VirtualIP IPaddr2 ip=198.51.100.3 cidr_netmask=24 --group apachegroup\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create Website apache configfile=\"/etc/httpd/conf/httpd.conf\" statusurl=\"http://127.0.0.1/server-status\" --group apachegroup\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tAfter you create the resources, the following command displays the \u003Ccode class=\"literal\">pcs\u003C/code> commands you can use to re-create those resources on a different system.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource config --output-format=cmd\u003C/strong>\u003C/span>\npcs resource create --no-default-ops --force -- my_lvm ocf:heartbeat:LVM-activate \\\n  vg_access_mode=system_id vgname=my_vg \\\n  op \\\n    monitor interval=30s id=my_lvm-monitor-interval-30s timeout=90s \\\n    start interval=0s id=my_lvm-start-interval-0s timeout=90s \\\n    stop interval=0s id=my_lvm-stop-interval-0s timeout=90s;\npcs resource create --no-default-ops --force -- my_fs ocf:heartbeat:Filesystem \\\n  device=/dev/my_vg/my_lv directory=/var/www fstype=xfs \\\n  op \\\n    monitor interval=20s id=my_fs-monitor-interval-20s timeout=40s \\\n    start interval=0s id=my_fs-start-interval-0s timeout=60s \\\n    stop interval=0s id=my_fs-stop-interval-0s timeout=60s;\npcs resource create --no-default-ops --force -- VirtualIP ocf:heartbeat:IPaddr2 \\\n  cidr_netmask=24 ip=198.51.100.3 \\\n  op \\\n    monitor interval=10s id=VirtualIP-monitor-interval-10s timeout=20s \\\n    start interval=0s id=VirtualIP-start-interval-0s timeout=20s \\\n    stop interval=0s id=VirtualIP-stop-interval-0s timeout=20s;\npcs resource create --no-default-ops --force -- Website ocf:heartbeat:apache \\\n  configfile=/etc/httpd/conf/httpd.conf statusurl=http://127.0.0.1/server-status \\\n  op \\\n    monitor interval=10s id=Website-monitor-interval-10s timeout=20s \\\n    start interval=0s id=Website-start-interval-0s timeout=40s \\\n    stop interval=0s id=Website-stop-interval-0s timeout=60s;\npcs resource group add apachegroup \\\n  my_lvm my_fs VirtualIP Website\u003C/pre>\u003Cp>\n\t\t\t\tTo display the \u003Ccode class=\"literal\">pcs\u003C/code> command or commands you can use to re-create only one configured resource, specify the resource ID for that resource.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource config VirtualIP --output-format=cmd\u003C/strong>\u003C/span>\npcs resource create --no-default-ops --force -- VirtualIP ocf:heartbeat:IPaddr2 \\\n  cidr_netmask=24 ip=198.51.100.3 \\\n  op \\\n    monitor interval=10s id=VirtualIP-monitor-interval-10s timeout=20s \\\n    start interval=0s id=VirtualIP-start-interval-0s timeout=20s \\\n    stop interval=0s id=VirtualIP-stop-interval-0s timeout=20s\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_modify-resource-parameters-managing-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">17.3. Modifying resource parameters\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tTo modify the parameters of a configured resource, use the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource update \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>resource_options\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\tThe following sequence of commands show the initial values of the configured parameters for resource \u003Ccode class=\"literal\">VirtualIP\u003C/code>, the command to change the value of the \u003Ccode class=\"literal\">ip\u003C/code> parameter, and the values following the update command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource config VirtualIP\u003C/strong>\u003C/span>\n Resource: VirtualIP (type=IPaddr2 class=ocf provider=heartbeat)\n  Attributes: ip=192.168.0.120 cidr_netmask=24\n  Operations: monitor interval=30s\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource update VirtualIP ip=192.169.0.120\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource config VirtualIP\u003C/strong>\u003C/span>\n Resource: VirtualIP (type=IPaddr2 class=ocf provider=heartbeat)\n  Attributes: ip=192.169.0.120 cidr_netmask=24\n  Operations: monitor interval=30s\u003C/pre>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tWhen you update a resource’s operation with the \u003Ccode class=\"literal\">pcs resource update\u003C/code> command, any options you do not specifically call out are reset to their default values.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/section>\u003Csection class=\"section\" id=\"proc_cleanup-cluster-resources-managing-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">17.4. Clearing failure status of cluster resources\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tIf a resource has failed, a failure message appears when you display the cluster status with the \u003Ccode class=\"literal\">pcs status\u003C/code> command. After attempting to resolve the cause of the failure, you can check the updated status of the resource by running the \u003Ccode class=\"literal\">pcs status\u003C/code> command again, and you can check the failure count for the cluster resources with the \u003Ccode class=\"literal\">pcs resource failcount show --full\u003C/code> command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can clear that failure status of a resource with the \u003Ccode class=\"literal command\">pcs resource cleanup\u003C/code> command. The \u003Ccode class=\"literal\">pcs resource cleanup\u003C/code> command resets the resource status and \u003Ccode class=\"literal\">failcount\u003C/code> value for the resource. This command also removes the operation history for the resource and re-detects its current state.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command resets the resource status and \u003Ccode class=\"literal\">failcount\u003C/code> value for the resource specified by \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource cleanup \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tIf you do not specify \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>, the \u003Ccode class=\"literal\">pcs resource cleanup\u003C/code> command resets the resource status and \u003Ccode class=\"literal\">failcount\u003C/code> value for all resources with a failure count.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIn addition to the \u003Ccode class=\"literal\">pcs resource cleanup \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>\u003C/code> command, you can also reset the resource status and clear the operation history of a resource with the \u003Ccode class=\"literal\">pcs resource refresh \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>\u003C/code> command. As with the \u003Ccode class=\"literal\">pcs resource cleanup\u003C/code> command, you can run the \u003Ccode class=\"literal\">pcs resource refresh\u003C/code> command with no options specified to reset the resource status and \u003Ccode class=\"literal\">failcount\u003C/code> value for all resources.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tBoth the \u003Ccode class=\"literal\">pcs resource cleanup\u003C/code> and the \u003Ccode class=\"literal\">pcs resource refresh\u003C/code> commands clear the operation history for a resource and re-detect the current state of the resource. The \u003Ccode class=\"literal\">pcs resource cleanup\u003C/code> command operates only on resources with failed actions as shown in the cluster status, while the \u003Ccode class=\"literal\">pcs resource refresh\u003C/code> command operates on resources regardless of their current state.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_moving-cluster-resources-managing-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">17.5. Moving resources in a cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tPacemaker provides a variety of mechanisms for configuring a resource to move from one node to another and to manually move a resource when needed.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can manually move resources in a cluster with the \u003Ccode class=\"literal command\">pcs resource move\u003C/code> and \u003Ccode class=\"literal command\">pcs resource relocate\u003C/code> commands, as described in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_manually-move-resources-cluster-maintenance\">Manually moving cluster resources\u003C/a>. In addition to these commands, you can also control the behavior of cluster resources by enabling, disabling, and banning resources, as described in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_disabling-resources-cluster-maintenance\">Disabling, enabling, and banning cluster resources\u003C/a>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can configure a resource so that it will move to a new node after a defined number of failures, and you can configure a cluster to move resources when external connectivity is lost.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"moving_resources_due_to_failure\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">17.5.1. Moving resources due to failure\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tWhen you create a resource, you can configure the resource so that it will move to a new node after a defined number of failures by setting the \u003Ccode class=\"literal\">migration-threshold\u003C/code> option for that resource. Once the threshold has been reached, this node will no longer be allowed to run the failed resource until:\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tThe resource’s \u003Ccode class=\"literal\">failure-timeout\u003C/code> value is reached.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tThe administrator manually resets the resource’s failure count by using the \u003Ccode class=\"literal\">pcs resource cleanup\u003C/code> command.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\t\tThe value of \u003Ccode class=\"literal\">migration-threshold\u003C/code> is set to \u003Ccode class=\"literal\">INFINITY\u003C/code> by default. \u003Ccode class=\"literal\">INFINITY\u003C/code> is defined internally as a very large but finite number. A value of 0 disables the \u003Ccode class=\"literal\">migration-threshold\u003C/code> feature.\n\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\tSetting a \u003Ccode class=\"literal\">migration-threshold\u003C/code> for a resource is not the same as configuring a resource for migration, in which the resource moves to another location without loss of state.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\t\tThe following example adds a migration threshold of 10 to the resource named \u003Ccode class=\"literal\">dummy_resource\u003C/code>, which indicates that the resource will move to a new node after 10 failures.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource meta dummy_resource migration-threshold=10\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tYou can add a migration threshold to the defaults for the whole cluster with the following command.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource defaults update migration-threshold=10\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tTo determine the resource’s current failure status and limits, use the \u003Ccode class=\"literal\">pcs resource failcount show\u003C/code> command.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThere are two exceptions to the migration threshold concept; they occur when a resource either fails to start or fails to stop. If the cluster property \u003Ccode class=\"literal\">start-failure-is-fatal\u003C/code> is set to \u003Ccode class=\"literal\">true\u003C/code> (which is the default), start failures cause the \u003Ccode class=\"literal\">failcount\u003C/code> to be set to \u003Ccode class=\"literal\">INFINITY\u003C/code> and always cause the resource to move immediately.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tStop failures are slightly different and crucial. If a resource fails to stop and STONITH is enabled, then the cluster will fence the node to be able to start the resource elsewhere. If STONITH is not enabled, then the cluster has no way to continue and will not try to start the resource elsewhere, but will try to stop it again after the failure timeout.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"moving_resources_due_to_connectivity_changes\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">17.5.2. Moving resources due to connectivity changes\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tSetting up the cluster to move resources when external connectivity is lost is a two step process.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tAdd a \u003Ccode class=\"literal\">ping\u003C/code> resource to the cluster. The \u003Ccode class=\"literal\">ping\u003C/code> resource uses the system utility of the same name to test if a list of machines (specified by DNS host name or IPv4/IPv6 address) are reachable and uses the results to maintain a node attribute called \u003Ccode class=\"literal\">pingd\u003C/code>.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tConfigure a location constraint for the resource that will move the resource to a different node when connectivity is lost.\n\t\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following table describes the properties you can set for a \u003Ccode class=\"literal\">ping\u003C/code> resource.\n\t\t\t\t\u003C/p>\u003Crh-table id=\"tb-pingoptions-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 17.1. Properties of a ping resources\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 50%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 50%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686140959024\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686140957936\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140959024\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">dampen\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140957936\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tThe time to wait (dampening) for further changes to occur. This prevents a resource from bouncing around the cluster when cluster nodes notice the loss of connectivity at slightly different times.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140959024\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">multiplier\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140957936\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tThe number of connected ping nodes gets multiplied by this value to get a score. Useful when there are multiple ping nodes configured.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140959024\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">host_list\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140957936\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tThe machines to contact to determine the current connectivity status. Allowed values include resolvable DNS host names, IPv4 and IPv6 addresses. The entries in the host list are space separated.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\t\tThe following example command creates a \u003Ccode class=\"literal\">ping\u003C/code> resource that verifies connectivity to \u003Ccode class=\"literal\">gateway.example.com\u003C/code>. In practice, you would verify connectivity to your network gateway/router. You configure the \u003Ccode class=\"literal\">ping\u003C/code> resource as a clone so that the resource will run on all cluster nodes.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create ping ocf:pacemaker:ping dampen=5s multiplier=1000 host_list=gateway.example.com clone\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tThe following example configures a location constraint rule for the existing resource named \u003Ccode class=\"literal\">Webserver\u003C/code>. This will cause the \u003Ccode class=\"literal\">Webserver\u003C/code> resource to move to a host that is able to ping \u003Ccode class=\"literal\">gateway.example.com\u003C/code> if the host that it is currently running on cannot ping \u003Ccode class=\"literal\">gateway.example.com\u003C/code>.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location Webserver rule score=-INFINITY pingd lt 1 or not_defined pingd\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"proc_disabling-monitor-operationmanaging-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">17.6. Disabling a monitor operation\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe easiest way to stop a recurring monitor is to delete it. However, there can be times when you only want to disable it temporarily. In such cases, add \u003Ccode class=\"literal\">enabled=\"false\"\u003C/code> to the operation’s definition. When you want to reinstate the monitoring operation, set \u003Ccode class=\"literal\">enabled=\"true\"\u003C/code> to the operation’s definition.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tWhen you update a resource’s operation with the \u003Ccode class=\"literal\">pcs resource update\u003C/code> command, any options you do not specifically call out are reset to their default values. For example, if you have configured a monitoring operation with a custom timeout value of 600, running the following commands will reset the timeout value to the default value of 20 (or whatever you have set the default value to with the \u003Ccode class=\"literal\">pcs resource op defaults\u003C/code> command).\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource update resourceXZY op monitor enabled=false\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource update resourceXZY op monitor enabled=true\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tIn order to maintain the original value of 600 for this option, when you reinstate the monitoring operation you must specify that value, as in the following example.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource update resourceXZY op monitor timeout=600 enabled=true\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_tagging-cluster-resources-managing-cluster-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">17.7. Configuring and managing cluster resource tags\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can use the \u003Ccode class=\"literal\">pcs\u003C/code> command to tag cluster resources. This allows you to enable, disable, manage, or unmanage a specified set of resources with a single command.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"tagging_cluster_resources_for_administration_by_category\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">17.7.1. Tagging cluster resources for administration by category\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following procedure tags two resources with a resource tag and disables the tagged resources. In this example, the existing resources to be tagged are named \u003Ccode class=\"literal\">d-01\u003C/code> and \u003Ccode class=\"literal\">d-02\u003C/code>.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tCreate a tag named \u003Ccode class=\"literal\">special-resources\u003C/code> for resources \u003Ccode class=\"literal\">d-01\u003C/code> and \u003Ccode class=\"literal\">d-02\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node-01]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs tag create special-resources d-01 d-02\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tDisplay the resource tag configuration.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node-01]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs tag config\u003C/strong>\u003C/span>\nspecial-resources\n  d-01\n  d-02\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tDisable all resources that are tagged with the \u003Ccode class=\"literal\">special-resources\u003C/code> tag.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node-01]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource disable special-resources\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tDisplay the status of the resources to confirm that resources \u003Ccode class=\"literal\">d-01\u003C/code> and \u003Ccode class=\"literal\">d-02\u003C/code> are disabled.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node-01]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource\u003C/strong>\u003C/span>\n  * d-01        (ocf::pacemaker:Dummy): Stopped (disabled)\n  * d-02        (ocf::pacemaker:Dummy): Stopped (disabled)\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\t\tIn addition to the \u003Ccode class=\"literal\">pcs resource disable\u003C/code> command, the \u003Ccode class=\"literal\">pcs resource enable\u003C/code>, \u003Ccode class=\"literal\">pcs resource manage\u003C/code>, and \u003Ccode class=\"literal\">pcs resource unmanage\u003C/code> commands support the administration of tagged resources.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tAfter you have created a resource tag:\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tYou can delete a resource tag with the \u003Ccode class=\"literal\">pcs tag delete\u003C/code> command.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tYou can modify resource tag configuration for an existing resource tag with the \u003Ccode class=\"literal\">pcs tag update\u003C/code> command.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"deleting_a_tagged_cluster_resource\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">17.7.2. Deleting a tagged cluster resource\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tYou cannot delete a tagged cluster resource with the \u003Ccode class=\"literal\">pcs\u003C/code> command. To delete a tagged resource, use the following procedure.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tRemove the resource tag.\n\t\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"a\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\t\tThe following command removes the resource tag \u003Ccode class=\"literal\">special-resources\u003C/code> from all resources with that tag,\n\t\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node-01]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs tag remove special-resources\u003C/strong>\u003C/span>\n[root@node-01]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs tag\u003C/strong>\u003C/span>\n No tags defined\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\t\tThe following command removes the resource tag \u003Ccode class=\"literal\">special-resources\u003C/code> from the resource \u003Ccode class=\"literal\">d-01\u003C/code> only.\n\t\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node-01]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs tag update special-resources remove d-01\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tDelete the resource.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node-01]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource delete d-01\u003C/strong>\u003C/span>\nAttempting to stop: d-01... Stopped\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_creating-multinode-resources-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 18. Creating cluster resources that are active on multiple nodes (cloned resources)\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tYou can clone a cluster resource so that the resource can be active on multiple nodes. For example, you can use cloned resources to configure multiple instances of an IP resource to distribute throughout a cluster for node balancing. You can clone any resource provided the resource agent supports it. A clone consists of one resource or one resource group.\n\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\tOnly resources that can be active on multiple nodes at the same time are suitable for cloning. For example, a \u003Ccode class=\"literal\">Filesystem\u003C/code> resource mounting a non-clustered file system such as \u003Ccode class=\"literal\">ext4\u003C/code> from a shared memory device should not be cloned. Since the \u003Ccode class=\"literal\">ext4\u003C/code> partition is not cluster aware, this file system is not suitable for read/write operations occurring from multiple nodes at the same time.\n\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Csection class=\"section\" id=\"proc_creating-cloned-resource-creating-multinode-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">18.1. Creating and removing a cloned resource\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can create a resource and a clone of that resource at the same time.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tTo create a resource and clone of the resource with the following single command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource create \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>standard\u003C/em>\u003C/span>:[\u003Cspan class=\"emphasis\">\u003Cem>provider\u003C/em>\u003C/span>:]]\u003Cspan class=\"emphasis\">\u003Cem>type\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>resource options\u003C/em>\u003C/span>] [meta \u003Cspan class=\"emphasis\">\u003Cem>resource meta options\u003C/em>\u003C/span>] clone [\u003Cspan class=\"emphasis\">\u003Cem>clone_id\u003C/em>\u003C/span>] [\u003Cspan class=\"emphasis\">\u003Cem>clone options\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\tYou can set a custom name for the clone by specifying a value for the \u003Cspan class=\"emphasis\">\u003Cem>clone_id\u003C/em>\u003C/span> option.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou cannot create a resource group and a clone of that resource group in a single command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tAlternately, you can create a clone of a previously-created resource or resource group with the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource clone \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> | \u003Cspan class=\"emphasis\">\u003Cem>group_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>clone_id\u003C/em>\u003C/span>][\u003Cspan class=\"emphasis\">\u003Cem>clone options\u003C/em>\u003C/span>]...\u003C/pre>\u003Cp>\n\t\t\t\tBy default, the name of the clone will be \u003Ccode class=\"literal\">\u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>-clone\u003C/code> or \u003Ccode class=\"literal\">\u003Cspan class=\"emphasis\">\u003Cem>group_name\u003C/em>\u003C/span>-clone\u003C/code>. You can set a custom name for the clone by specifying a value for the \u003Cspan class=\"emphasis\">\u003Cem>clone_id\u003C/em>\u003C/span> option.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tYou need to configure resource configuration changes on one node only.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tWhen configuring constraints, always use the name of the group or clone.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tWhen you create a clone of a resource, by default the clone takes on the name of the resource with \u003Ccode class=\"literal\">-clone\u003C/code> appended to the name. The following command creates a resource of type \u003Ccode class=\"literal\">apache\u003C/code> named \u003Ccode class=\"literal\">webfarm\u003C/code> and a clone of that resource named \u003Ccode class=\"literal\">webfarm-clone\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create webfarm apache clone\u003C/strong>\u003C/span>\u003C/pre>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tWhen you create a resource or resource group clone that will be ordered after another clone, you should almost always set the \u003Ccode class=\"literal\">interleave=true\u003C/code> option. This ensures that copies of the dependent clone can stop or start when the clone it depends on has stopped or started on the same node. If you do not set this option, if a cloned resource B depends on a cloned resource A and a node leaves the cluster, when the node returns to the cluster and resource A starts on that node, then all of the copies of resource B on all of the nodes will restart. This is because when a dependent cloned resource does not have the \u003Ccode class=\"literal\">interleave\u003C/code> option set, all instances of that resource depend on any running instance of the resource it depends on.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tUse the following command to remove a clone of a resource or a resource group. This does not remove the resource or resource group itself.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource unclone \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> | \u003Cspan class=\"emphasis\">\u003Cem>clone_id\u003C/em>\u003C/span> | \u003Cspan class=\"emphasis\">\u003Cem>group_name\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe following table describes the options you can specify for a cloned resource.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-resourcecloneoptions-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 18.1. Resource Clone Options\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 50%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 50%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686140682704\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686140681616\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140682704\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">priority, target-role, is-managed\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140681616\"> \u003Cp>\n\t\t\t\t\t\t\t\tOptions inherited from resource that is being cloned, as described in the \"Resource Meta Options\" table in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters#proc_configuring-resource-meta-options-configuring-cluster-resources\">Configuring resource meta options\u003C/a>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140682704\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">clone-max\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140681616\"> \u003Cp>\n\t\t\t\t\t\t\t\tHow many copies of the resource to start. Defaults to the number of nodes in the cluster.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140682704\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">clone-node-max\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140681616\"> \u003Cp>\n\t\t\t\t\t\t\t\tHow many copies of the resource can be started on a single node; the default value is \u003Ccode class=\"literal\">1\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140682704\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">notify\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140681616\"> \u003Cp>\n\t\t\t\t\t\t\t\tWhen stopping or starting a copy of the clone, tell all the other copies beforehand and when the action was successful. Allowed values: \u003Ccode class=\"literal\">false\u003C/code>, \u003Ccode class=\"literal\">true\u003C/code>. The default value is \u003Ccode class=\"literal\">false\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140682704\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">globally-unique\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140681616\"> \u003Cp>\n\t\t\t\t\t\t\t\tDoes each copy of the clone perform a different function? Allowed values: \u003Ccode class=\"literal\">false\u003C/code>, \u003Ccode class=\"literal\">true\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tIf the value of this option is \u003Ccode class=\"literal\">false\u003C/code>, these resources behave identically everywhere they are running and thus there can be only one copy of the clone active per machine.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tIf the value of this option is \u003Ccode class=\"literal\">true\u003C/code>, a copy of the clone running on one machine is not equivalent to another instance, whether that instance is running on another node or on the same node. The default value is \u003Ccode class=\"literal\">true\u003C/code> if the value of \u003Ccode class=\"literal\">clone-node-max\u003C/code> is greater than one; otherwise the default value is \u003Ccode class=\"literal\">false\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140682704\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">ordered\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140681616\"> \u003Cp>\n\t\t\t\t\t\t\t\tShould the copies be started in series (instead of in parallel). Allowed values: \u003Ccode class=\"literal\">false\u003C/code>, \u003Ccode class=\"literal\">true\u003C/code>. The default value is \u003Ccode class=\"literal\">false\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140682704\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">interleave\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140681616\"> \u003Cp>\n\t\t\t\t\t\t\t\tChanges the behavior of ordering constraints (between clones) so that copies of the first clone can start or stop as soon as the copy on the same node of the second clone has started or stopped (rather than waiting until every instance of the second clone has started or stopped). Allowed values: \u003Ccode class=\"literal\">false\u003C/code>, \u003Ccode class=\"literal\">true\u003C/code>. The default value is \u003Ccode class=\"literal\">false\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140682704\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">clone-min\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140681616\"> \u003Cp>\n\t\t\t\t\t\t\t\tIf a value is specified, any clones which are ordered after this clone will not be able to start until the specified number of instances of the original clone are running, even if the \u003Ccode class=\"literal\">interleave\u003C/code> option is set to \u003Ccode class=\"literal\">true\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tTo achieve a stable allocation pattern, clones are slightly sticky by default, which indicates that they have a slight preference for staying on the node where they are running. If no value for \u003Ccode class=\"literal\">resource-stickiness\u003C/code> is provided, the clone will use a value of 1. Being a small value, it causes minimal disturbance to the score calculations of other resources but is enough to prevent Pacemaker from needlessly moving copies around the cluster. For information about setting the \u003Ccode class=\"literal\">resource-stickiness\u003C/code> resource meta-option, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters#proc_configuring-resource-meta-options-configuring-cluster-resources\">Configuring resource meta options\u003C/a>.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-clone-constraints-creating-multinode-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">18.2. Configuring clone resource constraints\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIn most cases, a clone will have a single copy on each active cluster node. You can, however, set \u003Ccode class=\"literal\">clone-max\u003C/code> for the resource clone to a value that is less than the total number of nodes in the cluster. If this is the case, you can indicate which nodes the cluster should preferentially assign copies to with resource location constraints. These constraints are written no differently to those for regular resources except that the clone’s id must be used.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command creates a location constraint for the cluster to preferentially assign resource clone \u003Ccode class=\"literal\">webfarm-clone\u003C/code> to \u003Ccode class=\"literal\">node1\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location webfarm-clone prefers node1\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tOrdering constraints behave slightly differently for clones. In the example below, because the \u003Ccode class=\"literal\">interleave\u003C/code> clone option is left to default as \u003Ccode class=\"literal\">false\u003C/code>, no instance of \u003Ccode class=\"literal\">webfarm-stats\u003C/code> will start until all instances of \u003Ccode class=\"literal\">webfarm-clone\u003C/code> that need to be started have done so. Only if no copies of \u003Ccode class=\"literal\">webfarm-clone\u003C/code> can be started then \u003Ccode class=\"literal\">webfarm-stats\u003C/code> will be prevented from being active. Additionally, \u003Ccode class=\"literal\">webfarm-clone\u003C/code> will wait for \u003Ccode class=\"literal\">webfarm-stats\u003C/code> to be stopped before stopping itself.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint order start webfarm-clone then webfarm-stats\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tColocation of a regular (or group) resource with a clone means that the resource can run on any machine with an active copy of the clone. The cluster will choose a copy based on where the clone is running and the resource’s own location preferences.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tColocation between clones is also possible. In such cases, the set of allowed locations for the clone is limited to nodes on which the clone is (or will be) active. Allocation is then performed as normally.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command creates a colocation constraint to ensure that the resource \u003Ccode class=\"literal\">webfarm-stats\u003C/code> runs on the same node as an active copy of \u003Ccode class=\"literal\">webfarm-clone\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint colocation add webfarm-stats with webfarm-clone\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_creating-promotable-clone-resources-creating-multinode-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">18.3. Promotable clone resources\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tPromotable clone resources are clone resources with the \u003Ccode class=\"literal\">promotable\u003C/code> meta attribute set to \u003Ccode class=\"literal\">true\u003C/code>. They allow the instances to be in one of two operating modes; these are called \u003Ccode class=\"literal\">promoted\u003C/code> and \u003Ccode class=\"literal\">unpromoted\u003C/code>. The names of the modes do not have specific meanings, except for the limitation that when an instance is started, it must come up in the \u003Ccode class=\"literal\">Unpromoted\u003C/code> state. Note: The Promoted and Unpromoted role names are the functional equivalent of the Master and Slave Pacemaker roles in previous RHEL releases.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"creating_a_promotable_clone_resource\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">18.3.1. Creating a promotable clone resource\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tYou can create a resource as a promotable clone with the following single command.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource create \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>standard\u003C/em>\u003C/span>:[\u003Cspan class=\"emphasis\">\u003Cem>provider\u003C/em>\u003C/span>:]]\u003Cspan class=\"emphasis\">\u003Cem>type\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>resource options\u003C/em>\u003C/span>] promotable [\u003Cspan class=\"emphasis\">\u003Cem>clone_id\u003C/em>\u003C/span>] [\u003Cspan class=\"emphasis\">\u003Cem>clone options\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\t\tBy default, the name of the promotable clone will be \u003Ccode class=\"literal\">\u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>-clone\u003C/code>.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tYou can set a custom name for the clone by specifying a value for the \u003Cspan class=\"emphasis\">\u003Cem>clone_id\u003C/em>\u003C/span> option.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tAlternately, you can create a promotable resource from a previously-created resource or resource group with the following command.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource promotable \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>clone_id\u003C/em>\u003C/span>] [\u003Cspan class=\"emphasis\">\u003Cem>clone options\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\t\tBy default, the name of the promotable clone will be \u003Ccode class=\"literal\">\u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>-clone\u003C/code> or \u003Ccode class=\"literal\">\u003Cspan class=\"emphasis\">\u003Cem>group_name\u003C/em>\u003C/span>-clone\u003C/code>.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tYou can set a custom name for the clone by specifying a value for the \u003Cspan class=\"emphasis\">\u003Cem>clone_id\u003C/em>\u003C/span> option.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe following table describes the extra clone options you can specify for a promotable resource.\n\t\t\t\t\u003C/p>\u003Crh-table id=\"tb-promotablecloneoptions-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 18.2. Extra Clone Options Available for Promotable Clones\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 50%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 50%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686140565952\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686140564864\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140565952\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">promoted-max\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140564864\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tHow many copies of the resource can be promoted; default 1.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140565952\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">promoted-node-max\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140564864\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tHow many copies of the resource can be promoted on a single node; default 1.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003C/section>\u003Csection class=\"section\" id=\"configuring_promotable_resource_constraints\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">18.3.2. Configuring promotable resource constraints\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tIn most cases, a promotable resource will have a single copy on each active cluster node. If this is not the case, you can indicate which nodes the cluster should preferentially assign copies to with resource location constraints. These constraints are written no differently than those for regular resources.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tYou can create a colocation constraint which specifies whether the resources are operating in a promoted or unpromoted role. The following command creates a resource colocation constraint.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs constraint colocation add [promoted|unpromoted] \u003Cspan class=\"emphasis\">\u003Cem>source_resource\u003C/em>\u003C/span> with [promoted|unpromoted] \u003Cspan class=\"emphasis\">\u003Cem>target_resource\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>score\u003C/em>\u003C/span>] [\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\t\tFor information about colocation constraints, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters\">Colocating cluster resources\u003C/a>.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tWhen configuring an ordering constraint that includes promotable resources, one of the actions that you can specify for the resources is \u003Ccode class=\"literal\">promote\u003C/code>, indicating that the resource be promoted from unpromoted role to promoted role. Additionally, you can specify an action of \u003Ccode class=\"literal\">demote\u003C/code>, indicated that the resource be demoted from promoted role to unpromoted role.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe command for configuring an order constraint is as follows.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs constraint order [\u003Cspan class=\"emphasis\">\u003Cem>action\u003C/em>\u003C/span>] \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> then [\u003Cspan class=\"emphasis\">\u003Cem>action\u003C/em>\u003C/span>] \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\t\tFor information about resource order constraints, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters\">Determining the order in which cluster resources are run\u003C/a>.\n\t\t\t\t\u003C/p>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"proc_recovering-promoted-node-creating-multinode-resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">18.4. Demoting a promoted resource on failure\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can configure a promotable resource so that when a \u003Ccode class=\"literal\">promote\u003C/code> or \u003Ccode class=\"literal\">monitor\u003C/code> action fails for that resource, or the partition in which the resource is running loses quorum, the resource will be demoted but will not be fully stopped. This can prevent the need for manual intervention in situations where fully stopping the resource would require it.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tTo configure a promotable resource to be demoted when a \u003Ccode class=\"literal\">promote\u003C/code> action fails, set the \u003Ccode class=\"literal\">on-fail\u003C/code> operation meta option to \u003Ccode class=\"literal\">demote\u003C/code>, as in the following example.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource op add my-rsc promote on-fail=\"demote\"\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tTo configure a promotable resource to be demoted when a \u003Ccode class=\"literal\">monitor\u003C/code> action fails, set \u003Ccode class=\"literal\">interval\u003C/code> to a nonzero value, set the \u003Ccode class=\"literal\">on-fail\u003C/code> operation meta option to \u003Ccode class=\"literal\">demote\u003C/code>, and set \u003Ccode class=\"literal\">role\u003C/code> to \u003Ccode class=\"literal\">Promoted\u003C/code>, as in the following example.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource op add my-rsc monitor interval=\"10s\" on-fail=\"demote\" role=\"Promoted\"\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tTo configure a cluster so that when a cluster partition loses quorum any promoted resources will be demoted but left running and all other resources will be stopped, set the \u003Ccode class=\"literal\">no-quorum-policy\u003C/code> cluster property to \u003Ccode class=\"literal\">demote\u003C/code>\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tSetting the \u003Ccode class=\"literal\">on-fail\u003C/code> meta-attribute to \u003Ccode class=\"literal\">demote\u003C/code> for an operation does not affect how promotion of a resource is determined. If the affected node still has the highest promotion score, it will be selected to be promoted again.\n\t\t\t\u003C/p>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_clusternode-management-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 19. Managing cluster nodes\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tThere are a variety of \u003Ccode class=\"literal\">pcs\u003C/code> commands you can use to manage cluster nodes, including commands to start and stop cluster services and to add and remove cluster nodes.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_cluster-stop-clusternode-management\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">19.1. Stopping cluster services\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe following command stops cluster services on the specified node or nodes. As with the \u003Ccode class=\"literal command\">pcs cluster start\u003C/code>, the \u003Ccode class=\"literal\">--all\u003C/code> option stops cluster services on all nodes and if you do not specify any nodes, cluster services are stopped on the local node only.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs cluster stop [--all | \u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span>] [...]\u003C/pre>\u003Cp>\n\t\t\t\tYou can force a stop of cluster services on the local node with the following command, which performs a \u003Ccode class=\"literal command\">kill -9\u003C/code> command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs cluster kill\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_cluster-enable-clusternode-management\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">19.2. Enabling and disabling cluster services\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tEnable the cluster services with the following command. This configures the cluster services to run on startup on the specified node or nodes.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tEnabling allows nodes to automatically rejoin the cluster after they have been fenced, minimizing the time the cluster is at less than full strength. If the cluster services are not enabled, an administrator can manually investigate what went wrong before starting the cluster services manually, so that, for example, a node with hardware issues in not allowed back into the cluster when it is likely to fail again.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tIf you specify the \u003Ccode class=\"literal\">--all\u003C/code> option, the command enables cluster services on all nodes.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tIf you do not specify any nodes, cluster services are enabled on the local node only.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cpre class=\"literallayout\">pcs cluster enable [--all | \u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span>] [...]\u003C/pre>\u003Cp>\n\t\t\t\tUse the following command to configure the cluster services not to run on startup on the specified node or nodes.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tIf you specify the \u003Ccode class=\"literal\">--all\u003C/code> option, the command disables cluster services on all nodes.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tIf you do not specify any nodes, cluster services are disabled on the local node only.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cpre class=\"literallayout\">pcs cluster disable [--all | \u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span>] [...]\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_cluster-nodeadd-clusternode-management\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">19.3. Adding cluster nodes\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tAdd a new node to an existing cluster with the following procedure.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThis procedure adds standard clusters nodes running \u003Ccode class=\"literal\">corosync\u003C/code>. For information about integrating non-corosync nodes into a cluster, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_remote-node-management-configuring-and-managing-high-availability-clusters\">Integrating non-corosync nodes into a cluster: the pacemaker_remote service\u003C/a>.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tIt is recommended that you add nodes to existing clusters only during a production maintenance window. This allows you to perform appropriate resource and deployment testing for the new node and its fencing configuration.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tIn this example, the existing cluster nodes are \u003Ccode class=\"literal\">clusternode-01.example.com\u003C/code>, \u003Ccode class=\"literal\">clusternode-02.example.com\u003C/code>, and \u003Ccode class=\"literal\">clusternode-03.example.com\u003C/code>. The new node is \u003Ccode class=\"literal\">newnode.example.com\u003C/code>.\n\t\t\t\u003C/p>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\tOn the new node to add to the cluster, perform the following tasks.\n\t\t\t\t\u003C/p>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tInstall the cluster packages. If the cluster uses SBD, the Booth ticket manager, or a quorum device, you must manually install the respective packages (\u003Ccode class=\"literal\">sbd\u003C/code>, \u003Ccode class=\"literal\">booth-site\u003C/code>, \u003Ccode class=\"literal\">corosync-qdevice\u003C/code>) on the new node as well.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@newnode ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install -y pcs fence-agents-all\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIn addition to the cluster packages, you will also need to install and configure all of the services that you are running in the cluster, which you have installed on the existing cluster nodes. For example, if you are running an Apache HTTP server in a Red Hat high availability cluster, you will need to install the server on the node you are adding, as well as the \u003Ccode class=\"literal\">wget\u003C/code> tool that checks the status of the server.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf you are running the \u003Ccode class=\"literal command\">firewalld\u003C/code> daemon, execute the following commands to enable the ports that are required by the Red Hat High Availability Add-On.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --permanent --add-service=high-availability\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --add-service=high-availability\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSet a password for the user ID \u003Ccode class=\"literal\">hacluster\u003C/code>. It is recommended that you use the same password for each node in the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@newnode ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>passwd hacluster\u003C/strong>\u003C/span>\nChanging password for user hacluster.\nNew password:\nRetype new password:\npasswd: all authentication tokens updated successfully.\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tExecute the following commands to start the \u003Ccode class=\"literal\">pcsd\u003C/code> service and to enable \u003Ccode class=\"literal\">pcsd\u003C/code> at system start.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl start pcsd.service\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl enable pcsd.service\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\tOn a node in the existing cluster, perform the following tasks.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAuthenticate user \u003Ccode class=\"literal\">hacluster\u003C/code> on the new cluster node.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@clusternode-01 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs host auth newnode.example.com\u003C/strong>\u003C/span>\nUsername: hacluster\nPassword:\nnewnode.example.com: Authorized\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAdd the new node to the existing cluster. This command also syncs the cluster configuration file \u003Ccode class=\"literal\">corosync.conf\u003C/code> to all nodes in the cluster, including the new node you are adding.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@clusternode-01 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster node add newnode.example.com\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\tOn the new node to add to the cluster, perform the following tasks.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tStart and enable cluster services on the new node.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@newnode ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster start\u003C/strong>\u003C/span>\nStarting Cluster...\n[root@newnode ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster enable\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tEnsure that you configure and test a fencing device for the new cluster node.\n\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_cluster-noderemove-clusternode-management\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">19.4. Removing cluster nodes\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe following command shuts down the specified node and removes it from the cluster configuration file, \u003Ccode class=\"literal\">corosync.conf\u003C/code>, on all of the other nodes in the cluster.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs cluster node remove \u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_add-nodes-to-multiple-ip-cluster-clusternode-management\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">19.5. Adding a node to a cluster with multiple links\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen adding a node to a cluster with multiple links, you must specify addresses for all links.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following example adds the node \u003Ccode class=\"literal\">rh80-node3\u003C/code> to a cluster, specifying IP address 192.168.122.203 for the first link and 192.168.123.203 as the second link.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster node add rh80-node3 addr=192.168.122.203 addr=192.168.123.203\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_changing-links-in-multiple-ip-cluster-clusternode-management\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">19.6. Adding and modifying links in an existing cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIn most cases, you can add or modify the links in an existing cluster without restarting the cluster.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"adding_and_removing_links_in_an_existing_cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">19.6.1. Adding and removing links in an existing cluster\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tTo add a new link to a running cluster, use the \u003Ccode class=\"literal\">pcs cluster link add\u003C/code> command.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tWhen adding a link, you must specify an address for each node.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tAdding and removing a link is only possible when you are using the \u003Ccode class=\"literal\">knet\u003C/code> transport protocol.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tAt least one link in the cluster must be defined at any time.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tThe maximum number of links in a cluster is 8, numbered 0-7. It does not matter which links are defined, so, for example, you can define only links 3, 6 and 7.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tWhen you add a link without specifying its link number, \u003Ccode class=\"literal\">pcs\u003C/code> uses the lowest link available.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tThe link numbers of currently configured links are contained in the \u003Ccode class=\"literal\">corosync.conf\u003C/code> file. To display the \u003Ccode class=\"literal\">corosync.conf\u003C/code> file, run the \u003Ccode class=\"literal\">pcs cluster corosync\u003C/code> command or the \u003Ccode class=\"literal\">pcs cluster config show\u003C/code> command.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following command adds link number 5 to a three node cluster.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link add node1=10.0.5.11 node2=10.0.5.12 node3=10.0.5.31 options linknumber=5\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tTo remove an existing link, use the \u003Ccode class=\"literal\">pcs cluster link delete\u003C/code> or \u003Ccode class=\"literal\">pcs cluster link remove\u003C/code> command. Either of the following commands will remove link number 5 from the cluster.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link delete 5\u003C/strong>\u003C/span>\n\n[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link remove 5\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"modifying_a_link_in_a_cluster_with_multiple_links\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">19.6.2. Modifying a link in a cluster with multiple links\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tIf there are multiple links in the cluster and you want to change one of them, perform the following procedure.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tRemove the link you want to change.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link remove 2\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tAdd the link back to the cluster with the updated addresses and options.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link add node1=10.0.5.11 node2=10.0.5.12 node3=10.0.5.31 options linknumber=2\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"modifying_the_link_addresses_in_a_cluster_with_a_single_link\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">19.6.3. Modifying the link addresses in a cluster with a single link\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tIf your cluster uses only one link and you want to modify that link to use different addresses, perform the following procedure. In this example, the original link is link 1.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tAdd a new link with the new addresses and options.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link add node1=10.0.5.11 node2=10.0.5.12 node3=10.0.5.31 options linknumber=2\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tRemove the original link.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link remove 1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\t\tNote that you cannot specify addresses that are currently in use when adding links to a cluster. This means, for example, that if you have a two-node cluster with one link and you want to change the address for one node only, you cannot use the above procedure to add a new link that specifies one new address and one existing address. Instead, you can add a temporary link before removing the existing link and adding it back with the updated address, as in the following example.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tIn this example:\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tThe link for the existing cluster is link 1, which uses the address 10.0.5.11 for node 1 and the address 10.0.5.12 for node 2.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tYou would like to change the address for node 2 to 10.0.5.31.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\t\tTo update only one of the addresses for a two-node cluster with a single link, use the following procedure.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tAdd a new temporary link to the existing cluster, using addresses that are not currently in use.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link add node1=10.0.5.13 node2=10.0.5.14 options linknumber=2\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tRemove the original link.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link remove 1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tAdd the new, modified link.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link add node1=10.0.5.11 node2=10.0.5.31 options linknumber=1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tRemove the temporary link you created\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link remove 2\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"modifying_the_link_options_for_a_link_in_a_cluster_with_a_single_link\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">19.6.4. Modifying the link options for a link in a cluster with a single link\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tIf your cluster uses only one link and you want to modify the options for that link but you do not want to change the address to use, you can add a temporary link before removing and updating the link to modify.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tIn this example:\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tThe link for the existing cluster is link 1, which uses the address 10.0.5.11 for node 1 and the address 10.0.5.12 for node 2.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tYou would like to change the link option \u003Ccode class=\"literal\">link_priority\u003C/code> to 11.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\t\tModify the link option in a cluster with a single link with the following procedure.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tAdd a new temporary link to the existing cluster, using addresses that are not currently in use.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link add node1=10.0.5.13 node2=10.0.5.14 options linknumber=2\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tRemove the original link.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link remove 1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tAdd back the original link with the updated options.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link add node1=10.0.5.11 node2=10.0.5.12 options linknumber=1 link_priority=11\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tRemove the temporary link.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link remove 2\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"modifying_a_link_when_adding_a_new_link_is_not_possible\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">19.6.5. Modifying a link when adding a new link is not possible\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tIf for some reason adding a new link is not possible in your configuration and your only option is to modify a single existing link, you can use the following procedure, which requires that you shut your cluster down.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\t\tThe following example procedure updates link number 1 in the cluster and sets the \u003Ccode class=\"literal\">link_priority\u003C/code> option for the link to 11.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tStop the cluster services for the cluster.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster stop --all\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tUpdate the link addresses and options.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tThe \u003Ccode class=\"literal\">pcs cluster link update\u003C/code> command does not require that you specify all of the node addresses and options. Instead, you can specify only the addresses to change. This example modifies the addresses for \u003Ccode class=\"literal\">node1\u003C/code> and \u003Ccode class=\"literal\">node3\u003C/code> and the \u003Ccode class=\"literal\">link_priority\u003C/code> option only.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster link update 1 node1=10.0.5.11 node3=10.0.5.31 options link_priority=11\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tTo remove an option, you can set the option to a null value with the \u003Ccode class=\"literal\">\u003Cspan class=\"emphasis\">\u003Cem>option\u003C/em>\u003C/span>=\u003C/code> format.\n\t\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tRestart the cluster\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster start --all\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"proc_tracking-node-health-clusternode-management\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">19.7. Configuring a node health strategy\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\tA node might be functioning well enough to maintain its cluster membership and yet be unhealthy in some respect that makes it an undesirable location for resources. For example, a disk drive might be reporting SMART errors, or the CPU might be highly loaded. As of RHEL 9.1, You can use a node health strategy in Pacemaker to automatically move resources off unhealthy nodes.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can monitor a node’s health with the the following health node resource agents, which set node attributes based on CPU and disk status:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">ocf:pacemaker:HealthCPU\u003C/code>, which monitors CPU idling\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">ocf:pacemaker:HealthIOWait\u003C/code>, which monitors the CPU I/O wait\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">ocf:pacemaker:HealthSMART\u003C/code>, which monitors SMART status of a disk drive\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Ccode class=\"literal\">ocf:pacemaker:SysInfo\u003C/code>, which sets a variety of node attributes with local system information and also functions as a health agent monitoring disk space usage\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tAdditionally, any resource agent might provide node attributes that can be used to define a health node strategy.\n\t\t\t\u003C/p>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\tThe following procedure configures a health node strategy for a cluster that will move resources off of any node whose CPU I/O wait goes above 15%.\n\t\t\t\t\u003C/p>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSet the \u003Ccode class=\"literal\">health-node-strategy\u003C/code> cluster property to define how Pacemaker responds to changes in node health.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set node-health-strategy=migrate-on-red\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate a cloned cluster resource that uses a health node resource agent, setting the \u003Ccode class=\"literal\">allow-unhealthy-nodes\u003C/code> resource meta option to define whether the cluster will detect if the node’s health recovers and move resources back to the node. Configure this resource with a recurring monitor action, to continually check the health of all nodes.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThis example creates a \u003Ccode class=\"literal\">HealthIOWait\u003C/code> resource agent to monitor the CPU I/O wait, setting a red limit for moving resources off a node to 15%. This command sets the \u003Ccode class=\"literal\">allow-unhealthy-nodes\u003C/code> resource meta option to \u003Ccode class=\"literal\">true\u003C/code> and configures a recurring monitor interval of 10 seconds.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create io-monitor ocf:pacemaker:HealthIOWait red_limit=15 op monitor interval=10s meta allow-unhealthy-nodes=true clone\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-large-clusters-clusternode-management\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">19.8. Configuring a large cluster with many resources\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf the cluster you are deploying consists of a large number of nodes and many resources, you may need to modify the default values of the following parameters for your cluster.\n\t\t\t\u003C/p>\u003Cdiv class=\"variablelist\">\u003Cdl class=\"variablelist\">\u003Cdt>\u003Cspan class=\"term\">The \u003Ccode class=\"literal\">cluster-ipc-limit\u003C/code> cluster property\u003C/span>\u003C/dt>\u003Cdd>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tThe \u003Ccode class=\"literal\">cluster-ipc-limit\u003C/code> cluster property is the maximum IPC message backlog before one cluster daemon will disconnect another. When a large number of resources are cleaned up or otherwise modified simultaneously in a large cluster, a large number of CIB updates arrive at once. This could cause slower clients to be evicted if the Pacemaker service does not have time to process all of the configuration updates before the CIB event queue threshold is reached.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tThe recommended value of \u003Ccode class=\"literal\">cluster-ipc-limit\u003C/code> for use in large clusters is the number of resources in the cluster multiplied by the number of nodes. This value can be raised if you see \"Evicting client\" messages for cluster daemon PIDs in the logs.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tYou can increase the value of \u003Ccode class=\"literal\">cluster-ipc-limit\u003C/code> from its default value of 500 with the \u003Ccode class=\"literal\">pcs property set\u003C/code> command. For example, for a ten-node cluster with 200 resources you can set the value of \u003Ccode class=\"literal\">cluster-ipc-limit\u003C/code> to 2000 with the following command.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set cluster-ipc-limit=2000\u003C/strong>\u003C/span>\u003C/pre>\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">The \u003Ccode class=\"literal\">PCMK_ipc_buffer\u003C/code> Pacemaker parameter\u003C/span>\u003C/dt>\u003Cdd>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn very large deployments, internal Pacemaker messages may exceed the size of the message buffer. When this occurs, you will see a message in the system logs of the following format:\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">Compressed message exceeds \u003Cspan class=\"emphasis\">\u003Cem>X\u003C/em>\u003C/span>% of configured IPC limit (\u003Cspan class=\"emphasis\">\u003Cem>X\u003C/em>\u003C/span> bytes); consider setting PCMK_ipc_buffer to \u003Cspan class=\"emphasis\">\u003Cem>X\u003C/em>\u003C/span> or higher\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tWhen you see this message, you can increase the value of \u003Ccode class=\"literal\">PCMK_ipc_buffer\u003C/code> in the \u003Ccode class=\"literal\">/etc/sysconfig/pacemaker\u003C/code> configuration file on each node. For example, to increase the value of \u003Ccode class=\"literal\">PCMK_ipc_buffer\u003C/code> from its default value to 13396332 bytes, change the uncommented \u003Ccode class=\"literal\">PCMK_ipc_buffer\u003C/code> field in the \u003Ccode class=\"literal\">/etc/sysconfig/pacemaker\u003C/code> file on each node in the cluster as follows.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">PCMK_ipc_buffer=13396332\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tTo apply this change, run the following command.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl restart pacemaker\u003C/strong>\u003C/span>\u003C/pre>\u003C/dd>\u003C/dl>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_cluster-permissions-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 20. Setting user permissions for a Pacemaker cluster\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tYou can grant permission for specific users other than user \u003Ccode class=\"literal\">hacluster\u003C/code> to manage a Pacemaker cluster. There are two sets of permissions that you can grant to individual users:\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\tPermissions that allow individual users to manage the cluster through the Web UI and to run \u003Ccode class=\"literal command\">pcs\u003C/code> commands that connect to nodes over a network. Commands that connect to nodes over a network include commands to set up a cluster, or to add or remove nodes from a cluster.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tPermissions for local users to allow read-only or read-write access to the cluster configuration. Commands that do not require connecting over a network include commands that edit the cluster configuration, such as those that create resources and configure constraints.\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\tIn situations where both sets of permissions have been assigned, the permissions for commands that connect over a network are applied first, and then permissions for editing the cluster configuration on the local node are applied. Most \u003Ccode class=\"literal command\">pcs\u003C/code> commands do not require network access and in those cases the network permissions will not apply.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"proc_setting-cluster-access-over-network-cluster-permissions\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">20.1. Setting permissions for node access over a network\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tTo grant permission for specific users to manage the cluster through the Web UI and to run \u003Ccode class=\"literal command\">pcs\u003C/code> commands that connect to nodes over a network, add those users to the group \u003Ccode class=\"literal\">haclient\u003C/code>. This must be done on every node in the cluster.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_setting-local-cluster-permissions-cluster-permissions\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">20.2. Setting local permissions using ACLs\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can use the \u003Ccode class=\"literal command\">pcs acl\u003C/code> command to set permissions for local users to allow read-only or read-write access to the cluster configuration by using access control lists (ACLs).\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tBy default, ACLs are not enabled. When ACLs are not enabled, any user who is a member of the group \u003Ccode class=\"literal\">haclient\u003C/code> on all nodes has full local read/write access to the cluster configuration while users who are not members of \u003Ccode class=\"literal\">haclient\u003C/code> have no access. When ACLs are enabled, however, even users who are members of the \u003Ccode class=\"literal\">haclient\u003C/code> group have access only to what has been granted to that user by the ACLs. The root and \u003Ccode class=\"literal\">hacluster\u003C/code> user accounts always have full access to the cluster configuration, even when ACLs are enabled.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tSetting permissions for local users is a two step process:\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tExecute the \u003Ccode class=\"literal command\">pcs acl role create…​\u003C/code> command to create a \u003Cspan class=\"emphasis\">\u003Cem>role\u003C/em>\u003C/span> which defines the permissions for that role.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tAssign the role you created to a user with the \u003Ccode class=\"literal command\">pcs acl user create\u003C/code> command. If you assign multiple roles to the same user, any \u003Ccode class=\"literal\">deny\u003C/code> permission takes precedence, then \u003Ccode class=\"literal\">write\u003C/code>, then \u003Ccode class=\"literal\">read\u003C/code>.\n\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003Cdiv class=\"formalpara\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Cp>\n\t\t\t\t\tThe following example procedure provides read-only access for a cluster configuration to a local user named \u003Ccode class=\"literal\">rouser\u003C/code>. Note that it is also possible to restrict access to certain portions of the configuration only.\n\t\t\t\t\u003C/p>\u003C/div>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tIt is important to perform this procedure as root or to save all of the configuration updates to a working file which you can then push to the active CIB when you are finished. Otherwise, you can lock yourself out of making any further changes. For information on saving configuration updates to a working file, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_pcs-operation-configuring-and-managing-high-availability-clusters#proc_configure-testfile-pcs-operation\">Saving a configuration change to a working file\u003C/a>.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThis procedure requires that the user \u003Ccode class=\"literal\">rouser\u003C/code> exists on the local system and that the user \u003Ccode class=\"literal\">rouser\u003C/code> is a member of the group \u003Ccode class=\"literal\">haclient\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>adduser rouser\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>usermod -a -G haclient rouser\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tEnable Pacemaker ACLs with the \u003Ccode class=\"literal command\">pcs acl enable\u003C/code> command.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs acl enable\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate a role named \u003Ccode class=\"literal\">read-only\u003C/code> with read-only permissions for the cib.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs acl role create read-only description=\"Read access to cluster\" read xpath /cib\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate the user \u003Ccode class=\"literal\">rouser\u003C/code> in the pcs ACL system and assign that user the \u003Ccode class=\"literal\">read-only\u003C/code> role.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs acl user create rouser read-only\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tView the current ACLs.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs acl\u003C/strong>\u003C/span>\nUser: rouser\n  Roles: read-only\nRole: read-only\n  Description: Read access to cluster\n  Permission: read xpath /cib (read-only-read)\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn each node where \u003Ccode class=\"literal\">rouser\u003C/code> will run \u003Ccode class=\"literal\">pcs\u003C/code> commands, log in as \u003Ccode class=\"literal\">rouser\u003C/code> and authenticate to the local \u003Ccode class=\"literal\">pcsd\u003C/code> service. This is required in order to run certain \u003Ccode class=\"literal\">pcs\u003C/code> commands, such as \u003Ccode class=\"literal\">pcs status\u003C/code>, as the ACL user.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[rouser ~]$ \u003Cspan class=\"strong strong\">\u003Cstrong>pcs client local-auth\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_resource-monitoring-operations-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 21. Resource monitoring operations\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tTo ensure that resources remain healthy, you can add a monitoring operation to a resource’s definition. If you do not specify a monitoring operation for a resource, by default the \u003Ccode class=\"literal command\">pcs\u003C/code> command will create a monitoring operation, with an interval that is determined by the resource agent. If the resource agent does not provide a default monitoring interval, the pcs command will create a monitoring operation with an interval of 60 seconds.\n\t\t\u003C/p>\u003Cp>\n\t\t\tThe following table summarizes the properties of a resource monitoring operation.\n\t\t\u003C/p>\u003Crh-table id=\"idm140686144862608\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 21.1. Properties of an Operation\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 25%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 75%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686144857760\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686144856672\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686144857760\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">id\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686144856672\"> \u003Cp>\n\t\t\t\t\t\t\tUnique name for the action. The system assigns this when you configure an operation.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686144857760\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">name\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686144856672\"> \u003Cp>\n\t\t\t\t\t\t\tThe action to perform. Common values: \u003Ccode class=\"literal\">monitor\u003C/code>, \u003Ccode class=\"literal\">start\u003C/code>, \u003Ccode class=\"literal\">stop\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686144857760\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">interval\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686144856672\"> \u003Cp>\n\t\t\t\t\t\t\tIf set to a nonzero value, a recurring operation is created that repeats at this frequency, in seconds. A nonzero value makes sense only when the action \u003Ccode class=\"literal\">name\u003C/code> is set to \u003Ccode class=\"literal\">monitor\u003C/code>. A recurring monitor action will be executed immediately after a resource start completes, and subsequent monitor actions are scheduled starting at the time the previous monitor action completed. For example, if a monitor action with \u003Ccode class=\"literal\">interval=20s\u003C/code> is executed at 01:00:00, the next monitor action does not occur at 01:00:20, but at 20 seconds after the first monitor action completes.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\tIf set to zero, which is the default value, this parameter allows you to provide values to be used for operations created by the cluster. For example, if the \u003Ccode class=\"literal\">interval\u003C/code> is set to zero, the \u003Ccode class=\"literal\">name\u003C/code> of the operation is set to \u003Ccode class=\"literal\">start\u003C/code>, and the \u003Ccode class=\"literal\">timeout\u003C/code> value is set to 40, then Pacemaker will use a timeout of 40 seconds when starting this resource. A \u003Ccode class=\"literal\">monitor\u003C/code> operation with a zero interval allows you to set the \u003Ccode class=\"literal\">timeout\u003C/code>/\u003Ccode class=\"literal\">on-fail\u003C/code>/\u003Ccode class=\"literal\">enabled\u003C/code> values for the probes that Pacemaker does at startup to get the current status of all resources when the defaults are not desirable.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686144857760\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">timeout\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686144856672\"> \u003Cp>\n\t\t\t\t\t\t\tIf the operation does not complete in the amount of time set by this parameter, abort the operation and consider it failed. The default value is the value of \u003Ccode class=\"literal\">timeout\u003C/code> if set with the \u003Ccode class=\"literal command\">pcs resource op defaults\u003C/code> command, or 20 seconds if it is not set. If you find that your system includes a resource that requires more time than the system allows to perform an operation (such as \u003Ccode class=\"literal\">start\u003C/code>, \u003Ccode class=\"literal\">stop\u003C/code>, or \u003Ccode class=\"literal\">monitor\u003C/code>), investigate the cause and if the lengthy execution time is expected you can increase this value.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\tThe \u003Ccode class=\"literal\">timeout\u003C/code> value is not a delay of any kind, nor does the cluster wait the entire timeout period if the operation returns before the timeout period has completed.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686144857760\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">on-fail\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686144856672\"> \u003Cp>\n\t\t\t\t\t\t\tThe action to take if this action ever fails. Allowed values:\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">ignore\u003C/code> - Pretend the resource did not fail\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">block\u003C/code> - Do not perform any further operations on the resource\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">stop\u003C/code> - Stop the resource and do not start it elsewhere\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">restart\u003C/code> - Stop the resource and start it again (possibly on a different node)\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">fence\u003C/code> - STONITH the node on which the resource failed\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">standby\u003C/code> - Move \u003Cspan class=\"emphasis\">\u003Cem>all\u003C/em>\u003C/span> resources away from the node on which the resource failed\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">demote\u003C/code> - When a \u003Ccode class=\"literal\">promote\u003C/code> action fails for the resource, the resource will be demoted but will not be fully stopped. When a \u003Ccode class=\"literal\">monitor\u003C/code> action fails for a resource, if \u003Ccode class=\"literal\">interval\u003C/code> is set to a nonzero value and \u003Ccode class=\"literal\">role\u003C/code> is set to \u003Ccode class=\"literal\">Promoted\u003C/code> the resource will be demoted but will not be fully stopped.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\tThe default for the \u003Ccode class=\"literal\">stop\u003C/code> operation is \u003Ccode class=\"literal\">fence\u003C/code> when STONITH is enabled and \u003Ccode class=\"literal\">block\u003C/code> otherwise. All other operations default to \u003Ccode class=\"literal\">restart\u003C/code>.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686144857760\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">enabled\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686144856672\"> \u003Cp>\n\t\t\t\t\t\t\tIf \u003Ccode class=\"literal\">false\u003C/code>, the operation is treated as if it does not exist. Allowed values: \u003Ccode class=\"literal\">true\u003C/code>, \u003Ccode class=\"literal\">false\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Csection class=\"section\" id=\"proc_configuring-resource-monitoring-operations-resource-monitoring-operations\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">21.1. Configuring resource monitoring operations\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can configure monitoring operations when you create a resource with the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource create \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>standard:provider:type|type\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>resource_options\u003C/em>\u003C/span>] [op \u003Cspan class=\"emphasis\">\u003Cem>operation_action\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>operation_options\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>operation_type\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>operation_options\u003C/em>\u003C/span>]...]\u003C/pre>\u003Cp>\n\t\t\t\tFor example, the following command creates an \u003Ccode class=\"literal\">IPaddr2\u003C/code> resource with a monitoring operation. The new resource is called \u003Ccode class=\"literal\">VirtualIP\u003C/code> with an IP address of 192.168.0.99 and a netmask of 24 on \u003Ccode class=\"literal\">eth2\u003C/code>. A monitoring operation will be performed every 30 seconds.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.99 cidr_netmask=24 nic=eth2 op monitor interval=30s\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tAlternately, you can add a monitoring operation to an existing resource with the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource op add \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>operation_action\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>operation_properties\u003C/em>\u003C/span>]\u003C/pre>\u003Cp>\n\t\t\t\tUse the following command to delete a configured resource operation.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource op remove \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>operation_name\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>operation_properties\u003C/em>\u003C/span>\u003C/pre>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tYou must specify the exact operation properties to properly remove an existing operation.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tTo change the values of a monitoring option, you can update the resource. For example, you can create a \u003Ccode class=\"literal\">VirtualIP\u003C/code> with the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.99 cidr_netmask=24 nic=eth2\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tBy default, this command creates these operations.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">Operations: start interval=0s timeout=20s (VirtualIP-start-timeout-20s)\n            stop interval=0s timeout=20s (VirtualIP-stop-timeout-20s)\n            monitor interval=10s timeout=20s (VirtualIP-monitor-interval-10s)\u003C/pre>\u003Cp>\n\t\t\t\tTo change the stop timeout operation, execute the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource update VirtualIP op stop interval=0s timeout=40s\u003C/strong>\u003C/span>\n\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource config VirtualIP\u003C/strong>\u003C/span>\n Resource: VirtualIP (class=ocf provider=heartbeat type=IPaddr2)\n  Attributes: ip=192.168.0.99 cidr_netmask=24 nic=eth2\n  Operations: start interval=0s timeout=20s (VirtualIP-start-timeout-20s)\n              monitor interval=10s timeout=20s (VirtualIP-monitor-interval-10s)\n              stop interval=0s timeout=40s (VirtualIP-name-stop-interval-0s-timeout-40s)\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-global-resource-operation-defaults-resource-monitoring-operations\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">21.2. Configuring global resource operation defaults\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can change the default value of a resource operation for all resources with the \u003Ccode class=\"literal\">pcs resource op defaults update\u003C/code> command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command sets a global default of a \u003Ccode class=\"literal\">timeout\u003C/code> value of 240 seconds for all monitoring operations.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource op defaults update timeout=240s\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe original \u003Ccode class=\"literal\">pcs resource op defaults \u003Cspan class=\"emphasis\">\u003Cem>name\u003C/em>\u003C/span>=\u003Cspan class=\"emphasis\">\u003Cem>value\u003C/em>\u003C/span>\u003C/code> command, which set resource operation defaults for all resources in previous releases, remains supported unless there is more than one set of defaults configured. However, \u003Ccode class=\"literal\">pcs resource op defaults update\u003C/code> is now the preferred version of the command.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"overriding_resource_specific_operation_values\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">21.2.1. Overriding resource-specific operation values\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tNote that a cluster resource will use the global default only when the option is not specified in the cluster resource definition. By default, resource agents define the \u003Ccode class=\"literal\">timeout\u003C/code> option for all operations. For the global operation timeout value to be honored, you must create the cluster resource without the \u003Ccode class=\"literal\">timeout\u003C/code> option explicitly or you must remove the \u003Ccode class=\"literal\">timeout\u003C/code> option by updating the cluster resource, as in the following command.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource update VirtualIP op monitor interval=10s\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tFor example, after setting a global default of a \u003Ccode class=\"literal\">timeout\u003C/code> value of 240 seconds for all monitoring operations and updating the cluster resource \u003Ccode class=\"literal\">VirtualIP\u003C/code> to remove the timeout value for the \u003Ccode class=\"literal\">monitor\u003C/code> operation, the resource \u003Ccode class=\"literal\">VirtualIP\u003C/code> will then have timeout values for \u003Ccode class=\"literal\">start\u003C/code>, \u003Ccode class=\"literal\">stop\u003C/code>, and \u003Ccode class=\"literal\">monitor\u003C/code> operations of 20s, 40s and 240s, respectively. The global default value for timeout operations is applied here only on the \u003Ccode class=\"literal\">monitor\u003C/code> operation, where the default \u003Ccode class=\"literal\">timeout\u003C/code> option was removed by the previous command.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource config VirtualIP\u003C/strong>\u003C/span>\n Resource: VirtualIP (class=ocf provider=heartbeat type=IPaddr2)\n   Attributes: ip=192.168.0.99 cidr_netmask=24 nic=eth2\n   Operations: start interval=0s timeout=20s (VirtualIP-start-timeout-20s)\n               monitor interval=10s (VirtualIP-monitor-interval-10s)\n               stop interval=0s timeout=40s (VirtualIP-name-stop-interval-0s-timeout-40s)\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"changing_the_default_value_of_a_resource_operation_for_sets_of_resources\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">21.2.2. Changing the default value of a resource operation for sets of resources\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tYou can create multiple sets of resource operation defaults with the \u003Ccode class=\"literal\">pcs resource op defaults set create\u003C/code> command, which allows you to specify a rule that contains \u003Ccode class=\"literal\">resource\u003C/code> and operation expressions. All of the rule expressions supported by Pacemaker are allowed.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tWith this command, you can configure a default resource operation value for all resources of a particular type. For example, it is now possible to configure implicit \u003Ccode class=\"literal\">podman\u003C/code> resources created by Pacemaker when bundles are in use.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe following command sets a default timeout value of 90s for all operations for all \u003Ccode class=\"literal\">podman\u003C/code> resources. In this example, \u003Ccode class=\"literal\">::podman\u003C/code> means a resource of any class, any provider, of type \u003Ccode class=\"literal\">podman\u003C/code>.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThe \u003Ccode class=\"literal\">id\u003C/code> option, which names the set of resource operation defaults, is not mandatory. If you do not set this option, \u003Ccode class=\"literal\">pcs\u003C/code> will generate an ID automatically. Setting this value allows you to provide a more descriptive name.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource op defaults set create id=podman-timeout meta timeout=90s rule resource ::podman\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tThe following command sets a default timeout value of 120s for the \u003Ccode class=\"literal\">stop\u003C/code> operation for all resources.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource op defaults set create id=stop-timeout meta timeout=120s rule op stop\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tIt is possible to set the default timeout value for a specific operation for all resources of a particular type. The following example sets a default timeout value of 120s for the \u003Ccode class=\"literal\">stop\u003C/code> operation for all \u003Ccode class=\"literal\">podman\u003C/code> resources.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource op defaults set create id=podman-stop-timeout meta timeout=120s rule resource ::podman and op stop\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"displaying_currently_configured_resource_operation_default_values\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">21.2.3. Displaying currently configured resource operation default values\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe \u003Ccode class=\"literal\">pcs resource op defaults [config]\u003C/code> command displays a list of currently configured default values for resource operations, including any rules you specified. As of Red Hat Enterprise Linux 9.5, you can display the output of this command in text, JSON, and command formats.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tSpecifying \u003Ccode class=\"literal\">--output-format=text\u003C/code> displays the configured resource operation defaults in plain text format, which is the default value for this option.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tSpecifying \u003Ccode class=\"literal\">--output-format=cmd\u003C/code> displays the \u003Ccode class=\"literal\">pcs resource op defaults\u003C/code> commands created from the current cluster defaults configuration. You can use these commands to re-create configured resource operation defaults on a different system.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tSpecifying \u003Ccode class=\"literal\">--output-format=json\u003C/code> displays the configured resource operation defaults in JSON format, which is suitable for machine parsing.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following examples show the three different output formats of the \u003Ccode class=\"literal\">pcs resource op defaults config\u003C/code> command after the default resource operation values for any \u003Ccode class=\"literal\">ocf:pacemaker:podman\u003C/code> resource were reset with this example command:\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource op defaults set create id=op-set-1 score=100 meta timeout=30s rule op monitor and resource ocf:pacemaker:podman\u003C/strong>\u003C/span>\nWarning: Defaults do not apply to resources which override them with their own defined values\u003C/pre>\u003Cp>\n\t\t\t\t\tThis example displays the configured resource operation default values in plain text.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># pcs resource op defaults config\nMeta Attrs: op-set-1 score=100\n  timeout=30s\n  Rule: boolean-op=and score=INFINITY\n    Expression: op monitor\n    Expression: resource ocf:pacemaker:podman\u003C/pre>\u003Cp>\n\t\t\t\t\tThis example displays the \u003Ccode class=\"literal\">pcs resource op defaults\u003C/code> commands created from the current cluster defaults configuration.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># pcs resource op defaults config --output-format=cmd\npcs -- resource op defaults set create id=op-set-1 score=100 \\\n  meta timeout=30s \\\n  rule 'op monitor and resource ocf:pacemaker:podman'\u003C/pre>\u003Cp>\n\t\t\t\t\tThis example displays the configured resource operation default values in JSON format.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># pcs resource op defaults config --output-format=json\n{\"instance_attributes\": [], \"meta_attributes\": [{\"id\": \"op-set-1\", \"options\": {\"score\": \"100\"}, \"rule\": {\"id\": \"op-set-1-rule\", \"type\": \"RULE\", \"in_effect\": \"UNKNOWN\", \"options\": {\"boolean-op\": \"and\", \"score\": \"INFINITY\"}, \"date_spec\": null, \"duration\": null, \"expressions\": [{\"id\": \"op-set-1-rule-op-monitor\", \"type\": \"OP_EXPRESSION\", \"in_effect\": \"UNKNOWN\", \"options\": {\"name\": \"monitor\"}, \"date_spec\": null, \"duration\": null, \"expressions\": [], \"as_string\": \"op monitor\"}, {\"id\": \"op-set-1-rule-rsc-ocf-pacemaker-podman\", \"type\": \"RSC_EXPRESSION\", \"in_effect\": \"UNKNOWN\", \"options\": {\"class\": \"ocf\", \"provider\": \"pacemaker\", \"type\": \"podman\"}, \"date_spec\": null, \"duration\": null, \"expressions\": [], \"as_string\": \"resource ocf:pacemaker:podman\"}], \"as_string\": \"op monitor and resource ocf:pacemaker:podman\"}, \"nvpairs\": [{\"id\": \"op-set-1-timeout\", \"name\": \"timeout\", \"value\": \"30s\"}]}]}\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-multiple-monitoring-operations-resource-monitoring-operations\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">21.3. Configuring multiple monitoring operations\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can configure a single resource with as many monitor operations as a resource agent supports. In this way you can do a superficial health check every minute and progressively more intense ones at higher intervals.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tWhen configuring multiple monitor operations, you must ensure that no two operations are performed at the same interval.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tTo configure additional monitoring operations for a resource that supports more in-depth checks at different levels, you add an \u003Ccode class=\"literal\">OCF_CHECK_LEVEL=\u003Cspan class=\"emphasis\">\u003Cem>n\u003C/em>\u003C/span>\u003C/code> option.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFor example, if you configure the following \u003Ccode class=\"literal\">IPaddr2\u003C/code> resource, by default this creates a monitoring operation with an interval of 10 seconds and a timeout value of 20 seconds.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.0.99 cidr_netmask=24 nic=eth2\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tIf the Virtual IP supports a different check with a depth of 10, the following command causes Pacemaker to perform the more advanced monitoring check every 60 seconds in addition to the normal Virtual IP check every 10 seconds. (As noted, you should not configure the additional monitoring operation with a 10-second interval as well.)\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource op add VirtualIP monitor interval=60s OCF_CHECK_LEVEL=10\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_controlling-cluster-behavior-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 22. Pacemaker cluster properties\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tCluster properties control how the cluster behaves when confronted with situations that might occur during cluster operation.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"ref_cluster-properties-options-controlling-cluster-behavior\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">22.1. Summary of cluster properties and options\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe following table summaries the Pacemaker cluster properties, showing the default values of the properties and the possible values you can set for those properties.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThere are additional cluster properties that determine fencing behavior. For information about these properties, see the table of cluster properties that determine fencing behavior in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters#ref_general-fence-device-properties-configuring-fencing\">General properties of fencing devices\u003C/a>.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tIn addition to the properties described in this table, there are additional cluster properties that are exposed by the cluster software. For these properties, it is recommended that you not change their values from their defaults.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Crh-table id=\"tb-clusterprops-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 22.1. Cluster Properties\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 29%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 29%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 43%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686137714352\" scope=\"col\">Option\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686137713264\" scope=\"col\">Default\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686137712176\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">batch-limit\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\t0\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe number of resource actions that the cluster is allowed to execute in parallel. The \"correct\" value will depend on the speed and load of your network and cluster nodes. The default value of 0 means that the cluster will dynamically impose a limit when any node has a high CPU load.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">migration-limit\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\t-1 (unlimited)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe number of migration jobs that the cluster is allowed to execute in parallel on a node.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">no-quorum-policy\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\tstop\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tWhat to do when the cluster does not have quorum. Allowed values:\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* ignore - continue all resource management\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* freeze - continue resource management, but do not recover resources from nodes not in the affected partition\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* stop - stop all resources in the affected cluster partition\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* suicide - fence all nodes in the affected cluster partition\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* demote - if a cluster partition loses quorum, demote any promoted resources and stop all other resources\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">symmetric-cluster\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\ttrue\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tIndicates whether resources can run on any node by default.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">cluster-delay\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\t60s\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tRound trip delay over the network (excluding action execution). The \"correct\" value will depend on the speed and load of your network and cluster nodes.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">dc-deadtime\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\t20s\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tHow long to wait for a response from other nodes during startup. The \"correct\" value will depend on the speed and load of your network and the type of switches used.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">stop-orphan-resources\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\ttrue\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tIndicates whether deleted resources should be stopped.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">stop-orphan-actions\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\ttrue\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tIndicates whether deleted actions should be canceled.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">start-failure-is-fatal\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\ttrue\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tIndicates whether a failure to start a resource on a particular node prevents further start attempts on that node. When set to \u003Ccode class=\"literal\">false\u003C/code>, the cluster will decide whether to try starting on the same node again based on the resource’s current failure count and migration threshold. For information about setting the \u003Ccode class=\"literal\">migration-threshold\u003C/code> option for a resource, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters#proc_configuring-resource-meta-options-configuring-cluster-resources\">Configuring resource meta options\u003C/a>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tSetting \u003Ccode class=\"literal\">start-failure-is-fatal\u003C/code> to \u003Ccode class=\"literal\">false\u003C/code> incurs the risk that this will allow one faulty node that is unable to start a resource to hold up all dependent actions. This is why \u003Ccode class=\"literal\">start-failure-is-fatal\u003C/code> defaults to true. The risk of setting \u003Ccode class=\"literal\">start-failure-is-fatal=false\u003C/code> can be mitigated by setting a low migration threshold so that other actions can proceed after that many failures.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pe-error-series-max\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\t-1 (all)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe number of scheduler inputs resulting in ERRORs to save. Used when reporting problems.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pe-warn-series-max\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\t-1 (all)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe number of scheduler inputs resulting in WARNINGs to save. Used when reporting problems.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">pe-input-series-max\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\t-1 (all)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe number of \"normal\" scheduler inputs to save. Used when reporting problems.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">cluster-infrastructure\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe messaging stack on which Pacemaker is currently running. Used for informational and diagnostic purposes; not user-configurable.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">dc-version\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tVersion of Pacemaker on the cluster’s Designated Controller (DC). Used for diagnostic purposes; not user-configurable.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">cluster-recheck-interval\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\t15 minutes\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tPacemaker is primarily event-driven, and looks ahead to know when to recheck the cluster for failure timeouts and most time-based rules. Pacemaker will also recheck the cluster after the duration of inactivity specified by this property. This cluster recheck has two purposes: rules with \u003Ccode class=\"literal\">date-spec\u003C/code> are guaranteed to be checked this often, and it serves as a fail-safe for some kinds of scheduler bugs. A value of 0 disables this polling; positive values indicate a time interval.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">maintenance-mode\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\tfalse\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tMaintenance Mode tells the cluster to go to a \"hands off\" mode, and not start or stop any services until told otherwise. When maintenance mode is completed, the cluster does a sanity check of the current state of any services, and then stops or starts any that need it.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">shutdown-escalation\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\t20min\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe time after which to give up trying to shut down gracefully and just exit. Advanced use only.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">stop-all-resources\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\tfalse\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tShould the cluster stop all resources.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">enable-acl\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\tfalse\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tIndicates whether the cluster can use access control lists, as set with the \u003Ccode class=\"literal command\">pcs acl\u003C/code> command.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">placement-strategy\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\tdefault\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tIndicates whether and how the cluster will take utilization attributes into account when determining resource placement on cluster nodes.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137714352\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">node-health-strategy\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137713264\"> \u003Cp>\n\t\t\t\t\t\t\t\tnone\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137712176\"> \u003Cp>\n\t\t\t\t\t\t\t\tWhen used in conjunction with a health resource agent, controls how Pacemaker responds to changes in node health. Allowed values:\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">none\u003C/code> - Do not track node health.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">migrate-on-red\u003C/code> - Resources are moved off any node where a health agent has determined that the node’s status is \u003Ccode class=\"literal\">red\u003C/code>, based on the local conditions that the agent monitors.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">only-green\u003C/code> - Resources are moved off any node where a health agent has determined that the node’s status is \u003Ccode class=\"literal\">yellow\u003C/code> or \u003Ccode class=\"literal\">red\u003C/code>, based on the local conditions that the agent monitors.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\t* \u003Ccode class=\"literal\">progressive\u003C/code>, \u003Ccode class=\"literal\">custom\u003C/code> - Advanced node health strategies that offer finer-grained control over the cluster’s response to health conditions according to the internal numeric values of health attributes.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003C/section>\u003Csection class=\"section\" id=\"setting-cluster-properties-controlling-cluster-behavior\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">22.2. Setting and removing cluster properties\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tTo set the value of a cluster property, use the following \u003Cspan class=\"strong strong\">\u003Cstrong>\u003Cspan class=\"application application\">pcs\u003C/span>\u003C/strong>\u003C/span> command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs property set \u003Cspan class=\"emphasis\">\u003Cem>property\u003C/em>\u003C/span>=\u003Cspan class=\"emphasis\">\u003Cem>value\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tFor example, to set the value of \u003Ccode class=\"literal\">symmetric-cluster\u003C/code> to \u003Ccode class=\"literal\">false\u003C/code>, use the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set symmetric-cluster=false\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tYou can remove a cluster property from the configuration with the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs property unset \u003Cspan class=\"emphasis\">\u003Cem>property\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tAlternately, you can remove a cluster property from a configuration by leaving the value field of the \u003Ccode class=\"literal command\">pcs property set\u003C/code> command blank. This restores that property to its default value. For example, if you have previously set the \u003Ccode class=\"literal\">symmetric-cluster\u003C/code> property to \u003Ccode class=\"literal\">false\u003C/code>, the following command removes the value you have set from the configuration and restores the value of \u003Ccode class=\"literal\">symmetric-cluster\u003C/code> to \u003Ccode class=\"literal\">true\u003C/code>, which is its default value.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set symmetic-cluster=\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_querying-cluster-property-settings-controlling-cluster-behavior\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">22.3. Querying cluster property settings\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIn most cases, when you use the \u003Ccode class=\"literal command\">pcs\u003C/code> command to display values of the various cluster components, you can use \u003Ccode class=\"literal command\">pcs list\u003C/code> or \u003Ccode class=\"literal command\">pcs show\u003C/code> interchangeably. In the following examples, \u003Ccode class=\"literal command\">pcs list\u003C/code> is the format used to display an entire list of all settings for more than one property, while \u003Ccode class=\"literal command\">pcs show\u003C/code> is the format used to display the values of a specific property.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tTo display the values of the property settings that have been set for the cluster, use the following \u003Cspan class=\"strong strong\">\u003Cstrong>\u003Cspan class=\"application application\">pcs\u003C/span>\u003C/strong>\u003C/span> command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs property list\u003C/pre>\u003Cp>\n\t\t\t\tTo display all of the values of the property settings for the cluster, including the default values of the property settings that have not been explicitly set, use the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs property list --all\u003C/pre>\u003Cp>\n\t\t\t\tTo display the current value of a specific cluster property, use the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs property show \u003Cspan class=\"emphasis\">\u003Cem>property\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tFor example, to display the current value of the \u003Ccode class=\"literal\">cluster-infrastructure\u003C/code> property, execute the following command:\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property show cluster-infrastructure\u003C/strong>\u003C/span>\nCluster Properties:\n cluster-infrastructure: cman\u003C/pre>\u003Cp>\n\t\t\t\tFor informational purposes, you can display a list of all of the default values for the properties, whether they have been set to a value other than the default or not, by using the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs property [list|show] --defaults\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_exporting-properties-controlling-cluster-behavior\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">22.4. Exporting cluster properties as \u003Ccode class=\"literal\">pcs\u003C/code> commands\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tAs of Red Hat Enterprise Linux 9.3, you can display the \u003Ccode class=\"literal\">pcs\u003C/code> commands that can be used to re-create configured cluster properties on a different system using the \u003Ccode class=\"literal\">--output-format=cmd\u003C/code> option of the \u003Ccode class=\"literal\">pcs property config\u003C/code> command.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command sets the \u003Ccode class=\"literal\">migration-limit\u003C/code> cluster property to 10.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set migration-limit=10\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tAfter you set the cluster property, the following command displays the \u003Ccode class=\"literal\">pcs\u003C/code> command you can use to set the cluster property on a different system.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property config --output-format=cmd\u003C/strong>\u003C/span>\npcs property set --force -- \\\n migration-limit=10 \\\n placement-strategy=minimal\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-resources-to-remain-stopped-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 23. Configuring resources to remain stopped on clean node shutdown\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tWhen a cluster node shuts down, Pacemaker’s default response is to stop all resources running on that node and recover them elsewhere, even if the shutdown is a clean shutdown. You can configure Pacemaker so that when a node shuts down cleanly, the resources attached to the node will be locked to the node and unable to start elsewhere until they start again when the node that has shut down rejoins the cluster. This allows you to power down nodes during maintenance windows when service outages are acceptable without causing that node’s resources to fail over to other nodes in the cluster.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"ref_cluster-properties-shutdown-lock-configuring-resources-to-remain-stopped\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">23.1. Cluster properties to configure resources to remain stopped on clean node shutdown\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe ability to prevent resources from failing over on a clean node shutdown is implemented by means of the following cluster properties.\n\t\t\t\u003C/p>\u003Cdiv class=\"variablelist\">\u003Cdl class=\"variablelist\">\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">shutdown-lock\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tWhen this cluster property is set to the default value of \u003Ccode class=\"literal\">false\u003C/code>, the cluster will recover resources that are active on nodes being cleanly shut down. When this property is set to \u003Ccode class=\"literal\">true\u003C/code>, resources that are active on the nodes being cleanly shut down are unable to start elsewhere until they start on the node again after it rejoins the cluster.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tThe \u003Ccode class=\"literal\">shutdown-lock\u003C/code> property will work for either cluster nodes or remote nodes, but not guest nodes.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tIf \u003Ccode class=\"literal\">shutdown-lock\u003C/code> is set to \u003Ccode class=\"literal\">true\u003C/code>, you can remove the lock on one cluster resource when a node is down so that the resource can start elsewhere by performing a manual refresh on the node with the following command.\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\u003Ccode class=\"literal command\">pcs resource refresh \u003Cspan class=\"emphasis\">\u003Cem>resource\u003C/em>\u003C/span> node=\u003Cspan class=\"emphasis\">\u003Cem>nodename\u003C/em>\u003C/span>\u003C/code>\n\t\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tNote that once the resources are unlocked, the cluster is free to move the resources elsewhere. You can control the likelihood of this occurring by using stickiness values or location preferences for the resource.\n\t\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\tA manual refresh will work with remote nodes only if you first run the following commands:\n\t\t\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\tRun the \u003Ccode class=\"literal command\">systemctl stop pacemaker_remote\u003C/code> command on the remote node to stop the node.\n\t\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\tRun the \u003Ccode class=\"literal command\">pcs resource disable \u003Cspan class=\"emphasis\">\u003Cem>remote-connection-resource\u003C/em>\u003C/span>\u003C/code> command.\n\t\t\t\t\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\t\t\t\t\tYou can then perform a manual refresh on the remote node.\n\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/dd>\u003Cdt>\u003Cspan class=\"term\">\u003Ccode class=\"literal\">shutdown-lock-limit\u003C/code>\u003C/span>\u003C/dt>\u003Cdd>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tWhen this cluster property is set to a time other than the default value of 0, resources will be available for recovery on other nodes if the node does not rejoin within the specified time since the shutdown was initiated.\n\t\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\tThe \u003Ccode class=\"literal\">shutdown-lock-limit\u003C/code> property will work with remote nodes only if you first run the following commands:\n\t\t\t\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\tRun the \u003Ccode class=\"literal command\">systemctl stop pacemaker_remote\u003C/code> command on the remote node to stop the node.\n\t\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\tRun the \u003Ccode class=\"literal command\">pcs resource disable \u003Cspan class=\"emphasis\">\u003Cem>remote-connection-resource\u003C/em>\u003C/span>\u003C/code> command.\n\t\t\t\t\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\t\t\t\t\tAfter you run these commands, the resources that had been running on the remote node will be available for recovery on other nodes when the amount of time specified as the \u003Ccode class=\"literal\">shutdown-lock-limit\u003C/code> has passed.\n\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/dd>\u003C/dl>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_setting-shutdown-lock-configuring-resources-to-remain-stopped\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">23.2. Setting the shutdown-lock cluster property\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe following example sets the \u003Ccode class=\"literal\">shutdown-lock\u003C/code> cluster property to \u003Ccode class=\"literal\">true\u003C/code> in an example cluster and shows the effect this has when the node is shut down and started again. This example cluster consists of three nodes: \u003Ccode class=\"literal\">z1.example.com\u003C/code>, \u003Ccode class=\"literal\">z2.example.com\u003C/code>, and \u003Ccode class=\"literal\">z3.example.com\u003C/code>.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSet the \u003Ccode class=\"literal\">shutdown-lock\u003C/code> property to to \u003Ccode class=\"literal\">true\u003C/code> and verify its value. In this example the \u003Ccode class=\"literal\">shutdown-lock-limit\u003C/code> property maintains its default value of 0.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z3 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set shutdown-lock=true\u003C/strong>\u003C/span>\n[root@z3 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property list --all | grep shutdown-lock\u003C/strong>\u003C/span>\n shutdown-lock: true\n shutdown-lock-limit: 0\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCheck the status of the cluster. In this example, resources \u003Ccode class=\"literal\">third\u003C/code> and \u003Ccode class=\"literal\">fifth\u003C/code> are running on \u003Ccode class=\"literal\">z1.example.com\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z3 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\n...\nFull List of Resources:\n...\n * first\t(ocf::pacemaker:Dummy):\tStarted z3.example.com\n * second\t(ocf::pacemaker:Dummy):\tStarted z2.example.com\n * third\t(ocf::pacemaker:Dummy):\tStarted z1.example.com\n * fourth\t(ocf::pacemaker:Dummy):\tStarted z2.example.com\n * fifth\t(ocf::pacemaker:Dummy):\tStarted z1.example.com\n...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tShut down \u003Ccode class=\"literal\">z1.example.com\u003C/code>, which will stop the resources that are running on that node.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z3 ~] \u003Cspan class=\"strong strong\">\u003Cstrong># pcs cluster stop z1.example.com\u003C/strong>\u003C/span>\nStopping Cluster (pacemaker)...\nStopping Cluster (corosync)...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tRunning the \u003Ccode class=\"literal\">pcs status\u003C/code> command shows that node \u003Ccode class=\"literal\">z1.example.com\u003C/code> is offline and that the resources that had been running on \u003Ccode class=\"literal\">z1.example.com\u003C/code> are \u003Ccode class=\"literal\">LOCKED\u003C/code> while the node is down.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z3 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\n...\n\nNode List:\n * Online: [ z2.example.com z3.example.com ]\n * OFFLINE: [ z1.example.com ]\n\nFull List of Resources:\n...\n * first\t(ocf::pacemaker:Dummy):\tStarted z3.example.com\n * second\t(ocf::pacemaker:Dummy):\tStarted z2.example.com\n * third\t(ocf::pacemaker:Dummy):\tStopped z1.example.com (LOCKED)\n * fourth\t(ocf::pacemaker:Dummy):\tStarted z3.example.com\n * fifth\t(ocf::pacemaker:Dummy):\tStopped z1.example.com (LOCKED)\n\n...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tStart cluster services again on \u003Ccode class=\"literal\">z1.example.com\u003C/code> so that it rejoins the cluster. Locked resources should get started on that node, although once they start they will not not necessarily remain on the same node.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z3 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster start z1.example.com\u003C/strong>\u003C/span>\nStarting Cluster...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIn this example, resouces \u003Ccode class=\"literal\">third\u003C/code> and \u003Ccode class=\"literal\">fifth\u003C/code> are recovered on node \u003Ccode class=\"literal\">z1.example.com\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z3 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs status\u003C/strong>\u003C/span>\n...\n\nNode List:\n * Online: [ z1.example.com z2.example.com z3.example.com ]\n\nFull List of Resources:\n..\n * first\t(ocf::pacemaker:Dummy):\tStarted z3.example.com\n * second\t(ocf::pacemaker:Dummy):\tStarted z2.example.com\n * third\t(ocf::pacemaker:Dummy):\tStarted z1.example.com\n * fourth\t(ocf::pacemaker:Dummy):\tStarted z3.example.com\n * fifth\t(ocf::pacemaker:Dummy):\tStarted z1.example.com\n\n...\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 24. Configuring a node placement strategy\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tPacemaker decides where to place a resource according to the resource allocation scores on every node. The resource will be allocated to the node where the resource has the highest score. This allocation score is derived from a combination of factors, including resource constraints, \u003Ccode class=\"literal\">resource-stickiness\u003C/code> settings, prior failure history of a resource on each node, and utilization of each node.\n\t\t\u003C/p>\u003Cp>\n\t\t\tIf the resource allocation scores on all the nodes are equal, by the default placement strategy Pacemaker will choose a node with the least number of allocated resources for balancing the load. If the number of resources on each node is equal, the first eligible node listed in the CIB will be chosen to run the resource.\n\t\t\u003C/p>\u003Cp>\n\t\t\tOften, however, different resources use significantly different proportions of a node’s capacities (such as memory or I/O). You cannot always balance the load ideally by taking into account only the number of resources allocated to a node. In addition, if resources are placed such that their combined requirements exceed the provided capacity, they may fail to start completely or they may run with degraded performance. To take these factors into account, Pacemaker allows you to configure the following components:\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\tthe capacity a particular node provides\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tthe capacity a particular resource requires\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tan overall strategy for placement of resources\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Csection class=\"section\" id=\"configuring-utilization-attributes-configuring-node-placement-strategy\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">24.1. Utilization attributes and placement strategy\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tTo configure the capacity that a node provides or a resource requires, you can use \u003Cspan class=\"emphasis\">\u003Cem>utilization attributes\u003C/em>\u003C/span> for nodes and resources. You do this by setting a utilization variable for a resource and assigning a value to that variable to indicate what the resource requires, and then setting that same utilization variable for a node and assigning a value to that variable to indicate what that node provides.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can name utilization attributes according to your preferences and define as many name and value pairs as your configuration needs. The values of utilization attributes must be integers.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"configuring_node_and_resource_capacity\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">24.1.1. Configuring node and resource capacity\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following example configures a utilization attribute of CPU capacity for two nodes, setting this attribute as the variable \u003Ccode class=\"literal\">cpu\u003C/code>. It also configures a utilization attribute of RAM capacity, setting this attribute as the variable \u003Ccode class=\"literal\">memory\u003C/code>. In this example:\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tNode 1 is defined as providing a CPU capacity of two and a RAM capacity of 2048\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tNode 2 is defined as providing a CPU capacity of four and a RAM capacity of 2048\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node utilization node1 cpu=2 memory=2048\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs node utilization node2 cpu=4 memory=2048\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tThe following example specifies the same utilization attributes that three different resources require. In this example:\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tresource \u003Ccode class=\"literal\">dummy-small\u003C/code> requires a CPU capacity of 1 and a RAM capacity of 1024\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tresource \u003Ccode class=\"literal\">dummy-medium\u003C/code> requires a CPU capacity of 2 and a RAM capacity of 2048\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tresource \u003Ccode class=\"literal\">dummy-large\u003C/code> requires a CPU capacity of 1 and a RAM capacity of 3072\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource utilization dummy-small cpu=1 memory=1024\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource utilization dummy-medium cpu=2 memory=2048\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource utilization dummy-large cpu=3 memory=3072\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\t\tA node is considered eligible for a resource if it has sufficient free capacity to satisfy the resource’s requirements, as defined by the utilization attributes.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"configuring_placement_strategy\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">24.1.2. Configuring placement strategy\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tAfter you have configured the capacities your nodes provide and the capacities your resources require, you need to set the \u003Ccode class=\"literal\">placement-strategy\u003C/code> cluster property, otherwise the capacity configurations have no effect.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tFour values are available for the \u003Ccode class=\"literal\">placement-strategy\u003C/code> cluster property:\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">default\u003C/code> — Utilization values are not taken into account at all. Resources are allocated according to allocation scores. If scores are equal, resources are evenly distributed across nodes.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">utilization\u003C/code> — Utilization values are taken into account only when deciding whether a node is considered eligible (that is, whether it has sufficient free capacity to satisfy the resource’s requirements). Load-balancing is still done based on the number of resources allocated to a node.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">balanced\u003C/code> — Utilization values are taken into account when deciding whether a node is eligible to serve a resource and when load-balancing, so an attempt is made to spread the resources in a way that optimizes resource performance.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">minimal\u003C/code> — Utilization values are taken into account only when deciding whether a node is eligible to serve a resource. For load-balancing, an attempt is made to concentrate the resources on as few nodes as possible, thereby enabling possible power savings on the remaining nodes.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following example command sets the value of \u003Ccode class=\"literal\">placement-strategy\u003C/code> to \u003Ccode class=\"literal\">balanced\u003C/code>. After running this command, Pacemaker will ensure the load from your resources will be distributed evenly throughout the cluster, without the need for complicated sets of colocation constraints.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set placement-strategy=balanced\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"pacemaker-resource-allocation-configuring-node-placement-strategy\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">24.2. Pacemaker resource allocation\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tPacemaker allocates resources according to node preference, node capacity, and resource allocation preference.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"node_preference\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">24.2.1. Node preference\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tPacemaker determines which node is preferred when allocating resources according to the following strategy.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tThe node with the highest node weight gets consumed first. Node weight is a score maintained by the cluster to represent node health.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tIf multiple nodes have the same node weight:\n\t\t\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"circle\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\t\tIf the \u003Ccode class=\"literal\">placement-strategy\u003C/code> cluster property is \u003Ccode class=\"literal\">default\u003C/code> or \u003Ccode class=\"literal\">utilization\u003C/code>:\n\t\t\t\t\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"square\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\t\tThe node that has the least number of allocated resources gets consumed first.\n\t\t\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\t\tIf the numbers of allocated resources are equal, the first eligible node listed in the CIB gets consumed first.\n\t\t\t\t\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\t\tIf the \u003Ccode class=\"literal\">placement-strategy\u003C/code> cluster property is \u003Ccode class=\"literal\">balanced\u003C/code>:\n\t\t\t\t\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"square\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\t\tThe node that has the most free capacity gets consumed first.\n\t\t\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\t\tIf the free capacities of the nodes are equal, the node that has the least number of allocated resources gets consumed first.\n\t\t\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\t\t\tIf the free capacities of the nodes are equal and the number of allocated resources is equal, the first eligible node listed in the CIB gets consumed first.\n\t\t\t\t\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\tIf the \u003Ccode class=\"literal\">placement-strategy\u003C/code> cluster property is \u003Ccode class=\"literal\">minimal\u003C/code>, the first eligible node listed in the CIB gets consumed first.\n\t\t\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"node_capacity\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">24.2.2. Node capacity\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tPacemaker determines which node has the most free capacity according to the following strategy.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tIf only one type of utilization attribute has been defined, free capacity is a simple numeric comparison.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tIf multiple types of utilization attributes have been defined, then the node that is numerically highest in the most attribute types has the most free capacity. For example:\n\t\t\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"circle\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\tIf NodeA has more free CPUs, and NodeB has more free memory, then their free capacities are equal.\n\t\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\tIf NodeA has more free CPUs, while NodeB has more free memory and storage, then NodeB has more free capacity.\n\t\t\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"resource_allocation_preference\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">24.2.3. Resource allocation preference\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tPacemaker determines which resource is allocated first according to the following strategy.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tThe resource that has the highest priority gets allocated first. You can set a resource’s priority when you create the resource.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tIf the priorities of the resources are equal, the resource that has the highest score on the node where it is running gets allocated first, to prevent resource shuffling.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tIf the resource scores on the nodes where the resources are running are equal or the resources are not running, the resource that has the highest score on the preferred node gets allocated first. If the resource scores on the preferred node are equal in this case, the first runnable resource listed in the CIB gets allocated first.\n\t\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"resource-placement-strategy-guidelines-configuring-node-placement-strategy\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">24.3. Resource placement strategy guidelines\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tTo ensure that Pacemaker’s placement strategy for resources works most effectively, you should take the following considerations into account when configuring your system.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tMake sure that you have sufficient physical capacity.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf the physical capacity of your nodes is being used to near maximum under normal conditions, then problems could occur during failover. Even without the utilization feature, you may start to experience timeouts and secondary failures.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tBuild some buffer into the capabilities you configure for the nodes.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAdvertise slightly more node resources than you physically have, on the assumption the that a Pacemaker resource will not use 100% of the configured amount of CPU, memory, and so forth all the time. This practice is sometimes called overcommit.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSpecify resource priorities.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf the cluster is going to sacrifice services, it should be the ones you care about least. Ensure that resource priorities are properly set so that your most important resources are scheduled first.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"node-utilization-resource-agent-configuring-node-placement-strategy\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">24.4. The NodeUtilization resource agent\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe \u003Ccode class=\"literal\">NodeUtilization\u003C/code> resoure agent can detect the system parameters of available CPU, host memory availability, and hypervisor memory availability and add these parameters into the CIB. You can run the agent as a clone resource to have it automatically populate these parameters on each node.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFor information about the \u003Ccode class=\"literal\">NodeUtilization\u003C/code> resource agent and the resource options for this agent, run the \u003Ccode class=\"literal command\">pcs resource describe NodeUtilization\u003C/code> command.\n\t\t\t\u003C/p>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 25. Configuring a virtual domain as a resource\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tYou can configure a virtual domain that is managed by the \u003Ccode class=\"literal\">libvirt\u003C/code> virtualization framework as a cluster resource with the \u003Ccode class=\"literal command\">pcs resource create\u003C/code> command, specifying \u003Ccode class=\"literal\">VirtualDomain\u003C/code> as the resource type.\n\t\t\u003C/p>\u003Cp>\n\t\t\tWhen configuring a virtual domain as a resource, take the following considerations into account:\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\tA virtual domain should be stopped before you configure it as a cluster resource.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tOnce a virtual domain is a cluster resource, it should not be started, stopped, or migrated except through the cluster tools.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tDo not configure a virtual domain that you have configured as a cluster resource to start when its host boots.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tAll nodes allowed to run a virtual domain must have access to the necessary configuration files and storage devices for that virtual domain.\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\tIf you want the cluster to manage services within the virtual domain itself, you can configure the virtual domain as a guest node.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"ref_virtual-domain-resource-options-configuring-virtual-domain-as-a-resource\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">25.1. Virtual domain resource options\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe following table describes the resource options you can configure for a \u003Ccode class=\"literal\">VirtualDomain\u003C/code> resource.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-virtdomain-options-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 25.1. Resource Options for Virtual Domain Resources\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 38%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 25%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 38%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686139550576\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686139549488\" scope=\"col\">Default\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686139548400\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139550576\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">config\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139549488\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139548400\"> \u003Cp>\n\t\t\t\t\t\t\t\t(required) Absolute path to the \u003Ccode class=\"literal\">libvirt\u003C/code> configuration file for this virtual domain.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139550576\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">hypervisor\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139549488\"> \u003Cp>\n\t\t\t\t\t\t\t\tSystem dependent\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139548400\"> \u003Cp>\n\t\t\t\t\t\t\t\tHypervisor URI to connect to. You can determine the system’s default URI by running the \u003Ccode class=\"literal command\">virsh --quiet uri\u003C/code> command.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139550576\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">force_stop\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139549488\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">0\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139548400\"> \u003Cp>\n\t\t\t\t\t\t\t\tAlways forcefully shut down (\"destroy\") the domain on stop. The default behavior is to resort to a forceful shutdown only after a graceful shutdown attempt has failed. You should set this to \u003Ccode class=\"literal\">true\u003C/code> only if your virtual domain (or your virtualization back end) does not support graceful shutdown.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139550576\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">migration_transport\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139549488\"> \u003Cp>\n\t\t\t\t\t\t\t\tSystem dependent\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139548400\"> \u003Cp>\n\t\t\t\t\t\t\t\tTransport used to connect to the remote hypervisor while migrating. If this parameter is omitted, the resource will use \u003Ccode class=\"literal\">libvirt\u003C/code>'s default transport to connect to the remote hypervisor.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139550576\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">migration_network_suffix\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139549488\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139548400\"> \u003Cp>\n\t\t\t\t\t\t\t\tUse a dedicated migration network. The migration URI is composed by adding this parameter’s value to the end of the node name. If the node name is a fully qualified domain name (FQDN), insert the suffix immediately prior to the first period (.) in the FQDN. Ensure that this composed host name is locally resolvable and the associated IP address is reachable through the favored network.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139550576\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">monitor_scripts\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139549488\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139548400\"> \u003Cp>\n\t\t\t\t\t\t\t\tTo additionally monitor services within the virtual domain, add this parameter with a list of scripts to monitor. \u003Cspan class=\"emphasis\">\u003Cem>Note\u003C/em>\u003C/span>: When monitor scripts are used, the \u003Ccode class=\"literal\">start\u003C/code> and \u003Ccode class=\"literal\">migrate_from\u003C/code> operations will complete only when all monitor scripts have completed successfully. Be sure to set the timeout of these operations to accommodate this delay\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139550576\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">autoset_utilization_cpu\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139549488\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">true\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139548400\"> \u003Cp>\n\t\t\t\t\t\t\t\tIf set to \u003Ccode class=\"literal\">true\u003C/code>, the agent will detect the number of \u003Ccode class=\"literal\">domainU\u003C/code>'s \u003Ccode class=\"literal\">vCPU\u003C/code>s from \u003Ccode class=\"literal\">virsh\u003C/code>, and put it into the CPU utilization of the resource when the monitor is executed.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139550576\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">autoset_utilization_hv_memory\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139549488\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">true\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139548400\"> \u003Cp>\n\t\t\t\t\t\t\t\tIf set it true, the agent will detect the number of \u003Ccode class=\"literal\">Max memory\u003C/code> from \u003Ccode class=\"literal\">virsh\u003C/code>, and put it into the \u003Ccode class=\"literal\">hv_memory\u003C/code> utilization of the source when the monitor is executed.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139550576\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">migrateport\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139549488\"> \u003Cp>\n\t\t\t\t\t\t\t\trandom highport\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139548400\"> \u003Cp>\n\t\t\t\t\t\t\t\tThis port will be used in the \u003Ccode class=\"literal\">qemu\u003C/code> migrate URI. If unset, the port will be a random highport.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139550576\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">snapshot\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139549488\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139548400\"> \u003Cp>\n\t\t\t\t\t\t\t\tPath to the snapshot directory where the virtual machine image will be stored. When this parameter is set, the virtual machine’s RAM state will be saved to a file in the snapshot directory when stopped. If on start a state file is present for the domain, the domain will be restored to the same state it was in right before it stopped last. This option is incompatible with the \u003Ccode class=\"literal\">force_stop\u003C/code> option.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tIn addition to the \u003Ccode class=\"literal\">VirtualDomain\u003C/code> resource options, you can configure the \u003Ccode class=\"literal\">allow-migrate\u003C/code> metadata option to allow live migration of the resource to another node. When this option is set to \u003Ccode class=\"literal\">true\u003C/code>, the resource can be migrated without loss of state. When this option is set to \u003Ccode class=\"literal\">false\u003C/code>, which is the default state, the virtual domain will be shut down on the first node and then restarted on the second node when it is moved from one node to the other.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_creating-virtual-domain-resource-configuring-virtual-domain-as-a-resource\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">25.2. Creating the virtual domain resource\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe following procedure creates a \u003Ccode class=\"literal\">VirtualDomain\u003C/code> resource in a cluster for a virtual machine you have previously created.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tTo create the \u003Ccode class=\"literal\">VirtualDomain\u003C/code> resource agent for the management of the virtual machine, Pacemaker requires the virtual machine’s \u003Ccode class=\"literal\">xml\u003C/code> configuration file to be dumped to a file on disk. For example, if you created a virtual machine named \u003Ccode class=\"literal\">guest1\u003C/code>, dump the \u003Ccode class=\"literal\">xml\u003C/code> file to a file somewhere on one of the cluster nodes that will be allowed to run the guest. You can use a file name of your choosing; this example uses \u003Ccode class=\"literal\">/etc/pacemaker/guest1.xml\u003C/code>.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>virsh dumpxml guest1 &gt; /etc/pacemaker/guest1.xml\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tCopy the virtual machine’s \u003Ccode class=\"literal\">xml\u003C/code> configuration file to all of the other cluster nodes that will be allowed to run the guest, in the same location on each node.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tEnsure that all of the nodes allowed to run the virtual domain have access to the necessary storage devices for that virtual domain.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tSeparately test that the virtual domain can start and stop on each node that will run the virtual domain.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tIf it is running, shut down the guest node. Pacemaker will start the node when it is configured in the cluster. The virtual machine should not be configured to start automatically when the host boots.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tConfigure the \u003Ccode class=\"literal\">VirtualDomain\u003C/code> resource with the \u003Ccode class=\"literal command\">pcs resource create\u003C/code> command. For example, the following command configures a \u003Ccode class=\"literal\">VirtualDomain\u003C/code> resource named \u003Ccode class=\"literal\">VM\u003C/code>. Since the \u003Ccode class=\"literal\">allow-migrate\u003C/code> option is set to \u003Ccode class=\"literal\">true\u003C/code> a \u003Ccode class=\"literal\">pcs resource move VM \u003Cspan class=\"emphasis\">\u003Cem>nodeX\u003C/em>\u003C/span>\u003C/code> command would be done as a live migration.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIn this example \u003Ccode class=\"literal\">migration_transport\u003C/code> is set to \u003Ccode class=\"literal\">ssh\u003C/code>. Note that for SSH migration to work properly, keyless logging must work between nodes.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create VM VirtualDomain config=/etc/pacemaker/guest1.xml migration_transport=ssh meta allow-migrate=true\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 26. Configuring cluster quorum\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tA Red Hat Enterprise Linux High Availability Add-On cluster uses the \u003Ccode class=\"literal\">votequorum\u003C/code> service, in conjunction with fencing, to avoid split brain situations. A number of votes is assigned to each system in the cluster, and cluster operations are allowed to proceed only when a majority of votes is present. The service must be loaded into all nodes or none; if it is loaded into a subset of cluster nodes, the results will be unpredictable. For information about the configuration and operation of the \u003Ccode class=\"literal\">votequorum\u003C/code> service, see the \u003Ccode class=\"literal command\">votequorum\u003C/code>(5) man page.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"ref_quorum-options-configuring-cluster-quorum\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">26.1. Configuring quorum options\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThere are some special features of quorum configuration that you can set when you create a cluster with the \u003Ccode class=\"literal command\">pcs cluster setup\u003C/code> command. The following table summarizes these options.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-quorumoptions-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 26.1. Quorum Options\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 44%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 56%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686137345104\" scope=\"col\">Option\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686137344016\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137345104\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">auto_tie_breaker\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137344016\"> \u003Cp>\n\t\t\t\t\t\t\t\tWhen enabled, the cluster can suffer up to 50% of the nodes failing at the same time, in a deterministic fashion. The cluster partition, or the set of nodes that are still in contact with the \u003Ccode class=\"literal\">nodeid\u003C/code> configured in \u003Ccode class=\"literal\">auto_tie_breaker_node\u003C/code> (or lowest \u003Ccode class=\"literal\">nodeid\u003C/code> if not set), will remain quorate. The other nodes will be inquorate.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tThe \u003Ccode class=\"literal option\">auto_tie_breaker\u003C/code> option is principally used for clusters with an even number of nodes, as it allows the cluster to continue operation with an even split. For more complex failures, such as multiple, uneven splits, it is recommended that you use a quorum device.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tThe \u003Ccode class=\"literal option\">auto_tie_breaker\u003C/code> option is incompatible with quorum devices.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137345104\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">wait_for_all\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137344016\"> \u003Cp>\n\t\t\t\t\t\t\t\tWhen enabled, the cluster will be quorate for the first time only after all nodes have been visible at least once at the same time.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tThe \u003Ccode class=\"literal option\">wait_for_all\u003C/code> option is primarily used for two-node clusters and for even-node clusters using the quorum device \u003Ccode class=\"literal\">lms\u003C/code> (last man standing) algorithm.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t\tThe \u003Ccode class=\"literal option\">wait_for_all\u003C/code> option is automatically enabled when a cluster has two nodes, does not use a quorum device, and \u003Ccode class=\"literal option\">auto_tie_breaker\u003C/code> is disabled. You can override this by explicitly setting \u003Ccode class=\"literal option\">wait_for_all\u003C/code> to 0.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137345104\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">last_man_standing\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137344016\"> \u003Cp>\n\t\t\t\t\t\t\t\tWhen enabled, the cluster can dynamically recalculate \u003Ccode class=\"literal\">expected_votes\u003C/code> and quorum under specific circumstances. You must enable \u003Ccode class=\"literal\">wait_for_all\u003C/code> when you enable this option. The \u003Ccode class=\"literal\">last_man_standing\u003C/code> option is incompatible with quorum devices.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137345104\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">last_man_standing_window\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137344016\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe time, in milliseconds, to wait before recalculating \u003Ccode class=\"literal\">expected_votes\u003C/code> and quorum after a cluster loses nodes.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tFor further information about configuring and using these options, see the \u003Ccode class=\"literal command\">votequorum\u003C/code>(5) man page.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_modifying-quorum-options-configuring-cluster-quorum\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">26.2. Modifying quorum options\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can modify general quorum options for your cluster with the \u003Ccode class=\"literal command\">pcs quorum update\u003C/code> command. Executing this command requires that the cluster be stopped. For information on the quorum options, see the \u003Ccode class=\"literal command\">votequorum\u003C/code>(5) man page.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe format of the \u003Ccode class=\"literal command\">pcs quorum update\u003C/code> command is as follows.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs quorum update [auto_tie_breaker=[0|1]] [last_man_standing=[0|1]] [last_man_standing_window=[\u003Cspan class=\"emphasis\">\u003Cem>time-in-ms\u003C/em>\u003C/span>] [wait_for_all=[0|1]]\u003C/pre>\u003Cp>\n\t\t\t\tThe following series of commands modifies the \u003Ccode class=\"literal\">wait_for_all\u003C/code> quorum option and displays the updated status of the option. Note that the system does not allow you to execute this command while the cluster is running.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs quorum update wait_for_all=1\u003C/strong>\u003C/span>\nChecking corosync is not running on nodes...\nError: node1: corosync is running\nError: node2: corosync is running\n\n[root@node1:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster stop --all\u003C/strong>\u003C/span>\nnode2: Stopping Cluster (pacemaker)...\nnode1: Stopping Cluster (pacemaker)...\nnode1: Stopping Cluster (corosync)...\nnode2: Stopping Cluster (corosync)...\n\n[root@node1:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs quorum update wait_for_all=1\u003C/strong>\u003C/span>\nChecking corosync is not running on nodes...\nnode2: corosync is not running\nnode1: corosync is not running\nSending updated corosync.conf to nodes...\nnode1: Succeeded\nnode2: Succeeded\n\n[root@node1:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs quorum config\u003C/strong>\u003C/span>\nOptions:\n  wait_for_all: 1\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_displaying-quorum-configuration-status-configuring-cluster-quorum\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">26.3. Displaying quorum configuration and status\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tOnce a cluster is running, you can enter the following cluster quorum commands to display the quorum configuration and status.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command shows the quorum configuration.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs quorum [config]\u003C/pre>\u003Cp>\n\t\t\t\tThe following command shows the quorum runtime status.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs quorum status\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_running-inquorate-clusters-configuring-cluster-quorum\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">26.4. Running inquorate clusters\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf you take nodes out of a cluster for a long period of time and the loss of those nodes would cause quorum loss, you can change the value of the \u003Ccode class=\"literal\">expected_votes\u003C/code> parameter for the live cluster with the \u003Ccode class=\"literal\">pcs quorum expected-votes\u003C/code> command. This allows the cluster to continue operation when it does not have quorum.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tChanging the expected votes in a live cluster should be done with extreme caution. If less than 50% of the cluster is running because you have manually changed the expected votes, then the other nodes in the cluster could be started separately and run cluster services, causing data corruption and other unexpected results. If you change this value, you should ensure that the \u003Ccode class=\"literal\">wait_for_all\u003C/code> parameter is enabled.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tThe following command sets the expected votes in the live cluster to the specified value. This affects the live cluster only and does not change the configuration file; the value of \u003Ccode class=\"literal\">expected_votes\u003C/code> is reset to the value in the configuration file in the event of a reload.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs quorum expected-votes \u003Cspan class=\"emphasis\">\u003Cem>votes\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tIn a situation in which you know that the cluster is inquorate but you want the cluster to proceed with resource management, you can use the \u003Ccode class=\"literal command\">pcs quorum unblock\u003C/code> command to prevent the cluster from waiting for all nodes when establishing quorum.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tThis command should be used with extreme caution. Before issuing this command, it is imperative that you ensure that nodes that are not currently in the cluster are switched off and have no access to shared resources.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs quorum unblock\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-quorum-devices-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 27. Configuring quorum devices\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tYou can allow a cluster to sustain more node failures than standard quorum rules allows by configuring a separate quorum device which acts as a third-party arbitration device for the cluster. A quorum device is recommended for clusters with an even number of nodes. With two-node clusters, the use of a quorum device can better determine which node survives in a split-brain situation.\n\t\t\u003C/p>\u003Cp>\n\t\t\tYou must take the following into account when configuring a quorum device.\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\tIt is recommended that a quorum device be run on a different physical network at the same site as the cluster that uses the quorum device. Ideally, the quorum device host should be in a separate rack than the main cluster, or at least on a separate PSU and not on the same network segment as the corosync ring or rings.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tYou cannot use more than one quorum device in a cluster at the same time.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tAlthough you cannot use more than one quorum device in a cluster at the same time, a single quorum device may be used by several clusters at the same time. Each cluster using that quorum device can use different algorithms and quorum options, as those are stored on the cluster nodes themselves. For example, a single quorum device can be used by one cluster with an \u003Ccode class=\"literal\">ffsplit\u003C/code> (fifty/fifty split) algorithm and by a second cluster with an \u003Ccode class=\"literal\">lms\u003C/code> (last man standing) algorithm.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tA quorum device should not be run on an existing cluster node.\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Csection class=\"section\" id=\"proc_installing-quorum-device-packages-configuring-quorum-devices\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">27.1. Installing quorum device packages\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tConfiguring a quorum device for a cluster requires that you install the following packages:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tInstall \u003Ccode class=\"literal\">corosync-qdevice\u003C/code> on the nodes of an existing cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install corosync-qdevice\u003C/strong>\u003C/span>\n[root@node2:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install corosync-qdevice\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tInstall \u003Ccode class=\"literal\">pcs\u003C/code> and \u003Ccode class=\"literal\">corosync-qnetd\u003C/code> on the quorum device host.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@qdevice:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install pcs corosync-qnetd\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tStart the \u003Ccode class=\"literal\">pcsd\u003C/code> service and enable \u003Ccode class=\"literal\">pcsd\u003C/code> at system start on the quorum device host.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@qdevice:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl start pcsd.service\u003C/strong>\u003C/span>\n[root@qdevice:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl enable pcsd.service\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-quorum-device-configuring-quorum-devices\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">27.2. Configuring a quorum device\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tConfigure a quorum device and add it to the cluster with the following procedure.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIn this example:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe node used for a quorum device is \u003Ccode class=\"literal\">qdevice\u003C/code>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe quorum device model is \u003Ccode class=\"literal\">net\u003C/code>, which is currently the only supported model. The \u003Ccode class=\"literal\">net\u003C/code> model supports the following algorithms:\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"circle\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">ffsplit\u003C/code>: fifty-fifty split. This provides exactly one vote to the partition with the highest number of active nodes.\n\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">lms\u003C/code>: last-man-standing. If the node is the only one left in the cluster that can see the \u003Ccode class=\"literal\">qnetd\u003C/code> server, then it returns a vote.\n\t\t\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\t\tThe LMS algorithm allows the cluster to remain quorate even with only one remaining node, but it also means that the voting power of the quorum device is great since it is the same as number_of_nodes - 1. Losing connection with the quorum device means losing number_of_nodes - 1 votes, which means that only a cluster with all nodes active can remain quorate (by overvoting the quorum device); any other cluster becomes inquorate.\n\t\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tFor more detailed information about the implementation of these algorithms, see the \u003Ccode class=\"literal\">corosync-qdevice\u003C/code>(8) man page.\n\t\t\t\t\t\t\t\u003C/p>\u003C/li>\u003C/ul>\u003C/div>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe cluster nodes are \u003Ccode class=\"literal\">node1\u003C/code> and \u003Ccode class=\"literal\">node2\u003C/code>.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tOn the node that you will use to host your quorum device, configure the quorum device with the following command. This command configures and starts the quorum device model \u003Ccode class=\"literal\">net\u003C/code> and configures the device to start on boot.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@qdevice:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs qdevice setup model net --enable --start\u003C/strong>\u003C/span>\nQuorum device 'net' initialized\nquorum device enabled\nStarting quorum device...\nquorum device started\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAfter configuring the quorum device, you can check its status. This should show that the \u003Ccode class=\"literal\">corosync-qnetd\u003C/code> daemon is running and, at this point, there are no clients connected to it. The \u003Ccode class=\"literal option\">--full\u003C/code> command option provides detailed output.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@qdevice:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs qdevice status net --full\u003C/strong>\u003C/span>\nQNetd address:                  *:5403\nTLS:                            Supported (client certificate required)\nConnected clients:              0\nConnected clusters:             0\nMaximum send/receive size:      32768/32768 bytes\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tEnable the ports on the firewall needed by the \u003Ccode class=\"literal\">pcsd\u003C/code> daemon and the \u003Ccode class=\"literal\">net\u003C/code> quorum device by enabling the \u003Ccode class=\"literal\">high-availability\u003C/code> service on \u003Ccode class=\"literal\">firewalld\u003C/code> with following commands.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@qdevice:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --permanent --add-service=high-availability\u003C/strong>\u003C/span>\n[root@qdevice:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --add-service=high-availability\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tFrom one of the nodes in the existing cluster, authenticate user \u003Ccode class=\"literal\">hacluster\u003C/code> on the node that is hosting the quorum device. This allows \u003Ccode class=\"literal\">pcs\u003C/code> on the cluster to connect to \u003Ccode class=\"literal\">pcs\u003C/code> on the \u003Ccode class=\"literal\">qdevice\u003C/code> host, but does not allow \u003Ccode class=\"literal\">pcs\u003C/code> on the \u003Ccode class=\"literal\">qdevice\u003C/code> host to connect to \u003Ccode class=\"literal\">pcs\u003C/code> on the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1:~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs host auth qdevice\u003C/strong>\u003C/span>\nUsername: hacluster\nPassword:\nqdevice: Authorized\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAdd the quorum device to the cluster.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tBefore adding the quorum device, you can check the current configuration and status for the quorum device for later comparison. The output for these commands indicates that the cluster is not yet using a quorum device, and the \u003Ccode class=\"literal\">Qdevice\u003C/code> membership status for each node is \u003Ccode class=\"literal\">NR\u003C/code> (Not Registered).\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs quorum config\u003C/strong>\u003C/span>\nOptions:\u003C/pre>\u003Cpre class=\"literallayout\">[root@node1:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs quorum status\u003C/strong>\u003C/span>\nQuorum information\n------------------\nDate:             Wed Jun 29 13:15:36 2016\nQuorum provider:  corosync_votequorum\nNodes:            2\nNode ID:          1\nRing ID:          1/8272\nQuorate:          Yes\n\nVotequorum information\n----------------------\nExpected votes:   2\nHighest expected: 2\nTotal votes:      2\nQuorum:           1\nFlags:            2Node Quorate\n\nMembership information\n----------------------\n    Nodeid      Votes    Qdevice Name\n         1          1         NR node1 (local)\n         2          1         NR node2\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe following command adds the quorum device that you have previously created to the cluster. You cannot use more than one quorum device in a cluster at the same time. However, one quorum device can be used by several clusters at the same time. This example command configures the quorum device to use the \u003Ccode class=\"literal\">ffsplit\u003C/code> algorithm. For information about the configuration options for the quorum device, see the \u003Ccode class=\"literal\">corosync-qdevice\u003C/code>(8) man page.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs quorum device add model net host=qdevice algorithm=ffsplit\u003C/strong>\u003C/span>\nSetting up qdevice certificates on nodes...\nnode2: Succeeded\nnode1: Succeeded\nEnabling corosync-qdevice...\nnode1: corosync-qdevice enabled\nnode2: corosync-qdevice enabled\nSending updated corosync.conf to nodes...\nnode1: Succeeded\nnode2: Succeeded\nCorosync configuration reloaded\nStarting corosync-qdevice...\nnode1: corosync-qdevice started\nnode2: corosync-qdevice started\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCheck the configuration status of the quorum device.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tFrom the cluster side, you can execute the following commands to see how the configuration has changed.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe \u003Ccode class=\"literal command\">pcs quorum config\u003C/code> shows the quorum device that has been configured.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs quorum config\u003C/strong>\u003C/span>\nOptions:\nDevice:\n  Model: net\n    algorithm: ffsplit\n    host: qdevice\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThe \u003Ccode class=\"literal command\">pcs quorum status\u003C/code> command shows the quorum runtime status, indicating that the quorum device is in use. The meanings of of the \u003Ccode class=\"literal\">Qdevice\u003C/code> membership information status values for each cluster node are as follows:\n\t\t\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">A/NA\u003C/code> — The quorum device is alive or not alive, indicating whether there is a heartbeat between \u003Ccode class=\"literal\">qdevice\u003C/code> and \u003Ccode class=\"literal\">corosync\u003C/code>. This should always indicate that the quorum device is alive.\n\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">V/NV\u003C/code> — \u003Ccode class=\"literal\">V\u003C/code> is set when the quorum device has given a vote to a node. In this example, both nodes are set to \u003Ccode class=\"literal\">V\u003C/code> since they can communicate with each other. If the cluster were to split into two single-node clusters, one of the nodes would be set to \u003Ccode class=\"literal\">V\u003C/code> and the other node would be set to \u003Ccode class=\"literal\">NV\u003C/code>.\n\t\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">MW/NMW\u003C/code> — The internal quorum device flag is set (\u003Ccode class=\"literal\">MW\u003C/code>) or not set (\u003Ccode class=\"literal\">NMW\u003C/code>). By default the flag is not set and the value is \u003Ccode class=\"literal\">NMW\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs quorum status\u003C/strong>\u003C/span>\nQuorum information\n------------------\nDate:             Wed Jun 29 13:17:02 2016\nQuorum provider:  corosync_votequorum\nNodes:            2\nNode ID:          1\nRing ID:          1/8272\nQuorate:          Yes\n\nVotequorum information\n----------------------\nExpected votes:   3\nHighest expected: 3\nTotal votes:      3\nQuorum:           2\nFlags:            Quorate Qdevice\n\nMembership information\n----------------------\n    Nodeid      Votes    Qdevice Name\n         1          1    A,V,NMW node1 (local)\n         2          1    A,V,NMW node2\n         0          1            Qdevice\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tThe \u003Ccode class=\"literal command\">pcs quorum device status\u003C/code> shows the quorum device runtime status.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs quorum device status\u003C/strong>\u003C/span>\nQdevice information\n-------------------\nModel:                  Net\nNode ID:                1\nConfigured node list:\n    0   Node ID = 1\n    1   Node ID = 2\nMembership node list:   1, 2\n\nQdevice-net information\n----------------------\nCluster name:           mycluster\nQNetd host:             qdevice:5403\nAlgorithm:              ffsplit\nTie-breaker:            Node with lowest node ID\nState:                  Connected\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\t\tFrom the quorum device side, you can execute the following status command, which shows the status of the \u003Ccode class=\"literal\">corosync-qnetd\u003C/code> daemon.\n\t\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@qdevice:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs qdevice status net --full\u003C/strong>\u003C/span>\nQNetd address:                  *:5403\nTLS:                            Supported (client certificate required)\nConnected clients:              2\nConnected clusters:             1\nMaximum send/receive size:      32768/32768 bytes\nCluster \"mycluster\":\n    Algorithm:          ffsplit\n    Tie-breaker:        Node with lowest node ID\n    Node ID 2:\n        Client address:         ::ffff:192.168.122.122:50028\n        HB interval:            8000ms\n        Configured node list:   1, 2\n        Ring ID:                1.2050\n        Membership node list:   1, 2\n        TLS active:             Yes (client certificate verified)\n        Vote:                   ACK (ACK)\n    Node ID 1:\n        Client address:         ::ffff:192.168.122.121:48786\n        HB interval:            8000ms\n        Configured node list:   1, 2\n        Ring ID:                1.2050\n        Membership node list:   1, 2\n        TLS active:             Yes (client certificate verified)\n        Vote:                   ACK (ACK)\u003C/pre>\u003C/li>\u003C/ul>\u003C/div>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_managing-quorum-device-service-configuring-quorum-devices\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">27.3. Managing the quorum device service\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tPCS provides the ability to manage the quorum device service on the local host (\u003Ccode class=\"literal command\">corosync-qnetd\u003C/code>), as shown in the following example commands. Note that these commands affect only the \u003Ccode class=\"literal\">corosync-qnetd\u003C/code> service.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@qdevice:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs qdevice start net\u003C/strong>\u003C/span>\n[root@qdevice:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs qdevice stop net\u003C/strong>\u003C/span>\n[root@qdevice:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs qdevice enable net\u003C/strong>\u003C/span>\n[root@qdevice:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs qdevice disable net\u003C/strong>\u003C/span>\n[root@qdevice:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs qdevice kill net\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_managing-quorum-device-settings_configuring-quorum-devices\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">27.4. Managing a quorum device in a cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThere are a variety of \u003Ccode class=\"literal\">pcs\u003C/code> commands that you can use to change the quorum device settings in a cluster, disable a quorum device, and remove a quorum device.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"changing_quorum_device_settings\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">27.4.1. Changing quorum device settings\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tYou can change the setting of a quorum device with the \u003Ccode class=\"literal command\">pcs quorum device update\u003C/code> command.\n\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\tTo change the \u003Ccode class=\"literal\">host\u003C/code> option of quorum device model \u003Ccode class=\"literal\">net\u003C/code>, use the \u003Ccode class=\"literal command\">pcs quorum device remove\u003C/code> and the \u003Ccode class=\"literal command\">pcs quorum device add\u003C/code> commands to set up the configuration properly, unless the old and the new host are the same machine.\n\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\t\tThe following command changes the quorum device algorithm to \u003Ccode class=\"literal\">lms\u003C/code>.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs quorum device update model algorithm=lms\u003C/strong>\u003C/span>\nSending updated corosync.conf to nodes...\nnode1: Succeeded\nnode2: Succeeded\nCorosync configuration reloaded\nReloading qdevice configuration on nodes...\nnode1: corosync-qdevice stopped\nnode2: corosync-qdevice stopped\nnode1: corosync-qdevice started\nnode2: corosync-qdevice started\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"removing_a_quorum_device\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">27.4.2. Removing a quorum device\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following command removes a quorum device configured on a cluster node.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs quorum device remove\u003C/strong>\u003C/span>\nSending updated corosync.conf to nodes...\nnode1: Succeeded\nnode2: Succeeded\nCorosync configuration reloaded\nDisabling corosync-qdevice...\nnode1: corosync-qdevice disabled\nnode2: corosync-qdevice disabled\nStopping corosync-qdevice...\nnode1: corosync-qdevice stopped\nnode2: corosync-qdevice stopped\nRemoving qdevice certificates from nodes...\nnode1: Succeeded\nnode2: Succeeded\u003C/pre>\u003Cp>\n\t\t\t\t\tAfter you have removed a quorum device, you should see the following error message when displaying the quorum device status.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@node1:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs quorum device status\u003C/strong>\u003C/span>\nError: Unable to get quorum status: corosync-qdevice-tool: Can't connect to QDevice socket (is QDevice running?): No such file or directory\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"destroying_a_quorum_device\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">27.4.3. Destroying a quorum device\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following command disables and stops a quorum device on the quorum device host and deletes all of its configuration files.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@qdevice:~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs qdevice destroy net\u003C/strong>\u003C/span>\nStopping quorum device...\nquorum device stopped\nquorum device disabled\nQuorum device 'net' configuration files removed\u003C/pre>\u003C/section>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-pacemaker-alert-agents_configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 28. Triggering scripts for cluster events\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tA Pacemaker cluster is an event-driven system, where an event might be a resource or node failure, a configuration change, or a resource starting or stopping. You can configure Pacemaker cluster alerts to take some external action when a cluster event occurs by means of alert agents, which are external programs that the cluster calls in the same manner as the cluster calls resource agents to handle resource configuration and operation.\n\t\t\u003C/p>\u003Cp>\n\t\t\tThe cluster passes information about the event to the agent by means of environment variables. Agents can do anything with this information, such as send an email message or log to a file or update a monitoring system.\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\tPacemaker provides several sample alert agents, which are installed in \u003Ccode class=\"literal\">/usr/share/pacemaker/alerts\u003C/code> by default. These sample scripts may be copied and used as is, or they may be used as templates to be edited to suit your purposes. Refer to the source code of the sample agents for the full set of attributes they support.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tIf the sample alert agents do not meet your needs, you can write your own alert agents for a Pacemaker alert to call.\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Csection class=\"section\" id=\"using-sample-alert-agents-configuring-pacemaker-alert-agents\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">28.1. Installing and configuring sample alert agents\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen you use one of the sample alert agents, you should review the script to ensure that it suits your needs. These sample agents are provided as a starting point for custom scripts for specific cluster environments. Note that while Red Hat supports the interfaces that the alert agents scripts use to communicate with Pacemaker, Red Hat does not provide support for the custom agents themselves.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tTo use one of the sample alert agents, you must install the agent on each node in the cluster. For example, the following command installs the \u003Ccode class=\"literal\">alert_file.sh.sample\u003C/code> script as \u003Ccode class=\"literal\">alert_file.sh\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>install --mode=0755 /usr/share/pacemaker/alerts/alert_file.sh.sample /var/lib/pacemaker/alert_file.sh\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tAfter you have installed the script, you can create an alert that uses the script.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following example configures an alert that uses the installed \u003Ccode class=\"literal\">alert_file.sh\u003C/code> alert agent to log events to a file. Alert agents run as the user \u003Ccode class=\"literal\">hacluster\u003C/code>, which has a minimal set of permissions.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThis example creates the log file \u003Ccode class=\"literal\">pcmk_alert_file.log\u003C/code> that will be used to record the events. It then creates the alert agent and adds the path to the log file as its recipient.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>touch /var/log/pcmk_alert_file.log\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>chown hacluster:haclient /var/log/pcmk_alert_file.log\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>chmod 600 /var/log/pcmk_alert_file.log\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert create id=alert_file description=\"Log events to a file.\" path=/var/lib/pacemaker/alert_file.sh\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert recipient add alert_file id=my-alert_logfile value=/var/log/pcmk_alert_file.log\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tThe following example installs the \u003Ccode class=\"literal\">alert_snmp.sh.sample\u003C/code> script as \u003Ccode class=\"literal\">alert_snmp.sh\u003C/code> and configures an alert that uses the installed \u003Ccode class=\"literal\">alert_snmp.sh\u003C/code> alert agent to send cluster events as SNMP traps. By default, the script will send all events except successful monitor calls to the SNMP server. This example configures the timestamp format as a meta option. After configuring the alert, this example configures a recipient for the alert and displays the alert configuration.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>install --mode=0755 /usr/share/pacemaker/alerts/alert_snmp.sh.sample /var/lib/pacemaker/alert_snmp.sh\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert create id=snmp_alert path=/var/lib/pacemaker/alert_snmp.sh meta timestamp-format=\"%Y-%m-%d,%H:%M:%S.%01N\"\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert recipient add snmp_alert value=192.168.1.2\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert\u003C/strong>\u003C/span>\nAlerts:\n Alert: snmp_alert (path=/var/lib/pacemaker/alert_snmp.sh)\n  Meta options: timestamp-format=%Y-%m-%d,%H:%M:%S.%01N.\n  Recipients:\n   Recipient: snmp_alert-recipient (value=192.168.1.2)\u003C/pre>\u003Cp>\n\t\t\t\tThe following example installs the \u003Ccode class=\"literal\">alert_smtp.sh\u003C/code> agent and then configures an alert that uses the installed alert agent to send cluster events as email messages. After configuring the alert, this example configures a recipient and displays the alert configuration.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>install --mode=0755 /usr/share/pacemaker/alerts/alert_smtp.sh.sample /var/lib/pacemaker/alert_smtp.sh\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert create id=smtp_alert path=/var/lib/pacemaker/alert_smtp.sh options email_sender=donotreply@example.com\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert recipient add smtp_alert value=admin@example.com\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert\u003C/strong>\u003C/span>\nAlerts:\n Alert: smtp_alert (path=/var/lib/pacemaker/alert_smtp.sh)\n  Options: email_sender=donotreply@example.com\n  Recipients:\n   Recipient: smtp_alert-recipient (value=admin@example.com)\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"creating-cluster-alert-configuring-pacemaker-alert-agents\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">28.2. Creating a cluster alert\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe following command creates a cluster alert. The options that you configure are agent-specific configuration values that are passed to the alert agent script at the path you specify as additional environment variables. If you do not specify a value for \u003Ccode class=\"literal\">id\u003C/code>, one will be generated.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs alert create path=\u003Cspan class=\"emphasis\">\u003Cem>path\u003C/em>\u003C/span> [id=\u003Cspan class=\"emphasis\">\u003Cem>alert-id\u003C/em>\u003C/span>] [description=\u003Cspan class=\"emphasis\">\u003Cem>description\u003C/em>\u003C/span>] [options [\u003Cspan class=\"emphasis\">\u003Cem>option\u003C/em>\u003C/span>=\u003Cspan class=\"emphasis\">\u003Cem>value\u003C/em>\u003C/span>]...] [meta [\u003Cspan class=\"emphasis\">\u003Cem>meta-option\u003C/em>\u003C/span>=\u003Cspan class=\"emphasis\">\u003Cem>value\u003C/em>\u003C/span>]...]\u003C/pre>\u003Cp>\n\t\t\t\tMultiple alert agents may be configured; the cluster will call all of them for each event. Alert agents will be called only on cluster nodes. They will be called for events involving Pacemaker Remote nodes, but they will never be called on those nodes.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following example creates a simple alert that will call \u003Ccode class=\"literal\">myscript.sh\u003C/code> for each event.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert create id=my_alert path=/path/to/myscript.sh\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"managing-cluster-alert-configuring-pacemaker-alert-agents\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">28.3. Displaying, modifying, and removing cluster alerts\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThere are a variety of \u003Ccode class=\"literal\">pcs\u003C/code> commands you can use to display, modify, and remove cluster alerts.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command shows all configured alerts along with the values of the configured options.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs alert [config|show]\u003C/pre>\u003Cp>\n\t\t\t\tThe following command updates an existing alert with the specified \u003Cspan class=\"emphasis\">\u003Cem>alert-id\u003C/em>\u003C/span> value.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs alert update \u003Cspan class=\"emphasis\">\u003Cem>alert-id\u003C/em>\u003C/span> [path=\u003Cspan class=\"emphasis\">\u003Cem>path\u003C/em>\u003C/span>] [description=\u003Cspan class=\"emphasis\">\u003Cem>description\u003C/em>\u003C/span>] [options [\u003Cspan class=\"emphasis\">\u003Cem>option\u003C/em>\u003C/span>=\u003Cspan class=\"emphasis\">\u003Cem>value\u003C/em>\u003C/span>]...] [meta [\u003Cspan class=\"emphasis\">\u003Cem>meta-option\u003C/em>\u003C/span>=\u003Cspan class=\"emphasis\">\u003Cem>value\u003C/em>\u003C/span>]...]\u003C/pre>\u003Cp>\n\t\t\t\tThe following command removes an alert with the specified \u003Cspan class=\"emphasis\">\u003Cem>alert-id\u003C/em>\u003C/span> value.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs alert remove \u003Cspan class=\"emphasis\">\u003Cem>alert-id\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tAlternately, you can run the \u003Ccode class=\"literal\">pcs alert delete\u003C/code> command, which is identical to the \u003Ccode class=\"literal\">pcs alert remove\u003C/code> command. Both the \u003Ccode class=\"literal\">pcs alert delete\u003C/code> and the \u003Ccode class=\"literal\">pcs alert remove\u003C/code> commands allow you to specify more than one alert to be deleted.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"configuring-alert-recipients-configuring-pacemaker-alert-agents\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">28.4. Configuring cluster alert recipients\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tUsually alerts are directed towards a recipient. Thus each alert may be additionally configured with one or more recipients. The cluster will call the agent separately for each recipient.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe recipient may be anything the alert agent can recognize: an IP address, an email address, a file name, or whatever the particular agent supports.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command adds a new recipient to the specified alert.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs alert recipient add \u003Cspan class=\"emphasis\">\u003Cem>alert-id\u003C/em>\u003C/span> value=\u003Cspan class=\"emphasis\">\u003Cem>recipient-value\u003C/em>\u003C/span> [id=\u003Cspan class=\"emphasis\">\u003Cem>recipient-id\u003C/em>\u003C/span>] [description=\u003Cspan class=\"emphasis\">\u003Cem>description\u003C/em>\u003C/span>] [options [\u003Cspan class=\"emphasis\">\u003Cem>option\u003C/em>\u003C/span>=\u003Cspan class=\"emphasis\">\u003Cem>value\u003C/em>\u003C/span>]...] [meta [\u003Cspan class=\"emphasis\">\u003Cem>meta-option\u003C/em>\u003C/span>=\u003Cspan class=\"emphasis\">\u003Cem>value\u003C/em>\u003C/span>]...]\u003C/pre>\u003Cp>\n\t\t\t\tThe following command updates an existing alert recipient.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs alert recipient update \u003Cspan class=\"emphasis\">\u003Cem>recipient-id\u003C/em>\u003C/span> [value=\u003Cspan class=\"emphasis\">\u003Cem>recipient-value\u003C/em>\u003C/span>] [description=\u003Cspan class=\"emphasis\">\u003Cem>description\u003C/em>\u003C/span>] [options [\u003Cspan class=\"emphasis\">\u003Cem>option\u003C/em>\u003C/span>=\u003Cspan class=\"emphasis\">\u003Cem>value\u003C/em>\u003C/span>]...] [meta [\u003Cspan class=\"emphasis\">\u003Cem>meta-option\u003C/em>\u003C/span>=\u003Cspan class=\"emphasis\">\u003Cem>value\u003C/em>\u003C/span>]...]\u003C/pre>\u003Cp>\n\t\t\t\tThe following command removes the specified alert recipient.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs alert recipient remove \u003Cspan class=\"emphasis\">\u003Cem>recipient-id\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tAlternately, you can run the \u003Ccode class=\"literal\">pcs alert recipient delete\u003C/code> command, which is identical to the \u003Ccode class=\"literal\">pcs alert recipient remove\u003C/code> command. Both the \u003Ccode class=\"literal\">pcs alert recipient remove\u003C/code> and the \u003Ccode class=\"literal\">pcs alert recipient delete\u003C/code> commands allow you to remove more than one alert recipient.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following example command adds the alert recipient \u003Ccode class=\"literal\">my-alert-recipient\u003C/code> with a recipient ID of \u003Ccode class=\"literal\">my-recipient-id\u003C/code> to the alert \u003Ccode class=\"literal\">my-alert\u003C/code>. This will configure the cluster to call the alert script that has been configured for \u003Ccode class=\"literal\">my-alert\u003C/code> for each event, passing the recipient \u003Ccode class=\"literal\">some-address\u003C/code> as an environment variable.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">#  \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert recipient add my-alert value=my-alert-recipient id=my-recipient-id options value=some-address\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"cluster-alert-meta-options-configuring-pacemaker-alert-agents\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">28.5. Alert meta options\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tAs with resource agents, meta options can be configured for alert agents to affect how Pacemaker calls them. The following table describes the alert meta options. Meta options can be configured per alert agent as well as per recipient.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-alert-meta-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 28.1. Alert Meta Options\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 33%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686140229520\" scope=\"col\">Meta-Attribute\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686140228432\" scope=\"col\">Default\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686140227344\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140229520\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">enabled\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140228432\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">true\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140227344\"> \u003Cp>\n\t\t\t\t\t\t\t\t(RHEL 9.3 and later) If set to \u003Ccode class=\"literal\">false\u003C/code> for an alert, the alert will not be used. If set to \u003Ccode class=\"literal\">true\u003C/code> for an alert and \u003Ccode class=\"literal\">false\u003C/code> for a particular recipient of that alert, that recipient will not be used.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140229520\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">timestamp-format\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140228432\"> \u003Cp>\n\t\t\t\t\t\t\t\t%H:%M:%S.%06N\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140227344\"> \u003Cp>\n\t\t\t\t\t\t\t\tFormat the cluster will use when sending the event’s timestamp to the agent. This is a string as used with the \u003Ccode class=\"literal\">date\u003C/code>(1) command.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140229520\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">timeout\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140228432\"> \u003Cp>\n\t\t\t\t\t\t\t\t30s\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140227344\"> \u003Cp>\n\t\t\t\t\t\t\t\tIf the alert agent does not complete within this amount of time, it will be terminated.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tThe following example configures an alert that calls the script \u003Ccode class=\"literal\">myscript.sh\u003C/code> and then adds two recipients to the alert. The first recipient has an ID of \u003Ccode class=\"literal\">my-alert-recipient1\u003C/code> and the second recipient has an ID of \u003Ccode class=\"literal\">my-alert-recipient2\u003C/code>. The script will get called twice for each event, with each call using a 15-second timeout. One call will be passed to the recipient \u003Ccode class=\"literal\">someuser@example.com\u003C/code> with a timestamp in the format %D %H:%M, while the other call will be passed to the recipient \u003Ccode class=\"literal\">otheruser@example.com\u003C/code> with a timestamp in the format %c.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert create id=my-alert path=/path/to/myscript.sh meta timeout=15s\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert recipient add my-alert value=someuser@example.com id=my-alert-recipient1 meta timestamp-format=\"%D %H:%M\"\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert recipient add my-alert value=otheruser@example.com id=my-alert-recipient2 meta timestamp-format=\"%c\"\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"cluster-alert-configuration-configuring-pacemaker-alert-agents\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">28.6. Cluster alert configuration command examples\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe following sequential examples show some basic alert configuration commands to show the format to use to create alerts, add recipients, and display the configured alerts.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tNote that while you must install the alert agents themselves on each node in a cluster, you need to run the \u003Ccode class=\"literal\">pcs\u003C/code> commands only once.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following commands create a simple alert, add two recipients to the alert, and display the configured values.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tSince no alert ID value is specified, the system creates an alert ID value of \u003Ccode class=\"literal\">alert\u003C/code>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe first recipient creation command specifies a recipient of \u003Ccode class=\"literal\">rec_value\u003C/code>. Since this command does not specify a recipient ID, the value of \u003Ccode class=\"literal\">alert-recipient\u003C/code> is used as the recipient ID.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe second recipient creation command specifies a recipient of \u003Ccode class=\"literal\">rec_value2\u003C/code>. This command specifies a recipient ID of \u003Ccode class=\"literal\">my-recipient\u003C/code> for the recipient.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert create path=/my/path\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert recipient add alert value=rec_value\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert recipient add alert value=rec_value2 id=my-recipient\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert config\u003C/strong>\u003C/span>\nAlerts:\n Alert: alert (path=/my/path)\n  Recipients:\n   Recipient: alert-recipient (value=rec_value)\n   Recipient: my-recipient (value=rec_value2)\u003C/pre>\u003Cp>\n\t\t\t\tThis following commands add a second alert and a recipient for that alert. The alert ID for the second alert is \u003Ccode class=\"literal\">my-alert\u003C/code> and the recipient value is \u003Ccode class=\"literal\">my-other-recipient\u003C/code>. Since no recipient ID is specified, the system provides a recipient id of \u003Ccode class=\"literal\">my-alert-recipient\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert create id=my-alert path=/path/to/script description=alert_description options option1=value1 opt=val meta timeout=50s timestamp-format=\"%H%B%S\"\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert recipient add my-alert value=my-other-recipient\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert\u003C/strong>\u003C/span>\nAlerts:\n Alert: alert (path=/my/path)\n  Recipients:\n   Recipient: alert-recipient (value=rec_value)\n   Recipient: my-recipient (value=rec_value2)\n Alert: my-alert (path=/path/to/script)\n  Description: alert_description\n  Options: opt=val option1=value1\n  Meta options: timestamp-format=%H%B%S timeout=50s\n  Recipients:\n   Recipient: my-alert-recipient (value=my-other-recipient)\u003C/pre>\u003Cp>\n\t\t\t\tThe following commands modify the alert values for the alert \u003Ccode class=\"literal\">my-alert\u003C/code> and for the recipient \u003Ccode class=\"literal\">my-alert-recipient\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert update my-alert options option1=newvalue1 meta timestamp-format=\"%H%M%S\"\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert recipient update my-alert-recipient options option1=new meta timeout=60s\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert\u003C/strong>\u003C/span>\nAlerts:\n Alert: alert (path=/my/path)\n  Recipients:\n   Recipient: alert-recipient (value=rec_value)\n   Recipient: my-recipient (value=rec_value2)\n Alert: my-alert (path=/path/to/script)\n  Description: alert_description\n  Options: opt=val option1=newvalue1\n  Meta options: timestamp-format=%H%M%S timeout=50s\n  Recipients:\n   Recipient: my-alert-recipient (value=my-other-recipient)\n    Options: option1=new\n    Meta options: timeout=60s\u003C/pre>\u003Cp>\n\t\t\t\tThe following command removes the recipient \u003Ccode class=\"literal\">my-alert-recipient\u003C/code> from \u003Ccode class=\"literal\">alert\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert recipient remove my-recipient\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert\u003C/strong>\u003C/span>\nAlerts:\n Alert: alert (path=/my/path)\n  Recipients:\n   Recipient: alert-recipient (value=rec_value)\n Alert: my-alert (path=/path/to/script)\n  Description: alert_description\n  Options: opt=val option1=newvalue1\n  Meta options: timestamp-format=\"%M%B%S\" timeout=50s\n  Recipients:\n   Recipient: my-alert-recipient (value=my-other-recipient)\n    Options: option1=new\n    Meta options: timeout=60s\u003C/pre>\u003Cp>\n\t\t\t\tThe following command removes \u003Ccode class=\"literal\">myalert\u003C/code> from the configuration.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert remove myalert\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs alert\u003C/strong>\u003C/span>\nAlerts:\n Alert: alert (path=/my/path)\n  Recipients:\n   Recipient: alert-recipient (value=rec_value)\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"writing-cluster-alert-agent-configuring-pacemaker-alert-agents\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">28.7. Writing a cluster alert agent\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThere are three types of Pacemaker cluster alerts: node alerts, fencing alerts, and resource alerts. The environment variables that are passed to the alert agents can differ, depending on the type of alert. The following table describes the environment variables that are passed to alert agents and specifies when the environment variable is associated with a specific alert type.\n\t\t\t\u003C/p>\u003Crh-table id=\"tb-alert-environmentvariables-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 28.2. Environment Variables Passed to Alert Agents\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 44%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 56%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686137395072\" scope=\"col\">Environment Variable\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686137393984\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137395072\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">CRM_alert_kind\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137393984\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe type of alert (node, fencing, or resource)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137395072\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">CRM_alert_version\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137393984\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe version of Pacemaker sending the alert\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137395072\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">CRM_alert_recipient\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137393984\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe configured recipient\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137395072\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">CRM_alert_node_sequence\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137393984\"> \u003Cp>\n\t\t\t\t\t\t\t\tA sequence number increased whenever an alert is being issued on the local node, which can be used to reference the order in which alerts have been issued by Pacemaker. An alert for an event that happened later in time reliably has a higher sequence number than alerts for earlier events. Be aware that this number has no cluster-wide meaning.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137395072\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">CRM_alert_timestamp\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137393984\"> \u003Cp>\n\t\t\t\t\t\t\t\tA timestamp created prior to executing the agent, in the format specified by the \u003Ccode class=\"literal\">timestamp-format\u003C/code> meta option. This allows the agent to have a reliable, high-precision time of when the event occurred, regardless of when the agent itself was invoked (which could potentially be delayed due to system load or other circumstances).\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137395072\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">CRM_alert_node\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137393984\"> \u003Cp>\n\t\t\t\t\t\t\t\tName of affected node\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137395072\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">CRM_alert_desc\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137393984\"> \u003Cp>\n\t\t\t\t\t\t\t\tDetail about event. For node alerts, this is the node’s current state (member or lost). For fencing alerts, this is a summary of the requested fencing operation, including origin, target, and fencing operation error code, if any. For resource alerts, this is a readable string equivalent of \u003Ccode class=\"literal\">CRM_alert_status\u003C/code>.\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137395072\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">CRM_alert_nodeid\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137393984\"> \u003Cp>\n\t\t\t\t\t\t\t\tID of node whose status changed (provided with node alerts only)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137395072\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">CRM_alert_task\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137393984\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe requested fencing or resource operation (provided with fencing and resource alerts only)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137395072\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">CRM_alert_rc\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137393984\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe numerical return code of the fencing or resource operation (provided with fencing and resource alerts only)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137395072\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">CRM_alert_rsc\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137393984\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe name of the affected resource (resource alerts only)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137395072\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">CRM_alert_interval\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137393984\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe interval of the resource operation (resource alerts only)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137395072\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">CRM_alert_target_rc\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137393984\"> \u003Cp>\n\t\t\t\t\t\t\t\tThe expected numerical return code of the operation (resource alerts only)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137395072\"> \u003Cp>\n\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">CRM_alert_status\u003C/code>\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686137393984\"> \u003Cp>\n\t\t\t\t\t\t\t\tA numerical code used by Pacemaker to represent the operation result (resource alerts only)\n\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\t\tWhen writing an alert agent, you must take the following concerns into account.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tAlert agents may be called with no recipient (if none is configured), so the agent must be able to handle this situation, even if it only exits in that case. Users may modify the configuration in stages, and add a recipient later.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tIf more than one recipient is configured for an alert, the alert agent will be called once per recipient. If an agent is not able to run concurrently, it should be configured with only a single recipient. The agent is free, however, to interpret the recipient as a list.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tWhen a cluster event occurs, all alerts are fired off at the same time as separate processes. Depending on how many alerts and recipients are configured and on what is done within the alert agents, a significant load burst may occur. The agent could be written to take this into consideration, for example by queueing resource-intensive actions into some other instance, instead of directly executing them.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tAlert agents are run as the \u003Ccode class=\"literal\">hacluster\u003C/code> user, which has a minimal set of permissions. If an agent requires additional privileges, it is recommended to configure \u003Ccode class=\"literal command\">sudo\u003C/code> to allow the agent to run the necessary commands as another user with the appropriate privileges.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tTake care to validate and sanitize user-configured parameters, such as \u003Ccode class=\"literal\">CRM_alert_timestamp\u003C/code> (whose content is specified by the user-configured \u003Ccode class=\"literal\">timestamp-format\u003C/code>), \u003Ccode class=\"literal\">CRM_alert_recipient\u003C/code>, and all alert options. This is necessary to protect against configuration errors. In addition, if some user can modify the CIB without having \u003Ccode class=\"literal\">hacluster\u003C/code>-level access to the cluster nodes, this is a potential security concern as well, and you should avoid the possibility of code injection.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tIf a cluster contains resources with operations for which the \u003Ccode class=\"literal\">on-fail\u003C/code> parameter is set to \u003Ccode class=\"literal\">fence\u003C/code>, there will be multiple fence notifications on failure, one for each resource for which this parameter is set plus one additional notification. Both the \u003Ccode class=\"literal\">pacemaker-fenced\u003C/code> and \u003Ccode class=\"literal\">pacemaker-controld\u003C/code> will send notifications. Pacemaker performs only one actual fence operation in this case, however, no matter how many notifications are sent.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tThe alerts interface is designed to be backward compatible with the external scripts interface used by the \u003Ccode class=\"literal\">ocf:pacemaker:ClusterMon\u003C/code> resource. To preserve this compatibility, the environment variables passed to alert agents are available prepended with \u003Ccode class=\"literal\">CRM_notify_\u003C/code> as well as \u003Ccode class=\"literal\">CRM_alert_\u003C/code>. One break in compatibility is that the \u003Ccode class=\"literal\">ClusterMon\u003C/code> resource ran external scripts as the root user, while alert agents are run as the \u003Ccode class=\"literal\">hacluster\u003C/code> user.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-multisite-cluster-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 29. Multi-site Pacemaker clusters\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tWhen a cluster spans more than one site, issues with network connectivity between the sites can lead to split-brain situations. When connectivity drops, there is no way for a node on one site to determine whether a node on another site has failed or is still functioning with a failed site interlink. In addition, it can be problematic to provide high availability services across two sites which are too far apart to keep synchronous. To address these issues, Pacemaker provides full support for the ability to configure high availability clusters that span multiple sites through the use of a Booth cluster ticket manager.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"con_booth-cluster-ticket-manager-configuring-multisite-cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">29.1. Overview of Booth cluster ticket manager\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe Booth \u003Cspan class=\"emphasis\">\u003Cem>ticket manager\u003C/em>\u003C/span> is a distributed service that is meant to be run on a different physical network than the networks that connect the cluster nodes at particular sites. It yields another, loose cluster, a \u003Cspan class=\"emphasis\">\u003Cem>Booth formation\u003C/em>\u003C/span>, that sits on top of the regular clusters at the sites. This aggregated communication layer facilitates consensus-based decision processes for individual Booth tickets.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tA Booth \u003Cspan class=\"emphasis\">\u003Cem>ticket\u003C/em>\u003C/span> is a singleton in the Booth formation and represents a time-sensitive, movable unit of authorization. Resources can be configured to require a certain ticket to run. This can ensure that resources are run at only one site at a time, for which a ticket or tickets have been granted.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can think of a Booth formation as an overlay cluster consisting of clusters running at different sites, where all the original clusters are independent of each other. It is the Booth service which communicates to the clusters whether they have been granted a ticket, and it is Pacemaker that determines whether to run resources in a cluster based on a Pacemaker ticket constraint. This means that when using the ticket manager, each of the clusters can run its own resources as well as shared resources. For example there can be resources A, B and C running only in one cluster, resources D, E, and F running only in the other cluster, and resources G and H running in either of the two clusters as determined by a ticket. It is also possible to have an additional resource J that could run in either of the two clusters as determined by a separate ticket.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc-configuring-multisite-with-booth-configuring-multisite-cluster\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">29.2. Configuring multi-site clusters with Pacemaker\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can configure a multi-site configuration that uses the Booth ticket manager with the following procedure.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThese example commands use the following arrangement:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tCluster 1 consists of the nodes \u003Ccode class=\"literal\">cluster1-node1\u003C/code> and \u003Ccode class=\"literal\">cluster1-node2\u003C/code>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tCluster 1 has a floating IP address assigned to it of 192.168.11.100\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tCluster 2 consists of \u003Ccode class=\"literal\">cluster2-node1\u003C/code> and \u003Ccode class=\"literal\">cluster2-node2\u003C/code>\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tCluster 2 has a floating IP address assigned to it of 192.168.22.100\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe arbitrator node is \u003Ccode class=\"literal\">arbitrator-node\u003C/code> with an ip address of 192.168.99.100\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe name of the Booth ticket that this configuration uses is \u003Ccode class=\"literal\">apacheticket\u003C/code>\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tThese example commands assume that the cluster resources for an Apache service have been configured as part of the resource group \u003Ccode class=\"literal\">apachegroup\u003C/code> for each cluster. It is not required that the resources and resource groups be the same on each cluster to configure a ticket constraint for those resources, since the Pacemaker instance for each cluster is independent, but that is a common failover scenario.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tNote that at any time in the configuration procedure you can enter the \u003Ccode class=\"literal command\">pcs booth config\u003C/code> command to display the booth configuration for the current node or cluster or the \u003Ccode class=\"literal command\">pcs booth status\u003C/code> command to display the current status of booth on the local node.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tInstall the \u003Ccode class=\"literal\">booth-site\u003C/code> Booth ticket manager package on each node of both clusters.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@cluster1-node1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install -y booth-site\u003C/strong>\u003C/span>\n[root@cluster1-node2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install -y booth-site\u003C/strong>\u003C/span>\n[root@cluster2-node1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install -y booth-site\u003C/strong>\u003C/span>\n[root@cluster2-node2 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install -y booth-site\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tInstall the \u003Ccode class=\"literal\">pcs\u003C/code>, \u003Ccode class=\"literal\">booth-core\u003C/code>, and \u003Ccode class=\"literal\">booth-arbitrator\u003C/code> packages on the arbitrator node.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@arbitrator-node ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install -y pcs booth-core booth-arbitrator\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tIf you are running the \u003Ccode class=\"literal\">firewalld\u003C/code> daemon, execute the following commands on all nodes in both clusters as well as on the arbitrator node to enable the ports that are required by the Red Hat High Availability Add-On.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --permanent --add-service=high-availability\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --add-service=high-availability\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tYou may need to modify which ports are open to suit local conditions. For more information about the ports that are required by the Red Hat High-Availability Add-On, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters#proc_enabling-ports-for-high-availability-creating-high-availability-cluster\">Enabling ports for the High Availability Add-On\u003C/a>.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate a Booth configuration on one node of one cluster. The addresses you specify for each cluster and for the arbitrator must be IP addresses. For each cluster, you specify a floating IP address.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[cluster1-node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs booth setup sites 192.168.11.100 192.168.22.100 arbitrators 192.168.99.100\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThis command creates the configuration files \u003Ccode class=\"literal\">/etc/booth/booth.conf\u003C/code> and \u003Ccode class=\"literal\">/etc/booth/booth.key\u003C/code> on the node from which it is run.\n\t\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate a ticket for the Booth configuration. This is the ticket that you will use to define the resource constraint that will allow resources to run only when this ticket has been granted to the cluster.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tThis basic failover configuration procedure uses only one ticket, but you can create additional tickets for more complicated scenarios where each ticket is associated with a different resource or resources.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[cluster1-node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs booth ticket add apacheticket\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tSynchronize the Booth configuration to all nodes in the current cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[cluster1-node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs booth sync\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tFrom the arbitrator node, pull the Booth configuration to the arbitrator. If you have not previously done so, you must first authenticate \u003Ccode class=\"literal\">pcs\u003C/code> to the node from which you are pulling the configuration.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[arbitrator-node ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs host auth cluster1-node1\u003C/strong>\u003C/span>\n[arbitrator-node ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs booth pull cluster1-node1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tPull the Booth configuration to the other cluster and synchronize to all the nodes of that cluster. As with the arbitrator node, if you have not previously done so, you must first authenticate \u003Ccode class=\"literal\">pcs\u003C/code> to the node from which you are pulling the configuration.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[cluster2-node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs host auth cluster1-node1\u003C/strong>\u003C/span>\n[cluster2-node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs booth pull cluster1-node1\u003C/strong>\u003C/span>\n[cluster2-node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs booth sync\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tStart and enable Booth on the arbitrator.\n\t\t\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\tYou must not manually start or enable Booth on any of the nodes of the clusters since Booth runs as a Pacemaker resource in those clusters.\n\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cpre class=\"literallayout\">[arbitrator-node ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs booth start\u003C/strong>\u003C/span>\n[arbitrator-node ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs booth enable\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tConfigure Booth to run as a cluster resource on both cluster sites, using the floating IP addresses assigned to each cluster. This creates a resource group with \u003Ccode class=\"literal\">booth-ip\u003C/code> and \u003Ccode class=\"literal\">booth-service\u003C/code> as members of that group.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[cluster1-node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs booth create ip 192.168.11.100\u003C/strong>\u003C/span>\n[cluster2-node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs booth create ip 192.168.22.100\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAdd a ticket constraint to the resource group you have defined for each cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[cluster1-node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint ticket add apacheticket apachegroup\u003C/strong>\u003C/span>\n[cluster2-node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint ticket add apacheticket apachegroup\u003C/strong>\u003C/span>\u003C/pre>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tYou can enter the following command to display the currently configured ticket constraints.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs constraint ticket [show]\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tGrant the ticket you created for this setup to the first cluster.\n\t\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\t\tNote that it is not necessary to have defined ticket constraints before granting a ticket. Once you have initially granted a ticket to a cluster, then Booth takes over ticket management unless you override this manually with the \u003Ccode class=\"literal command\">pcs booth ticket revoke\u003C/code> command. For information about the \u003Ccode class=\"literal command\">pcs booth\u003C/code> administration commands, see the PCS help screen for the \u003Ccode class=\"literal command\">pcs booth\u003C/code> command.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[cluster1-node1 ~] # \u003Cspan class=\"strong strong\">\u003Cstrong>pcs booth ticket grant apacheticket\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\tIt is possible to add or remove tickets at any time, even after completing this procedure. After adding or removing a ticket, however, you must synchronize the configuration files to the other nodes and clusters as well as to the arbitrator and grant the ticket as is shown in this procedure.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tFor information about additional Booth administration commands that you can use for cleaning up and removing Booth configuration files, tickets, and resources, see the PCS help screen for the \u003Ccode class=\"literal command\">pcs booth\u003C/code> command.\n\t\t\t\u003C/p>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_remote-node-management-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 30. Integrating non-corosync nodes into a cluster: the pacemaker_remote service\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tThe \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service allows nodes not running \u003Ccode class=\"literal\">corosync\u003C/code> to integrate into the cluster and have the cluster manage their resources just as if they were real cluster nodes.\n\t\t\u003C/p>\u003Cp>\n\t\t\tAmong the capabilities that the \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service provides are the following:\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\tThe \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service allows you to scale beyond the Red Hat support limit of 32 nodes.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tThe \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service allows you to manage a virtual environment as a cluster resource and also to manage individual services within the virtual environment as cluster resources.\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\tThe following terms are used to describe the \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service.\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\u003Cspan class=\"emphasis\">\u003Cem>cluster node\u003C/em>\u003C/span> — A node running the High Availability services (\u003Ccode class=\"literal\">pacemaker\u003C/code> and \u003Ccode class=\"literal\">corosync\u003C/code>).\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\u003Cspan class=\"emphasis\">\u003Cem>remote node\u003C/em>\u003C/span> — A node running \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> to remotely integrate into the cluster without requiring \u003Ccode class=\"literal\">corosync\u003C/code> cluster membership. A remote node is configured as a cluster resource that uses the \u003Ccode class=\"literal\">ocf:pacemaker:remote\u003C/code> resource agent.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\u003Cspan class=\"emphasis\">\u003Cem>guest node\u003C/em>\u003C/span> — A virtual guest node running the \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service. The virtual guest resource is managed by the cluster; it is both started by the cluster and integrated into the cluster as a remote node.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\u003Cspan class=\"emphasis\">\u003Cem>pacemaker_remote\u003C/em>\u003C/span> — A service daemon capable of performing remote application management within remote nodes and KVM guest nodes in a Pacemaker cluster environment. This service is an enhanced version of Pacemaker’s local executor daemon (\u003Ccode class=\"literal\">pacemaker-execd\u003C/code>) that is capable of managing resources remotely on a node not running corosync.\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\tA Pacemaker cluster running the \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service has the following characteristics.\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\tRemote nodes and guest nodes run the \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service (with very little configuration required on the virtual machine side).\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tThe cluster stack (\u003Ccode class=\"literal\">pacemaker\u003C/code> and \u003Ccode class=\"literal\">corosync\u003C/code>), running on the cluster nodes, connects to the \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service on the remote nodes, allowing them to integrate into the cluster.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tThe cluster stack (\u003Ccode class=\"literal\">pacemaker\u003C/code> and \u003Ccode class=\"literal\">corosync\u003C/code>), running on the cluster nodes, launches the guest nodes and immediately connects to the \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service on the guest nodes, allowing them to integrate into the cluster.\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\tThe key difference between the cluster nodes and the remote and guest nodes that the cluster nodes manage is that the remote and guest nodes are not running the cluster stack. This means the remote and guest nodes have the following limitations:\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\tthey do not take place in quorum\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tthey do not execute fencing device actions\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tthey are not eligible to be the cluster’s Designated Controller (DC)\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tthey do not themselves run the full range of \u003Ccode class=\"literal command\">pcs\u003C/code> commands\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\tOn the other hand, remote nodes and guest nodes are not bound to the scalability limits associated with the cluster stack.\n\t\t\u003C/p>\u003Cp>\n\t\t\tOther than these noted limitations, the remote and guest nodes behave just like cluster nodes in respect to resource management, and the remote and guest nodes can themselves be fenced. The cluster is fully capable of managing and monitoring resources on each remote and guest node: You can build constraints against them, put them in standby, or perform any other action you perform on cluster nodes with the \u003Ccode class=\"literal command\">pcs\u003C/code> commands. Remote and guest nodes appear in cluster status output just as cluster nodes do.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"ref_host-and-guest-authentication-of-remote-nodes-remote-node-management\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">30.1. Host and guest authentication of pacemaker_remote nodes\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tThe connection between cluster nodes and pacemaker_remote is secured using Transport Layer Security (TLS) with pre-shared key (PSK) encryption and authentication over TCP (using port 3121 by default). This means both the cluster node and the node running \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> must share the same private key. By default this key must be placed at \u003Ccode class=\"literal\">/etc/pacemaker/authkey\u003C/code> on both cluster nodes and remote nodes.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe \u003Ccode class=\"literal command\">pcs cluster node add-guest\u003C/code> command sets up the \u003Ccode class=\"literal\">authkey\u003C/code> for guest nodes and the \u003Ccode class=\"literal command\">pcs cluster node add-remote\u003C/code> command sets up the \u003Ccode class=\"literal\">authkey\u003C/code> for remote nodes.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-kvm-guest-nodes-remote-node-management\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">30.2. Configuring KVM guest nodes\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tA Pacemaker guest node is a virtual guest node running the \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service. The virtual guest node is managed by the cluster.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"guest_node_resource_options\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">30.2.1. Guest node resource options\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tWhen configuring a virtual machine to act as a guest node, you create a \u003Ccode class=\"literal\">VirtualDomain\u003C/code> resource, which manages the virtual machine. For descriptions of the options you can set for a \u003Ccode class=\"literal\">VirtualDomain\u003C/code> resource, see the \"Resource Options for Virtual Domain Resources\" table in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters#ref_virtual-domain-resource-options-configuring-virtual-domain-as-a-resource\">Virtual domain resource options\u003C/a>.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tIn addition to the \u003Ccode class=\"literal\">VirtualDomain\u003C/code> resource options, metadata options define the resource as a guest node and define the connection parameters. You set these resource options with the \u003Ccode class=\"literal command\">pcs cluster node add-guest\u003C/code> command. The following table describes these metadata options.\n\t\t\t\t\u003C/p>\u003Crh-table id=\"tb-remoteklm-options-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 30.1. Metadata Options for Configuring KVM Resources as Remote Nodes\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 33%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686138489296\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686138488208\" scope=\"col\">Default\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686138487120\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686138489296\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">remote-node\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686138488208\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t&lt;none&gt;\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686138487120\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tThe name of the guest node this resource defines. This both enables the resource as a guest node and defines the unique name used to identify the guest node. \u003Cspan class=\"emphasis\">\u003Cem>WARNING\u003C/em>\u003C/span>: This value cannot overlap with any resource or node IDs.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686138489296\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">remote-port\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686138488208\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t3121\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686138487120\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tConfigures a custom port to use for the guest connection to \u003Ccode class=\"literal\">pacemaker_remote\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686138489296\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">remote-addr\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686138488208\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tThe address provided in the \u003Ccode class=\"literal\">pcs host auth\u003C/code> command\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686138487120\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tThe IP address or host name to connect to\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686138489296\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">remote-connect-timeout\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686138488208\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t60s\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686138487120\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tAmount of time before a pending guest connection will time out\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003C/section>\u003Csection class=\"section\" id=\"integrating_a_virtual_machine_as_a_guest_node\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">30.2.2. Integrating a virtual machine as a guest node\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following procedure is a high-level summary overview of the steps to perform to have Pacemaker launch a virtual machine and to integrate that machine as a guest node, using \u003Ccode class=\"literal\">libvirt\u003C/code> and KVM virtual guests.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tConfigure the \u003Ccode class=\"literal\">VirtualDomain\u003C/code> resources.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tEnter the following commands on every virtual machine to install \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> packages, start the \u003Ccode class=\"literal\">pcsd\u003C/code> service and enable it to run on startup, and allow TCP port 3121 through the firewall.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install pacemaker-remote resource-agents pcs\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl start pcsd.service\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl enable pcsd.service\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --add-port 3121/tcp --permanent\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --add-port 2224/tcp --permanent\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --reload\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tGive each virtual machine a static network address and unique host name, which should be known to all nodes.\n\t\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tIf you have not already done so, authenticate \u003Ccode class=\"literal\">pcs\u003C/code> to the node you will be integrating as a quest node.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs host auth \u003Cspan class=\"emphasis\">\u003Cem>nodename\u003C/em>\u003C/span>\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tUse the following command to convert an existing \u003Ccode class=\"literal\">VirtualDomain\u003C/code> resource into a guest node. This command must be run on a cluster node and not on the guest node which is being added. In addition to converting the resource, this command copies the \u003Ccode class=\"literal\">/etc/pacemaker/authkey\u003C/code> to the guest node and starts and enables the \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> daemon on the guest node. The node name for the guest node, which you can define arbitrarily, can differ from the host name for the node.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster node add-guest \u003Cspan class=\"emphasis\">\u003Cem>nodename\u003C/em>\u003C/span> \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>options\u003C/em>\u003C/span>]\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tAfter creating the \u003Ccode class=\"literal\">VirtualDomain\u003C/code> resource, you can treat the guest node just as you would treat any other node in the cluster. For example, you can create a resource and place a resource constraint on the resource to run on the guest node as in the following commands, which are run from a cluster node. You can include guest nodes in groups, which allows you to group a storage device, file system, and VM.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create webserver apache configfile=/etc/httpd/conf/httpd.conf op monitor interval=30s\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location webserver prefers \u003Cspan class=\"emphasis\">\u003Cem>nodename\u003C/em>\u003C/span>\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"proc_configuring-remote-nodes-remote-node-management\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">30.3. Configuring Pacemaker remote nodes\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tA remote node is defined as a cluster resource with \u003Ccode class=\"literal\">ocf:pacemaker:remote\u003C/code> as the resource agent. You create this resource with the \u003Ccode class=\"literal command\">pcs cluster node add-remote\u003C/code> command.\n\t\t\t\u003C/p>\u003Csection class=\"section\" id=\"remote_node_resource_options\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">30.3.1. Remote node resource options\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following table describes the resource options you can configure for a \u003Ccode class=\"literal\">remote\u003C/code> resource.\n\t\t\t\t\u003C/p>\u003Crh-table id=\"tb-remotenode-options-HAAR\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 30.2. Resource Options for Remote Nodes\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 38%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 25%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 38%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686140461792\" scope=\"col\">Field\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686140460704\" scope=\"col\">Default\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686140459616\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140461792\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">reconnect_interval\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140460704\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t0\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140459616\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tTime in seconds to wait before attempting to reconnect to a remote node after an active connection to the remote node has been severed. This wait is recurring. If reconnect fails after the wait period, a new reconnect attempt will be made after observing the wait time. When this option is in use, Pacemaker will keep attempting to reach out and connect to the remote node indefinitely after each wait interval.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140461792\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">server\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140460704\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tAddress specified with \u003Ccode class=\"literal\">pcs host auth\u003C/code> command\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140459616\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tServer to connect to. This can be an IP address or host name.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140461792\"> \u003Cp>\n\t\t\t\t\t\t\t\t\t\u003Ccode class=\"literal\">port\u003C/code>\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140460704\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686140459616\"> \u003Cp>\n\t\t\t\t\t\t\t\t\tTCP port to connect to.\n\t\t\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003C/section>\u003Csection class=\"section\" id=\"remote_node_configuration_overview\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">30.3.2. Remote node configuration overview\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tThe following procedure provides a high-level summary overview of the steps to perform to configure a Pacemaker Remote node and to integrate that node into an existing Pacemaker cluster environment.\n\t\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tOn the node that you will be configuring as a remote node, allow cluster-related services through the local firewall.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --permanent --add-service=high-availability\u003C/strong>\u003C/span>\nsuccess\n# \u003Cspan class=\"strong strong\">\u003Cstrong>firewall-cmd --reload\u003C/strong>\u003C/span>\nsuccess\u003C/pre>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\tIf you are using \u003Ccode class=\"literal\">iptables\u003C/code> directly, or some other firewall solution besides \u003Ccode class=\"literal\">firewalld\u003C/code>, simply open the following ports: TCP ports 2224 and 3121.\n\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tInstall the \u003Ccode class=\"literal command\">pacemaker_remote\u003C/code> daemon on the remote node.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>dnf install -y pacemaker-remote resource-agents pcs\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tStart and enable \u003Ccode class=\"literal\">pcsd\u003C/code> on the remote node.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl start pcsd.service\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>systemctl enable pcsd.service\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tIf you have not already done so, authenticate \u003Ccode class=\"literal\">pcs\u003C/code> to the node you will be adding as a remote node.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs host auth remote1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tAdd the remote node resource to the cluster with the following command. This command also syncs all relevant configuration files to the new node, starts the node, and configures it to start \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> on boot. This command must be run on a cluster node and not on the remote node which is being added.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster node add-remote remote1\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\t\tAfter adding the \u003Ccode class=\"literal\">remote\u003C/code> resource to the cluster, you can treat the remote node just as you would treat any other node in the cluster. For example, you can create a resource and place a resource constraint on the resource to run on the remote node as in the following commands, which are run from a cluster node.\n\t\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs resource create webserver apache configfile=/etc/httpd/conf/httpd.conf op monitor interval=30s\u003C/strong>\u003C/span>\n# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs constraint location webserver prefers remote1\u003C/strong>\u003C/span>\u003C/pre>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\t\t\t\tNever involve a remote node connection resource in a resource group, colocation constraint, or order constraint.\n\t\t\t\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\tConfigure fencing resources for the remote node. Remote nodes are fenced the same way as cluster nodes. Configure fencing resources for use with remote nodes the same as you would with cluster nodes. Note, however, that remote nodes can never initiate a fencing action. Only cluster nodes are capable of actually executing a fencing operation against another node.\n\t\t\t\t\t\t\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"proc_changing-default-port-location-remote-node-management\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">30.4. Changing the default port location\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf you need to change the default port location for either Pacemaker or \u003Ccode class=\"literal\">pacemaker_remote\u003C/code>, you can set the \u003Ccode class=\"literal\">PCMK_remote_port\u003C/code> environment variable that affects both of these daemons. This environment variable can be enabled by placing it in the \u003Ccode class=\"literal\">/etc/sysconfig/pacemaker\u003C/code> file as follows.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">\\#==#==# Pacemaker Remote\n...\n#\n# Specify a custom port for Pacemaker Remote connections\nPCMK_remote_port=3121\u003C/pre>\u003Cp>\n\t\t\t\tWhen changing the default port used by a particular guest node or remote node, the \u003Ccode class=\"literal\">PCMK_remote_port\u003C/code> variable must be set in that node’s \u003Ccode class=\"literal\">/etc/sysconfig/pacemaker\u003C/code> file, and the cluster resource creating the guest node or remote node connection must also be configured with the same port number (using the \u003Ccode class=\"literal\">remote-port\u003C/code> metadata option for guest nodes, or the \u003Ccode class=\"literal\">port\u003C/code> option for remote nodes).\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_upgrading-systems-with-pacemaker-remote-remote-node-management\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">30.5. Upgrading systems with pacemaker_remote nodes\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf the \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service is stopped on an active Pacemaker Remote node, the cluster will gracefully migrate resources off the node before stopping the node. This allows you to perform software upgrades and other routine maintenance procedures without removing the node from the cluster. Once \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> is shut down, however, the cluster will immediately try to reconnect. If \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> is not restarted within the resource’s monitor timeout, the cluster will consider the monitor operation as failed.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf you wish to avoid monitor failures when the \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service is stopped on an active Pacemaker Remote node, you can use the following procedure to take the node out of the cluster before performing any system administration that might stop \u003Ccode class=\"literal\">pacemaker_remote\u003C/code>.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tStop the node’s connection resource with the \u003Ccode class=\"literal command\">pcs resource disable \u003Cspan class=\"emphasis\">\u003Cem>resourcename\u003C/em>\u003C/span>\u003C/code> command, which will move all services off the node. The connection resource would be the \u003Ccode class=\"literal\">ocf:pacemaker:remote\u003C/code> resource for a remote node or, commonly, the \u003Ccode class=\"literal\">ocf:heartbeat:VirtualDomain\u003C/code> resource for a guest node. For guest nodes, this command will also stop the VM, so the VM must be started outside the cluster (for example, using \u003Ccode class=\"literal command\">virsh\u003C/code>) to perform any maintenance.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource disable \u003Cspan class=\"emphasis\">\u003Cem>resourcename\u003C/em>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tPerform the required maintenance.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tWhen ready to return the node to the cluster, re-enable the resource with the \u003Ccode class=\"literal command\">pcs resource enable\u003C/code> command.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource enable \u003Cspan class=\"emphasis\">\u003Cem>resourcename\u003C/em>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 31. Performing cluster maintenance\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tIn order to perform maintenance on the nodes of your cluster, you may need to stop or move the resources and services running on that cluster. Or you may need to stop the cluster software while leaving the services untouched. Pacemaker provides a variety of methods for performing system maintenance.\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\tIf you need to stop a node in a cluster while continuing to provide the services running on that cluster on another node, you can put the cluster node in standby mode. A node that is in standby mode is no longer able to host resources. Any resource currently active on the node will be moved to another node, or stopped if no other node is eligible to run the resource. For information about standby mode, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_stopping-individual-node-cluster-maintenance\">Putting a node into standby mode\u003C/a>.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\tIf you need to move an individual resource off the node on which it is currently running without stopping that resource, you can use the \u003Ccode class=\"literal command\">pcs resource move\u003C/code> command to move the resource to a different node.\n\t\t\t\t\u003C/p>\u003Cp class=\"simpara\">\n\t\t\t\t\tWhen you execute the \u003Ccode class=\"literal command\">pcs resource move\u003C/code> command, this adds a constraint to the resource to prevent it from running on the node on which it is currently running. When you are ready to move the resource back, you can execute the \u003Ccode class=\"literal command\">pcs resource clear\u003C/code> or the \u003Ccode class=\"literal command\">pcs constraint delete\u003C/code> command to remove the constraint. This does not necessarily move the resources back to the original node, however, since where the resources can run at that point depends on how you have configured your resources initially. You can relocate a resource to its preferred node with the \u003Ccode class=\"literal command\">pcs resource relocate run\u003C/code> command.\n\t\t\t\t\u003C/p>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tIf you need to stop a running resource entirely and prevent the cluster from starting it again, you can use the \u003Ccode class=\"literal command\">pcs resource disable\u003C/code> command. For information on the \u003Ccode class=\"literal command\">pcs resource disable\u003C/code> command, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_disabling-resources-cluster-maintenance\">Disabling, enabling, and banning cluster resources\u003C/a>.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tIf you want to prevent Pacemaker from taking any action for a resource (for example, if you want to disable recovery actions while performing maintenance on the resource, or if you need to reload the \u003Ccode class=\"literal\">/etc/sysconfig/pacemaker\u003C/code> settings), use the \u003Ccode class=\"literal command\">pcs resource unmanage\u003C/code> command, as described in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_unmanaging-resources-cluster-maintenance\">Setting a resource to unmanaged mode\u003C/a>. Pacemaker Remote connection resources should never be unmanaged.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tIf you need to put the cluster in a state where no services will be started or stopped, you can set the \u003Ccode class=\"literal\">maintenance-mode\u003C/code> cluster property. Putting the cluster into maintenance mode automatically unmanages all resources. For information about putting the cluster in maintenance mode, see \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_setting-maintenance-mode-cluster-maintenance\">Putting a cluster in maintenance mode\u003C/a>.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tIf you need to update the packages that make up the RHEL High Availability and Resilient Storage Add-Ons, you can update the packages on one node at a time or on the entire cluster as a whole, as summarized in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_updating-cluster-packages-cluster-maintenance\">Updating a RHEL high availability cluster\u003C/a>.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tIf you need to perform maintenance on a Pacemaker remote node, you can remove that node from the cluster by disabling the remote node resource, as described in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_upgrading-remote-nodes-cluster-maintenance\">Upgrading remote nodes and guest nodes\u003C/a>.\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\tIf you need to migrate a VM in a RHEL cluster, you will first need to stop the cluster services on the VM to remove the node from the cluster and then start the cluster back up after performing the migration. as described in \u003Ca class=\"link\" href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters#proc_migrating-cluster-vms-cluster-maintenance\">Migrating VMs in a RHEL cluster\u003C/a>.\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Csection class=\"section\" id=\"proc_stopping-individual-node-cluster-maintenance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">31.1. Putting a node into standby mode\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen a cluster node is in standby mode, the node is no longer able to host resources. Any resources currently active on the node will be moved to another node.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command puts the specified node into standby mode. If you specify the \u003Ccode class=\"literal option\">--all\u003C/code>, this command puts all nodes into standby mode.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can use this command when updating a resource’s packages. You can also use this command when testing a configuration, to simulate recovery without actually shutting down a node.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs node standby \u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span> | --all\u003C/pre>\u003Cp>\n\t\t\t\tThe following command removes the specified node from standby mode. After running this command, the specified node is then able to host resources. If you specify the \u003Ccode class=\"literal option\">--all\u003C/code>, this command removes all nodes from standby mode.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs node unstandby \u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span> | --all\u003C/pre>\u003Cp>\n\t\t\t\tNote that when you execute the \u003Ccode class=\"literal command\">pcs node standby\u003C/code> command, this prevents resources from running on the indicated node. When you execute the \u003Ccode class=\"literal command\">pcs node unstandby\u003C/code> command, this allows resources to run on the indicated node. This does not necessarily move the resources back to the indicated node; where the resources can run at that point depends on how you have configured your resources initially.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_manually-move-resources-cluster-maintenance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">31.2. Manually moving cluster resources\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tYou can override the cluster and force resources to move from their current location. There are two occasions when you would want to do this:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tWhen a node is under maintenance, and you need to move all resources running on that node to a different node\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tWhen individually specified resources needs to be moved\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tTo move all resources running on a node to a different node, you put the node in standby mode.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can move individually specified resources in either of the following ways.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tYou can use the \u003Ccode class=\"literal command\">pcs resource move\u003C/code> command to move a resource off a node on which it is currently running.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tYou can use the \u003Ccode class=\"literal command\">pcs resource relocate run\u003C/code> command to move a resource to its preferred node, as determined by current cluster status, constraints, location of resources and other settings.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Csection class=\"section\" id=\"moving_a_resource_from_its_current_node\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">31.2.1. Moving a resource from its current node\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tTo move a resource off the node on which it is currently running, use the following command, specifying the \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> of the resource as defined. Specify the \u003Ccode class=\"literal\">destination_node\u003C/code> if you want to indicate on which node to run the resource that you are moving.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource move \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>destination_node\u003C/em>\u003C/span>] [--promoted] [--strict] [--wait[=\u003Cspan class=\"emphasis\">\u003Cem>n\u003C/em>\u003C/span>]]\u003C/pre>\u003Cp>\n\t\t\t\t\tWhen you execute the \u003Ccode class=\"literal command\">pcs resource move\u003C/code> command, this adds a constraint to the resource to prevent it from running on the node on which it is currently running. By default, the location constraint that the command creates is automatically removed once the resource has been moved. If removing the constraint would cause the resource to move back to the original node, as might happen if the \u003Ccode class=\"literal\">resource-stickiness\u003C/code> value for the resource is 0, the \u003Ccode class=\"literal\">pcs resource move\u003C/code> command fails. If you would like to move a resource and leave the resulting constraint in place, use the \u003Ccode class=\"literal\">pcs resource move-with-constraint\u003C/code> command.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tIf you specify the \u003Ccode class=\"literal\">--promoted\u003C/code> parameter of the \u003Ccode class=\"literal command\">pcs resource move\u003C/code> command, the constraint applies only to promoted instances of the resource.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tIf you specify the \u003Ccode class=\"literal\">--strict\u003C/code> parameter of the \u003Ccode class=\"literal command\">pcs resource move\u003C/code> command, the command will fail if other resources than the one specified in the command would be affected.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tYou can optionally configure a \u003Ccode class=\"literal\">--wait[=\u003Cspan class=\"emphasis\">\u003Cem>n\u003C/em>\u003C/span>]\u003C/code> parameter for the \u003Ccode class=\"literal\">pcs resource move\u003C/code> command to indicate the number of seconds to wait for the resource to start on the destination node before returning 0 if the resource is started or 1 if the resource has not yet started. If you do not specify n, it defaults to a value of 60 minutes.\n\t\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"moving_a_resource_to_its_preferred_node\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch4 class=\"title\">31.2.2. Moving a resource to its preferred node\u003C/h4>\u003C/div>\u003C/div>\u003C/div>\u003Cp>\n\t\t\t\t\tAfter a resource has moved, either due to a failover or to an administrator manually moving the node, it will not necessarily move back to its original node even after the circumstances that caused the failover have been corrected. To relocate resources to their preferred node, use the following command. A preferred node is determined by the current cluster status, constraints, resource location, and other settings and may change over time.\n\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource relocate run [\u003Cspan class=\"emphasis\">\u003Cem>resource1\u003C/em>\u003C/span>] [\u003Cspan class=\"emphasis\">\u003Cem>resource2\u003C/em>\u003C/span>] ...\u003C/pre>\u003Cp>\n\t\t\t\t\tIf you do not specify any resources, all resource are relocated to their preferred nodes.\n\t\t\t\t\u003C/p>\u003Cp>\n\t\t\t\t\tThis command calculates the preferred node for each resource while ignoring resource stickiness. After calculating the preferred node, it creates location constraints which will cause the resources to move to their preferred nodes. Once the resources have been moved, the constraints are deleted automatically. To remove all constraints created by the \u003Ccode class=\"literal command\">pcs resource relocate run\u003C/code> command, you can enter the \u003Ccode class=\"literal command\">pcs resource relocate clear\u003C/code> command. To display the current status of resources and their optimal node ignoring resource stickiness, enter the \u003Ccode class=\"literal command\">pcs resource relocate show\u003C/code> command.\n\t\t\t\t\u003C/p>\u003C/section>\u003C/section>\u003Csection class=\"section\" id=\"proc_disabling-resources-cluster-maintenance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">31.3. Disabling, enabling, and banning cluster resources\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIn addition to the \u003Ccode class=\"literal command\">pcs resource move\u003C/code> and \u003Ccode class=\"literal command\">pcs resource relocate\u003C/code> commands, there are a variety of other commands you can use to control the behavior of cluster resources.\n\t\t\t\u003C/p>\u003Ch5 id=\"disabling_a_cluster_resource\">Disabling a cluster resource\u003C/h5>\u003Cp>\n\t\t\t\tYou can manually stop a running resource and prevent the cluster from starting it again with the following command. Depending on the rest of the configuration (constraints, options, failures, and so on), the resource may remain started. If you specify the \u003Ccode class=\"literal option\">--wait\u003C/code> option, \u003Cspan class=\"strong strong\">\u003Cstrong>\u003Cspan class=\"application application\">pcs\u003C/span>\u003C/strong>\u003C/span> will wait up to 'n' seconds for the resource to stop and then return 0 if the resource is stopped or 1 if the resource has not stopped. If 'n' is not specified it defaults to 60 minutes.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource disable \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [--wait[=\u003Cspan class=\"emphasis\">\u003Cem>n\u003C/em>\u003C/span>]]\u003C/pre>\u003Cp>\n\t\t\t\tYou can specify that a resource be disabled only if disabling the resource would not have an effect on other resources. Ensuring that this would be the case can be impossible to do by hand when complex resource relations are set up.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe \u003Ccode class=\"literal command\">pcs resource disable --simulate\u003C/code> command shows the effects of disabling a resource while not changing the cluster configuration.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe \u003Ccode class=\"literal command\">pcs resource disable --safe\u003C/code> command disables a resource only if no other resources would be affected in any way, such as being migrated from one node to another. The \u003Ccode class=\"literal command\">pcs resource safe-disable\u003C/code> command is an alias for the \u003Ccode class=\"literal\">pcs resource disable --safe\u003C/code> command.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe \u003Ccode class=\"literal command\">pcs resource disable --safe --no-strict\u003C/code> command disables a resource only if no other resources would be stopped or demoted\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tYou can specify the \u003Ccode class=\"literal\">--brief\u003C/code> option for the \u003Ccode class=\"literal\">pcs resource disable --safe\u003C/code> command to print errors only. The error report that the \u003Ccode class=\"literal\">pcs resource disable --safe\u003C/code> command generates if the safe disable operation fails contains the affected resource IDs. If you need to know only the resource IDs of resources that would be affected by disabling a resource, use the \u003Ccode class=\"literal\">--brief\u003C/code> option, which does not provide the full simulation result.\n\t\t\t\u003C/p>\u003Ch5 id=\"enabling_a_cluster_resource\">Enabling a cluster resource\u003C/h5>\u003Cp>\n\t\t\t\tUse the following command to allow the cluster to start a resource. Depending on the rest of the configuration, the resource may remain stopped. If you specify the \u003Ccode class=\"literal option\">--wait\u003C/code> option, \u003Cspan class=\"strong strong\">\u003Cstrong>\u003Cspan class=\"application application\">pcs\u003C/span>\u003C/strong>\u003C/span> will wait up to 'n' seconds for the resource to start and then return 0 if the resource is started or 1 if the resource has not started. If 'n' is not specified it defaults to 60 minutes.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource enable \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [--wait[=\u003Cspan class=\"emphasis\">\u003Cem>n\u003C/em>\u003C/span>]]\u003C/pre>\u003Ch5 id=\"preventing_a_resource_from_running_on_a_particular_node\">Preventing a resource from running on a particular node\u003C/h5>\u003Cp>\n\t\t\t\tUse the following command to prevent a resource from running on a specified node, or on the current node if no node is specified.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource ban \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span> [\u003Cspan class=\"emphasis\">\u003Cem>node\u003C/em>\u003C/span>] [--promoted] [lifetime=\u003Cspan class=\"emphasis\">\u003Cem>lifetime\u003C/em>\u003C/span>] [--wait[=\u003Cspan class=\"emphasis\">\u003Cem>n\u003C/em>\u003C/span>]]\u003C/pre>\u003Cp>\n\t\t\t\tNote that when you execute the \u003Ccode class=\"literal command\">pcs resource ban\u003C/code> command, this adds a -INFINITY location constraint to the resource to prevent it from running on the indicated node. You can execute the \u003Ccode class=\"literal command\">pcs resource clear\u003C/code> or the \u003Ccode class=\"literal command\">pcs constraint delete\u003C/code> command to remove the constraint. This does not necessarily move the resources back to the indicated node; where the resources can run at that point depends on how you have configured your resources initially.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf you specify the \u003Ccode class=\"literal\">--promoted\u003C/code> parameter of the \u003Ccode class=\"literal command\">pcs resource ban\u003C/code> command, the scope of the constraint is limited to the promoted role and you must specify \u003Cspan class=\"emphasis\">\u003Cem>promotable_id\u003C/em>\u003C/span> rather than \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can optionally configure a \u003Ccode class=\"literal\">lifetime\u003C/code> parameter for the \u003Ccode class=\"literal\">pcs resource ban\u003C/code> command to indicate a period of time the constraint should remain.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tYou can optionally configure a \u003Ccode class=\"literal\">--wait[=\u003Cspan class=\"emphasis\">\u003Cem>n\u003C/em>\u003C/span>]\u003C/code> parameter for the \u003Ccode class=\"literal\">pcs resource ban\u003C/code> command to indicate the number of seconds to wait for the resource to start on the destination node before returning 0 if the resource is started or 1 if the resource has not yet started. If you do not specify n, the default resource timeout will be used.\n\t\t\t\u003C/p>\u003Ch5 id=\"forcing_a_resource_to_start_on_the_current_node\">Forcing a resource to start on the current node\u003C/h5>\u003Cp>\n\t\t\t\tUse the \u003Ccode class=\"literal command\">debug-start\u003C/code> parameter of the \u003Ccode class=\"literal command\">pcs resource\u003C/code> command to force a specified resource to start on the current node, ignoring the cluster recommendations and printing the output from starting the resource. This is mainly used for debugging resources; starting resources on a cluster is (almost) always done by Pacemaker and not directly with a \u003Ccode class=\"literal command\">pcs\u003C/code> command. If your resource is not starting, it is usually due to either a misconfiguration of the resource (which you debug in the system log), constraints that prevent the resource from starting, or the resource being disabled. You can use this command to test resource configuration, but it should not normally be used to start resources in a cluster.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe format of the \u003Ccode class=\"literal command\">debug-start\u003C/code> command is as follows.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource debug-start \u003Cspan class=\"emphasis\">\u003Cem>resource_id\u003C/em>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_unmanaging-resources-cluster-maintenance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">31.4. Setting a resource to unmanaged mode\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen a resource is in \u003Ccode class=\"literal\">unmanaged\u003C/code> mode, the resource is still in the configuration but Pacemaker does not manage the resource.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following command sets the indicated resources to \u003Ccode class=\"literal\">unmanaged\u003C/code> mode.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource unmanage \u003Cspan class=\"emphasis\">\u003Cem>resource1\u003C/em>\u003C/span>  [\u003Cspan class=\"emphasis\">\u003Cem>resource2\u003C/em>\u003C/span>] ...\u003C/pre>\u003Cp>\n\t\t\t\tThe following command sets resources to \u003Ccode class=\"literal\">managed\u003C/code> mode, which is the default state.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource manage \u003Cspan class=\"emphasis\">\u003Cem>resource1\u003C/em>\u003C/span>  [\u003Cspan class=\"emphasis\">\u003Cem>resource2\u003C/em>\u003C/span>] ...\u003C/pre>\u003Cp>\n\t\t\t\tYou can specify the name of a resource group with the \u003Ccode class=\"literal command\">pcs resource manage\u003C/code> or \u003Ccode class=\"literal command\">pcs resource unmanage\u003C/code> command. The command will act on all of the resources in the group, so that you can set all of the resources in a group to \u003Ccode class=\"literal\">managed\u003C/code> or \u003Ccode class=\"literal\">unmanaged\u003C/code> mode with a single command and then manage the contained resources individually.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_setting-maintenance-mode-cluster-maintenance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">31.5. Putting a cluster in maintenance mode\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen a cluster is in maintenance mode, the cluster does not start or stop any services until told otherwise. When maintenance mode is completed, the cluster does a sanity check of the current state of any services, and then stops or starts any that need it.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tTo put a cluster in maintenance mode, use the following command to set the \u003Ccode class=\"literal\">maintenance-mode\u003C/code> cluster property to \u003Ccode class=\"literal\">true\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set maintenance-mode=true\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tTo remove a cluster from maintenance mode, use the following command to set the \u003Ccode class=\"literal\">maintenance-mode\u003C/code> cluster property to \u003Ccode class=\"literal\">false\u003C/code>.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set maintenance-mode=false\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tYou can remove a cluster property from the configuration with the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs property unset \u003Cspan class=\"emphasis\">\u003Cem>property\u003C/em>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tAlternately, you can remove a cluster property from a configuration by leaving the value field of the \u003Ccode class=\"literal command\">pcs property set\u003C/code> command blank. This restores that property to its default value. For example, if you have previously set the \u003Ccode class=\"literal\">symmetric-cluster\u003C/code> property to \u003Ccode class=\"literal\">false\u003C/code>, the following command removes the value you have set from the configuration and restores the value of \u003Ccode class=\"literal\">symmetric-cluster\u003C/code> to \u003Ccode class=\"literal\">true\u003C/code>, which is its default value.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs property set symmetric-cluster=\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003Csection class=\"section\" id=\"proc_updating-cluster-packages-cluster-maintenance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">31.6. Updating a RHEL high availability cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tUpdating packages that make up the RHEL High Availability and Resilient Storage Add-Ons, either individually or as a whole, can be done in one of two general ways:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Cspan class=\"emphasis\">\u003Cem>Rolling Updates\u003C/em>\u003C/span>: Remove one node at a time from service, update its software, then integrate it back into the cluster. This allows the cluster to continue providing service and managing resources while each node is updated.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\t\u003Cspan class=\"emphasis\">\u003Cem>Entire Cluster Update\u003C/em>\u003C/span>: Stop the entire cluster, apply updates to all nodes, then start the cluster back up.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Crh-alert class=\"admonition warning\" state=\"danger\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Warning\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tIt is critical that when performing software update procedures for Red Hat Enterprise Linux High Availability and Resilient Storage clusters, you ensure that any node that will undergo updates is not an active member of the cluster before those updates are initiated.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tFor a full description of each of these methods and the procedures to follow for the updates, see \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/2059253/\">Recommended Practices for Applying Software Updates to a RHEL High Availability or Resilient Storage Cluster\u003C/a>.\n\t\t\t\u003C/p>\u003C/section>\u003Csection class=\"section\" id=\"proc_upgrading-remote-nodes-cluster-maintenance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">31.7. Upgrading remote nodes and guest nodes\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tIf the \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service is stopped on an active remote node or guest node, the cluster will gracefully migrate resources off the node before stopping the node. This allows you to perform software upgrades and other routine maintenance procedures without removing the node from the cluster. Once \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> is shut down, however, the cluster will immediately try to reconnect. If \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> is not restarted within the resource’s monitor timeout, the cluster will consider the monitor operation as failed.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tIf you wish to avoid monitor failures when the \u003Ccode class=\"literal\">pacemaker_remote\u003C/code> service is stopped on an active Pacemaker Remote node, you can use the following procedure to take the node out of the cluster before performing any system administration that might stop \u003Ccode class=\"literal\">pacemaker_remote\u003C/code>.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tStop the node’s connection resource with the \u003Ccode class=\"literal command\">pcs resource disable \u003Cspan class=\"emphasis\">\u003Cem>resourcename\u003C/em>\u003C/span>\u003C/code> command, which will move all services off the node. The connection resource would be the \u003Ccode class=\"literal\">ocf:pacemaker:remote\u003C/code> resource for a remote node or, commonly, the \u003Ccode class=\"literal\">ocf:heartbeat:VirtualDomain\u003C/code> resource for a guest node. For guest nodes, this command will also stop the VM, so the VM must be started outside the cluster (for example, using \u003Ccode class=\"literal command\">virsh\u003C/code>) to perform any maintenance.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource disable \u003Cspan class=\"emphasis\">\u003Cem>resourcename\u003C/em>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tPerform the required maintenance.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tWhen ready to return the node to the cluster, re-enable the resource with the \u003Ccode class=\"literal command\">pcs resource enable\u003C/code> command.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">pcs resource enable \u003Cspan class=\"emphasis\">\u003Cem>resourcename\u003C/em>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_migrating-cluster-vms-cluster-maintenance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">31.8. Migrating VMs in a RHEL cluster\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tRed Hat does not support live migration of active cluster nodes across hypervisors or hosts, as noted in \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/3131111/\">Support Policies for RHEL High Availability Clusters - General Conditions with Virtualized Cluster Members\u003C/a>. If you need to perform a live migration, you will first need to stop the cluster services on the VM to remove the node from the cluster, and then start the cluster back up after performing the migration. The following steps outline the procedure for removing a VM from a cluster, migrating the VM, and restoring the VM to the cluster.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThe following steps outline the procedure for removing a VM from a cluster, migrating the VM, and restoring the VM to the cluster.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tThis procedure applies to VMs that are used as full cluster nodes, not to VMs managed as cluster resources (including VMs used as guest nodes) which can be live-migrated without special precautions. For general information about the fuller procedure required for updating packages that make up the RHEL High Availability and Resilient Storage Add-Ons, either individually or as a whole, see \u003Ca class=\"link\" href=\"https://access.redhat.com/articles/2059253/\">Recommended Practices for Applying Software Updates to a RHEL High Availability or Resilient Storage Cluster\u003C/a>.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tBefore performing this procedure, consider the effect on cluster quorum of removing a cluster node. For example, if you have a three-node cluster and you remove one node, your cluster can not withstand any node failure. This is because if one node of a three-node cluster is already down, removing a second node will lose quorum.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tIf any preparations need to be made before stopping or moving the resources or software running on the VM to migrate, perform those steps.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tRun the following command on the VM to stop the cluster software on the VM.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster stop\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tPerform the live migration of the VM.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tStart cluster services on the VM.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster start\u003C/strong>\u003C/span>\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"identifying-cluster-by-uuid-cluster-maintenance\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">31.9. Identifying clusters by UUID\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tAs of Red Hat Enterprise Linux 9.1, when you create a cluster it has an associated UUID. Since a cluster name is not a unique cluster identifier, a third-party tool such as a configuration management database that manages multiple clusters with the same name can uniquely identify a cluster by means of its UUID. You can display the current cluster UUID with the \u003Ccode class=\"literal\">pcs cluster config [show]\u003C/code> command, which includes the cluster UUID in its output.\n\t\t\t\u003C/p>\u003Cp>\n\t\t\t\tTo add a UUID to an existing cluster, run the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster config uuid generate\u003C/strong>\u003C/span>\u003C/pre>\u003Cp>\n\t\t\t\tTo regenerate a UUID for a cluster with an existing UUID, run the following command.\n\t\t\t\u003C/p>\u003Cpre class=\"literallayout\"># \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster config uuid generate --force\u003C/strong>\u003C/span>\u003C/pre>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"assembly_configuring-disaster-recovery-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 32. Configuring disaster recovery clusters\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tOne method of providing disaster recovery for a high availability cluster is to configure two clusters. You can then configure one cluster as your primary site cluster, and the second cluster as your disaster recovery cluster.\n\t\t\u003C/p>\u003Cp>\n\t\t\tIn normal circumstances, the primary cluster is running resources in production mode. The disaster recovery cluster has all the resources configured as well and is either running them in demoted mode or not at all. For example, there may be a database running in the primary cluster in promoted mode and running in the disaster recovery cluster in demoted mode. The database in this setup would be configured so that data is synchronized from the primary to disaster recovery site. This is done through the database configuration itself rather than through the \u003Ccode class=\"literal\">pcs\u003C/code> command interface.\n\t\t\u003C/p>\u003Cp>\n\t\t\tWhen the primary cluster goes down, users can use the \u003Ccode class=\"literal\">pcs\u003C/code> command interface to manually fail the resources over to the disaster recovery site. They can then log in to the disaster site and promote and start the resources there. Once the primary cluster has recovered, users can use the \u003Ccode class=\"literal\">pcs\u003C/code> command interface to manually move resources back to the primary site.\n\t\t\u003C/p>\u003Cp>\n\t\t\tYou can use the \u003Ccode class=\"literal\">pcs\u003C/code> command to display the status of both the primary and the disaster recovery site cluster from a single node on either site.\n\t\t\u003C/p>\u003Csection class=\"section\" id=\"ref_recovery-considerations-configuring-disaster-recovery\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">32.1. Considerations for disaster recovery clusters\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tWhen planning and configuring a disaster recovery site that you will manage and monitor with the \u003Ccode class=\"literal\">pcs\u003C/code> command interface, note the following considerations.\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe disaster recovery site must be a cluster. This makes it possible to configure it with same tools and similar procedures as the primary site.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe primary and disaster recovery clusters are created by independent \u003Ccode class=\"literal\">pcs cluster setup\u003C/code> commands.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe clusters and their resources must be configured so that that the data is synchronized and failover is possible.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe cluster nodes in the recovery site can not have the same names as the nodes in the primary site.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe pcs user \u003Ccode class=\"literal\">hacluster\u003C/code> must be authenticated for each node in both clusters on the node from which you will be running \u003Ccode class=\"literal\">pcs\u003C/code> commands.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Csection class=\"section\" id=\"proc_disaster-recovery-display-configuring-disaster-recovery\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch3 class=\"title\">32.2. Displaying status of recovery clusters\u003C/h3>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\t\tTo configure a primary and a disaster recovery cluster so that you can display the status of both clusters, perform the following procedure.\n\t\t\t\u003C/p>\u003Crh-alert class=\"admonition note\" state=\"info\">\u003Cdiv class=\"admonition_header\" slot=\"header\">Note\u003C/div>\u003Cdiv>\u003Cp>\n\t\t\t\t\tSetting up a disaster recovery cluster does not automatically configure resources or replicate data. Those items must be configured manually by the user.\n\t\t\t\t\u003C/p>\u003C/div>\u003C/rh-alert>\u003Cp>\n\t\t\t\tIn this example:\n\t\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe primary cluster will be named \u003Ccode class=\"literal\">PrimarySite\u003C/code> and will consist of the nodes \u003Ccode class=\"literal\">z1.example.com\u003C/code>. and \u003Ccode class=\"literal\">z2.example.com\u003C/code>.\n\t\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\tThe disaster recovery site cluster will be named \u003Ccode class=\"literal\">DRsite\u003C/code> and will consist of the nodes \u003Ccode class=\"literal\">z3.example.com\u003C/code> and \u003Ccode class=\"literal\">z4.example.com\u003C/code>.\n\t\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\t\tThis example sets up a basic cluster with no resources or fencing configured.\n\t\t\t\u003C/p>\u003Cdiv class=\"orderedlist\">\u003Cp class=\"title\">\u003Cstrong>Procedure\u003C/strong>\u003C/p>\u003Col class=\"orderedlist\" type=\"1\">\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tAuthenticate all of the nodes that will be used for both clusters.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs host auth z1.example.com z2.example.com z3.example.com z4.example.com -u hacluster -p password\u003C/strong>\u003C/span>\nz1.example.com: Authorized\nz2.example.com: Authorized\nz3.example.com: Authorized\nz4.example.com: Authorized\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate the cluster that will be used as the primary cluster and start cluster services for the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster setup PrimarySite z1.example.com z2.example.com --start\u003C/strong>\u003C/span>\n{...}\nCluster has been successfully set up.\nStarting cluster on hosts: 'z1.example.com', 'z2.example.com'...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCreate the cluster that will be used as the disaster recovery cluster and start cluster services for the cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs cluster setup DRSite z3.example.com z4.example.com --start\u003C/strong>\u003C/span>\n{...}\nCluster has been successfully set up.\nStarting cluster on hosts: 'z3.example.com', 'z4.example.com'...\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tFrom a node in the primary cluster, set up the second cluster as the recovery site. The recovery site is defined by a name of one of its nodes.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs dr set-recovery-site z3.example.com\u003C/strong>\u003C/span>\nSending 'disaster-recovery config' to 'z3.example.com', 'z4.example.com'\nz3.example.com: successful distribution of the file 'disaster-recovery config'\nz4.example.com: successful distribution of the file 'disaster-recovery config'\nSending 'disaster-recovery config' to 'z1.example.com', 'z2.example.com'\nz1.example.com: successful distribution of the file 'disaster-recovery config'\nz2.example.com: successful distribution of the file 'disaster-recovery config'\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCheck the disaster recovery configuration.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs dr config\u003C/strong>\u003C/span>\nLocal site:\n  Role: Primary\nRemote site:\n  Role: Recovery\n  Nodes:\n    z3.example.com\n    z4.example.com\u003C/pre>\u003C/li>\u003Cli class=\"listitem\">\u003Cp class=\"simpara\">\n\t\t\t\t\t\tCheck the status of the primary cluster and the disaster recovery cluster from a node in the primary cluster.\n\t\t\t\t\t\u003C/p>\u003Cpre class=\"literallayout\">[root@z1 ~]# \u003Cspan class=\"strong strong\">\u003Cstrong>pcs dr status\u003C/strong>\u003C/span>\n--- Local cluster - Primary site ---\nCluster name: PrimarySite\n\nWARNINGS:\nNo stonith devices and stonith-enabled is not false\n\nCluster Summary:\n  * Stack: corosync\n  * Current DC: z2.example.com (version 2.0.3-2.el8-2c9cea563e) - partition with quorum\n  * Last updated: Mon Dec  9 04:10:31 2019\n  * Last change:  Mon Dec  9 04:06:10 2019 by hacluster via crmd on z2.example.com\n  * 2 nodes configured\n  * 0 resource instances configured\n\nNode List:\n  * Online: [ z1.example.com z2.example.com ]\n\nFull List of Resources:\n  * No resources\n\nDaemon Status:\n  corosync: active/disabled\n  pacemaker: active/disabled\n  pcsd: active/enabled\n\n\n--- Remote cluster - Recovery site ---\nCluster name: DRSite\n\nWARNINGS:\nNo stonith devices and stonith-enabled is not false\n\nCluster Summary:\n  * Stack: corosync\n  * Current DC: z4.example.com (version 2.0.3-2.el8-2c9cea563e) - partition with quorum\n  * Last updated: Mon Dec  9 04:10:34 2019\n  * Last change:  Mon Dec  9 04:09:55 2019 by hacluster via crmd on z4.example.com\n  * 2 nodes configured\n  * 0 resource instances configured\n\nNode List:\n  * Online: [ z3.example.com z4.example.com ]\n\nFull List of Resources:\n  * No resources\n\nDaemon Status:\n  corosync: active/disabled\n  pacemaker: active/disabled\n  pcsd: active/enabled\u003C/pre>\u003C/li>\u003C/ol>\u003C/div>\u003Cp>\n\t\t\t\tFor additional display options for a disaster recovery configuration, see the help screen for the \u003Ccode class=\"literal\">pcs dr\u003C/code> command.\n\t\t\t\u003C/p>\u003C/section>\u003C/section>\u003Csection class=\"chapter\" id=\"ref_interpreting-resource-exit-codes-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 33. Interpreting resource agent OCF return codes\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tPacemaker resource agents conform to the Open Cluster Framework (OCF) Resource Agent API. This following tables describe the OCF return codes and how they are interpreted by Pacemaker.\n\t\t\u003C/p>\u003Cp>\n\t\t\tThe first thing the cluster does when an agent returns a code is to check the return code against the expected result. If the result does not match the expected value, then the operation is considered to have failed, and recovery action is initiated.\n\t\t\u003C/p>\u003Cp>\n\t\t\tFor any invocation, resource agents must exit with a defined return code that informs the caller of the outcome of the invoked action.\n\t\t\u003C/p>\u003Cp>\n\t\t\tThere are three types of failure recovery, as described in the following table.\n\t\t\u003C/p>\u003Crh-table id=\"idm140686139848096\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 33.1. Types of Recovery Performed by the Cluster\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 14%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 43%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 43%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686139842304\" scope=\"col\">Type\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686139841216\" scope=\"col\">Description\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686139840128\" scope=\"col\">Action Taken by the Cluster\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139842304\"> \u003Cp>\n\t\t\t\t\t\t\tsoft\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139841216\"> \u003Cp>\n\t\t\t\t\t\t\tA transient error occurred.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139840128\"> \u003Cp>\n\t\t\t\t\t\t\tRestart the resource or move it to a new location .\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139842304\"> \u003Cp>\n\t\t\t\t\t\t\thard\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139841216\"> \u003Cp>\n\t\t\t\t\t\t\tA non-transient error that may be specific to the current node occurred.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139840128\"> \u003Cp>\n\t\t\t\t\t\t\tMove the resource elsewhere and prevent it from being retried on the current node.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139842304\"> \u003Cp>\n\t\t\t\t\t\t\tfatal\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139841216\"> \u003Cp>\n\t\t\t\t\t\t\tA non-transient error that will be common to all cluster nodes occurred (for example, a bad configuration was specified).\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686139840128\"> \u003Cp>\n\t\t\t\t\t\t\tStop the resource and prevent it from being started on any cluster node.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003Cp>\n\t\t\tThe following table provides The OCF return codes and the type of recovery the cluster will initiate when a failure code is received.Note that even actions that return 0 (OCF alias \u003Ccode class=\"literal\">OCF_SUCCESS\u003C/code>) can be considered to have failed, if 0 was not the expected return value.\n\t\t\u003C/p>\u003Crh-table id=\"idm140686136972976\">\u003Ctable class=\"lt-4-cols lt-7-rows\">\u003Ccaption>Table 33.2. OCF Return Codes\u003C/caption>\u003Ccolgroup>\u003Ccol style=\"width: 11%; \" class=\"col_1\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 33%; \" class=\"col_2\">\u003C!--Empty-->\u003C/col>\u003Ccol style=\"width: 56%; \" class=\"col_3\">\u003C!--Empty-->\u003C/col>\u003C/colgroup>\u003Cthead>\u003Ctr>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686136967216\" scope=\"col\">Return Code\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686136966128\" scope=\"col\">OCF Label\u003C/th>\u003Cth align=\"left\" valign=\"top\" id=\"idm140686136965040\" scope=\"col\">Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136967216\"> \u003Cp>\n\t\t\t\t\t\t\t0\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136966128\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">OCF_SUCCESS\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136965040\"> \u003Cp>\n\t\t\t\t\t\t\t* The action completed successfully. This is the expected return code for any successful start, stop, promote, and demote command.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* Type if unexpected: soft\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136967216\"> \u003Cp>\n\t\t\t\t\t\t\t1\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136966128\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">OCF_ERR_GENERIC\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136965040\"> \u003Cp>\n\t\t\t\t\t\t\t* The action returned a generic error.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* Type: soft\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* The resource manager will attempt to recover the resource or move it to a new location.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136967216\"> \u003Cp>\n\t\t\t\t\t\t\t2\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136966128\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">OCF_ERR_ARGS\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136965040\"> \u003Cp>\n\t\t\t\t\t\t\t* The resource’s configuration is not valid on this machine. For example, it refers to a location not found on the node.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* Type: hard\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* The resource manager will move the resource elsewhere and prevent it from being retried on the current node\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136967216\"> \u003Cp>\n\t\t\t\t\t\t\t3\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136966128\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">OCF_ERR_UNIMPLEMENTED\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136965040\"> \u003Cp>\n\t\t\t\t\t\t\t* The requested action is not implemented.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* Type: hard\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136967216\"> \u003Cp>\n\t\t\t\t\t\t\t4\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136966128\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">OCF_ERR_PERM\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136965040\"> \u003Cp>\n\t\t\t\t\t\t\t* The resource agent does not have sufficient privileges to complete the task. This may be due, for example, to the agent not being able to open a certain file, to listen on a specific socket, or to write to a directory.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* Type: hard\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* Unless specifically configured otherwise, the resource manager will attempt to recover a resource which failed with this error by restarting the resource on a different node (where the permission problem may not exist).\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136967216\"> \u003Cp>\n\t\t\t\t\t\t\t5\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136966128\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">OCF_ERR_INSTALLED\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136965040\"> \u003Cp>\n\t\t\t\t\t\t\t* A required component is missing on the node where the action was executed. This may be due to a required binary not being executable, or a vital configuration file being unreadable.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* Type: hard\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* Unless specifically configured otherwise, the resource manager will attempt to recover a resource which failed with this error by restarting the resource on a different node (where the required files or binaries may be present).\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136967216\"> \u003Cp>\n\t\t\t\t\t\t\t6\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136966128\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">OCF_ERR_CONFIGURED\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136965040\"> \u003Cp>\n\t\t\t\t\t\t\t* The resource’s configuration on the local node is invalid.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* Type: fatal\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* When this code is returned, Pacemaker will prevent the resource from running on any node in the cluster, even if the service configuraiton is valid on some other node.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136967216\"> \u003Cp>\n\t\t\t\t\t\t\t7\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136966128\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">OCF_NOT_RUNNING\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136965040\"> \u003Cp>\n\t\t\t\t\t\t\t* The resource is safely stopped. This implies that the resource has either gracefully shut down, or has never been started.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* Type if unexpected: soft\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* The cluster will not attempt to stop a resource that returns this for any action.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136967216\"> \u003Cp>\n\t\t\t\t\t\t\t8\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136966128\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">OCF_RUNNING_PROMOTED\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136965040\"> \u003Cp>\n\t\t\t\t\t\t\t* The resource is running in promoted role.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* Type if unexpected: soft\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136967216\"> \u003Cp>\n\t\t\t\t\t\t\t9\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136966128\"> \u003Cp>\n\t\t\t\t\t\t\t\u003Ccode class=\"literal\">OCF_FAILED_PROMOTED\u003C/code>\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136965040\"> \u003Cp>\n\t\t\t\t\t\t\t* The resource is (or might be) in promoted role but has failed.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* Type: soft\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003Cp>\n\t\t\t\t\t\t\t* The resource will be demoted, stopped and then started (and possibly promoted) again.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136967216\"> \u003Cp>\n\t\t\t\t\t\t\t190\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136966128\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136965040\"> \u003Cp>\n\t\t\t\t\t\t\t* The service is found to be properly active, but in such a condition that future failures are more likely.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136967216\"> \u003Cp>\n\t\t\t\t\t\t\t191\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136966128\"> \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136965040\"> \u003Cp>\n\t\t\t\t\t\t\t* The resource agent supports roles and the service is found to be properly active in the promoted role, but in such a condition that future failures are more likely.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136967216\"> \u003Cp>\n\t\t\t\t\t\t\tother\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136966128\"> \u003Cp>\n\t\t\t\t\t\t\tN/A\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003Ctd align=\"left\" valign=\"top\" headers=\"idm140686136965040\"> \u003Cp>\n\t\t\t\t\t\t\tCustom error code.\n\t\t\t\t\t\t\u003C/p>\n\t\t\t\t\t\t \u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/rh-table>\u003C/section>\u003Csection class=\"chapter\" id=\"ref_ibmz-configuring-and-managing-high-availability-clusters\">\u003Cdiv class=\"titlepage\">\u003Cdiv>\u003Cdiv>\u003Ch2 class=\"title\">Chapter 34. Configuring a Red Hat High Availability cluster with IBM z/VM instances as cluster members\u003C/h2>\u003C/div>\u003C/div>\u003C/div>\u003Cp class=\"_abstract _abstract\">\n\t\t\tRed Hat provides several articles that may be useful when designing, configuring, and administering a Red Hat High Availability cluster running on z/VM virtual machines.\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\u003Ca class=\"link\" href=\"https://access.redhat.com/articles/1543363\">Design Guidance for RHEL High Availability Clusters - IBM z/VM Instances as Cluster Members\u003C/a>\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\u003Ca class=\"link\" href=\"https://access.redhat.com/articles/3331981\">Administrative Procedures for RHEL High Availability Clusters - Configuring z/VM SMAPI Fencing with fence_zvmip for RHEL 7 or 8 IBM z Systems Cluster Members\u003C/a>\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\u003Ca class=\"link\" href=\"https://access.redhat.com/solutions/3555071\">RHEL High Availability cluster nodes on IBM z Systems experience STONITH-device timeouts around midnight on a nightly basis\u003C/a> (Red Hat Knowledgebase)\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\u003Ca class=\"link\" href=\"https://access.redhat.com/articles/3332491\">Administrative Procedures for RHEL High Availability Clusters - Preparing a dasd Storage Device for Use by a Cluster of IBM z Systems Members\u003C/a>\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003Cp>\n\t\t\tYou may also find the following articles useful when designing a Red Hat High Availability cluster in general.\n\t\t\u003C/p>\u003Cdiv class=\"itemizedlist\">\u003Cul class=\"itemizedlist\" type=\"disc\">\u003Cli class=\"listitem\">\n\t\t\t\t\t\u003Ca class=\"link\" href=\"https://access.redhat.com/articles/2912891\">Support Policies for RHEL High Availability Clusters\u003C/a>\n\t\t\t\t\u003C/li>\u003Cli class=\"listitem\">\n\t\t\t\t\t\u003Ca class=\"link\" href=\"https://access.redhat.com/articles/3099541\">Exploring Concepts of RHEL High Availability Clusters - Fencing/STONITH\u003C/a>\n\t\t\t\t\u003C/li>\u003C/ul>\u003C/div>\u003C/section>\u003Cdiv>\u003Cdiv xml:lang=\"en-US\" class=\"legalnotice\" id=\"idm140686156711152\">\u003Ch2 class=\"legalnotice\">Legal Notice\u003C/h2>\u003Cdiv class=\"para\">\n\t\tCopyright \u003Cspan class=\"trademark\">\u003C!--Empty-->\u003C/span>© 2024 Red Hat, Inc.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\tThe text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license (\"CC-BY-SA\"). An explanation of CC-BY-SA is available at \u003Ca class=\"uri\" href=\"http://creativecommons.org/licenses/by-sa/3.0/\">http://creativecommons.org/licenses/by-sa/3.0/\u003C/a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\tRed Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\tRed Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\t\u003Cspan class=\"trademark\">Linux\u003C/span>® is the registered trademark of Linus Torvalds in the United States and other countries.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\t\u003Cspan class=\"trademark\">Java\u003C/span>® is a registered trademark of Oracle and/or its affiliates.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\t\u003Cspan class=\"trademark\">XFS\u003C/span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\t\u003Cspan class=\"trademark\">MySQL\u003C/span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\t\u003Cspan class=\"trademark\">Node.js\u003C/span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\tThe \u003Cspan class=\"trademark\">OpenStack\u003C/span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.\n\t\u003C/div>\u003Cdiv class=\"para\">\n\t\tAll other trademarks are the property of their respective owners.\n\t\u003C/div>\u003C/div>\u003C/div>\u003C/div>\u003C/body>",[14,21,26,84,97,138,171,192,222,249,271,285,362,425,459,481,499,504,531,583,614,673,687,718,740,754,798,812,834,869,903,917,961,1012,1026,1031,1036],{"title":11,"visible":15,"weight":16,"urlFragment":17,"anchor":18,"singlePageAnchor":18,"docTitle":19,"url":20},true,1,"index",null,"configuring_and_managing_high_availability_clusters","#",{"title":22,"visible":15,"weight":23,"urlFragment":24,"anchor":18,"singlePageAnchor":24,"docTitle":19,"url":25},"Providing feedback on Red Hat documentation",2,"proc_providing-feedback-on-red-hat-documentation_configuring-and-managing-high-availability-clusters","#proc_providing-feedback-on-red-hat-documentation_configuring-and-managing-high-availability-clusters",{"title":27,"visible":15,"weight":28,"urlFragment":29,"anchor":18,"singlePageAnchor":29,"sections":30,"docTitle":19,"url":83},"1. High Availability Add-On overview",3,"assembly_overview-of-high-availability-configuring-and-managing-high-availability-clusters",[31,35,52,69],{"title":32,"visible":15,"weight":16,"urlFragment":29,"anchor":33,"singlePageAnchor":33,"docTitle":19,"url":34},"1.1. High Availability Add-On components","con_high-availability-add-on-components-overview-of-high-availability","#con_high-availability-add-on-components-overview-of-high-availability",{"title":36,"visible":15,"weight":23,"urlFragment":29,"anchor":37,"singlePageAnchor":37,"sections":38,"docTitle":19,"url":51},"1.2. High Availability Add-On concepts","con_high-availability-add-on-concepts-overview-of-high-availability",[39,43,47],{"title":40,"visible":15,"weight":16,"urlFragment":29,"anchor":41,"singlePageAnchor":41,"docTitle":19,"url":42},"1.2.1. Fencing","fencing","#fencing",{"title":44,"visible":15,"weight":23,"urlFragment":29,"anchor":45,"singlePageAnchor":45,"docTitle":19,"url":46},"1.2.2. Quorum","quorum","#quorum",{"title":48,"visible":15,"weight":28,"urlFragment":29,"anchor":49,"singlePageAnchor":49,"docTitle":19,"url":50},"1.2.3. Cluster resources","cluster_resources","#cluster_resources","#con_high-availability-add-on-concepts-overview-of-high-availability",{"title":53,"visible":15,"weight":28,"urlFragment":29,"anchor":54,"singlePageAnchor":54,"sections":55,"docTitle":19,"url":68},"1.3. Pacemaker overview","con_pacemaker-overview-overview-of-high-availability",[56,60,64],{"title":57,"visible":15,"weight":16,"urlFragment":29,"anchor":58,"singlePageAnchor":58,"docTitle":19,"url":59},"1.3.1. Pacemaker architecture components","pacemaker_architecture_components","#pacemaker_architecture_components",{"title":61,"visible":15,"weight":23,"urlFragment":29,"anchor":62,"singlePageAnchor":62,"docTitle":19,"url":63},"1.3.2. Pacemaker configuration and management tools","pacemaker_configuration_and_management_tools","#pacemaker_configuration_and_management_tools",{"title":65,"visible":15,"weight":28,"urlFragment":29,"anchor":66,"singlePageAnchor":66,"docTitle":19,"url":67},"1.3.3. The cluster and Pacemaker configuration files","the_cluster_and_pacemaker_configuration_files","#the_cluster_and_pacemaker_configuration_files","#con_pacemaker-overview-overview-of-high-availability",{"title":70,"visible":15,"weight":71,"urlFragment":29,"anchor":72,"singlePageAnchor":72,"sections":73,"docTitle":19,"url":82},"1.4. LVM logical volumes in a Red Hat high availability cluster",4,"con_HA-lvm-shared-volumes-overview-of-high-availability",[74,78],{"title":75,"visible":15,"weight":16,"urlFragment":29,"anchor":76,"singlePageAnchor":76,"docTitle":19,"url":77},"1.4.1. Choosing HA-LVM or shared volumes","choosing_ha_lvm_or_shared_volumes","#choosing_ha_lvm_or_shared_volumes",{"title":79,"visible":15,"weight":23,"urlFragment":29,"anchor":80,"singlePageAnchor":80,"docTitle":19,"url":81},"1.4.2. Configuring LVM volumes in a cluster","configuring_lvm_volumes_in_a_cluster","#configuring_lvm_volumes_in_a_cluster","#con_HA-lvm-shared-volumes-overview-of-high-availability","#assembly_overview-of-high-availability-configuring-and-managing-high-availability-clusters",{"title":85,"visible":15,"weight":71,"urlFragment":86,"anchor":18,"singlePageAnchor":86,"sections":87,"docTitle":19,"url":96},"2. Getting started with Pacemaker","assembly_getting-started-with-pacemaker-configuring-and-managing-high-availability-clusters",[88,92],{"title":89,"visible":15,"weight":16,"urlFragment":86,"anchor":90,"singlePageAnchor":90,"docTitle":19,"url":91},"2.1. Learning to use Pacemaker","proc_learning-to-use-pacemaker-getting-started-with-pacemaker","#proc_learning-to-use-pacemaker-getting-started-with-pacemaker",{"title":93,"visible":15,"weight":23,"urlFragment":86,"anchor":94,"singlePageAnchor":94,"docTitle":19,"url":95},"2.2. Learning to configure failover","proc_learning-to-configure-failover-getting-started-with-pacemaker","#proc_learning-to-configure-failover-getting-started-with-pacemaker","#assembly_getting-started-with-pacemaker-configuring-and-managing-high-availability-clusters",{"title":98,"visible":15,"weight":99,"urlFragment":100,"anchor":18,"singlePageAnchor":100,"sections":101,"docTitle":19,"url":137},"3. The pcs command-line interface",5,"assembly_pcs-operation-configuring-and-managing-high-availability-clusters",[102,106,110,114,118,122,127,132],{"title":103,"visible":15,"weight":16,"urlFragment":100,"anchor":104,"singlePageAnchor":104,"docTitle":19,"url":105},"3.1. pcs help display","proc_pcs-help-pcs-operation","#proc_pcs-help-pcs-operation",{"title":107,"visible":15,"weight":23,"urlFragment":100,"anchor":108,"singlePageAnchor":108,"docTitle":19,"url":109},"3.2. Viewing the raw cluster configuration","proc_raw-config-pcs-operation","#proc_raw-config-pcs-operation",{"title":111,"visible":15,"weight":28,"urlFragment":100,"anchor":112,"singlePageAnchor":112,"docTitle":19,"url":113},"3.3. Saving a configuration change to a working file","proc_configure-testfile-pcs-operation","#proc_configure-testfile-pcs-operation",{"title":115,"visible":15,"weight":71,"urlFragment":100,"anchor":116,"singlePageAnchor":116,"docTitle":19,"url":117},"3.4. Displaying cluster status","proc_cluster-status-pcs-operation","#proc_cluster-status-pcs-operation",{"title":119,"visible":15,"weight":99,"urlFragment":100,"anchor":120,"singlePageAnchor":120,"docTitle":19,"url":121},"3.5. Displaying the full cluster configuration","proc_cluster-config-display-pcs-operation","#proc_cluster-config-display-pcs-operation",{"title":123,"visible":15,"weight":124,"urlFragment":100,"anchor":125,"singlePageAnchor":125,"docTitle":19,"url":126},"3.6. Displaying resource status",6,"proc_resource-status-pcs-operation","#proc_resource-status-pcs-operation",{"title":128,"visible":15,"weight":129,"urlFragment":100,"anchor":130,"singlePageAnchor":130,"docTitle":19,"url":131},"3.7. Modifying the corosync.conf file with the pcs command",7,"proc_pcs-corosync-manage-pcs-operation","#proc_pcs-corosync-manage-pcs-operation",{"title":133,"visible":15,"weight":134,"urlFragment":100,"anchor":135,"singlePageAnchor":135,"docTitle":19,"url":136},"3.8. Displaying the corosync.conf file with the pcs command",8,"proc_pcs-corosync-display-pcs-operation","#proc_pcs-corosync-display-pcs-operation","#assembly_pcs-operation-configuring-and-managing-high-availability-clusters",{"title":139,"visible":15,"weight":124,"urlFragment":140,"anchor":18,"singlePageAnchor":140,"sections":141,"docTitle":19,"url":170},"4. Creating a Red Hat High-Availability cluster with Pacemaker","assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters",[142,146,150,154,158,162,166],{"title":143,"visible":15,"weight":16,"urlFragment":140,"anchor":144,"singlePageAnchor":144,"docTitle":19,"url":145},"4.1. Installing cluster software","proc_installing-cluster-software-creating-high-availability-cluster","#proc_installing-cluster-software-creating-high-availability-cluster",{"title":147,"visible":15,"weight":23,"urlFragment":140,"anchor":148,"singlePageAnchor":148,"docTitle":19,"url":149},"4.2. Installing the pcp-zeroconf package (recommended)","proc_installing-pcp-zeroconf-creating-high-availability-cluster","#proc_installing-pcp-zeroconf-creating-high-availability-cluster",{"title":151,"visible":15,"weight":28,"urlFragment":140,"anchor":152,"singlePageAnchor":152,"docTitle":19,"url":153},"4.3. Creating a high availability cluster","proc_creating-high-availability-cluster-creating-high-availability-cluster","#proc_creating-high-availability-cluster-creating-high-availability-cluster",{"title":155,"visible":15,"weight":71,"urlFragment":140,"anchor":156,"singlePageAnchor":156,"docTitle":19,"url":157},"4.4. Creating a high availability cluster with multiple links","proc_configure-multiple-ip-cluster-creating-high-availability-cluster","#proc_configure-multiple-ip-cluster-creating-high-availability-cluster",{"title":159,"visible":15,"weight":99,"urlFragment":140,"anchor":160,"singlePageAnchor":160,"docTitle":19,"url":161},"4.5. Configuring fencing","proc_configuring-fencing-creating-high-availability-cluster","#proc_configuring-fencing-creating-high-availability-cluster",{"title":163,"visible":15,"weight":124,"urlFragment":140,"anchor":164,"singlePageAnchor":164,"docTitle":19,"url":165},"4.6. Backing up and restoring a cluster configuration","proc_cluster-backup-creating-high-availability-cluster","#proc_cluster-backup-creating-high-availability-cluster",{"title":167,"visible":15,"weight":129,"urlFragment":140,"anchor":168,"singlePageAnchor":168,"docTitle":19,"url":169},"4.7. Enabling ports for the High Availability Add-On","proc_enabling-ports-for-high-availability-creating-high-availability-cluster","#proc_enabling-ports-for-high-availability-creating-high-availability-cluster","#assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters",{"title":172,"visible":15,"weight":129,"urlFragment":173,"anchor":18,"singlePageAnchor":173,"sections":174,"docTitle":19,"url":191},"5. Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster","assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters",[175,179,183,187],{"title":176,"visible":15,"weight":16,"urlFragment":173,"anchor":177,"singlePageAnchor":177,"docTitle":19,"url":178},"5.1. Configuring an LVM volume with an XFS file system in a Pacemaker cluster","proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http","#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http",{"title":180,"visible":15,"weight":23,"urlFragment":173,"anchor":181,"singlePageAnchor":181,"docTitle":19,"url":182},"5.2. Configuring an Apache HTTP Server","proc_configuring-apache-http-web-server-configuring-ha-http","#proc_configuring-apache-http-web-server-configuring-ha-http",{"title":184,"visible":15,"weight":28,"urlFragment":173,"anchor":185,"singlePageAnchor":185,"docTitle":19,"url":186},"5.3. Creating the resources and resource groups","proc_configuring-resources-for-http-server-in-a-cluster-configuring-ha-http","#proc_configuring-resources-for-http-server-in-a-cluster-configuring-ha-http",{"title":188,"visible":15,"weight":71,"urlFragment":173,"anchor":189,"singlePageAnchor":189,"docTitle":19,"url":190},"5.4. Testing the resource configuration","proc_testing-resource-configuration-in-a-cluster-configuring-ha-http","#proc_testing-resource-configuration-in-a-cluster-configuring-ha-http","#assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters",{"title":193,"visible":15,"weight":134,"urlFragment":194,"anchor":18,"singlePageAnchor":194,"sections":195,"docTitle":19,"url":221},"6. Configuring an active/passive NFS server in a Red Hat High Availability cluster","assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters",[196,200,204,208],{"title":197,"visible":15,"weight":16,"urlFragment":194,"anchor":198,"singlePageAnchor":198,"docTitle":19,"url":199},"6.1. Configuring an LVM volume with an XFS file system in a Pacemaker cluster","proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs","#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs",{"title":201,"visible":15,"weight":23,"urlFragment":194,"anchor":202,"singlePageAnchor":202,"docTitle":19,"url":203},"6.2. Configuring an NFS share","proc_configuring-nfs-share-configuring-ha-nfs","#proc_configuring-nfs-share-configuring-ha-nfs",{"title":205,"visible":15,"weight":28,"urlFragment":194,"anchor":206,"singlePageAnchor":206,"docTitle":19,"url":207},"6.3. Configuring the resources and resource group for an NFS server in a cluster","proc_configuring_resources_for_nfs_server_in_a_cluster-configuring-ha-nfs","#proc_configuring_resources_for_nfs_server_in_a_cluster-configuring-ha-nfs",{"title":209,"visible":15,"weight":71,"urlFragment":194,"anchor":210,"singlePageAnchor":210,"sections":211,"docTitle":19,"url":220},"6.4. Testing the NFS resource configuration","proc_testing-nfs-resource-configuration-configuring-ha-nfs",[212,216],{"title":213,"visible":15,"weight":16,"urlFragment":194,"anchor":214,"singlePageAnchor":214,"docTitle":19,"url":215},"6.4.1. Testing the NFS export","testing_the_nfs_export","#testing_the_nfs_export",{"title":217,"visible":15,"weight":23,"urlFragment":194,"anchor":218,"singlePageAnchor":218,"docTitle":19,"url":219},"6.4.2. Testing for failover","testing_for_failover","#testing_for_failover","#proc_testing-nfs-resource-configuration-configuring-ha-nfs","#assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters",{"title":223,"visible":15,"weight":224,"urlFragment":225,"anchor":18,"singlePageAnchor":225,"sections":226,"docTitle":19,"url":248},"7. GFS2 file systems in a cluster",9,"assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters",[227,231],{"title":228,"visible":15,"weight":16,"urlFragment":225,"anchor":229,"singlePageAnchor":229,"docTitle":19,"url":230},"7.1. Configuring a GFS2 file system in a cluster","proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster","#proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster",{"title":232,"visible":15,"weight":23,"urlFragment":225,"anchor":233,"singlePageAnchor":233,"sections":234,"docTitle":19,"url":247},"7.2. Configuring an encrypted GFS2 file system in a cluster","proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster",[235,239,243],{"title":236,"visible":15,"weight":16,"urlFragment":225,"anchor":237,"singlePageAnchor":237,"docTitle":19,"url":238},"7.2.1. Configure a shared logical volume in a Pacemaker cluster","configure_a_shared_logical_volume_in_a_pacemaker_cluster","#configure_a_shared_logical_volume_in_a_pacemaker_cluster",{"title":240,"visible":15,"weight":23,"urlFragment":225,"anchor":241,"singlePageAnchor":241,"docTitle":19,"url":242},"7.2.2. Encrypt the logical volume and create a crypt resource","encrypt_the_logical_volume_and_create_a_crypt_resource","#encrypt_the_logical_volume_and_create_a_crypt_resource",{"title":244,"visible":15,"weight":28,"urlFragment":225,"anchor":245,"singlePageAnchor":245,"docTitle":19,"url":246},"7.2.3. Format the encrypted logical volume with a GFS2 file system and create a file system resource for the cluster","format_the_encrypted_logical_volume_with_a_gfs2_file_system_and_create_a_file_system_resource_for_the_cluster","#format_the_encrypted_logical_volume_with_a_gfs2_file_system_and_create_a_file_system_resource_for_the_cluster","#proc_configuring-encrypted-gfs2.adoc-configuring-gfs2-cluster","#assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters",{"title":250,"visible":15,"weight":251,"urlFragment":252,"anchor":18,"singlePageAnchor":252,"sections":253,"docTitle":19,"url":270},"8. Configuring an active/active Samba server in a Red Hat High Availability cluster",10,"assembly_configuring-active-active-samba-in-a-cluster-configuring-and-managing-high-availability-clusters",[254,258,262,266],{"title":255,"visible":15,"weight":16,"urlFragment":252,"anchor":256,"singlePageAnchor":256,"docTitle":19,"url":257},"8.1. Configuring a GFS2 file system for a Samba service in a high availability cluster","proc_configuring-gfs2-for-clustered-samba_adoc-configuring-ha-samba","#proc_configuring-gfs2-for-clustered-samba_adoc-configuring-ha-samba",{"title":259,"visible":15,"weight":23,"urlFragment":252,"anchor":260,"singlePageAnchor":260,"docTitle":19,"url":261},"8.2. Configuring Samba in a high availability cluster","proc_configuring-samba-for-ha-cluster_adoc-configuring-ha-samba","#proc_configuring-samba-for-ha-cluster_adoc-configuring-ha-samba",{"title":263,"visible":15,"weight":28,"urlFragment":252,"anchor":264,"singlePageAnchor":264,"docTitle":19,"url":265},"8.3. Configuring Samba cluster resources","proc_configuring-samba-cluster-resources_adoc-configuring-ha-samba","#proc_configuring-samba-cluster-resources_adoc-configuring-ha-samba",{"title":267,"visible":15,"weight":71,"urlFragment":252,"anchor":268,"singlePageAnchor":268,"docTitle":19,"url":269},"8.4. Verifying clustered Samba configuration","proc_verifying-clustered-samba-configuration.adoc-configuring-ha-samba","#proc_verifying-clustered-samba-configuration.adoc-configuring-ha-samba","#assembly_configuring-active-active-samba-in-a-cluster-configuring-and-managing-high-availability-clusters",{"title":272,"visible":15,"weight":273,"urlFragment":274,"anchor":18,"singlePageAnchor":274,"sections":275,"docTitle":19,"url":284},"9. Getting started with the pcsd Web UI",11,"assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters",[276,280],{"title":277,"visible":15,"weight":16,"urlFragment":274,"anchor":278,"singlePageAnchor":278,"docTitle":19,"url":279},"9.1. Setting up the pcsd Web UI","proc_setting-up-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui","#proc_setting-up-the-pcsd-web-ui-getting-started-with-the-pcsd-web-ui",{"title":281,"visible":15,"weight":23,"urlFragment":274,"anchor":282,"singlePageAnchor":282,"docTitle":19,"url":283},"9.2. Configuring a high availability pcsd Web UI","proc_configuring-ha-pcsd-web-ui-getting-started-with-the-pcsd-web-ui","#proc_configuring-ha-pcsd-web-ui-getting-started-with-the-pcsd-web-ui","#assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters",{"title":286,"visible":15,"weight":287,"urlFragment":288,"anchor":18,"singlePageAnchor":288,"sections":289,"docTitle":19,"url":361},"10. Configuring fencing in a Red Hat High Availability cluster",12,"assembly_configuring-fencing-configuring-and-managing-high-availability-clusters",[290,294,298,302,306,310,314,318,322,326,330,334,338,343],{"title":291,"visible":15,"weight":16,"urlFragment":288,"anchor":292,"singlePageAnchor":292,"docTitle":19,"url":293},"10.1. Displaying available fence agents and their options","proc_displaying-fence-agents-configuring-fencing","#proc_displaying-fence-agents-configuring-fencing",{"title":295,"visible":15,"weight":23,"urlFragment":288,"anchor":296,"singlePageAnchor":296,"docTitle":19,"url":297},"10.2. Creating a fence device","proc_creating-fence-devices-configuring-fencing","#proc_creating-fence-devices-configuring-fencing",{"title":299,"visible":15,"weight":28,"urlFragment":288,"anchor":300,"singlePageAnchor":300,"docTitle":19,"url":301},"10.3. General properties of fencing devices","ref_general-fence-device-properties-configuring-fencing","#ref_general-fence-device-properties-configuring-fencing",{"title":303,"visible":15,"weight":71,"urlFragment":288,"anchor":304,"singlePageAnchor":304,"docTitle":19,"url":305},"10.4. Fencing delays","ref_fence-delays-configuring-fencing","#ref_fence-delays-configuring-fencing",{"title":307,"visible":15,"weight":99,"urlFragment":288,"anchor":308,"singlePageAnchor":308,"docTitle":19,"url":309},"10.5. Testing a fence device","proc_testing-fence-devices-configuring-fencing","#proc_testing-fence-devices-configuring-fencing",{"title":311,"visible":15,"weight":124,"urlFragment":288,"anchor":312,"singlePageAnchor":312,"docTitle":19,"url":313},"10.6. Configuring fencing levels","proc_configuring-fencing-levels-configuring-fencing","#proc_configuring-fencing-levels-configuring-fencing",{"title":315,"visible":15,"weight":129,"urlFragment":288,"anchor":316,"singlePageAnchor":316,"docTitle":19,"url":317},"10.7. Configuring fencing for redundant power supplies","proc_configuring-fencing-for-redundant-power-configuring-fencing","#proc_configuring-fencing-for-redundant-power-configuring-fencing",{"title":319,"visible":15,"weight":134,"urlFragment":288,"anchor":320,"singlePageAnchor":320,"docTitle":19,"url":321},"10.8. Displaying configured fence devices","proc_displaying-configuring-fence-devices-configuring-fencing","#proc_displaying-configuring-fence-devices-configuring-fencing",{"title":323,"visible":15,"weight":224,"urlFragment":288,"anchor":324,"singlePageAnchor":324,"docTitle":19,"url":325},"10.9. Exporting fence devices as pcs commands","proc_exporting-fence-devices-configuring-fencing","#proc_exporting-fence-devices-configuring-fencing",{"title":327,"visible":15,"weight":251,"urlFragment":288,"anchor":328,"singlePageAnchor":328,"docTitle":19,"url":329},"10.10. Modifying and deleting fence devices","proc_modifying-fence-devices-configuring-fencing","#proc_modifying-fence-devices-configuring-fencing",{"title":331,"visible":15,"weight":273,"urlFragment":288,"anchor":332,"singlePageAnchor":332,"docTitle":19,"url":333},"10.11. Manually fencing a cluster node","proc_manually-fencing-a-node-configuring-fencing","#proc_manually-fencing-a-node-configuring-fencing",{"title":335,"visible":15,"weight":287,"urlFragment":288,"anchor":336,"singlePageAnchor":336,"docTitle":19,"url":337},"10.12. Disabling a fence device","proc_disabling-a-fence-device-configuring-fencing","#proc_disabling-a-fence-device-configuring-fencing",{"title":339,"visible":15,"weight":340,"urlFragment":288,"anchor":341,"singlePageAnchor":341,"docTitle":19,"url":342},"10.13. Preventing a node from using a fencing device",13,"proc_preventing-a-node-from-using-a-fence-device-configuring-fencing","#proc_preventing-a-node-from-using-a-fence-device-configuring-fencing",{"title":344,"visible":15,"weight":345,"urlFragment":288,"anchor":346,"singlePageAnchor":346,"sections":347,"docTitle":19,"url":360},"10.14. Configuring ACPI for use with integrated fence devices",14,"proc_configuring-acpi-for-fence-devices-configuring-fencing",[348,352,356],{"title":349,"visible":15,"weight":16,"urlFragment":288,"anchor":350,"singlePageAnchor":350,"docTitle":19,"url":351},"10.14.1. Disabling ACPI Soft-Off with the BIOS","s2-bios-setting-CA","#s2-bios-setting-CA",{"title":353,"visible":15,"weight":23,"urlFragment":288,"anchor":354,"singlePageAnchor":354,"docTitle":19,"url":355},"10.14.2. Disabling ACPI Soft-Off in the logind.conf file","s2-acpi-disable-logind-CA","#s2-acpi-disable-logind-CA",{"title":357,"visible":15,"weight":28,"urlFragment":288,"anchor":358,"singlePageAnchor":358,"docTitle":19,"url":359},"10.14.3. Disabling ACPI completely in the GRUB 2 file","s2-acpi-disable-boot-CA","#s2-acpi-disable-boot-CA","#proc_configuring-acpi-for-fence-devices-configuring-fencing","#assembly_configuring-fencing-configuring-and-managing-high-availability-clusters",{"title":363,"visible":15,"weight":340,"urlFragment":364,"anchor":18,"singlePageAnchor":364,"sections":365,"docTitle":19,"url":424},"11. Configuring cluster resources","assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters",[366,370,374,395,420],{"title":367,"visible":15,"weight":16,"urlFragment":364,"anchor":368,"singlePageAnchor":368,"docTitle":19,"url":369},"11.1. Resource agent identifiers","ref_resource-properties.adoc-configuring-cluster-resources","#ref_resource-properties.adoc-configuring-cluster-resources",{"title":371,"visible":15,"weight":23,"urlFragment":364,"anchor":372,"singlePageAnchor":372,"docTitle":19,"url":373},"11.2. Displaying resource-specific parameters","proc_displaying-resource-specific-parameters-configuring-cluster-resources","#proc_displaying-resource-specific-parameters-configuring-cluster-resources",{"title":375,"visible":15,"weight":28,"urlFragment":364,"anchor":376,"singlePageAnchor":376,"sections":377,"docTitle":19,"url":394},"11.3. Configuring resource meta options","proc_configuring-resource-meta-options-configuring-cluster-resources",[378,382,386,390],{"title":379,"visible":15,"weight":16,"urlFragment":364,"anchor":380,"singlePageAnchor":380,"docTitle":19,"url":381},"11.3.1. Changing the default value of a resource option","changing_the_default_value_of_a_resource_option","#changing_the_default_value_of_a_resource_option",{"title":383,"visible":15,"weight":23,"urlFragment":364,"anchor":384,"singlePageAnchor":384,"docTitle":19,"url":385},"11.3.2. Changing the default value of a resource option for sets of resources","changing_the_default_value_of_a_resource_option_for_sets_of_resources","#changing_the_default_value_of_a_resource_option_for_sets_of_resources",{"title":387,"visible":15,"weight":28,"urlFragment":364,"anchor":388,"singlePageAnchor":388,"docTitle":19,"url":389},"11.3.3. Displaying currently configured resource defaults","displaying_currently_configured_resource_defaults","#displaying_currently_configured_resource_defaults",{"title":391,"visible":15,"weight":71,"urlFragment":364,"anchor":392,"singlePageAnchor":392,"docTitle":19,"url":393},"11.3.4. Setting meta options on resource creation","setting_meta_options_on_resource_creation","#setting_meta_options_on_resource_creation","#proc_configuring-resource-meta-options-configuring-cluster-resources",{"title":396,"visible":15,"weight":71,"urlFragment":364,"anchor":397,"singlePageAnchor":397,"sections":398,"docTitle":19,"url":419},"11.4. Configuring resource groups","proc_creating-resource-groups-configuring-cluster-resources",[399,403,407,411,415],{"title":400,"visible":15,"weight":16,"urlFragment":364,"anchor":401,"singlePageAnchor":401,"docTitle":19,"url":402},"11.4.1. Creating a resource group","creating_a_resource_group","#creating_a_resource_group",{"title":404,"visible":15,"weight":23,"urlFragment":364,"anchor":405,"singlePageAnchor":405,"docTitle":19,"url":406},"11.4.2. Removing a resource group","removing_a_resource_group","#removing_a_resource_group",{"title":408,"visible":15,"weight":28,"urlFragment":364,"anchor":409,"singlePageAnchor":409,"docTitle":19,"url":410},"11.4.3. Displaying resource groups","displaying_resource_groups","#displaying_resource_groups",{"title":412,"visible":15,"weight":71,"urlFragment":364,"anchor":413,"singlePageAnchor":413,"docTitle":19,"url":414},"11.4.4. Group options","s2-group_options-HAAR","#s2-group_options-HAAR",{"title":416,"visible":15,"weight":99,"urlFragment":364,"anchor":417,"singlePageAnchor":417,"docTitle":19,"url":418},"11.4.5. Group stickiness","s2-group_stickiness-HAAR","#s2-group_stickiness-HAAR","#proc_creating-resource-groups-configuring-cluster-resources",{"title":421,"visible":15,"weight":99,"urlFragment":364,"anchor":422,"singlePageAnchor":422,"docTitle":19,"url":423},"11.5. Determining resource behavior","con_determining-resource-behavior-configuring-cluster-resources","#con_determining-resource-behavior-configuring-cluster-resources","#assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters",{"title":426,"visible":15,"weight":345,"urlFragment":427,"anchor":18,"singlePageAnchor":427,"sections":428,"docTitle":19,"url":458},"12. Determining which nodes a resource can run on","assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters",[429,433,437,450,454],{"title":430,"visible":15,"weight":16,"urlFragment":427,"anchor":431,"singlePageAnchor":431,"docTitle":19,"url":432},"12.1. Configuring location constraints","proc_configuring-location-constraints-determining-which-node-a-resource-runs-on","#proc_configuring-location-constraints-determining-which-node-a-resource-runs-on",{"title":434,"visible":15,"weight":23,"urlFragment":427,"anchor":435,"singlePageAnchor":435,"docTitle":19,"url":436},"12.2. Limiting resource discovery to a subset of nodes","proc_limiting-resource-discovery-to-a-subset-of-nodes-determining-which-node-a-resource-runs-on","#proc_limiting-resource-discovery-to-a-subset-of-nodes-determining-which-node-a-resource-runs-on",{"title":438,"visible":15,"weight":28,"urlFragment":427,"anchor":439,"singlePageAnchor":439,"sections":440,"docTitle":19,"url":449},"12.3. Configuring a location constraint strategy","proc_configuring-location-constraint-strategy-determining-which-node-a-resource-runs-on",[441,445],{"title":442,"visible":15,"weight":16,"urlFragment":427,"anchor":443,"singlePageAnchor":443,"docTitle":19,"url":444},"12.3.1. Configuring an \"Opt-In\" cluster","s3-optin-clusters-HAAR","#s3-optin-clusters-HAAR",{"title":446,"visible":15,"weight":23,"urlFragment":427,"anchor":447,"singlePageAnchor":447,"docTitle":19,"url":448},"12.3.2. Configuring an \"Opt-Out\" cluster","s3-optout-clusters-HAAR","#s3-optout-clusters-HAAR","#proc_configuring-location-constraint-strategy-determining-which-node-a-resource-runs-on",{"title":451,"visible":15,"weight":71,"urlFragment":427,"anchor":452,"singlePageAnchor":452,"docTitle":19,"url":453},"12.4. Configuring a resource to prefer its current node","proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on","#proc_setting-resource-stickiness-determining-which-node-a-resource-runs-on",{"title":455,"visible":15,"weight":99,"urlFragment":427,"anchor":456,"singlePageAnchor":456,"docTitle":19,"url":457},"12.5. Exporting resource constraints as pcs commands","proc_exporting-constraints-determining-which-node-a-resource-runs-on","#proc_exporting-constraints-determining-which-node-a-resource-runs-on","#assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters",{"title":460,"visible":15,"weight":461,"urlFragment":462,"anchor":18,"singlePageAnchor":462,"sections":463,"docTitle":19,"url":480},"13. Determining the order in which cluster resources are run",15,"assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters",[464,468,472,476],{"title":465,"visible":15,"weight":16,"urlFragment":462,"anchor":466,"singlePageAnchor":466,"docTitle":19,"url":467},"13.1. Configuring mandatory ordering","con_configuring-mandatory-ordering-determining-resource-order","#con_configuring-mandatory-ordering-determining-resource-order",{"title":469,"visible":15,"weight":23,"urlFragment":462,"anchor":470,"singlePageAnchor":470,"docTitle":19,"url":471},"13.2. Configuring advisory ordering","proc_configuring-advisory-ordering-determining-resource-order","#proc_configuring-advisory-ordering-determining-resource-order",{"title":473,"visible":15,"weight":28,"urlFragment":462,"anchor":474,"singlePageAnchor":474,"docTitle":19,"url":475},"13.3. Configuring ordered resource sets","proc_configuring-ordered-resource-sets.adocdetermining-resource-order","#proc_configuring-ordered-resource-sets.adocdetermining-resource-order",{"title":477,"visible":15,"weight":71,"urlFragment":462,"anchor":478,"singlePageAnchor":478,"docTitle":19,"url":479},"13.4. Configuring startup order for resource dependencies not managed by Pacemaker","proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order","#proc_configuring-nonpacemaker-dependencies.adoc-determining-resource-order","#assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters",{"title":482,"visible":15,"weight":483,"urlFragment":484,"anchor":18,"singlePageAnchor":484,"sections":485,"docTitle":19,"url":498},"14. Colocating cluster resources",16,"assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters",[486,490,494],{"title":487,"visible":15,"weight":16,"urlFragment":484,"anchor":488,"singlePageAnchor":488,"docTitle":19,"url":489},"14.1. Specifying mandatory placement of resources","proc_specifying-mandatory-placement.adoc-colocating-cluster-resources","#proc_specifying-mandatory-placement.adoc-colocating-cluster-resources",{"title":491,"visible":15,"weight":23,"urlFragment":484,"anchor":492,"singlePageAnchor":492,"docTitle":19,"url":493},"14.2. Specifying advisory placement of resources","con_specifying-advisory-placement-colocating-cluster-resources","#con_specifying-advisory-placement-colocating-cluster-resources",{"title":495,"visible":15,"weight":28,"urlFragment":484,"anchor":496,"singlePageAnchor":496,"docTitle":19,"url":497},"14.3. Colocating sets of resources","proc_colocating-resource-sets.adoc-colocating-cluster-resources","#proc_colocating-resource-sets.adoc-colocating-cluster-resources","#assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters",{"title":500,"visible":15,"weight":501,"urlFragment":502,"anchor":18,"singlePageAnchor":502,"docTitle":19,"url":503},"15. Displaying resource constraints and resource dependencies",17,"proc_displaying-resource-constraints.adoc-configuring-and-managing-high-availability-clusters","#proc_displaying-resource-constraints.adoc-configuring-and-managing-high-availability-clusters",{"title":505,"visible":15,"weight":506,"urlFragment":507,"anchor":18,"singlePageAnchor":507,"sections":508,"docTitle":19,"url":530},"16. Determining resource location with rules",18,"assembly_determining-resource-location-with-rules-configuring-and-managing-high-availability-clusters",[509,526],{"title":510,"visible":15,"weight":16,"urlFragment":507,"anchor":511,"singlePageAnchor":511,"sections":512,"docTitle":19,"url":525},"16.1. Pacemaker rules","ref_pacemaker-rules.adoc-determining-resource-location-with-rules",[513,517,521],{"title":514,"visible":15,"weight":16,"urlFragment":507,"anchor":515,"singlePageAnchor":515,"docTitle":19,"url":516},"16.1.1. Node attribute expressions","node_attribute_expressions","#node_attribute_expressions",{"title":518,"visible":15,"weight":23,"urlFragment":507,"anchor":519,"singlePageAnchor":519,"docTitle":19,"url":520},"16.1.2. Time/date based expressions","time_date_based_expressions","#time_date_based_expressions",{"title":522,"visible":15,"weight":28,"urlFragment":507,"anchor":523,"singlePageAnchor":523,"docTitle":19,"url":524},"16.1.3. Date specifications","date_specifications","#date_specifications","#ref_pacemaker-rules.adoc-determining-resource-location-with-rules",{"title":527,"visible":15,"weight":23,"urlFragment":507,"anchor":528,"singlePageAnchor":528,"docTitle":19,"url":529},"16.2. Configuring a Pacemaker location constraint using rules","ref_configuring-constraint-using-rules.adoc-determining-resource-location-with-rules","#ref_configuring-constraint-using-rules.adoc-determining-resource-location-with-rules","#assembly_determining-resource-location-with-rules-configuring-and-managing-high-availability-clusters",{"title":532,"visible":15,"weight":533,"urlFragment":534,"anchor":18,"singlePageAnchor":534,"sections":535,"docTitle":19,"url":582},"17. Managing cluster resources",19,"assembly_managing-cluster-resources-configuring-and-managing-high-availability-clusters",[536,540,544,548,552,565,569],{"title":537,"visible":15,"weight":16,"urlFragment":534,"anchor":538,"singlePageAnchor":538,"docTitle":19,"url":539},"17.1. Displaying configured resources","proc_display-configured-resources-managing-cluster-resources","#proc_display-configured-resources-managing-cluster-resources",{"title":541,"visible":15,"weight":23,"urlFragment":534,"anchor":542,"singlePageAnchor":542,"docTitle":19,"url":543},"17.2. Exporting cluster resources as pcs commands","proc_exporting-resources-managing-cluster-resources","#proc_exporting-resources-managing-cluster-resources",{"title":545,"visible":15,"weight":28,"urlFragment":534,"anchor":546,"singlePageAnchor":546,"docTitle":19,"url":547},"17.3. Modifying resource parameters","proc_modify-resource-parameters-managing-cluster-resources","#proc_modify-resource-parameters-managing-cluster-resources",{"title":549,"visible":15,"weight":71,"urlFragment":534,"anchor":550,"singlePageAnchor":550,"docTitle":19,"url":551},"17.4. Clearing failure status of cluster resources","proc_cleanup-cluster-resources-managing-cluster-resources","#proc_cleanup-cluster-resources-managing-cluster-resources",{"title":553,"visible":15,"weight":99,"urlFragment":534,"anchor":554,"singlePageAnchor":554,"sections":555,"docTitle":19,"url":564},"17.5. Moving resources in a cluster","proc_moving-cluster-resources-managing-cluster-resources",[556,560],{"title":557,"visible":15,"weight":16,"urlFragment":534,"anchor":558,"singlePageAnchor":558,"docTitle":19,"url":559},"17.5.1. Moving resources due to failure","moving_resources_due_to_failure","#moving_resources_due_to_failure",{"title":561,"visible":15,"weight":23,"urlFragment":534,"anchor":562,"singlePageAnchor":562,"docTitle":19,"url":563},"17.5.2. Moving resources due to connectivity changes","moving_resources_due_to_connectivity_changes","#moving_resources_due_to_connectivity_changes","#proc_moving-cluster-resources-managing-cluster-resources",{"title":566,"visible":15,"weight":124,"urlFragment":534,"anchor":567,"singlePageAnchor":567,"docTitle":19,"url":568},"17.6. Disabling a monitor operation","proc_disabling-monitor-operationmanaging-cluster-resources","#proc_disabling-monitor-operationmanaging-cluster-resources",{"title":570,"visible":15,"weight":129,"urlFragment":534,"anchor":571,"singlePageAnchor":571,"sections":572,"docTitle":19,"url":581},"17.7. Configuring and managing cluster resource tags","proc_tagging-cluster-resources-managing-cluster-resources",[573,577],{"title":574,"visible":15,"weight":16,"urlFragment":534,"anchor":575,"singlePageAnchor":575,"docTitle":19,"url":576},"17.7.1. Tagging cluster resources for administration by category","tagging_cluster_resources_for_administration_by_category","#tagging_cluster_resources_for_administration_by_category",{"title":578,"visible":15,"weight":23,"urlFragment":534,"anchor":579,"singlePageAnchor":579,"docTitle":19,"url":580},"17.7.2. Deleting a tagged cluster resource","deleting_a_tagged_cluster_resource","#deleting_a_tagged_cluster_resource","#proc_tagging-cluster-resources-managing-cluster-resources","#assembly_managing-cluster-resources-configuring-and-managing-high-availability-clusters",{"title":584,"visible":15,"weight":585,"urlFragment":586,"anchor":18,"singlePageAnchor":586,"sections":587,"docTitle":19,"url":613},"18. Creating cluster resources that are active on multiple nodes (cloned resources)",20,"assembly_creating-multinode-resources-configuring-and-managing-high-availability-clusters",[588,592,596,609],{"title":589,"visible":15,"weight":16,"urlFragment":586,"anchor":590,"singlePageAnchor":590,"docTitle":19,"url":591},"18.1. Creating and removing a cloned resource","proc_creating-cloned-resource-creating-multinode-resources","#proc_creating-cloned-resource-creating-multinode-resources",{"title":593,"visible":15,"weight":23,"urlFragment":586,"anchor":594,"singlePageAnchor":594,"docTitle":19,"url":595},"18.2. Configuring clone resource constraints","proc_configuring-clone-constraints-creating-multinode-resources","#proc_configuring-clone-constraints-creating-multinode-resources",{"title":597,"visible":15,"weight":28,"urlFragment":586,"anchor":598,"singlePageAnchor":598,"sections":599,"docTitle":19,"url":608},"18.3. Promotable clone resources","proc_creating-promotable-clone-resources-creating-multinode-resources",[600,604],{"title":601,"visible":15,"weight":16,"urlFragment":586,"anchor":602,"singlePageAnchor":602,"docTitle":19,"url":603},"18.3.1. Creating a promotable clone resource","creating_a_promotable_clone_resource","#creating_a_promotable_clone_resource",{"title":605,"visible":15,"weight":23,"urlFragment":586,"anchor":606,"singlePageAnchor":606,"docTitle":19,"url":607},"18.3.2. Configuring promotable resource constraints","configuring_promotable_resource_constraints","#configuring_promotable_resource_constraints","#proc_creating-promotable-clone-resources-creating-multinode-resources",{"title":610,"visible":15,"weight":71,"urlFragment":586,"anchor":611,"singlePageAnchor":611,"docTitle":19,"url":612},"18.4. Demoting a promoted resource on failure","proc_recovering-promoted-node-creating-multinode-resources","#proc_recovering-promoted-node-creating-multinode-resources","#assembly_creating-multinode-resources-configuring-and-managing-high-availability-clusters",{"title":615,"visible":15,"weight":616,"urlFragment":617,"anchor":18,"singlePageAnchor":617,"sections":618,"docTitle":19,"url":672},"19. Managing cluster nodes",21,"assembly_clusternode-management-configuring-and-managing-high-availability-clusters",[619,623,627,631,635,639,664,668],{"title":620,"visible":15,"weight":16,"urlFragment":617,"anchor":621,"singlePageAnchor":621,"docTitle":19,"url":622},"19.1. Stopping cluster services","proc_cluster-stop-clusternode-management","#proc_cluster-stop-clusternode-management",{"title":624,"visible":15,"weight":23,"urlFragment":617,"anchor":625,"singlePageAnchor":625,"docTitle":19,"url":626},"19.2. Enabling and disabling cluster services","proc_cluster-enable-clusternode-management","#proc_cluster-enable-clusternode-management",{"title":628,"visible":15,"weight":28,"urlFragment":617,"anchor":629,"singlePageAnchor":629,"docTitle":19,"url":630},"19.3. Adding cluster nodes","proc_cluster-nodeadd-clusternode-management","#proc_cluster-nodeadd-clusternode-management",{"title":632,"visible":15,"weight":71,"urlFragment":617,"anchor":633,"singlePageAnchor":633,"docTitle":19,"url":634},"19.4. Removing cluster nodes","proc_cluster-noderemove-clusternode-management","#proc_cluster-noderemove-clusternode-management",{"title":636,"visible":15,"weight":99,"urlFragment":617,"anchor":637,"singlePageAnchor":637,"docTitle":19,"url":638},"19.5. Adding a node to a cluster with multiple links","proc_add-nodes-to-multiple-ip-cluster-clusternode-management","#proc_add-nodes-to-multiple-ip-cluster-clusternode-management",{"title":640,"visible":15,"weight":124,"urlFragment":617,"anchor":641,"singlePageAnchor":641,"sections":642,"docTitle":19,"url":663},"19.6. Adding and modifying links in an existing cluster","proc_changing-links-in-multiple-ip-cluster-clusternode-management",[643,647,651,655,659],{"title":644,"visible":15,"weight":16,"urlFragment":617,"anchor":645,"singlePageAnchor":645,"docTitle":19,"url":646},"19.6.1. Adding and removing links in an existing cluster","adding_and_removing_links_in_an_existing_cluster","#adding_and_removing_links_in_an_existing_cluster",{"title":648,"visible":15,"weight":23,"urlFragment":617,"anchor":649,"singlePageAnchor":649,"docTitle":19,"url":650},"19.6.2. Modifying a link in a cluster with multiple links","modifying_a_link_in_a_cluster_with_multiple_links","#modifying_a_link_in_a_cluster_with_multiple_links",{"title":652,"visible":15,"weight":28,"urlFragment":617,"anchor":653,"singlePageAnchor":653,"docTitle":19,"url":654},"19.6.3. Modifying the link addresses in a cluster with a single link","modifying_the_link_addresses_in_a_cluster_with_a_single_link","#modifying_the_link_addresses_in_a_cluster_with_a_single_link",{"title":656,"visible":15,"weight":71,"urlFragment":617,"anchor":657,"singlePageAnchor":657,"docTitle":19,"url":658},"19.6.4. Modifying the link options for a link in a cluster with a single link","modifying_the_link_options_for_a_link_in_a_cluster_with_a_single_link","#modifying_the_link_options_for_a_link_in_a_cluster_with_a_single_link",{"title":660,"visible":15,"weight":99,"urlFragment":617,"anchor":661,"singlePageAnchor":661,"docTitle":19,"url":662},"19.6.5. Modifying a link when adding a new link is not possible","modifying_a_link_when_adding_a_new_link_is_not_possible","#modifying_a_link_when_adding_a_new_link_is_not_possible","#proc_changing-links-in-multiple-ip-cluster-clusternode-management",{"title":665,"visible":15,"weight":129,"urlFragment":617,"anchor":666,"singlePageAnchor":666,"docTitle":19,"url":667},"19.7. Configuring a node health strategy","proc_tracking-node-health-clusternode-management","#proc_tracking-node-health-clusternode-management",{"title":669,"visible":15,"weight":134,"urlFragment":617,"anchor":670,"singlePageAnchor":670,"docTitle":19,"url":671},"19.8. Configuring a large cluster with many resources","proc_configuring-large-clusters-clusternode-management","#proc_configuring-large-clusters-clusternode-management","#assembly_clusternode-management-configuring-and-managing-high-availability-clusters",{"title":674,"visible":15,"weight":675,"urlFragment":676,"anchor":18,"singlePageAnchor":676,"sections":677,"docTitle":19,"url":686},"20. Setting user permissions for a Pacemaker cluster",22,"assembly_cluster-permissions-configuring-and-managing-high-availability-clusters",[678,682],{"title":679,"visible":15,"weight":16,"urlFragment":676,"anchor":680,"singlePageAnchor":680,"docTitle":19,"url":681},"20.1. Setting permissions for node access over a network","proc_setting-cluster-access-over-network-cluster-permissions","#proc_setting-cluster-access-over-network-cluster-permissions",{"title":683,"visible":15,"weight":23,"urlFragment":676,"anchor":684,"singlePageAnchor":684,"docTitle":19,"url":685},"20.2. Setting local permissions using ACLs","proc_setting-local-cluster-permissions-cluster-permissions","#proc_setting-local-cluster-permissions-cluster-permissions","#assembly_cluster-permissions-configuring-and-managing-high-availability-clusters",{"title":688,"visible":15,"weight":689,"urlFragment":690,"anchor":18,"singlePageAnchor":690,"sections":691,"docTitle":19,"url":717},"21. Resource monitoring operations",23,"assembly_resource-monitoring-operations-configuring-and-managing-high-availability-clusters",[692,696,713],{"title":693,"visible":15,"weight":16,"urlFragment":690,"anchor":694,"singlePageAnchor":694,"docTitle":19,"url":695},"21.1. Configuring resource monitoring operations","proc_configuring-resource-monitoring-operations-resource-monitoring-operations","#proc_configuring-resource-monitoring-operations-resource-monitoring-operations",{"title":697,"visible":15,"weight":23,"urlFragment":690,"anchor":698,"singlePageAnchor":698,"sections":699,"docTitle":19,"url":712},"21.2. Configuring global resource operation defaults","proc_configuring-global-resource-operation-defaults-resource-monitoring-operations",[700,704,708],{"title":701,"visible":15,"weight":16,"urlFragment":690,"anchor":702,"singlePageAnchor":702,"docTitle":19,"url":703},"21.2.1. Overriding resource-specific operation values","overriding_resource_specific_operation_values","#overriding_resource_specific_operation_values",{"title":705,"visible":15,"weight":23,"urlFragment":690,"anchor":706,"singlePageAnchor":706,"docTitle":19,"url":707},"21.2.2. Changing the default value of a resource operation for sets of resources","changing_the_default_value_of_a_resource_operation_for_sets_of_resources","#changing_the_default_value_of_a_resource_operation_for_sets_of_resources",{"title":709,"visible":15,"weight":28,"urlFragment":690,"anchor":710,"singlePageAnchor":710,"docTitle":19,"url":711},"21.2.3. Displaying currently configured resource operation default values","displaying_currently_configured_resource_operation_default_values","#displaying_currently_configured_resource_operation_default_values","#proc_configuring-global-resource-operation-defaults-resource-monitoring-operations",{"title":714,"visible":15,"weight":28,"urlFragment":690,"anchor":715,"singlePageAnchor":715,"docTitle":19,"url":716},"21.3. Configuring multiple monitoring operations","proc_configuring-multiple-monitoring-operations-resource-monitoring-operations","#proc_configuring-multiple-monitoring-operations-resource-monitoring-operations","#assembly_resource-monitoring-operations-configuring-and-managing-high-availability-clusters",{"title":719,"visible":15,"weight":720,"urlFragment":721,"anchor":18,"singlePageAnchor":721,"sections":722,"docTitle":19,"url":739},"22. Pacemaker cluster properties",24,"assembly_controlling-cluster-behavior-configuring-and-managing-high-availability-clusters",[723,727,731,735],{"title":724,"visible":15,"weight":16,"urlFragment":721,"anchor":725,"singlePageAnchor":725,"docTitle":19,"url":726},"22.1. Summary of cluster properties and options","ref_cluster-properties-options-controlling-cluster-behavior","#ref_cluster-properties-options-controlling-cluster-behavior",{"title":728,"visible":15,"weight":23,"urlFragment":721,"anchor":729,"singlePageAnchor":729,"docTitle":19,"url":730},"22.2. Setting and removing cluster properties","setting-cluster-properties-controlling-cluster-behavior","#setting-cluster-properties-controlling-cluster-behavior",{"title":732,"visible":15,"weight":28,"urlFragment":721,"anchor":733,"singlePageAnchor":733,"docTitle":19,"url":734},"22.3. Querying cluster property settings","proc_querying-cluster-property-settings-controlling-cluster-behavior","#proc_querying-cluster-property-settings-controlling-cluster-behavior",{"title":736,"visible":15,"weight":71,"urlFragment":721,"anchor":737,"singlePageAnchor":737,"docTitle":19,"url":738},"22.4. Exporting cluster properties as pcs commands","proc_exporting-properties-controlling-cluster-behavior","#proc_exporting-properties-controlling-cluster-behavior","#assembly_controlling-cluster-behavior-configuring-and-managing-high-availability-clusters",{"title":741,"visible":15,"weight":742,"urlFragment":743,"anchor":18,"singlePageAnchor":743,"sections":744,"docTitle":19,"url":753},"23. Configuring resources to remain stopped on clean node shutdown",25,"assembly_configuring-resources-to-remain-stopped-configuring-and-managing-high-availability-clusters",[745,749],{"title":746,"visible":15,"weight":16,"urlFragment":743,"anchor":747,"singlePageAnchor":747,"docTitle":19,"url":748},"23.1. Cluster properties to configure resources to remain stopped on clean node shutdown","ref_cluster-properties-shutdown-lock-configuring-resources-to-remain-stopped","#ref_cluster-properties-shutdown-lock-configuring-resources-to-remain-stopped",{"title":750,"visible":15,"weight":23,"urlFragment":743,"anchor":751,"singlePageAnchor":751,"docTitle":19,"url":752},"23.2. Setting the shutdown-lock cluster property","proc_setting-shutdown-lock-configuring-resources-to-remain-stopped","#proc_setting-shutdown-lock-configuring-resources-to-remain-stopped","#assembly_configuring-resources-to-remain-stopped-configuring-and-managing-high-availability-clusters",{"title":755,"visible":15,"weight":756,"urlFragment":757,"anchor":18,"singlePageAnchor":757,"sections":758,"docTitle":19,"url":797},"24. Configuring a node placement strategy",26,"assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters",[759,772,789,793],{"title":760,"visible":15,"weight":16,"urlFragment":757,"anchor":761,"singlePageAnchor":761,"sections":762,"docTitle":19,"url":771},"24.1. Utilization attributes and placement strategy","configuring-utilization-attributes-configuring-node-placement-strategy",[763,767],{"title":764,"visible":15,"weight":16,"urlFragment":757,"anchor":765,"singlePageAnchor":765,"docTitle":19,"url":766},"24.1.1. Configuring node and resource capacity","configuring_node_and_resource_capacity","#configuring_node_and_resource_capacity",{"title":768,"visible":15,"weight":23,"urlFragment":757,"anchor":769,"singlePageAnchor":769,"docTitle":19,"url":770},"24.1.2. Configuring placement strategy","configuring_placement_strategy","#configuring_placement_strategy","#configuring-utilization-attributes-configuring-node-placement-strategy",{"title":773,"visible":15,"weight":23,"urlFragment":757,"anchor":774,"singlePageAnchor":774,"sections":775,"docTitle":19,"url":788},"24.2. Pacemaker resource allocation","pacemaker-resource-allocation-configuring-node-placement-strategy",[776,780,784],{"title":777,"visible":15,"weight":16,"urlFragment":757,"anchor":778,"singlePageAnchor":778,"docTitle":19,"url":779},"24.2.1. Node preference","node_preference","#node_preference",{"title":781,"visible":15,"weight":23,"urlFragment":757,"anchor":782,"singlePageAnchor":782,"docTitle":19,"url":783},"24.2.2. Node capacity","node_capacity","#node_capacity",{"title":785,"visible":15,"weight":28,"urlFragment":757,"anchor":786,"singlePageAnchor":786,"docTitle":19,"url":787},"24.2.3. Resource allocation preference","resource_allocation_preference","#resource_allocation_preference","#pacemaker-resource-allocation-configuring-node-placement-strategy",{"title":790,"visible":15,"weight":28,"urlFragment":757,"anchor":791,"singlePageAnchor":791,"docTitle":19,"url":792},"24.3. Resource placement strategy guidelines","resource-placement-strategy-guidelines-configuring-node-placement-strategy","#resource-placement-strategy-guidelines-configuring-node-placement-strategy",{"title":794,"visible":15,"weight":71,"urlFragment":757,"anchor":795,"singlePageAnchor":795,"docTitle":19,"url":796},"24.4. The NodeUtilization resource agent","node-utilization-resource-agent-configuring-node-placement-strategy","#node-utilization-resource-agent-configuring-node-placement-strategy","#assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters",{"title":799,"visible":15,"weight":800,"urlFragment":801,"anchor":18,"singlePageAnchor":801,"sections":802,"docTitle":19,"url":811},"25. Configuring a virtual domain as a resource",27,"assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters",[803,807],{"title":804,"visible":15,"weight":16,"urlFragment":801,"anchor":805,"singlePageAnchor":805,"docTitle":19,"url":806},"25.1. Virtual domain resource options","ref_virtual-domain-resource-options-configuring-virtual-domain-as-a-resource","#ref_virtual-domain-resource-options-configuring-virtual-domain-as-a-resource",{"title":808,"visible":15,"weight":23,"urlFragment":801,"anchor":809,"singlePageAnchor":809,"docTitle":19,"url":810},"25.2. Creating the virtual domain resource","proc_creating-virtual-domain-resource-configuring-virtual-domain-as-a-resource","#proc_creating-virtual-domain-resource-configuring-virtual-domain-as-a-resource","#assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters",{"title":813,"visible":15,"weight":814,"urlFragment":815,"anchor":18,"singlePageAnchor":815,"sections":816,"docTitle":19,"url":833},"26. Configuring cluster quorum",28,"assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters",[817,821,825,829],{"title":818,"visible":15,"weight":16,"urlFragment":815,"anchor":819,"singlePageAnchor":819,"docTitle":19,"url":820},"26.1. Configuring quorum options","ref_quorum-options-configuring-cluster-quorum","#ref_quorum-options-configuring-cluster-quorum",{"title":822,"visible":15,"weight":23,"urlFragment":815,"anchor":823,"singlePageAnchor":823,"docTitle":19,"url":824},"26.2. Modifying quorum options","proc_modifying-quorum-options-configuring-cluster-quorum","#proc_modifying-quorum-options-configuring-cluster-quorum",{"title":826,"visible":15,"weight":28,"urlFragment":815,"anchor":827,"singlePageAnchor":827,"docTitle":19,"url":828},"26.3. Displaying quorum configuration and status","proc_displaying-quorum-configuration-status-configuring-cluster-quorum","#proc_displaying-quorum-configuration-status-configuring-cluster-quorum",{"title":830,"visible":15,"weight":71,"urlFragment":815,"anchor":831,"singlePageAnchor":831,"docTitle":19,"url":832},"26.4. Running inquorate clusters","proc_running-inquorate-clusters-configuring-cluster-quorum","#proc_running-inquorate-clusters-configuring-cluster-quorum","#assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters",{"title":835,"visible":15,"weight":836,"urlFragment":837,"anchor":18,"singlePageAnchor":837,"sections":838,"docTitle":19,"url":868},"27. Configuring quorum devices",29,"assembly_configuring-quorum-devices-configuring-and-managing-high-availability-clusters",[839,843,847,851],{"title":840,"visible":15,"weight":16,"urlFragment":837,"anchor":841,"singlePageAnchor":841,"docTitle":19,"url":842},"27.1. Installing quorum device packages","proc_installing-quorum-device-packages-configuring-quorum-devices","#proc_installing-quorum-device-packages-configuring-quorum-devices",{"title":844,"visible":15,"weight":23,"urlFragment":837,"anchor":845,"singlePageAnchor":845,"docTitle":19,"url":846},"27.2. Configuring a quorum device","proc_configuring-quorum-device-configuring-quorum-devices","#proc_configuring-quorum-device-configuring-quorum-devices",{"title":848,"visible":15,"weight":28,"urlFragment":837,"anchor":849,"singlePageAnchor":849,"docTitle":19,"url":850},"27.3. Managing the quorum device service","proc_managing-quorum-device-service-configuring-quorum-devices","#proc_managing-quorum-device-service-configuring-quorum-devices",{"title":852,"visible":15,"weight":71,"urlFragment":837,"anchor":853,"singlePageAnchor":853,"sections":854,"docTitle":19,"url":867},"27.4. Managing a quorum device in a cluster","proc_managing-quorum-device-settings_configuring-quorum-devices",[855,859,863],{"title":856,"visible":15,"weight":16,"urlFragment":837,"anchor":857,"singlePageAnchor":857,"docTitle":19,"url":858},"27.4.1. Changing quorum device settings","changing_quorum_device_settings","#changing_quorum_device_settings",{"title":860,"visible":15,"weight":23,"urlFragment":837,"anchor":861,"singlePageAnchor":861,"docTitle":19,"url":862},"27.4.2. Removing a quorum device","removing_a_quorum_device","#removing_a_quorum_device",{"title":864,"visible":15,"weight":28,"urlFragment":837,"anchor":865,"singlePageAnchor":865,"docTitle":19,"url":866},"27.4.3. Destroying a quorum device","destroying_a_quorum_device","#destroying_a_quorum_device","#proc_managing-quorum-device-settings_configuring-quorum-devices","#assembly_configuring-quorum-devices-configuring-and-managing-high-availability-clusters",{"title":870,"visible":15,"weight":871,"urlFragment":872,"anchor":18,"singlePageAnchor":872,"sections":873,"docTitle":19,"url":902},"28. Triggering scripts for cluster events",30,"assembly_configuring-pacemaker-alert-agents_configuring-and-managing-high-availability-clusters",[874,878,882,886,890,894,898],{"title":875,"visible":15,"weight":16,"urlFragment":872,"anchor":876,"singlePageAnchor":876,"docTitle":19,"url":877},"28.1. Installing and configuring sample alert agents","using-sample-alert-agents-configuring-pacemaker-alert-agents","#using-sample-alert-agents-configuring-pacemaker-alert-agents",{"title":879,"visible":15,"weight":23,"urlFragment":872,"anchor":880,"singlePageAnchor":880,"docTitle":19,"url":881},"28.2. Creating a cluster alert","creating-cluster-alert-configuring-pacemaker-alert-agents","#creating-cluster-alert-configuring-pacemaker-alert-agents",{"title":883,"visible":15,"weight":28,"urlFragment":872,"anchor":884,"singlePageAnchor":884,"docTitle":19,"url":885},"28.3. Displaying, modifying, and removing cluster alerts","managing-cluster-alert-configuring-pacemaker-alert-agents","#managing-cluster-alert-configuring-pacemaker-alert-agents",{"title":887,"visible":15,"weight":71,"urlFragment":872,"anchor":888,"singlePageAnchor":888,"docTitle":19,"url":889},"28.4. Configuring cluster alert recipients","configuring-alert-recipients-configuring-pacemaker-alert-agents","#configuring-alert-recipients-configuring-pacemaker-alert-agents",{"title":891,"visible":15,"weight":99,"urlFragment":872,"anchor":892,"singlePageAnchor":892,"docTitle":19,"url":893},"28.5. Alert meta options","cluster-alert-meta-options-configuring-pacemaker-alert-agents","#cluster-alert-meta-options-configuring-pacemaker-alert-agents",{"title":895,"visible":15,"weight":124,"urlFragment":872,"anchor":896,"singlePageAnchor":896,"docTitle":19,"url":897},"28.6. Cluster alert configuration command examples","cluster-alert-configuration-configuring-pacemaker-alert-agents","#cluster-alert-configuration-configuring-pacemaker-alert-agents",{"title":899,"visible":15,"weight":129,"urlFragment":872,"anchor":900,"singlePageAnchor":900,"docTitle":19,"url":901},"28.7. Writing a cluster alert agent","writing-cluster-alert-agent-configuring-pacemaker-alert-agents","#writing-cluster-alert-agent-configuring-pacemaker-alert-agents","#assembly_configuring-pacemaker-alert-agents_configuring-and-managing-high-availability-clusters",{"title":904,"visible":15,"weight":905,"urlFragment":906,"anchor":18,"singlePageAnchor":906,"sections":907,"docTitle":19,"url":916},"29. Multi-site Pacemaker clusters",31,"assembly_configuring-multisite-cluster-configuring-and-managing-high-availability-clusters",[908,912],{"title":909,"visible":15,"weight":16,"urlFragment":906,"anchor":910,"singlePageAnchor":910,"docTitle":19,"url":911},"29.1. Overview of Booth cluster ticket manager","con_booth-cluster-ticket-manager-configuring-multisite-cluster","#con_booth-cluster-ticket-manager-configuring-multisite-cluster",{"title":913,"visible":15,"weight":23,"urlFragment":906,"anchor":914,"singlePageAnchor":914,"docTitle":19,"url":915},"29.2. Configuring multi-site clusters with Pacemaker","proc-configuring-multisite-with-booth-configuring-multisite-cluster","#proc-configuring-multisite-with-booth-configuring-multisite-cluster","#assembly_configuring-multisite-cluster-configuring-and-managing-high-availability-clusters",{"title":918,"visible":15,"weight":919,"urlFragment":920,"anchor":18,"singlePageAnchor":920,"sections":921,"docTitle":19,"url":960},"30. Integrating non-corosync nodes into a cluster: the pacemaker_remote service",32,"assembly_remote-node-management-configuring-and-managing-high-availability-clusters",[922,926,939,952,956],{"title":923,"visible":15,"weight":16,"urlFragment":920,"anchor":924,"singlePageAnchor":924,"docTitle":19,"url":925},"30.1. Host and guest authentication of pacemaker_remote nodes","ref_host-and-guest-authentication-of-remote-nodes-remote-node-management","#ref_host-and-guest-authentication-of-remote-nodes-remote-node-management",{"title":927,"visible":15,"weight":23,"urlFragment":920,"anchor":928,"singlePageAnchor":928,"sections":929,"docTitle":19,"url":938},"30.2. Configuring KVM guest nodes","proc_configuring-kvm-guest-nodes-remote-node-management",[930,934],{"title":931,"visible":15,"weight":16,"urlFragment":920,"anchor":932,"singlePageAnchor":932,"docTitle":19,"url":933},"30.2.1. Guest node resource options","guest_node_resource_options","#guest_node_resource_options",{"title":935,"visible":15,"weight":23,"urlFragment":920,"anchor":936,"singlePageAnchor":936,"docTitle":19,"url":937},"30.2.2. Integrating a virtual machine as a guest node","integrating_a_virtual_machine_as_a_guest_node","#integrating_a_virtual_machine_as_a_guest_node","#proc_configuring-kvm-guest-nodes-remote-node-management",{"title":940,"visible":15,"weight":28,"urlFragment":920,"anchor":941,"singlePageAnchor":941,"sections":942,"docTitle":19,"url":951},"30.3. Configuring Pacemaker remote nodes","proc_configuring-remote-nodes-remote-node-management",[943,947],{"title":944,"visible":15,"weight":16,"urlFragment":920,"anchor":945,"singlePageAnchor":945,"docTitle":19,"url":946},"30.3.1. Remote node resource options","remote_node_resource_options","#remote_node_resource_options",{"title":948,"visible":15,"weight":23,"urlFragment":920,"anchor":949,"singlePageAnchor":949,"docTitle":19,"url":950},"30.3.2. Remote node configuration overview","remote_node_configuration_overview","#remote_node_configuration_overview","#proc_configuring-remote-nodes-remote-node-management",{"title":953,"visible":15,"weight":71,"urlFragment":920,"anchor":954,"singlePageAnchor":954,"docTitle":19,"url":955},"30.4. Changing the default port location","proc_changing-default-port-location-remote-node-management","#proc_changing-default-port-location-remote-node-management",{"title":957,"visible":15,"weight":99,"urlFragment":920,"anchor":958,"singlePageAnchor":958,"docTitle":19,"url":959},"30.5. Upgrading systems with pacemaker_remote nodes","proc_upgrading-systems-with-pacemaker-remote-remote-node-management","#proc_upgrading-systems-with-pacemaker-remote-remote-node-management","#assembly_remote-node-management-configuring-and-managing-high-availability-clusters",{"title":962,"visible":15,"weight":963,"urlFragment":964,"anchor":18,"singlePageAnchor":964,"sections":965,"docTitle":19,"url":1011},"31. Performing cluster maintenance",33,"assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters",[966,970,983,987,991,995,999,1003,1007],{"title":967,"visible":15,"weight":16,"urlFragment":964,"anchor":968,"singlePageAnchor":968,"docTitle":19,"url":969},"31.1. Putting a node into standby mode","proc_stopping-individual-node-cluster-maintenance","#proc_stopping-individual-node-cluster-maintenance",{"title":971,"visible":15,"weight":23,"urlFragment":964,"anchor":972,"singlePageAnchor":972,"sections":973,"docTitle":19,"url":982},"31.2. Manually moving cluster resources","proc_manually-move-resources-cluster-maintenance",[974,978],{"title":975,"visible":15,"weight":16,"urlFragment":964,"anchor":976,"singlePageAnchor":976,"docTitle":19,"url":977},"31.2.1. Moving a resource from its current node","moving_a_resource_from_its_current_node","#moving_a_resource_from_its_current_node",{"title":979,"visible":15,"weight":23,"urlFragment":964,"anchor":980,"singlePageAnchor":980,"docTitle":19,"url":981},"31.2.2. Moving a resource to its preferred node","moving_a_resource_to_its_preferred_node","#moving_a_resource_to_its_preferred_node","#proc_manually-move-resources-cluster-maintenance",{"title":984,"visible":15,"weight":28,"urlFragment":964,"anchor":985,"singlePageAnchor":985,"docTitle":19,"url":986},"31.3. Disabling, enabling, and banning cluster resources","proc_disabling-resources-cluster-maintenance","#proc_disabling-resources-cluster-maintenance",{"title":988,"visible":15,"weight":71,"urlFragment":964,"anchor":989,"singlePageAnchor":989,"docTitle":19,"url":990},"31.4. Setting a resource to unmanaged mode","proc_unmanaging-resources-cluster-maintenance","#proc_unmanaging-resources-cluster-maintenance",{"title":992,"visible":15,"weight":99,"urlFragment":964,"anchor":993,"singlePageAnchor":993,"docTitle":19,"url":994},"31.5. Putting a cluster in maintenance mode","proc_setting-maintenance-mode-cluster-maintenance","#proc_setting-maintenance-mode-cluster-maintenance",{"title":996,"visible":15,"weight":124,"urlFragment":964,"anchor":997,"singlePageAnchor":997,"docTitle":19,"url":998},"31.6. Updating a RHEL high availability cluster","proc_updating-cluster-packages-cluster-maintenance","#proc_updating-cluster-packages-cluster-maintenance",{"title":1000,"visible":15,"weight":129,"urlFragment":964,"anchor":1001,"singlePageAnchor":1001,"docTitle":19,"url":1002},"31.7. Upgrading remote nodes and guest nodes","proc_upgrading-remote-nodes-cluster-maintenance","#proc_upgrading-remote-nodes-cluster-maintenance",{"title":1004,"visible":15,"weight":134,"urlFragment":964,"anchor":1005,"singlePageAnchor":1005,"docTitle":19,"url":1006},"31.8. Migrating VMs in a RHEL cluster","proc_migrating-cluster-vms-cluster-maintenance","#proc_migrating-cluster-vms-cluster-maintenance",{"title":1008,"visible":15,"weight":224,"urlFragment":964,"anchor":1009,"singlePageAnchor":1009,"docTitle":19,"url":1010},"31.9. Identifying clusters by UUID","identifying-cluster-by-uuid-cluster-maintenance","#identifying-cluster-by-uuid-cluster-maintenance","#assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters",{"title":1013,"visible":15,"weight":1014,"urlFragment":1015,"anchor":18,"singlePageAnchor":1015,"sections":1016,"docTitle":19,"url":1025},"32. Configuring disaster recovery clusters",34,"assembly_configuring-disaster-recovery-configuring-and-managing-high-availability-clusters",[1017,1021],{"title":1018,"visible":15,"weight":16,"urlFragment":1015,"anchor":1019,"singlePageAnchor":1019,"docTitle":19,"url":1020},"32.1. Considerations for disaster recovery clusters","ref_recovery-considerations-configuring-disaster-recovery","#ref_recovery-considerations-configuring-disaster-recovery",{"title":1022,"visible":15,"weight":23,"urlFragment":1015,"anchor":1023,"singlePageAnchor":1023,"docTitle":19,"url":1024},"32.2. Displaying status of recovery clusters","proc_disaster-recovery-display-configuring-disaster-recovery","#proc_disaster-recovery-display-configuring-disaster-recovery","#assembly_configuring-disaster-recovery-configuring-and-managing-high-availability-clusters",{"title":1027,"visible":15,"weight":1028,"urlFragment":1029,"anchor":18,"singlePageAnchor":1029,"docTitle":19,"url":1030},"33. Interpreting resource agent OCF return codes",35,"ref_interpreting-resource-exit-codes-configuring-and-managing-high-availability-clusters","#ref_interpreting-resource-exit-codes-configuring-and-managing-high-availability-clusters",{"title":1032,"visible":15,"weight":1033,"urlFragment":1034,"anchor":18,"singlePageAnchor":1034,"docTitle":19,"url":1035},"34. Configuring a Red Hat High Availability cluster with IBM z/VM instances as cluster members",36,"ref_ibmz-configuring-and-managing-high-availability-clusters","#ref_ibmz-configuring-and-managing-high-availability-clusters",{"title":1037,"visible":15,"weight":1038,"urlFragment":1039,"anchor":18,"singlePageAnchor":1040,"docTitle":19,"url":1041},"Legal Notice",37,"legal-notice","idm140686156711152","#idm140686156711152",[1043,1046,1049],{"text":1044,"link":1045},"Red Hat Enterprise Linux","/documentation/red_hat_enterprise_linux/",{"text":1047,"link":1048},"9","/documentation/red_hat_enterprise_linux/9/",{"text":11},{"name":11,"translations":1051,"productVersion":1052,"singlePage":1053,"pdf":1056,"publishingStatus":1058},[5,6,7,8,9],{"name":1047},{"contentUrl":1054,"name":11,"new":1055,"url":19},"https://d2bhdhkti9t3uj.cloudfront.net/html/c94df1f8-46f0-4929-a205-b7964b66746c/39aab062ce7a9fdc724e0d7758b1c3b7.html","https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/configuring_and_managing_high_availability_clusters/index",{"url":1057},"https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/pdf/configuring_and_managing_high_availability_clusters/Red_Hat_Enterprise_Linux-9-Configuring_and_managing_high_availability_clusters-en-US.pdf","PUBLISHED",[1060,1065,1068,1072,1076,1080,1084],{"name":1061,"new":1062,"url":1063,"urlAliases":1064},"10.0-Beta","https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/10-beta","10-beta",[],{"name":1047,"new":1066,"url":1047,"urlAliases":1067},"https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9",[],{"name":1069,"new":1070,"url":1069,"urlAliases":1071},"8","https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8",[],{"name":1073,"new":1074,"url":1073,"urlAliases":1075},"7","https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7",[],{"name":1077,"new":1078,"url":1077,"urlAliases":1079},"6","https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6",[],{"name":1081,"new":1082,"url":1081,"urlAliases":1083},"5","https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/5",[],{"name":1085,"new":1086,"url":1085,"urlAliases":1087},"4","https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/4",[],{"configuring_and_managing_high_availability_clusters/index":1089,"configuring_and_managing_high_availability_clusters/proc_providing-feedback-on-red-hat-documentation_configuring-and-managing-high-availability-clusters":1090,"configuring_and_managing_high_availability_clusters/assembly_overview-of-high-availability-configuring-and-managing-high-availability-clusters":1091,"configuring_and_managing_high_availability_clusters/assembly_getting-started-with-pacemaker-configuring-and-managing-high-availability-clusters":1092,"configuring_and_managing_high_availability_clusters/assembly_pcs-operation-configuring-and-managing-high-availability-clusters":1093,"configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters":1094,"configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters":1095,"configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters":1096,"configuring_and_managing_high_availability_clusters/assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters":1097,"configuring_and_managing_high_availability_clusters/assembly_configuring-active-active-samba-in-a-cluster-configuring-and-managing-high-availability-clusters":1098,"configuring_and_managing_high_availability_clusters/assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters":1099,"configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters":1100,"configuring_and_managing_high_availability_clusters/assembly_configuring-cluster-resources-configuring-and-managing-high-availability-clusters":1101,"configuring_and_managing_high_availability_clusters/assembly_determining-which-node-a-resource-runs-on-configuring-and-managing-high-availability-clusters":1102,"configuring_and_managing_high_availability_clusters/assembly_determining-resource-order.adoc-configuring-and-managing-high-availability-clusters":1103,"configuring_and_managing_high_availability_clusters/assembly_colocating-cluster-resources.adoc_configuring-and-managing-high-availability-clusters":1104,"configuring_and_managing_high_availability_clusters/proc_displaying-resource-constraints.adoc-configuring-and-managing-high-availability-clusters":1105,"configuring_and_managing_high_availability_clusters/assembly_determining-resource-location-with-rules-configuring-and-managing-high-availability-clusters":1106,"configuring_and_managing_high_availability_clusters/assembly_managing-cluster-resources-configuring-and-managing-high-availability-clusters":1107,"configuring_and_managing_high_availability_clusters/assembly_creating-multinode-resources-configuring-and-managing-high-availability-clusters":1108,"configuring_and_managing_high_availability_clusters/assembly_clusternode-management-configuring-and-managing-high-availability-clusters":1109,"configuring_and_managing_high_availability_clusters/assembly_cluster-permissions-configuring-and-managing-high-availability-clusters":1110,"configuring_and_managing_high_availability_clusters/assembly_resource-monitoring-operations-configuring-and-managing-high-availability-clusters":1111,"configuring_and_managing_high_availability_clusters/assembly_controlling-cluster-behavior-configuring-and-managing-high-availability-clusters":1112,"configuring_and_managing_high_availability_clusters/assembly_configuring-resources-to-remain-stopped-configuring-and-managing-high-availability-clusters":1113,"configuring_and_managing_high_availability_clusters/assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters":1114,"configuring_and_managing_high_availability_clusters/assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters":1115,"configuring_and_managing_high_availability_clusters/assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters":1116,"configuring_and_managing_high_availability_clusters/assembly_configuring-quorum-devices-configuring-and-managing-high-availability-clusters":1117,"configuring_and_managing_high_availability_clusters/assembly_configuring-pacemaker-alert-agents_configuring-and-managing-high-availability-clusters":1118,"configuring_and_managing_high_availability_clusters/assembly_configuring-multisite-cluster-configuring-and-managing-high-availability-clusters":1119,"configuring_and_managing_high_availability_clusters/assembly_remote-node-management-configuring-and-managing-high-availability-clusters":1120,"configuring_and_managing_high_availability_clusters/assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters":1121,"configuring_and_managing_high_availability_clusters/assembly_configuring-disaster-recovery-configuring-and-managing-high-availability-clusters":1122,"configuring_and_managing_high_availability_clusters/ref_interpreting-resource-exit-codes-configuring-and-managing-high-availability-clusters":1123,"configuring_and_managing_high_availability_clusters/ref_ibmz-configuring-and-managing-high-availability-clusters":1124,"configuring_and_managing_high_availability_clusters/legal-notice":1125},{"prevt":18,"next":24},{"prevt":17,"next":29},{"prevt":18,"next":29},{"prevt":18,"next":86},{"prevt":18,"next":100},{"prevt":18,"next":140},{"prevt":18,"next":173},{"prevt":18,"next":194},{"prevt":18,"next":225},{"prevt":18,"next":252},{"prevt":18,"next":274},{"prevt":18,"next":288},{"prevt":18,"next":364},{"prevt":18,"next":427},{"prevt":18,"next":462},{"prevt":18,"next":484},{"prevt":484,"next":507},{"prevt":18,"next":507},{"prevt":18,"next":534},{"prevt":18,"next":586},{"prevt":18,"next":617},{"prevt":18,"next":676},{"prevt":18,"next":690},{"prevt":18,"next":721},{"prevt":18,"next":743},{"prevt":18,"next":757},{"prevt":18,"next":801},{"prevt":18,"next":815},{"prevt":18,"next":837},{"prevt":18,"next":872},{"prevt":18,"next":906},{"prevt":18,"next":920},{"prevt":18,"next":964},{"prevt":18,"next":1015},{"prevt":1015,"next":1034},{"prevt":1029,"next":1039},{"prevt":1034,"next":18},{"product":18,"version":18},{"configuring_and_managing_high_availability_clusters":1128},[1055],{"products":1130},[1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143],"builds_for_red_hat_openshift","migration_toolkit_for_virtualization","openshift_container_platform","openshift_sandboxed_containers","red_hat_advanced_cluster_security_for_kubernetes","red_hat_advanced_cluster_management_for_kubernetes","red_hat_openshift_data_foundation","red_hat_openshift_dev_spaces","red_hat_openshift_gitops","red_hat_openshift_local","red_hat_openshift_pipelines","red_hat_openshift_serverless","workload_availability_for_red_hat_openshift",[],{"default":1146},[1147,1156,1162,1171,1179,1187,1194,1202,1209],{"nid":1148,"type":1149,"langcode":1150,"Published":16,"title":1151,"Created":1152,"Updated":1153,"body_value":18,"field_documentation banner_text_value":18,"field_documentation banner_text_format":18,"field_paths_value":1154,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":1155},580,"rebrand banner","en","OpenShift Container Storage is now OpenShift Data Foundation starting with version 4.9.","2023-01-11 15:38:32","2023-01-11 15:40:04","/documentation/red_hat_openshift_container_storage\r\n/documentation/red_hat_openshift_container_storage/\r\n/documentation/red_hat_openshift_container_storage/*","internal:/documentation/red_hat_openshift_data_foundation/",{"nid":1157,"type":1149,"langcode":1150,"Published":16,"title":1151,"Created":1158,"Updated":1159,"body_value":18,"field_documentation banner_text_value":18,"field_documentation banner_text_format":18,"field_paths_value":1160,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":1161},581,"2023-01-11 15:41:31","2023-01-11 15:42:04","/documentation/red_hat_openshift_data_foundation/4.9\r\n/documentation/red_hat_openshift_data_foundation/4.9/\r\n/documentation/red_hat_openshift_data_foundation/4.9/*\r\n/documentation/red_hat_openshift_data_foundation/4.10\r\n/documentation/red_hat_openshift_data_foundation/4.10/\r\n/documentation/red_hat_openshift_data_foundation/4.10/*\r\n/documentation/red_hat_openshift_data_foundation/4.11\r\n/documentation/red_hat_openshift_data_foundation/4.11/\r\n/documentation/red_hat_openshift_data_foundation/4.11/*","internal:/documentation/red_hat_openshift_container_storage/",{"nid":1163,"type":1164,"langcode":1150,"Published":16,"title":1165,"Created":1166,"Updated":1167,"body_value":18,"field_documentation banner_text_value":1168,"field_documentation banner_text_format":1169,"field_paths_value":1170,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":18},582,"developer preview banner","MicroShift is Developer Preview software only","2023-01-11 15:50:24","2023-01-30 19:00:52","\u003Cp slot=header>MicroShift is Developer Preview software only.\u003C/p>For more information about the support scope of Red Hat Developer Preview software, see \u003Ca href=\"https://access.redhat.com/support/offerings/devpreview/\">Developer Preview Support Scope\u003C/a>.","documentation banner","/documentation/microshift/4.12\r\n/documentation/microshift/4.12/*\r\n/documentation/red_hat_build_of_microshift/4.12\r\n/documentation/red_hat_build_of_microshift/4.12/*",{"nid":1172,"type":1173,"langcode":1150,"Published":16,"title":1174,"Created":1175,"Updated":1176,"body_value":18,"field_documentation banner_text_value":1177,"field_documentation banner_text_format":1169,"field_paths_value":1178,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":18},583,"obsolete documentation banner","RHACS EOL - DAT-3433","2023-01-23 16:36:43","2023-01-23 16:39:14","\u003Cp slot=header>You are viewing documentation for a release that is no longer maintained. To view the documentation for the most recent version, see the \u003Ca href=\"/documentation/red_hat_advanced_cluster_security_for_kubernetes/\">latest RHACS docs\u003C/a>.\u003C/p>","/documentation/red_hat_advanced_cluster_security_for_kubernetes/3.69\r\n/documentation/red_hat_advanced_cluster_security_for_kubernetes/3.69/*\r\n/documentation/red_hat_advanced_cluster_security_for_kubernetes/3.70\r\n/documentation/red_hat_advanced_cluster_security_for_kubernetes/3.70/*\r\n/documentation/red_hat_advanced_cluster_security_for_kubernetes/3.71\r\n/documentation/red_hat_advanced_cluster_security_for_kubernetes/3.71/*",{"nid":1180,"type":1181,"langcode":1150,"Published":16,"title":1182,"Created":1183,"Updated":1184,"body_value":18,"field_documentation banner_text_value":1185,"field_documentation banner_text_format":1169,"field_paths_value":1186,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":18},584,"end of life banner","EOL banner for RHV","2023-05-23 14:58:05","2023-05-24 15:19:42","\u003Cp slot=header>The Red Hat Virtualization\u003C/p>Maintenance Phase runs until August 31, 2024, followed by the Extended Life Phase with no more software fixes through August 31, 2026. See \u003Ca href=\"https://access.redhat.com/articles/6975303\">Migration Paths for OpenShift Container Platform deployed on Red Hat Virtualization\u003C/a> for details.","/documentation/red_hat_virtualization/4.4",{"nid":1188,"type":1181,"langcode":1150,"Published":16,"title":1189,"Created":1190,"Updated":1191,"body_value":18,"field_documentation banner_text_value":1192,"field_documentation banner_text_format":1169,"field_paths_value":1193,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":18},585,"RHHI-V EOL","2023-06-01 16:52:57","2023-06-01 17:03:44","\u003Cp slot=header>Red Hat Hyperconverged Infrastructure for Virtualization\u003C/p> is in the \u003Ca href=\"https://access.redhat.com/support/policy/updates/rhhiv\">Maintenance Support Phase\u003C/a> of its lifecycle until October 31, 2024. After that date, the product will be End of Life. See the \u003Ca href=\"https://access.redhat.com/announcements/6972521\">RHHI-V announcement\u003C/a> for next steps.","/documentation/red_hat_hyperconverged_infrastructure_for_virtualization/1.8",{"nid":1195,"type":1196,"langcode":1150,"Published":16,"title":1197,"Created":1198,"Updated":1199,"body_value":18,"field_documentation banner_text_value":1200,"field_documentation banner_text_format":1169,"field_paths_value":1201,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":18},586,"preview banner","MicroShift is Technology Preview software only","2024-03-18 16:53:05","2024-03-18 16:54:56","\u003Cp slot=header>MicroShift is Technology Preview software only.\u003C/p>For more information about the support scope of Red Hat Technology Preview software, see \u003Ca href=\"https://access.redhat.com/support/offerings/techpreview/\">Technology Preview Support Scope\u003C/a>.","/documentation/red_hat_build_of_microshift/4.13\r\n/documentation/red_hat_build_of_microshift/4.13/*",{"nid":1203,"type":1196,"langcode":1150,"Published":16,"title":1204,"Created":1205,"Updated":1206,"body_value":18,"field_documentation banner_text_value":1207,"field_documentation banner_text_format":1169,"field_paths_value":1208,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":18},588,"Ansible 2.5 upgrade limitation","2024-09-30 16:53:05","2024-10-28 16:54:56","\u003Cp slot=\"header\">Support added for upgrades from 2.4\u003C/p>Ansible Automation Platform 2.5-3, released on October 28, 2024, adds support for upgrades from 2.4. See the upgrade documentation for more information.","",{"nid":1210,"alertType":1211,"type":1212,"langcode":1150,"Published":16,"title":1213,"Created":1208,"Updated":1208,"body_value":18,"field_documentation banner_text_value":1214,"field_documentation banner_text_format":1169,"field_paths_value":1215,"field_documentation_training_url_uri":18,"field_title_title_id":18,"field_rebrand banner_link_uri":18},587,"warning","Warning banner","Red Hat build of Apache Camel K","\u003Cp slot=header>Red Hat Camel K is deprecated\u003C/p>Red Hat Camel K is deprecated and the End of Life date for this product is June 30, 2025. For help migrating to the current go-to solution, \u003Ca target=_blank href=\"https://docs.redhat.com/en/documentation/red_hat_build_of_apache_camel\">Red Hat build of Apache Camel\u003C/a>, see the \u003Ca target=_blank href=\"https://docs.redhat.com/en/documentation/red_hat_build_of_apache_camel_k/1.10.7/html/migration_guide_camel_k_to_camel_extensions_for_quarkus/index\">Migration Guide\u003C/a>.","/documentation/red_hat_build_of_apache_camel_k/*",{"product":1044},["Reactive",1218],{"$snuxt-i18n-meta":1219,"$sisLoading":1220,"$sisSinglePage":15,"$sisInFocusMode":1220,"$smobileTocOpen":1220,"$sisLargeTOC":1220,"$scurrentChapter":17,"$scurrentSection":17,"$scurrentSubSection":1208},{},false,["Set"],["ShallowReactive",1223],{"s8LoCEfG4A":18,"rFVLKcOK8e":18,"uUstF4AIyn":18,"MdHNSZP4nR":18,"Pn02PlJOas":18},"/en/documentation/red_hat_enterprise_linux/9/html-single/configuring_and_managing_high_availability_clusters/index"]</script>
<script>window.__NUXT__={};window.__NUXT__.config={public:{contentEnv:"",collectFeedback:true,i18n:{baseUrl:"",defaultLocale:"en",defaultDirection:"ltr",strategy:"prefix",lazy:false,rootRedirect:"",routesNameSeparator:"___",defaultLocaleRouteNameSuffix:"default",skipSettingLocaleOnNavigate:false,differentDomains:false,trailingSlash:false,configLocales:[{code:"en",name:"English",iso:"en-US"},{code:"fr",name:"Français",iso:"fr-FR"},{code:"ko",name:"한국어",iso:"ko-KR"},{code:"ja",name:"日本語",iso:"ja-JP"},{code:"zh-cn",name:"中文 (中国)",iso:"zh-CN"},{code:"de",name:"Deutsch",iso:"de_DE"},{code:"it",name:"Italiano",iso:"it_IT"},{code:"pt-br",name:"Português",iso:"pt_BR"},{code:"es",name:"Español",iso:"es-ES"}],locales:{en:{domain:""},fr:{domain:""},ko:{domain:""},ja:{domain:""},"zh-cn":{domain:""},de:{domain:""},it:{domain:""},"pt-br":{domain:""},es:{domain:""}},detectBrowserLanguage:{alwaysRedirect:false,cookieCrossOrigin:false,cookieDomain:"",cookieKey:"i18n_redirected",cookieSecure:false,fallbackLocale:"",redirectOn:"root",useCookie:true},experimental:{localeDetector:"",switchLocalePathLinkSSR:false,autoImportTranslationFunctions:false}}},app:{baseURL:"/",buildId:"5ff82c51-785e-4a7d-aca6-77d6692e8c7a",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body></html>
